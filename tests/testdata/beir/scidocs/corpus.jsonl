{"_id":"033b62167e7358c429738092109311af696e9137","title":"Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews","text":"This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \u201csubtle nuances\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201cvery cavalier\u201d). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \u201cexcellent\u201d minus the mutual information between the given phrase and the word \u201cpoor\u201d. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews."}
{"_id":"105a0b3826710356e218685f87b20fe39c64c706","title":"Opinion observer: analyzing and comparing opinions on the Web","text":"The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he\/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him\/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly."}
{"_id":"2ae40898406df0a3732acc54f147c1d377f54e2a","title":"Query by Committee","text":"We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms."}
{"_id":"49e85869fa2cbb31e2fd761951d0cdfa741d95f3","title":"Adaptive Manifold Learning","text":"Manifold learning algorithms seek to find a low-dimensional parameterization of high-dimensional data. They heavily rely on the notion of what can be considered as local, how accurately the manifold can be approximated locally, and, last but not least, how the local structures can be patched together to produce the global parameterization. In this paper, we develop algorithms that address two key issues in manifold learning: 1) the adaptive selection of the local neighborhood sizes when imposing a connectivity structure on the given set of high-dimensional data points and 2) the adaptive bias reduction in the local low-dimensional embedding by accounting for the variations in the curvature of the manifold as well as its interplay with the sampling density of the data set. We demonstrate the effectiveness of our methods for improving the performance of manifold learning algorithms using both synthetic and real-world data sets."}
{"_id":"bf07d60ba6d6c6b8cabab72dfce06f203782df8f","title":"Manifold-Learning-Based Feature Extraction for Classification of Hyperspectral Data: A Review of Advances in Manifold Learning","text":"Advances in hyperspectral sensing provide new capability for characterizing spectral signatures in a wide range of physical and biological systems, while inspiring new methods for extracting information from these data. HSI data often lie on sparse, nonlinear manifolds whose geometric and topological structures can be exploited via manifold-learning techniques. In this article, we focused on demonstrating the opportunities provided by manifold learning for classification of remotely sensed data. However, limitations and opportunities remain both for research and applications. Although these methods have been demonstrated to mitigate the impact of physical effects that affect electromagnetic energy traversing the atmosphere and reflecting from a target, nonlinearities are not always exhibited in the data, particularly at lower spatial resolutions, so users should always evaluate the inherent nonlinearity in the data. Manifold learning is data driven, and as such, results are strongly dependent on the characteristics of the data, and one method will not consistently provide the best results. Nonlinear manifold-learning methods require parameter tuning, although experimental results are typically stable over a range of values, and have higher computational overhead than linear methods, which is particularly relevant for large-scale remote sensing data sets. Opportunities for advancing manifold learning also exist for analysis of hyperspectral and multisource remotely sensed data. Manifolds are assumed to be inherently smooth, an assumption that some data sets may violate, and data often contain classes whose spectra are distinctly different, resulting in multiple manifolds or submanifolds that cannot be readily integrated with a single manifold representation. Developing appropriate characterizations that exploit the unique characteristics of these submanifolds for a particular data set is an open research problem for which hierarchical manifold structures appear to have merit. To date, most work in manifold learning has focused on feature extraction from single images, assuming stationarity across the scene. Research is also needed in joint exploitation of global and local embedding methods in dynamic, multitemporal environments and integration with semisupervised and active learning."}
{"_id":"01996726f44253807537cec68393f1fce6a9cafa","title":"Stochastic Neighbor Embedding","text":"We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional \u201cimages\u201d of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word \u201cbank\u201d, to have versions close to the images of both \u201criver\u201d and \u201cfinance\u201d without forcing the images of outdoor concepts to be located close to those of corporate concepts."}
{"_id":"0e1431fa42d76c44911b07078610d4b9254bd4ce","title":"Nonlinear Component Analysis as a Kernel Eigenvalue Problem","text":"A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."}
{"_id":"13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986","title":"Random Forests","text":"Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression."}
{"_id":"10d710c01acb10c4aea702926d21697935656c3d","title":"Infrared Colorization Using Deep Convolutional Neural Networks","text":"This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR\/RGB camera, which ensures a perfect pixel to pixel registration."}
{"_id":"325d145af5f38943e469da6369ab26883a3fd69e","title":"Colorful Image Colorization","text":"Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a \u201ccolorization Turing test,\u201d asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks."}
{"_id":"326a0914dcdf7f42b5e1c2887174476728ca1b9d","title":"Wasserstein GAN","text":"The problem this paper is concerned with is that of unsupervised learning. Mainly, what does it mean to learn a probability distribution? The classical answer to this is to learn a probability density. This is often done by defining a parametric family of densities (P\u03b8)\u03b8\u2208Rd and finding the one that maximized the likelihood on our data: if we have real data examples {x}i=1, we would solve the problem"}
{"_id":"5287d8fef49b80b8d500583c07e935c7f9798933","title":"Generative Adversarial Text to Image Synthesis","text":"Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions."}
{"_id":"57bbbfea63019a57ef658a27622c357978400a50","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation","text":""}
{"_id":"20f5b475effb8fd0bf26bc72b4490b033ac25129","title":"Real time detection of lane markers in urban streets","text":"We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques."}
{"_id":"27edbcf8c6023905db4de18a4189c2093ab39b23","title":"Robust Lane Detection and Tracking in Challenging Scenarios","text":"A lane-detection system is an important component of many intelligent transportation systems. We present a robust lane-detection-and-tracking algorithm to deal with challenging scenarios such as a lane curvature, worn lane markings, lane changes, and emerging, ending, merging, and splitting lanes. We first present a comparative study to find a good real-time lane-marking classifier. Once detection is done, the lane markings are grouped into lane-boundary hypotheses. We group left and right lane boundaries separately to effectively handle merging and splitting lanes. A fast and robust algorithm, based on random-sample consensus and particle filtering, is proposed to generate a large number of hypotheses in real time. The generated hypotheses are evaluated and grouped based on a probabilistic framework. The suggested framework effectively combines a likelihood-based object-recognition algorithm with a Markov-style process (tracking) and can also be applied to general-part-based object-tracking problems. An experimental result on local streets and highways shows that the suggested algorithm is very reliable."}
{"_id":"4d2cd0b25c5b0f69b6976752ebca43ec5f04a461","title":"Lane detection and tracking using B-Snake","text":"In this paper, we proposed a B-Snake based lane detection and tracking algorithm without any cameras\u2019 parameters. Compared with other lane models, the B-Snake based lane model is able to describe a wider range of lane structures since B-Spline can form any arbitrary shape by a set of control points. The problems of detecting both sides of lane markings (or boundaries) have been merged here as the problem of detecting the mid-line of the lane, by using the knowledge of the perspective parallel lines. Furthermore, a robust algorithm, called CHEVP, is presented for providing a good initial position for the B-Snake. Also, a minimum error method by Minimum Mean Square Error (MMSE) is proposed to determine the control points of the B-Snake model by the overall image forces on two sides of lane. Experimental results show that the proposed method is robust against noise, shadows, and illumination variations in the captured road images. It is also applicable to the marked and the unmarked roads, as well as the dash and the solid paint line roads. q 2003 Elsevier B.V. All rights reserved."}
{"_id":"1c0f7854c14debcc34368e210568696a01c40573","title":"Using vanishing points for camera calibration","text":"In this article a new method for the calibration of a vision system which consists of two (or more) cameras is presented. The proposed method, which uses simple properties of vanishing points, is divided into two steps. In the first step, the intrinsic parameters of each camera, that is, the focal length and the location of the intersection between the optical axis and the image plane, are recovered from a single image of a cube. In the second step, the extrinsic parameters of a pair of cameras, that is, the rotation matrix and the translation vector which describe the rigid motion between the coordinate systems fixed in the two cameras are estimated from an image stereo pair of a suitable planar pattern. Firstly, by matching the corresponding vanishing points in the two images the rotation matrix can be computed, then the translation vector is estimated by means of a simple triangulation. The robustness of the method against noise is discussed, and the conditions for optimal estimation of the rotation matrix are derived. Extensive experimentation shows that the precision that can be achieved with the proposed method is sufficient to efficiently perform machine vision tasks that require camera calibration, like depth from stereo and motion from image sequence."}
{"_id":"235aff8bdb65654163110b35f268de6933814c49","title":"Realtime lane tracking of curved local road","text":"A lane detection system is an important component of many intelligent transportation systems. We present a robust realtime lane tracking algorithm for a curved local road. First, we present a comparative study to find a good realtime lane marking classifier. Once lane markings are detected, they are grouped into many lane boundary hypotheses represented by constrained cubic spline curves. We present a robust hypothesis generation algorithm using a particle filtering technique and a RANSAC (random sample concensus) algorithm. We introduce a probabilistic approach to group lane boundary hypotheses into left and right lane boundaries. The proposed grouping approach can be applied to general part-based object tracking problems. It incorporates a likelihood-based object recognition technique into a Markov-style process. An experimental result on local streets shows that the suggested algorithm is very reliable"}
{"_id":"26d4ab9b60b91bb610202b58fa1766951fedb9e9","title":"DRAW: A Recurrent Neural Network For Image Generation","text":"This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye."}
{"_id":"cbcd9f32b526397f88d18163875d04255e72137f","title":"Gradient-based learning applied to document recognition","text":""}
{"_id":"14829636fee5a1cf8dee9737849a8e2bdaf9a91f","title":"Bitter to Better - How to Make Bitcoin a Better Currency","text":"Bitcoin is a distributed digital currency which has attracted a substantial number of users. We perform an in-depth investigation to understand what made Bitcoin so successful, while decades of research on cryptographic e-cash has not lead to a large-scale deployment. We ask also how Bitcoin could become a good candidate for a long-lived stable currency. In doing so, we identify several issues and attacks of Bitcoin, and propose suitable techniques to address them."}
{"_id":"35fe18606529d82ce3fc90961dd6813c92713b3c","title":"SoK: Research Perspectives and Challenges for Bitcoin and Cryptocurrencies","text":"Bit coin has emerged as the most successful cryptographic currency in history. Within two years of its quiet launch in 2009, Bit coin grew to comprise billions of dollars of economic value despite only cursory analysis of the system's design. Since then a growing literature has identified hidden-but-important properties of the system, discovered attacks, proposed promising alternatives, and singled out difficult future challenges. Meanwhile a large and vibrant open-source community has proposed and deployed numerous modifications and extensions. We provide the first systematic exposition Bit coin and the many related crypto currencies or 'altcoins.' Drawing from a scattered body of knowledge, we identify three key components of Bit coin's design that can be decoupled. This enables a more insightful analysis of Bit coin's properties and future stability. We map the design space for numerous proposed modifications, providing comparative analyses for alternative consensus mechanisms, currency allocation mechanisms, computational puzzles, and key management tools. We survey anonymity issues in Bit coin and provide an evaluation framework for analyzing a variety of privacy-enhancing proposals. Finally we provide new insights on what we term disinter mediation protocols, which absolve the need for trusted intermediaries in an interesting set of applications. We identify three general disinter mediation strategies and provide a detailed comparison."}
{"_id":"3d16ed355757fc13b7c6d7d6d04e6e9c5c9c0b78","title":"Majority Is Not Enough: Bitcoin Mining Is Vulnerable","text":""}
{"_id":"34feeafb5ff7757b67cf5c46da0869ffb9655310","title":"Perpetual environmentally powered sensor networks","text":"Environmental energy is an attractive power source for low power wireless sensor networks. We present Prometheus, a system that intelligently manages energy transfer for perpetual operation without human intervention or servicing. Combining positive attributes of different energy storage elements and leveraging the intelligence of the microprocessor, we introduce an efficient multi-stage energy transfer system that reduces the common limitations of single energy storage systems to achieve near perpetual operation. We present our design choices, tradeoffs, circuit evaluations, performance analysis, and models. We discuss the relationships between system components and identify optimal hardware choices to meet an application's needs. Finally we present our implementation of a real system that uses solar energy to power Berkeley's Telos Mote. Our analysis predicts the system will operate for 43 years under 1% load, 4 years under 10% load, and 1 year under 100% load. Our implementation uses a two stage storage system consisting of supercapacitors (primary buffer) and a lithium rechargeable battery (secondary buffer). The mote has full knowledge of power levels and intelligently manages energy transfer to maximize lifetime."}
{"_id":"3689220c58f89e9e19cc0df51c0a573884486708","title":"AmbiMax: Autonomous Energy Harvesting Platform for Multi-Supply Wireless Sensor Nodes","text":"AmbiMax is an energy harvesting circuit and a supercapacitor based energy storage system for wireless sensor nodes (WSN). Previous WSNs attempt to harvest energy from various sources, and some also use supercapacitors instead of batteries to address the battery aging problem. However, they either waste much available energy due to impedance mismatch, or they require active digital control that incurs overhead, or they work with only one specific type of source. AmbiMax addresses these problems by first performing maximum power point tracking (MPPT) autonomously, and then charges supercapacitors at maximum efficiency. Furthermore, AmbiMax is modular and enables composition of multiple energy harvesting sources including solar, wind, thermal, and vibration, each with a different optimal size. Experimental results on a real WSN platform, Eco, show that AmbiMax successfully manages multiple power sources simultaneously and autonomously at several times the efficiency of the current state-of-the-art for WSNs"}
{"_id":"4833d690f7e0a4020ef48c1a537dbb5b8b9b04c6","title":"Integrated photovoltaic maximum power point tracking converter","text":"A low-power low-cost highly efficient maximum power point tracker (MPPT) to be integrated into a photovoltaic (PV) panel is proposed. This can result in a 25% energy enhancement compared to a standard photovoltaic panel, while performing functions like battery voltage regulation and matching of the PV array with the load. Instead of using an externally connected MPPT, it is proposed to use an integrated MPPT converter as part of the PV panel. It is proposed that this integrated MPPT uses a simple controller in order to be cost effective. Furthermore, the converter has to be very efficient, in order to transfer more energy to the load than a directly coupled system. This is achieved by using a simple soft-switched topology. A much higher conversion efficiency at lower cost will then result, making the MPPT an affordable solution for small PV energy systems."}
{"_id":"61c1d66defb225eda47462d1bc393906772c9196","title":"Hardware design experiences in ZebraNet","text":"The enormous potential for wireless sensor networks to make a positive impact on our society has spawned a great deal of research on the topic, and this research is now producing environment-ready systems. Current technology limits coupled with widely-varying application requirements lead to a diversity of hardware platforms for different portions of the design space. In addition, the unique energy and reliability constraints of a system that must function for months at a time without human intervention mean that demands on sensor network hardware are different from the demands on standard integrated circuits. This paper describes our experiences designing sensor nodes and low level software to control them.\n In the ZebraNet system we use GPS technology to record fine-grained position data in order to track long term animal migrations [14]. The ZebraNet hardware is composed of a 16-bit TI microcontroller, 4 Mbits of off-chip flash memory, a 900 MHz radio, and a low-power GPS chip. In this paper, we discuss our techniques for devising efficient power supplies for sensor networks, methods of managing the energy consumption of the nodes, and methods of managing the peripheral devices including the radio, flash, and sensors. We conclude by evaluating the design of the ZebraNet nodes and discussing how it can be improved. Our lessons learned in developing this hardware can be useful both in designing future sensor nodes and in using them in real systems."}
{"_id":"bb17e8858b0d3a5eba2bb91f45f4443d3e10b7cd","title":"The Balanced Scorecard: Translating Strategy Into Action","text":""}
{"_id":"bb6e6e3251bbb80587bdb5064e24b55d728529b1","title":"Mixed Methods Research : A Research Paradigm Whose Time Has Come","text":"14 The purposes of this article are to position mixed methods research (mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm \u201cwars\u201d and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research (mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it."}
{"_id":"0b3cfbf79d50dae4a16584533227bb728e3522aa","title":"Long Short-Term Memory","text":"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."}
{"_id":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database","text":"WordNet is perhaps the most important and widely used lexical resource for natural language processing systems up to now. WordNet: An Electronic Lexical Database, edited by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive Sciences Laboratory of Princeton University, where WordNet was produced, and nine chapters contributed by scientists from elsewhere. Miller's foreword offers a fascinating account of the history of WordNet. He discusses the presuppositions of such a lexical database, how the top-level noun categories were determined, and the sources of the words in WordNet. He also writes about the evolution of WordNet from its original incarnation as a dictionary browser to a broad-coverage lexicon, and the involvement of different people during its various stages of development over a decade. It makes very interesting reading for casual and serious users of WordNet and anyone who is grateful for the existence of WordNet. The book is organized in three parts. Part I is about WordNet itself and consists of four chapters: \"Nouns in WordNet\" by George Miller, \"Modifiers in WordNet\" by Katherine Miller, \"A semantic network of English verbs\" by Christiane Fellbaum, and \"Design and implementation of the WordNet lexical database and search software\" by Randee Tengi. These chapters are essentially updated versions of four papers from Miller (1990). Compared with the earlier papers, the chapters in this book focus more on the underlying assumptions and rationales behind the design decisions. The description of the information contained in WordNet, however, is not as detailed as in Miller (1990). The main new additions in these chapters include an explanation of sense grouping in George Miller's chapter, a section about adverbs in Katherine Miller's chapter, observations about autohyponymy (one sense of a word being a hyponym of another sense of the same word) and autoantonymy (one sense of a word being an antonym of another sense of the same word) in Fellbaum's chapter, and Tengi's description of the Grinder, a program that converts the files the lexicographers work with to searchable lexical databases. The three papers in Part II are characterized as \"extensions, enhancements and"}
{"_id":"b49af9c4ab31528d37122455e4caf5fdeefec81a","title":"Smart homes and their users: a systematic analysis and key challenges","text":"Published research on smart homes and their users is growing exponentially, yet a clear understanding of who these users are and how they might use smart home technologies is missing from a field being overwhelmingly pushed by technology developers. Through a systematic analysis of peer-reviewed literature on smart homes and their users, this paper takes stock of the dominant research themes and the linkages and disconnects between them. Key findings within each of nine themes are analysed, grouped into three: (1) views of the smart home\u2014functional, instrumental, socio-technical; (2) users and the use of the smart home\u2014prospective users, interactions and decisions, using technologies in the home; and (3) challenges for realising the smart home\u2014hardware and software, design, domestication. These themes are integrated into an organising framework for future research that identifies the presence or absence of cross-cutting relationships between different understandings of smart homes and their users. The usefulness of the organising framework is illustrated in relation to two major concerns\u2014privacy and control\u2014that have been narrowly interpreted to date, precluding deeper insights and potential solutions. Future research on smart homes and their users can benefit by exploring and developing cross-cutting relationships between the research themes identified."}
{"_id":"052b1d8ce63b07fec3de9dbb583772d860b7c769","title":"Learning representations by back-propagating errors","text":"We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."}
{"_id":"07f3f736d90125cb2b04e7408782af411c67dd5a","title":"Convolutional Neural Network Architectures for Matching Natural Language Sentences","text":"Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layerby-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models."}
{"_id":"0af737eae02032e66e035dfed7f853ccb095d6f5","title":"ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs","text":"How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence\u2019s representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks. We release code at: https:\/\/github.com\/yinwenpeng\/Answer_Selection."}
{"_id":"1c059493904b2244d2280b8b4c0c7d3ca115be73","title":"node2vec: Scalable Feature Learning for Networks","text":"Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.\n We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks."}
{"_id":"468b9055950c428b17f0bf2ff63fe48a6cb6c998","title":"A Neural Attention Model for Abstractive Sentence Summarization","text":"Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."}
{"_id":"81eb0a1ea90a6f6d5e7f14cb3397a4ee0f77824a","title":"Question\/Answer Matching for CQA System via Combining Lexical and Sequential Information","text":"Community-based Question Answering (CQA) has become popular in knowledge sharing sites since it allows users to get answers to complex, detailed, and personal questions directly from other users. Large archives of historical questions and associated answers have been accumulated. Retrieving relevant historical answers that best match a question is an essential component of a CQA service. Most state of the art approaches are based on bag-of-words models, which have been proven successful in a range of text matching tasks, but are insufficient for capturing the important word sequence information in short text matching. In this paper, a new architecture is proposed to more effectively model the complicated matching relations between questions and answers. It utilises a similarity matrix which contains both lexical and sequential information. Afterwards the information is put into a deep architecture to find potentially suitable answers. The experimental study shows its potential in improving matching accuracy of question and answer."}
{"_id":"02227c94dd41fe0b439e050d377b0beb5d427cda","title":"Reading Digits in Natural Images with Unsupervised Feature Learning","text":"Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks."}
{"_id":"0ee47ca8e90f3dd2107b6791c0da42357c56f5bc","title":"Agile Software Development: The Business of Innovation","text":"T he rise and fall of the dot-com-driven Internet economy shouldn't distract us from seeing that the business environment continues to change at a dramatically increasing pace. To thrive in this turbulent environment, we must confront the business need for relentless innovation and forge the future workforce culture. Agile software development approaches such as Extreme Programming , Crystal methods, Lean Development, Scrum, Adaptive Software Development (ASD), and others view change from a perspective that mirrors today's turbulent business and technology environment. In a recent study of more than 200 software development projects, QSM Associates' Michael Mah reported that the researchers couldn't find nearly half of the projects' original plans to measure against. Why? Conforming to plan was no longer the primary goal; instead, satisfying customers\u2014at the time of delivery , not at project initiation\u2014took precedence. In many projects we review, major changes in the requirements, scope, and technology that are outside the development team's control often occur within a project's life span. Accepting that Barry Boehm's life cycle cost differentials theory\u2014the cost of change grows through the software's development life cycle\u2014remains valid, the question today is not how to stop change early in a project but how to better handle inevitable changes throughout its life cycle. Traditional approaches assumed that if we just tried hard enough, we could anticipate the complete set of requirements early and reduce cost by eliminating change. Today, eliminating change early means being unresponsive to business con-ditions\u2014in other words, business failure. Similarly, traditional process manage-ment\u2014by continuous measurement, error identification, and process refine-ments\u2014strove to drive variations out of processes. This approach assumes that variations are the result of errors. Today, while process problems certainly cause some errors, external environmental changes cause critical variations. Because we cannot eliminate these changes, driving down the cost of responding to them is the only viable strategy. Rather than eliminating rework, the new strategy is to reduce its cost. However, in not just accommodating change, but embracing it, we also must be careful to retain quality. Expectations have grown over the years. The market demands and expects innovative, high-quality software that meets its needs\u2014 and soon. Agile methods are a response to this expectation. Their strategy is to reduce the cost of change throughout a project. Extreme Programming (XP), for example, calls for the software development team to \u2022 produce the first delivery in weeks, to achieve an early win and rapid \u2026"}
{"_id":"32cbd065ac9405530ce0b1832a9a58c7444ba305","title":"Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments","text":"We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets."}
{"_id":"7e8a5e0a87fab337d71ce04ba02b7a5ded392421","title":"Detecting and Tracking Political Abuse in Social Media","text":"We study astroturf political campaigns on microblogging platforms: politically-motivated individuals and organizations that use multiple centrally-controlled accounts to create the appearance of widespread support for a candidate or opinion. We describe a machine learning framework that combines topological, content-based and crowdsourced features of information diffusion networks on Twitter to detect the early stages of viral spreading of political misinformation. We present promising preliminary results with better than 96% accuracy in the detection of astroturf content in the run-up to the 2010 U.S. midterm elections."}
{"_id":"1319bf6218cbcd85ac7512991447ecd9d776577d","title":"Task constraints in visual working memory","text":"This paper examines the nature of visual representations that direct ongoing performance in sensorimotor tasks. Performance of such natural tasks requires relating visual information from different gaze positions. To explore this we used the technique of making task relevant display changes during saccadic eye movements. Subjects copied a pattern of colored blocks on a computer monitor, using the mouse to drag the blocks across the screen. Eye position was monitored using a dual-purkinje eye tracker, and the color of blocks in the pattern was changed at different points in task performance. When the target of the saccade changed color during the saccade, the duration of fixations on the model pattern increased, depending on the point in the task that the change was made. Thus different fixations on the same visual stimulus served a different purpose. The results also indicated that the visual information that is retained across successive fixations depends on moment by moment task demands. This is consistent with previous suggestions that visual representations are limited and task dependent. Changes in blocks in addition to the saccade target led to greater increases in fixation duration. This indicated that some global aspect of the pattern was retained across different fixations. Fixation durations revealed effects of the display changes that were not revealed in perceptual report. This can be understood by distinguishing between processes that operate at different levels of description and different time scales. Our conscious experience of the world may reflect events over a longer time scale than those underlying the substructure of the perceptuo-motor machinery."}
{"_id":"03406ec0118137ca1ab734a8b6b3678a35a43415","title":"A Morphable Model for the Synthesis of 3D Faces","text":"In this paper, a new technique for modeling textured 3D faces is introduced. 3D faces can either be generated automatically from one or more photographs, or modeled directly through an intuitive user interface. Users are assisted in two key problems of computer aided face modeling. First, new face images or new 3D face models can be registered automatically by computing dense one-to-one correspondence to an internal face model. Second, the approach regulates the naturalness of modeled faces avoiding faces with an \u201cunlikely\u201d appearance. Starting from an example set of 3D face models, we derive a morphable face model by transforming the shape and texture of the examples into a vector space representation. New faces and expressions can be modeled by forming linear combinations of the prototypes. Shape and texture constraints derived from the statistics of our example faces are used to guide manual modeling or automated matching algorithms. We show 3D face reconstructions from single images and their applications for photo-realistic image manipulations. We also demonstrate face manipulations according to complex parameters such as gender, fullness of a face or its distinctiveness."}
{"_id":"5425f7109dca2022ff9bde0ed3f113080d0d606b","title":"DFD: Efficient Functional Dependency Discovery","text":"The discovery of unknown functional dependencies in a dataset is of great importance for database redesign, anomaly detection and data cleansing applications. However, as the nature of the problem is exponential in the number of attributes none of the existing approaches can be applied on large datasets. We present a new algorithm DFD for discovering all functional dependencies in a dataset following a depth-first traversal strategy of the attribute lattice that combines aggressive pruning and efficient result verification. Our approach is able to scale far beyond existing algorithms for up to 7.5 million tuples, and is up to three orders of magnitude faster than existing approaches on smaller datasets."}
{"_id":"c2f4c6d7e06da14c4b3ce3a9b97394a64708dc52","title":"Database Dependency Discovery: A Machine Learning Approach","text":"Database dependencies, such as functional and multivalued dependencies, express the presence of structure in databas e relations, that can be utilised in the database design proce ss. The discovery of database dependencies can be viewed as an induction problem, in which general rules (dependencies) a re obtained from specific facts (the relation). This viewpoint has the advantage of abstracting away as much as possible from the particulars of the dependencies. The algorithms in this paper are designed such that they can easily be generalised to other kinds of dependencies. Like in current approaches to computational induction such as inductive logic programming, we distinguish between top down algorithms and bottom-up algorithms. In a top-down approach, hypotheses are generated in a systematic way and then tested against the given relation. In a bottom-up approach, the relation is inspected in order to see what dependencies it may satisfy or violate. We give algorithms for bot h approaches."}
{"_id":"28f840416cfe7aed3cda11e266119d247fcc3f9e","title":"GORDIAN: Efficient and Scalable Discovery of Composite Keys","text":"Identification of (composite) key attributes is of fundamental importance for many different data management tasks such as data modeling, data integration, anomaly detection, query formulation, query optimization, and indexing. However, information about keys is often missing or incomplete in many real-world database scenarios. Surprisingly, the fundamental problem of automatic key discovery has received little attention in the existing literature. Existing solutions ignore composite keys, due to the complexity associated with their discovery. Even for simple keys, current algorithms take a brute-force approach; the resulting exponential CPU and memory requirements limit the applicability of these methods to small datasets. In this paper, we describe GORDIAN, a scalable algorithm for automatic discovery of keys in large datasets, including composite keys. GORDIAN can provide exact results very efficiently for both real-world and synthetic datasets. GORDIAN can be used to find (composite) key attributes in any collection of entities, e.g., key column-groups in relational data, or key leaf-node sets in a collection of XML documents with a common schema. We show empirically that GORDIAN can be combined with sampling to efficiently obtain high quality sets of approximate keys even in very large datasets."}
{"_id":"5288d14f6a3937df5e10109d4e23d79b7ddf080f","title":"Fast Algorithms for Mining Association Rules in Large Databases","text":""}
{"_id":"57fb4b0c63400dc984893461b1f5a73244b3e3eb","title":"Logic and Databases: A Deductive Approach","text":"ion, Databases and Conceptual Modeling (Pingree Park, Colo., June), pp. 112-114; ACM SZGMOD Rec. 11, 2 (Feb.). CODD, E. F. 1982. Relational database: A practical foundation for productivity. Commun. ACM 25, 2 (Feb.), 109-117. COLMERAUER, A. 1973. Un systeme de communication homme-machine en francais. Rapport, Groupe Intelligence Artificielle, Universite d\u2019AixMarseille-Luminy, Marseilles, France. COLMERAUER, A., AND PIQUE, J. F. 1981. About natural logic. In Advances in Database Theory vol. 1, H. Gallaire, J. Minker, and J.-M. Nicolas, Eds. Plenum, New York, pp. 343-365. COOPER, E. C. 1980. On the expressive power of query languages for relational databases. Tech. Rep. 14-80, Computer Science Dept., Harvard Univ., Cambridge, Mass. DAHL, V. 1982. On database systems development through logic. ACM Trans. Database Syst. 7, 1 (Mar.), 102-123. DATE, C. J. 1977. An Introduction to Database Systems. Addison-Wesley, Reading, Mass. DATE, C. J. 1981. An Introduction to Database Systems, 3rd ed. Addison-Wesley, Reading, Mass. DELIYANNI, A., AND KOWALSKI, R. A. 1979. Logic and semantic networks. Commun. ACM 22, 3"}
{"_id":"14318685b5959b51d0f1e3db34643eb2855dc6d9","title":"Going deeper with convolutions","text":"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."}
{"_id":"1827de6fa9c9c1b3d647a9d707042e89cf94abf0","title":"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","text":"Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}
{"_id":"741fd80f0a31fe77f91b1cce3d91c544d6d5b1b2","title":"Affective outcomes of virtual reality exposure therapy for anxiety and specific phobias: a meta-analysis.","text":"Virtual reality exposure therapy (VRET) is an increasingly common treatment for anxiety and specific phobias. Lacking is a quantitative meta-analysis that enhances understanding of the variability and clinical significance of anxiety reduction outcomes after VRET. Searches of electronic databases yielded 52 studies, and of these, 21 studies (300 subjects) met inclusion criteria. Although meta-analysis revealed large declines in anxiety symptoms following VRET, moderator analyses were limited due to inconsistent reporting in the VRET literature. This highlights the need for future research studies that report uniform and detailed information regarding presence, immersion, anxiety and\/or phobia duration, and demographics."}
{"_id":"53b85e4066944b1753aae8e3418028a67d9372e1","title":"The Chemical Basis of Morphogenesis","text":"The paper discussed is by Alan Turing. It was published in 1952 and presents an idea of how periodic patterns could be formed in nature. Looking on periodic structures \u2013 like the stripes on tigers, the dots on leopards or the whirly leaves on woodruff \u2013 it is hard to imagine those patterns are formated by pure chance. On the other hand, thinking of the unbelievable multitude of possible realizations, the patterns can not all be exactly encoded in the genes. The paper \u201cThe Chemical Basis of Morphogenesis\u201d proposes a possible mechanism due to an interaction of two \u201cmorphogenes\u201d which react and diffuse through the tissue. Fulfilling some constrains regarding the diffusibilities and the behaviour of the reactions, this mechanism \u2013 called Turing mechanism \u2013 can lead to a pattern of concentrations defining the structure we see."}
{"_id":"9141d85998eadb1bca5cca027ae07670cfafb015","title":"Determining the Sentiment of Opinions","text":"Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results."}
{"_id":"0cfe588996f1bc319f87c6f75160d1cf1542d9a9","title":"ON THE EMERGENCE OF GR 4 AMMAR FROM THE LEXICON","text":""}
{"_id":"162cd1259ad106990c6bfd36db19751f940274d3","title":"Rapid word learning under uncertainty via cross-situational statistics.","text":"There are an infinite number of possible word-to-word pairings in naturalistic learning environments. Previous proposals to solve this mapping problem have focused on linguistic, social, representational, and attentional constraints at a single moment. This article discusses a cross-situational learning strategy based on computing distributional statistics across words, across referents, and, most important, across the co-occurrences of words and referents at multiple moments. We briefly exposed adults to a set of trials that each contained multiple spoken words and multiple pictures of individual objects; no information about word-picture correspondences was given within a trial. Nonetheless, over trials, subjects learned the word-picture mappings through cross-trial statistical relations. Different learning conditions varied the degree of within-trial reference uncertainty, the number of trials, and the length of trials. Overall, the remarkable performance of learners in various learning conditions suggests that they calculate cross-trial statistics with sufficient fidelity and by doing so rapidly learn word-referent pairs even in highly ambiguous learning contexts."}
{"_id":"53dd71dc5598d41c06d3eef1315e098dc4cbca28","title":"Word Segmentation : The Role of Distributional Cues","text":"One of the infant\u2019s first tasks in language acquisition is to discover the words embedded in a mostly continuous speech stream. This learning problem might be solved by using distributional cues to word boundaries\u2014for example, by computing the transitional probabilities between sounds in the language input and using the relative strengths of these probabilities to hypothesize word boundaries. The learner might be further aided by language-specific prosodic cues correlated with word boundaries. As a first step in testing these hypotheses, we briefly exposed adults to an artificial language in which the only cues available for word segmentation were the transitional probabilities between syllables. Subjects were able to learn the words of this language. Furthermore, the addition of certain prosodic cues served to enhance performance. These results suggest that distributional cues may play an important role in the initial word segmentation of language learners. q 1996 Academic Press, Inc."}
{"_id":"7085126d3d21b559e38231f3fa283aae0ca50cd8","title":"Statistical learning by 8-month-old infants.","text":"Learners rely on a combination of experience-independent and experience-dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-month-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 minutes of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input."}
{"_id":"a6ad17f9df9346c56bab090b35ef73ff94f56c01","title":"A computational study of cross-situational techniques for learning word-to-meaning mappings","text":"This paper presents a computational study of part of the lexical-acquisition task faced by children, namely the acquisition of word-to-meaning mappings. It first approximates this task as a formal mathematical problem. It then presents an implemented algorithm for solving this problem, illustrating its operation on a small example. This algorithm offers one precise interpretation of the intuitive notions of cross-situational learning and the principle of contrast applied between words in an utterance. It robustly learns a homonymous lexicon despite noisy multi-word input, in the presence of referential uncertainty, with no prior knowledge that is specific to the language being learned. Computational simulations demonstrate the robustness of this algorithm and illustrate how algorithms based on cross-situational learning and the principle of contrast might be able to solve lexical-acquisition problems of the size faced by children, under weak, worst-case assumptions about the type and quantity of data available."}
{"_id":"21fb86020f68bf2dd57cd1b8a0e8adead5d9a9ae","title":"Data Mining : Concepts and Techniques","text":"Association rule mining was first proposed by Agrawal, Imielinski, and Swami [AIS93]. The Apriori algorithm discussed in Section 5.2.1 for frequent itemset mining was presented in Agrawal and Srikant [AS94b]. A variation of the algorithm using a similar pruning heuristic was developed independently by Mannila, Tiovonen, and Verkamo [MTV94]. A joint publication combining these works later appeared in Agrawal, Mannila, Srikant, Toivonen, and Verkamo [AMS96]. A method for generating association rules from frequent itemsets is described in Agrawal and Srikant [AS94a]."}
{"_id":"4eae6ee36de5f9ae3c05c6ca385938de98cd5ef8","title":"Combining Text and Linguistic Document Representations for Authorship Attribution","text":"In this paper, we provide several alternatives to the classical Bag-Of-Words model for automatic authorship attribution. To this end, we consider linguistic and writing style information such as grammatical structures to construct different document representations. Furthermore we describe two techniques to combine the obtained representations: combination vectors and ensemble based meta classification. Our experiments show the viability of our approach."}
{"_id":"288c67457f09c0c30cadd7439040114e9c377bc3","title":"Finding Interesting Rules from Large Sets of Discovered Association Rules","text":"Association rules, introduced by Agrawal, Imielinski, and Swami, are rules of the form \u201cfor 90% of the rows of the relation, if the row has value 1 in the columns in set W, then it has 1 also in column B\u201d. Efficient methods exist for discovering association rules from large collections of data. The number of discovered rules can, however, be so large that browsing the rule set and finding interesting rules from it can be quite difficult for the user. We show how a simple formalism of rule templates makes it possible to easily describe the structure of interesting rules. We also give examples of visualization of rules, and show how a visualization tool interfaces with rule templates."}
{"_id":"384bb3944abe9441dcd2cede5e7cd7353e9ee5f7","title":"Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods","text":""}
{"_id":"49fa97db6b7f3ab2b3a623c3552aa680b80c8dd2","title":"Automatically Categorizing Written Texts by Author Gender","text":"The problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution. Nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80% accuracy. The same techniques can be used to determine if a document is fiction or non-fiction with approximately 98% accuracy."}
{"_id":"cc54251f84c8577ca862fec41a1766c9a0d4a7b8","title":"Updating P300: An integrative theory of P3a and P3b","text":"The empirical and theoretical development of the P300 event-related brain potential (ERP) is reviewed by considering factors that contribute to its amplitude, latency, and general characteristics. The neuropsychological origins of the P3a and P3b subcomponents are detailed, and how target\/standard discrimination difficulty modulates scalp topography is discussed. The neural loci of P3a and P3b generation are outlined, and a cognitive model is proffered: P3a originates from stimulus-driven frontal attention mechanisms during task processing, whereas P3b originates from temporal-parietal activity associated with attention and appears related to subsequent memory processing. Neurotransmitter actions associating P3a to frontal\/dopaminergic and P3b to parietal\/norepinephrine pathways are highlighted. Neuroinhibition is suggested as an overarching theoretical mechanism for P300, which is elicited when stimulus detection engages memory operations."}
{"_id":"56e362c661d575b908e8a9f9bbb48f535a9312a5","title":"On Managing Very Large Sensor-Network Data Using Bigtable","text":"Recent advances and innovations in smart sensor technologies, energy storage, data communications, and distributed computing paradigms are enabling technological breakthroughs in very large sensor networks. There is an emerging surge of next-generation sensor-rich computers in consumer mobile devices as well as tailor-made field platforms wirelessly connected to the Internet. Billions of such sensor computers are posing both challenges and opportunities in relation to scalable and reliable management of the peta- and exa-scale time series being generated over time. This paper presents a Cloud-computing approach to this issue based on the two well-known data storage and processing paradigms: Bigtable and MapReduce."}
{"_id":"36927265f588ed093c2cbdbf7bf95ddd72f000a9","title":"Performance Evaluation of Bridgeless PFC Boost Rectifiers","text":"In this paper, a systematic review of bridgeless power factor correction (PFC) boost rectifiers, also called dual boost PFC rectifiers, is presented. Performance comparison between the conventional PFC boost rectifier and a representative member of the bridgeless PFC boost rectifier family is performed. Loss analysis and experimental efficiency evaluation for both CCM and DCM\/CCM boundary operations are provided."}
{"_id":"77c87f82a73edab2c46d600fc3d7821cdb15359a","title":"State-of-the-art, single-phase, active power-factor-correction techniques for high-power applications - an overview","text":"A review of high-performance, state-of-the-art, active power-factor-correction (PFC) techniques for high-power, single-phase applications is presented. The merits and limitations of several PFC techniques that are used in today's network-server and telecom power supplies to maximize their conversion efficiencies are discussed. These techniques include various zero-voltage-switching and zero-current-switching, active-snubber approaches employed to reduce reverse-recovery-related switching losses, as well as techniques for the minimization of the conduction losses. Finally, the effect of recent advancements in semiconductor technology, primarily silicon-carbide technology, on the performance and design considerations of PFC converters is discussed."}
{"_id":"864e1700594dfdf46a4981b5bc07a54ebeab11ba","title":"Bridgeless PFC implementation using one cycle control technique","text":"Conventional boost PFC suffers from the high conduction loss in the input rectifier-bridge. Higher efficiency can be achieved by using the bridgeless boost topology. This new circuit has issues such as voltage sensing, current sensing and EMI noise. In this paper, one cycle control technique is used to solve the issues of the voltage sensing and current sensing. Experimental results show efficiency improvement and EMI performance"}
{"_id":"c0b7e09f212ec85da22974c481e7b93efeba1504","title":"Common mode noise modeling and analysis of dual boost PFC circuit","text":"To achieve high efficiency PFC front stage in switching mode power supply (SMPS), dual boost PFC (DBPFC) topology shows superior characteristics compared with traditional boost PFC, but it by nature brings higher EMI noise, especially common mode (CM) noise. This paper deals with the modeling and analysis of DBPFC CM noise based on and compared with boost PFC, noise propagation equivalent circuits of both topologies are deduced, and theoretical analysis illustrates the difference. Experiments are performed to validate the EMI model and analysis."}
{"_id":"60ba158cb1a619726db31b684a7bf818e2f8256b","title":"Common mode EMI noise suppression in bridgeless boost PFC converter","text":"Bridgeless boost PFC converter has high efficiency by eliminating the input diode bridge. However, Common Mode (CM) conducted EMI becomes a great issue. The goal of this paper is to study the possibility to minimize the CM noise in this converter. First the noise model is studied. Then a balance concept is introduced and applied to cancel the CM noise. Two approaches to minimize CM noise are introduced and compared. Experiments verify the effectiveness of both approaches."}
{"_id":"0825788b9b5a18e3dfea5b0af123b5e939a4f564","title":"Glove: Global Vectors for Word Representation","text":"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."}
{"_id":"10cfa5bfab3da9c8026d3a358695ea2a5eba0f33","title":"Parallel Tracking and Mapping for Small AR Workspaces","text":"This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems."}
{"_id":"272216c1f097706721096669d85b2843c23fa77d","title":"Adam: A Method for Stochastic Optimization","text":"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and\/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and\/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}
{"_id":"05357314fe2da7c2248b03d89b7ab9e358cbf01e","title":"Learning with kernels","text":"All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."}
{"_id":"067c7857753e21e7317b556c86e30be60aa7cac0","title":"Xen and the art of virtualization","text":"Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests."}
{"_id":"0c668ee24d58ecca165f788d40765e79ed615471","title":"Classification and Regression Trees","text":""}
{"_id":"31864e13a9b3473ebb07b4f991f0ae3363517244","title":"A Computational Approach to Edge Detection","text":"This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge."}
{"_id":"15cf63f8d44179423b4100531db4bb84245aa6f1","title":"Deep Learning","text":"Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress."}
{"_id":"505c58c2c100e7512b7f7d906a9d4af72f6e8415","title":"Genetic programming - on the programming of computers by means of natural selection","text":"Page ii Complex Adaptive Systems John H. Holland, Christopher Langton, and Stewart W. Wilson, advisors Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence, MIT Press edition John H. Holland Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life edited by Francisco J. Varela and Paul Bourgine Genetic Programming: On the Programming of Computers by Means of Natural Selection John R. Koza"}
{"_id":"1a090df137014acab572aa5dc23449b270db64b4","title":"LIBSVM: a library for support vector machines","text":""}
{"_id":"1e56ed3d2c855f848ffd91baa90f661772a279e1","title":"Latent Dirichlet Allocation","text":"We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes\/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification."}
{"_id":"00844516c86828a4cc81471b573cb1a1696fcde9","title":"Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion","text":"Here, we demonstrate that subject motion produces substantial changes in the timecourses of resting state functional connectivity MRI (rs-fcMRI) data despite compensatory spatial registration and regression of motion estimates from the data. These changes cause systematic but spurious correlation structures throughout the brain. Specifically, many long-distance correlations are decreased by subject motion, whereas many short-distance correlations are increased. These changes in rs-fcMRI correlations do not arise from, nor are they adequately countered by, some common functional connectivity processing steps. Two indices of data quality are proposed, and a simple method to reduce motion-related effects in rs-fcMRI analyses is demonstrated that should be flexibly implementable across a variety of software platforms. We demonstrate how application of this technique impacts our own data, modifying previous conclusions about brain development. These results suggest the need for greater care in dealing with subject motion, and the need to critically revisit previous rs-fcMRI work that may not have adequately controlled for effects of transient subject movements."}
{"_id":"21bfc289cf7e2309e70f390ae14d89df7c911a67","title":"Modeling regional and psychophysiologic interactions in fMRI: the importance of hemodynamic deconvolution","text":"The analysis of functional magnetic resonance imaging (fMRI) time-series data can provide information not only about task-related activity, but also about the connectivity (functional or effective) among regions and the influences of behavioral or physiologic states on that connectivity. Similar analyses have been performed in other imaging modalities, such as positron emission tomography. However, fMRI is unique because the information about the underlying neuronal activity is filtered or convolved with a hemodynamic response function. Previous studies of regional connectivity in fMRI have overlooked this convolution and have assumed that the observed hemodynamic response approximates the neuronal response. In this article, this assumption is revisited using estimates of underlying neuronal activity. These estimates use a parametric empirical Bayes formulation for hemodynamic deconvolution."}
{"_id":"261208c69aeca0243e43511845a0d8023d31acbe","title":"Common regions of the human frontal lobe recruited by diverse cognitive demands","text":"Though many neuroscientific methods have been brought to bear in the search for functional specializations within prefrontal cortex, little consensus has emerged. To assess the contribution of functional neuroimaging, this article reviews patterns of frontal-lobe activation associated with a broad range of different cognitive demands, including aspects of perception, response selection, executive control, working memory, episodic memory and problem solving. The results show a striking regularity: for many demands, there is a similar recruitment of mid-dorsolateral, mid-ventrolateral and dorsal anterior cingulate cortex. Much of the remainder of frontal cortex, including most of the medial and orbital surfaces, is largely insensitive to these demands. Undoubtedly, these results provide strong evidence for regional specialization of function within prefrontal cortex. This specialization, however, takes an unexpected form: a specific frontal-lobe network that is consistently recruited for solution of diverse cognitive problems."}
{"_id":"2ce0d2f6efe74b9df4c0eccb434322d931c5dd47","title":"Prefrontal cortical function and anxiety: controlling attention to threat-related stimuli","text":"Threat-related stimuli are strong competitors for attention, particularly in anxious individuals. We used functional magnetic resonance imaging (fMRI) with healthy human volunteers to study how the processing of threat-related distractors is controlled and whether this alters as anxiety levels increase. Our work builds upon prior analyses of the cognitive control functions of lateral prefrontal cortex (lateral PFC) and anterior cingulate cortex (ACC). We found that rostral ACC was strongly activated by infrequent threat-related distractors, consistent with a role for this area in responding to unexpected processing conflict caused by salient emotional stimuli. Participants with higher anxiety levels showed both less rostral ACC activity overall and reduced recruitment of lateral PFC as expectancy of threat-related distractors was established. This supports the proposal that anxiety is associated with reduced top-down control over threat-related distractors. Our results suggest distinct roles for rostral ACC and lateral PFC in governing the processing of task-irrelevant, threat-related stimuli, and indicate reduced recruitment of this circuitry in anxiety."}
{"_id":"0ca9e60d077c97f8f9f9e43110e899ed45284ecd","title":"Other minds in the brain: a functional imaging study of \u201ctheory of mind\u201d in story comprehension","text":"The ability of normal children and adults to attribute independent mental states to self and others in order to explain and predict behaviour (\"theory of mind\") has been a focus of much recent research. Autism is a biologically based disorder which appears to be characterised by a specific impairment in this \"mentalising\" process. The present paper reports a functional neuroimaging study with positron emission tomography in which we studied brain activity in normal volunteers while they performed story comprehension tasks necessitating the attribution of mental states. The resultant brain activity was compared with that measured in two control tasks: \"physical\" stories which did not require this mental attribution, and passages of unlinked sentences. Both story conditions, when compared to the unlinked sentences, showed significantly increased regional cerebral blood flow in the following regions: the temporal poles bilaterally, the left superior temporal gyrus and the posterior cingulate cortex. Comparison of the \"theory of mind\" stories with \"physical\" stores revealed a specific pattern of activation associated with mental state attribution: it was only this task which produced activation in the medial frontal gyrus on the left (Brodmann's area 8). This comparison also showed significant activation in the posterior cingulate cortex. These surprisingly clear-cut findings are discussed in relation to previous studies of brain activation during story comprehension. The localisation of brain regions involved in normal attribution of mental states and contextual problem solving is feasible and may have implication for the neural basis of autism."}
{"_id":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition","text":"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."}
{"_id":"7c3a4b84214561d8a6e4963bbb85a17a5b1e003a","title":"Programs for Machine Learning. Part I","text":""}
{"_id":"167e1359943b96b9e92ee73db1df69a1f65d731d","title":"A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts","text":"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \u201cthumbs up\u201d or \u201cthumbs down\u201d. To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints. Publication info: Proceedings of the ACL, 2004."}
{"_id":"045a975c1753724b3a0780673ee92b37b9827be6","title":"Wait-Free Synchronization","text":"A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, \u201cthere is no wait-free implementation of X by Y.\u201d We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that atomic read\/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest&set and fetch&add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object."}
{"_id":"0541d5338adc48276b3b8cd3a141d799e2d40150","title":"MapReduce: Simplified Data Processing on Large Clusters","text":"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day."}
{"_id":"2032be0818be583f159cc75f2022ed78222fb772","title":"Salt-and-pepper noise removal by median-type noise detectors and detail-preserving regularization","text":"This paper proposes a two-phase scheme for removing salt-and-pepper impulse noise. In the first phase, an adaptive median filter is used to identify pixels which are likely to be contaminated by noise (noise candidates). In the second phase, the image is restored using a specialized regularization method that applies only to those selected noise candidates. In terms of edge preservation and noise suppression, our restored images show a significant improvement compared to those restored by using just nonlinear filters or regularization methods only. Our scheme can remove salt-and-pepper-noise with a noise level as high as 90%."}
{"_id":"43089ffed8c6c653f6994fb96f7f48bbcff2a598","title":"Adaptive median filters: new algorithms and results","text":"Based on two types of image models corrupted by impulse noise, we propose two new algorithms for adaptive median filters. They have variable window size for removal of impulses while preserving sharpness. The first one, called the ranked-order based adaptive median filter (RAMF), is based on a test for the presence of impulses in the center pixel itself followed by a test for the presence of residual impulses in the median filter output. The second one, called the impulse size based adaptive median filter (SAMF), is based on the detection of the size of the impulse noise. It is shown that the RAMF is superior to the nonlinear mean L(p) filter in removing positive and negative impulses while simultaneously preserving sharpness; the SAMF is superior to Lin's (1988) adaptive scheme because it is simpler with better performance in removing the high density impulsive noise as well as nonimpulsive noise and in preserving the fine details. Simulations on standard images confirm that these algorithms are superior to standard median filters."}
{"_id":"2bdcc4dbf14e13d33740531ea8954463ca7e68a2","title":"Social coding in GitHub: transparency and collaboration in an open software repository","text":"Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation."}
{"_id":"3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","title":"Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors","text":"Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts."}
{"_id":"1b90ee5c846aafe7feb38b439a3e8fa212757899","title":"Detection and analysis of drive-by-download attacks and malicious JavaScript code","text":"JavaScript is a browser scripting language that allows developers to create sophisticated client-side interfaces for web applications. However, JavaScript code is also used to carry out attacks against the user's browser and its extensions. These attacks usually result in the download of additional malware that takes complete control of the victim's platform, and are, therefore, called \"drive-by downloads.\" Unfortunately, the dynamic nature of the JavaScript language and its tight integration with the browser make it difficult to detect and block malicious JavaScript code.\n This paper presents a novel approach to the detection and analysis of malicious JavaScript code. Our approach combines anomaly detection with emulation to automatically identify malicious JavaScript code and to support its analysis. We developed a system that uses a number of features and machine-learning techniques to establish the characteristics of normal JavaScript code. Then, during detection, the system is able to identify anomalous JavaScript code by emulating its behavior and comparing it to the established profiles. In addition to identifying malicious code, the system is able to support the analysis of obfuscated code and to generate detection signatures for signature-based systems. The system has been made publicly available and has been used by thousands of analysts."}
{"_id":"3032182c47b75d9c1d16877815dab8f8637631a2","title":"Beyond blacklists: learning to detect malicious web sites from suspicious URLs","text":"Malicious Web sites are a cornerstone of Internet criminal activities. As a result, there has been broad interest in developing systems to prevent the end user from visiting such sites. In this paper, we describe an approach to this problem based on automated URL classification, using statistical methods to discover the tell-tale lexical and host-based properties of malicious Web site URLs. These methods are able to learn highly predictive models by extracting and automatically analyzing tens of thousands of features potentially indicative of suspicious URLs. The resulting classifiers obtain 95-99% accuracy, detecting large numbers of malicious Web sites from their URLs, with only modest false positives."}
{"_id":"6ba2b0a92408789eec23c008a9beb1b574b42470","title":"Anomaly Based Web Phishing Page Detection","text":"Many anti-phishing schemes have recently been proposed in literature. Despite all those efforts, the threat of phishing attacks is not mitigated. One of the main reasons is that phishing attackers have the adaptability to change their tactics with little cost. In this paper, we propose a novel approach, which is independent of any specific phishing implementation. Our idea is to examine the anomalies in Web pages, in particular, the discrepancy between a Web site's identity and its structural features and HTTP transactions. It demands neither user expertise nor prior knowledge of the Web site. The evasion of our phishing detection entails high cost to the adversary. As shown by the experiments, our phishing detector functions with low miss rate and low false-positive rate"}
{"_id":"9cbe8c8ba680a4e55517a8cf322603334ac68be1","title":"Effective analysis, characterization, and detection of malicious web pages","text":"The steady evolution of the Web has paved the way for miscreants to take advantage of vulnerabilities to embed malicious content into web pages. Up on a visit, malicious web pages steal sensitive data, redirect victims to other malicious targets, or cease control of victim's system to mount future attacks. Approaches to detect malicious web pages have been reactively effective at special classes of attacks like drive-by-downloads. However, the prevalence and complexity of attacks by malicious web pages is still worrisome. The main challenges in this problem domain are (1) fine-grained capturing and characterization of attack payloads (2) evolution of web page artifacts and (3) exibility and scalability of detection techniques with a fast-changing threat landscape. To this end, we proposed a holistic approach that leverages static analysis, dynamic analysis, machine learning, and evolutionary searching and optimization to effectively analyze and detect malicious web pages. We do so by: introducing novel features to capture fine-grained snapshot of malicious web pages, holistic characterization of malicious web pages, and application of evolutionary techniques to fine-tune learning-based detection models pertinent to evolution of attack payloads. In this paper, we present key intuition and details of our approach, results obtained so far, and future work."}
{"_id":"d69ae114a54a0295fe0a882d205611a121f981e1","title":"ADAM: Detecting Intrusions by Data Mining","text":"Intrusion detection systems have traditionally been based on the characterization of an attack and the tracking of the activity on the system to see if it matches that characterization. Recently, new intrusion detection systems based on data mining are making their appearance in the field. This paper describes the design and experiences with the ADAM ( Audit Data Analysis and Mining) system, which we use as a testbed to study how useful data mining techniques can be in intrusion detection. Keywords\u2014Intrusion Detection, Data Mining, Association Rules, Classifiers."}
{"_id":"03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","title":"A Convolutional Neural Network for Modelling Sentences","text":"The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline."}
{"_id":"74ec3d4cbb22453ce1d128c42ea66d2bdced64d6","title":"Novel Multilevel Inverter Carrier-Based PWM Method","text":"The advent of the transformerless multilevel inverter topology has brought forth various pulsewidth modulation (PWM) schemes as a means to control the switching of the active devices in each of the multiple voltage levels in the inverter. An analysis of how existing multilevel carrier-based PWM affects switch utilization for the different levels of a diode-clamped inverter is conducted. Two novel carrier-based multilevel PWM schemes are presented which help to optimize or balance the switch utilization in multilevel inverters. A 10-kW prototype sixlevel diode-clamped inverter has been built and controlled with the novel PWM strategies proposed in this paper to act as a voltage-source inverter for a motor drive."}
{"_id":"e68a6132f5536aad264ba62052005d0eca3356d5","title":"A New Neutral-Point-Clamped PWM Inverter","text":"A new neutral-point-clamped pulsewidth modulation (PWM) inverter composed of main switching devices which operate as switches for PWM and auxiliary switching devices to clamp the output terminal potential to the neutral point potential has been developed. This inverter output contains less harmonic content as compared with that of a conventional type. Two inverters are compared analytically and experimentally. In addition, a new PWM technique suitable for an ac drive system is applied to this inverter. The neutral-point-clamped PWM inverter adopting the new PWM technique shows an excellent drive system efficiency, including motor efficiency, and is appropriate for a wide-range variable-speed drive system."}
{"_id":"ff5c193fd7142b3f426baf997b43937eca1bbbad","title":"Multilevel inverters: a survey of topologies, controls, and applications","text":"Multilevel inverter technology has emerged recently as a very important alternative in the area of high-power medium-voltage energy control. This paper presents the most important topologies like diode-clamped inverter (neutral-point clamped), capacitor-clamped (flying capacitor), and cascaded multicell with separate dc sources. Emerging topologies like asymmetric hybrid cells and soft-switched multilevel inverters are also discussed. This paper also presents the most relevant control and modulation methods developed for this family of converters: multilevel sinusoidal pulsewidth modulation, multilevel selective harmonic elimination, and space-vector modulation. Special attention is dedicated to the latest and more relevant applications of these converters such as laminators, conveyor belts, and unified power-flow controllers. The need of an active front end at the input side for those inverters supplying regenerative loads is also discussed, and the circuit topology options are also presented. Finally, the peripherally developing areas such as high-voltage high-power devices and optical sensors and other opportunities for future development are addressed."}
{"_id":"3d8a29cf3843f92bf9897c4f2d3c02d96d59540a","title":"Multilevel PWM Methods at Low Modulation Indices","text":"When utilized at low amplitude modulation indices, existing multilevel carrier-based PWM strategies have no special provisions for this operating region, and several levels of the inverter go unused. This paper proposes some novel multilevel PWM strategies to take advantage of the multiple levels in both a diodeclamped inverter and a cascaded H-bridges inverter by utilizing all of the levels in the inverter even at low modulation indices. Simulation results show what effects the different strategies have on the active device utilization. A prototype 6-level diode-clamped inverter and an 11-level cascaded H-bridges inverter have been built and controlled with the novel PWM strategies proposed in this paper."}
{"_id":"40baa5d4632d807cc5841874be73415775b500fd","title":"Multilevel Converters for Large Electric Drives","text":"Traditional two-level high-frequency pulse width modulation (PWM) inverters for motor drives have several problems associated with their high frequency switching which produces common-mode voltage and high voltage change (dV\/dt) rates to the motor windings. Multilevel inverters solve these problems because their devices can switch at a much lower frequency. Two different multilevel topologies are identified for use as a converter for electric drives, a cascade inverter with separate dc sources and a back-to-back diode clamped converter. The cascade inverter is a natural fit for large automotive allelectric drives because of the high VA ratings possible and because it uses several levels of dc voltage sources which would be available from batteries or fuel cells. The back-to-back diode clamped converter is ideal where a source of ac voltage is available such as a hybrid electric vehicle. Simulation and experimental results show the superiority of these two converters over PWM based drives."}
{"_id":"0757817bf5714bb91c3d4f30cf3144e0837e57e5","title":"Tangible Bits: Towards Seamless Interfaces between People, Bits and Atoms","text":"This paper presents our vision of Human Computer Interaction (HCI): \"Tangible Bits.\" Tangible Bits allows users to \"grasp & manipulate\" bits in the center of users\u2019 attention by coupling the bits with everyday physical objects and architectural surfaces. Tangible Bits also enables users to be aware of background bits at the periphery of human perception using ambient display media such as light, sound, airflow, and water movement in an augmented space. The goal of Tangible Bits is to bridge the gaps between both cyberspace and the physical environment, as well as the foreground and background of human activities. This paper describes three key concepts of Tangible Bits: interactive surfaces; the coupling of bits with graspable physical objects; and ambient media for background awareness. We illustrate these concepts with three prototype systems \u2013 the metaDESK, transBOARD and ambientROOM \u2013 to identify underlying research issues."}
{"_id":"18d7a36d953480adba60c21e4b2a3f3208fedc77","title":"HERB: a home exploring robotic butler","text":"We describe the architecture, algorithms, and experiments with HERB, an autonomous mobile manipulator that performs useful manipulation tasks in the home. We present new algorithms for searching for objects, learning to navigate in cluttered dynamic indoor scenes, recognizing and registering objects accurately in high clutter using vision, manipulating doors and other constrained objects using caging grasps, grasp planning and execution in clutter, and manipulation on pose and torque constraint manifolds. S.S. Srinivasa ( ) \u00b7 D. Ferguson \u00b7 C.J. Helfrich Intel Research Pittsburgh, 4720 Forbes Avenue, Suite 410, Pittsburgh, PA 15213, USA e-mail: siddhartha.srinivasa@intel.com C.J. Helfrich e-mail: casey.j.helfrich@intel.com D. Berenson \u00b7 A. Collet \u00b7 R. Diankov \u00b7 G. Gallagher \u00b7 G. Hollinger \u00b7 J. Kuffner \u00b7 M.V. Weghe The Robotics Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA D. Berenson e-mail: dberenso@ri.cmu.edu A. Collet e-mail: acollet@ri.cmu.edu R. Diankov e-mail: rdiankov@ri.cmu.edu G. Gallagher e-mail: ggallagh@ri.cmu.edu G. Hollinger e-mail: gholling@ri.cmu.edu J. Kuffner e-mail: kuffner@ri.cmu.edu M.V. Weghe e-mail: vandeweg@ri.cmu.edu We also present numerous severe real-world test results from the integration of these algorithms into a single mobile manipulator."}
{"_id":"be9336fd5642e57b6c147c6eb97612b052fd43d4","title":"Projected texture stereo","text":"Passive stereo vision is widely used as a range sensing technology in robots, but suffers from dropouts: areas of low texture where stereo matching fails. By supplementing a stereo system with a strong texture projector, dropouts can be eliminated or reduced. This paper develops a practical stereo projector system, first by finding good patterns to project in the ideal case, then by analyzing the effects of system blur and phase noise on these patterns, and finally by designing a compact projector that is capable of good performance out to 3m in indoor scenes. The system has been implemented and has excellent depth precision and resolution, especially in the range out to 1.5m."}
{"_id":"cf2a86994505a96c19c73dbbaa4a39801bdee088","title":"Real-time 3D object pose estimation and tracking for natural landmark based visual servo","text":"A real-time solution for estimating and tracking the 3D pose of a rigid object is presented for image-based visual servo with natural landmarks. The many state-of-the-art technologies that are available for recognizing the 3D pose of an object in a natural setting are not suitable for real-time servo due to their time lags. This paper demonstrates that a real-time solution of 3D pose estimation become feasible by combining a fast tracker such as KLT [7] [8] with a method of determining the 3D coordinates of tracking points on an object at the time of SIFT based tracking point initiation, assuming that a 3D geometric model with SIFT description of an object is known a-priori. Keeping track of tracking points with KLT, removing the tracking point outliers automatically, and reinitiating the tracking points using SIFT once deteriorated, the 3D pose of an object can be estimated and tracked in real-time. This method can be applied to both mono and stereo camera based 3D pose estimation and tracking. The former guarantees higher frame rates with about 1 ms of local pose estimation, while the latter assures of more precise pose results but with about 16 ms of local pose estimation. The experimental investigations have shown the effectiveness of the proposed approach with real-time performance."}
{"_id":"0674c1e2fd78925a1baa6a28216ee05ed7b48ba0","title":"Object Recognition from Local Scale-Invariant Features","text":"Proc. of the International Conference on Computer Vision, Corfu (Sept. 1999) An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds."}
{"_id":"12cedcc79bec6403ffab5d4c85a1bf7500683eca","title":"Algorithmic Complexity in Coding Theory and the Minimum Distance Problem","text":"We startwithan overviewof algorithmiccomplexity problemsin coding theory We then show that the problemof computing the minimumdiktanceof a binaryIinwr code is NP-hard,and the correspondingdeci~\u201donproblemis W-complete. Thisconstitutes a proof of the conjecture Bedekamp, McEliece,vanTilborg, dating back to 1978. Extensionsand applicationsof this result to other problemsin codingtheqv are discussed."}
{"_id":"18ca2837d280a6b2250024b6b0e59345601064a7","title":"Nonlinear dimensionality reduction by locally linear embedding.","text":"Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."}
{"_id":"949b3eb7d26afeb1585729b8a78575f2dbc925b1","title":"Feature Selection and Kernel Learning for Local Learning-Based Clustering","text":"The performance of the most clustering algorithms highly relies on the representation of data in the input space or the Hilbert space of kernel methods. This paper is to obtain an appropriate data representation through feature selection or kernel learning within the framework of the Local Learning-Based Clustering (LLC) (Wu and Scho\u0308lkopf 2006) method, which can outperform the global learning-based ones when dealing with the high-dimensional data lying on manifold. Specifically, we associate a weight to each feature or kernel and incorporate it into the built-in regularization of the LLC algorithm to take into account the relevance of each feature or kernel for the clustering. Accordingly, the weights are estimated iteratively in the clustering process. We show that the resulting weighted regularization with an additional constraint on the weights is equivalent to a known sparse-promoting penalty. Hence, the weights of those irrelevant features or kernels can be shrunk toward zero. Extensive experiments show the efficacy of the proposed methods on the benchmark data sets."}
{"_id":"15bdf3f1412cd762c40ad41dee5485de38ab0120","title":"On the Robust Control of Buck-Converter DC-Motor Combinations","text":"The concepts of active disturbance rejection control and flatness-based control are used in this paper to regulate the response of a dc-to-dc buck power converter affected by unknown, exogenous, time-varying load current demands. The generalized proportional integral observer is used to estimate and cancel the time-varying disturbance signals. A key element in the proposed control for the buck converter-dc motor combination is that even if the control input gain is imprecisely known, the control strategy still provides proper regulation and tracking. The robustness of this method is further extended to the case of a double buck topology driving two different dc motors affected by different load torque disturbances. Simulation results are provided."}
{"_id":"2d78fbe680b4501b0c21fbd49eb7652592cf077d","title":"Comparative study of Proportional Integral and Backstepping controller for Buck converter","text":"This paper describes the comparative study of Proportional Integral (PI) and Backstepping controller for Buck converter with R-load and DC motor. Backstepping approach is an efficient control design procedure for both regulation and tracking problems. This approach is based upon a systematic procedure which guarantees global regulation and tracking. The proposed control scheme is to stabilize the output (voltage or speed) and tracking error to converge zero asymptotically. Buck converter system is simulated in MATLAB, using state reconstruction techniques. Simulation results of buck converter with R-load and PMDC motor reveals that, settling time of Backstepping controller is less than PI controller"}
{"_id":"bba5386f9210f2996d403f09224926d860c763d7","title":"Robust Passivity-Based Control of a Buck\u2013Boost-Converter\/DC-Motor System: An Active Disturbance Rejection Approach","text":"This paper presents an active disturbance rejection (ADR) approach for the control of a buck-boost-converter feeding a dc motor. The presence of arbitrary, time-varying, load torque inputs on the dc motor and the lack of direct measurability of the motor's angular velocity variable prompts a generalized proportional integral (GPI) observer-based ADR controller which is synthesized on the basis of passivity considerations. The GPI observer simultaneously estimates the angular velocity and the exogenous disturbance torque input in an on-line cancellation scheme, known as the ADR control. The proposed control scheme is thus a sensorless one with robustness features added to the traditional energy shaping plus damping injection methodology. The discrete switching control realization of the designed continuous feedback control law is accomplished by means of a traditional PWM-modulation scheme. Additionally, an input to state stability property of the closed-loop system is established. Experimental and simulation results are provided."}
{"_id":"d8ca9a094f56e3fe542269ea272b46f5e46bdd99","title":"Closed-Loop Analysis and Cascade Control of a Nonminimum Phase Boost Converter","text":"In this paper, a cascade controller is designed and analyzed for a boost converter. The fast inner current loop uses sliding-mode control. The slow outer voltage loop uses the proportional-integral (PI) control. Stability analysis and selection of PI gains are based on the nonlinear closed-loop error dynamics. It is proven that the closed-loop system has a nonminimum phase behavior. The voltage transients and reference voltage are predictable. The current ripple and system sensitivity are studied. The controller is validated by a simulation circuit with nonideal circuit parameters, different circuit parameters, and various maximum switching frequencies. The simulation results show that the reference output voltage is well tracked under parametric changes, system uncertainties, or external disturbances with fast dynamic transients, confirming the validity of the proposed controller."}
{"_id":"06d22950a79a839d864b575569a0de91ded33135","title":"A general approach to control a Positive Buck-Boost converter to achieve robustness against input voltage fluctuations and load changes","text":"A positive buck-boost converter is a known DC- DC converter which may be controlled to act as buck or boost converter with same polarity of the input voltage. This converter has four switching states which include all the switching states of the above mentioned DC-DC converters. In addition there is one switching state which provides a degree of freedom for the positive buck-boost converter in comparison to the buck, boost, and inverting buck-boost converters. In other words the positive buck- boost converter shows a higher level of flexibility for its inductor current control compared to the other DC-DC converters. In this paper this extra degree of freedom is utilised to increase the robustness against input voltage fluctuations and load changes. To address this capacity of the positive buck-boost converter, two different control strategies are proposed which control the inductor current and output voltage against any fluctuations in input voltage and load changes. Mathematical analysis for dynamic and steady state conditions are presented in this paper and simulation results verify the proposed method."}
{"_id":"1a07186bc10592f0330655519ad91652125cd907","title":"A unified architecture for natural language processing: deep neural networks with multitask learning","text":"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."}
{"_id":"303b0b6e6812c60944a4ac9914222ac28b0813a2","title":"Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis","text":"This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify thecontextual polarityfor a large subset of sentiment expressions, achieving results that are significantly better than baseline."}
{"_id":"3bf22713709f58c8a64dd56a69257ceae8532013","title":"Robust real-time lane and road detection in critical shadow conditions","text":"This paper presents the vision-based road detection system currently installed onto the MOB-LAB land vehicle. Based on a geometrical transform and on a fast morphological processing, the system is capable to detect road markings even in extremely severe shadow conditions on at and structured roads. The use of a special-purpose massively architecture (PAPRICA) allows to achieve a processing rate of about 17 Hz."}
{"_id":"21968ae000669eb4cf03718a0d97e23a6bf75926","title":"Learning influence probabilities in social networks","text":"Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance."}
{"_id":"24986435c73066aaea0b21066db4539270106bee","title":"Novelty and redundancy detection in adaptive filtering","text":"This paper addresses the problem of extending an adaptive information filtering system to make decisions about the novelty and redundancy of relevant documents. It argues that relevance and redundance should each be modelled explicitly and separately. A set of five redundancy measures are proposed and evaluated in experiments with and without redundancy thresholds. The experimental results demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language models are both effective for identifying redundant documents."}
{"_id":"39282ff070f62ceeaa6495815098cbac8411101f","title":"Collaborative location and activity recommendations with GPS history data","text":"With the increasing popularity of location-based services, such as tour guide and location-based social network, we now have accumulated many location data on the Web. In this paper, we show that, by using the location data based on GPS and users' comments at various locations, we can discover interesting locations and possible activities that can be performed there for recommendations. Our research is highlighted in the following location-related queries in our daily life: 1) if we want to do something such as sightseeing or food-hunting in a large city such as Beijing, where should we go? 2) If we have already visited some places such as the Bird's Nest building in Beijing's Olympic park, what else can we do there? By using our system, for the first question, we can recommend her to visit a list of interesting locations such as Tiananmen Square, Bird's Nest, etc. For the second question, if the user visits Bird's Nest, we can recommend her to not only do sightseeing but also to experience its outdoor exercise facilities or try some nice food nearby. To achieve this goal, we first model the users' location and activity histories that we take as input. We then mine knowledge, such as the location features and activity-activity correlations from the geographical databases and the Web, to gather additional inputs. Finally, we apply a collective matrix factorization method to mine interesting locations and activities, and use them to recommend to the users where they can visit if they want to perform some specific activities and what they can do if they visit some specific places. We empirically evaluated our system using a large GPS dataset collected by 162 users over a period of 2.5 years in the real-world. We extensively evaluated our system and showed that our system can outperform several state-of-the-art baselines."}
{"_id":"03f9b5389df52f42cabcf0c4a9ac6e10ff6d4395","title":"A mobile application framework for the geospatial web","text":"In this paper we present an application framework that leverages geospatial content on the World Wide Web by enabling innovative modes of interaction and novel types of user interfaces on advanced mobile phones and PDAs. We discuss the current development steps involved in building mobile geospatial Web applications and derive three technological pre-requisites for our framework: spatial query operations based on visibility and field of view, a 2.5D environment model, and a presentationindependent data exchange format for geospatial query results. We propose the Local Visibility Model as a suitable XML-based candidate and present a prototype implementation."}
{"_id":"08a8c653b4f20f2b63ac6734f24fa5f5f819782a","title":"Mining interesting locations and travel sequences from GPS trajectories","text":"The increasing availability of GPS-enabled devices is changing the way people interact with the Web, and brings us a large amount of GPS trajectories representing people's location histories. In this paper, based on multiple users' GPS trajectories, we aim to mine interesting locations and classical travel sequences in a given geospatial region. Here, interesting locations mean the culturally important places, such as Tiananmen Square in Beijing, and frequented public areas, like shopping malls and restaurants, etc. Such information can help users understand surrounding locations, and would enable travel recommendation. In this work, we first model multiple individuals' location histories with a tree-based hierarchical graph (TBHG). Second, based on the TBHG, we propose a HITS (Hypertext Induced Topic Search)-based inference model, which regards an individual's access on a location as a directed link from the user to that location. This model infers the interest of a location by taking into account the following three factors. 1) The interest of a location depends on not only the number of users visiting this location but also these users' travel experiences. 2) Users' travel experiences and location interests have a mutual reinforcement relationship. 3) The interest of a location and the travel experience of a user are relative values and are region-related. Third, we mine the classical travel sequences among locations considering the interests of these locations and users' travel experiences. We evaluated our system using a large GPS dataset collected by 107 users over a period of one year in the real world. As a result, our HITS-based inference model outperformed baseline approaches like rank-by-count and rank-by-frequency. Meanwhile, when considering the users' travel experiences and location interests, we achieved a better performance beyond baselines, such as rank-by-count and rank-by-interest, etc."}
{"_id":"11651db02c4a243b5177516e62a45f952dc54430","title":"Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding","text":"The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques."}
{"_id":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging","text":"Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce stateof-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result."}
{"_id":"ff1577528a34a11c2a81d2451d346c412c674c02","title":"Character-based Neural Machine Translation","text":"We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention\/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing\/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models."}
{"_id":"00a28138c74869cfb8236a18a4dbe3a896f7a812","title":"Better Word Representations with Recursive Neural Networks for Morphology","text":"Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way."}
{"_id":"0f52a233d2e20e7b270a4eed9e06aff1840a46d6","title":"The mirror-neuron system.","text":"A category of stimuli of great importance for primates, humans in particular, is that formed by actions done by other individuals. If we want to survive, we must understand the actions of others. Furthermore, without action understanding, social organization is impossible. In the case of humans, there is another faculty that depends on the observation of others' actions: imitation learning. Unlike most species, we are able to learn by imitation, and this faculty is at the basis of human culture. In this review we present data on a neurophysiological mechanism--the mirror-neuron mechanism--that appears to play a fundamental role in both action understanding and imitation. We describe first the functional properties of mirror neurons in monkeys. We review next the characteristics of the mirror-neuron system in humans. We stress, in particular, those properties specific to the human mirror-neuron system that might explain the human capacity to learn by imitation. We conclude by discussing the relationship between the mirror-neuron system and language."}
{"_id":"0015fa48e4ab633985df789920ef1e0c75d4b7a8","title":"Training Support Vector Machines: an Application to Face Detection","text":"Detection (To appear in the Proceedings of CVPR'97, June 17-19, 1997, Puerto Rico.) Edgar Osunay? Robert Freund? Federico Girosiy yCenter for Biological and Computational Learning and ?Operations Research Center Massachusetts Institute of Technology Cambridge, MA, 02139, U.S.A. Abstract We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs.) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classi ers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points."}
{"_id":"0224e11e8582dd35b32203e9da064d4a3935a792","title":"Fast Pose Estimation with Parameter-Sensitive Hashing","text":"Example-basedmethodsareeffectivefor parameterestimationproblemswhentheunderlyingsystemis simpleor thedimensionalityof the input is low. For complex andhigh-dimensional problemssuch asposeestimation,thenumberof required examplesand the computationalcomplexity rapidly becmeprohibitivelyhigh. We introducea new algorithm that learnsa setof hashingfunctionsthat efficiently index examplesrelevant to a particular estimationtask. Our algorithm extendsa recentlydevelopedmethodfor locality-sensitivehashing, which findsapproximateneighborsin timesublinearin thenumber of examples.Thismethoddependscritically on thechoiceof hashfunctions;weshowhowto find thesetof hashfunctions thatare optimallyrelevantto a particular estimationproblem.Experimentsdemonstr atethat theresultingalgorithm,which wecall Parameter -SensitiveHashing, canrapidlyandaccuratelyestimatethearticulatedposeof humanfiguresfroma large databaseof exampleimages. 0Part of thiswork wasdonewhenG.S.andP.V. werewith MitsubishiElectricResearchLabs,Cambridge,MA."}
{"_id":"0122e063ca5f0f9fb9d144d44d41421503252010","title":"Large Scale Distributed Deep Networks","text":"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestlysized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."}
{"_id":"15b45650fa30c56bdc4c595a5afd31663f7f3eb4","title":"Does Language Shape Thought?: Mandarin and English Speakers' Conceptions of Time","text":"Does the language you speak affect how you think about the world? This question is taken up in three experiments. English and Mandarin talk about time differently--English predominantly talks about time as if it were horizontal, while Mandarin also commonly describes time as vertical. This difference between the two languages is reflected in the way their speakers think about time. In one study, Mandarin speakers tended to think about time vertically even when they were thinking for English (Mandarin speakers were faster to confirm that March comes earlier than April if they had just seen a vertical array of objects than if they had just seen a horizontal array, and the reverse was true for English speakers). Another study showed that the extent to which Mandarin-English bilinguals think about time vertically is related to how old they were when they first began to learn English. In another experiment native English speakers were taught to talk about time using vertical spatial terms in a way similar to Mandarin. On a subsequent test, this group of English speakers showed the same bias to think about time vertically as was observed with Mandarin speakers. It is concluded that (1) language is a powerful tool in shaping thought about abstract domains and (2) one's native language plays an important role in shaping habitual thought (e.g., how one tends to think about time) but does not entirely determine one's thinking in the strong Whorfian sense."}
{"_id":"0e2795b1329b25ba3709584b96fd5cb4c96f6f22","title":"A Systematic Comparison of Various Statistical Alignment Models","text":"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."}
{"_id":"fc40ad1238fba787dd8a58a7aed57a8d020a6fdc","title":"Artificial neural networks: fundamentals, computing, design, and application.","text":"Artificial neural networks (ANNs) are relatively new computational tools that have found extensive utilization in solving many complex real-world problems. The attractiveness of ANNs comes from their remarkable information processing characteristics pertinent mainly to nonlinearity, high parallelism, fault and noise tolerance, and learning and generalization capabilities. This paper aims to familiarize the reader with ANN-based computing (neurocomputing) and to serve as a useful companion practical guide and toolkit for the ANNs modeler along the course of ANN project development. The history of the evolution of neurocomputing and its relation to the field of neurobiology is briefly discussed. ANNs are compared to both expert systems and statistical regression and their advantages and limitations are outlined. A bird's eye review of the various types of ANNs and the related learning rules is presented, with special emphasis on backpropagation (BP) ANNs theory and design. A generalized methodology for developing successful ANNs projects from conceptualization, to design, to implementation, is described. The most common problems that BPANNs developers face during training are summarized in conjunction with possible causes and remedies. Finally, as a practical application, BPANNs were used to model the microbial growth curves of S. flexneri. The developed model was reasonably accurate in simulating both training and test time-dependent growth curves as affected by temperature and pH."}
{"_id":"25406e6733a698bfc4ac836f8e74f458e75dad4f","title":"What Size Net Gives Valid Generalization?","text":"We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1\/8. We show that if m O(W\/\u220a log N\/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a\/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W\/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."}
{"_id":"656a33c1db546da8490d6eba259e2a849d73a001","title":"Learning in Artificial Neural Networks: A Statistical Perspective","text":"The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."}
{"_id":"fbe24a2d9598c620324e3bd51e2f817cd35e9c81","title":"Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights","text":"A two-layer neural network can be used to approximate any nonlinear function. T h e behavior of the hidden nodes tha t allows the network to do this is described. Networks w i th one input are analyzed first, and the analysis is then extended to networks w i t h mult iple inputs. T h e result of th is analysis is used to formulate a method for ini t ial izat ion o f the weights o f neural networks to reduce t ra in ing t ime. Training examples are given and the learning curve for these examples are shown to illustrate the decrease in necessary training t ime. Introduction Two-layer feed forward neural networks have been proven capable of approximating any arbitrary functions [l], given that they have sufficient numbers of nodes in their hidden layers. We offer a description of how this works, along with a method of speeding up the training process by choosing the networks\u2019 initial weights. The relationship between the inputs and the output of a two-layer neural network may be described by Equation (1) H l y = wi . sigmoid(LqX + W b i ) (1) i=O where y is the network\u2019s output, X is the input vector, H is the number of hidden nodes, Wi is the weight vector of the i th node of the hidden layer, Wbi is the bias weight of the ith hidden node, w i is the weight of the output layer which connects the i th hidden unit to the output. The behavior of hidden nodes in two-layer networks with one input To illustrate the behavior of the hidden nodes, a two-layer network with one input is trained to approximate a function of one variable d(z) . That is, the network is trained to produce d(z) given z as input using the back-propagation algorithm [2]. The output of the network is given as"}
{"_id":"0b47b6ffe714303973f40851d975c042ff4fcde1","title":"Distributional Clustering of English Words","text":"We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \u201csoft\u201d clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."}
{"_id":"146bb2ea1fbdd86f81cd0dae7d3fd63decac9f5c","title":"Genetic Algorithms in Search Optimization and Machine Learning","text":"This book brings together-in an informal and tutorial fashion-the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems..."}
{"_id":"a204471ad4722a5e4ade844f8a25aa1c1037e1c1","title":"Brain regions with mirror properties: A meta-analysis of 125 human fMRI studies","text":"Mirror neurons in macaque area F5 fire when an animal performs an action, such as a mouth or limb movement, and also when the animal passively observes an identical or similar action performed by another individual. Brain-imaging studies in humans conducted over the last 20 years have repeatedly attempted to reveal analogous brain regions with mirror properties in humans, with broad and often speculative claims about their functional significance across a range of cognitive domains, from language to social cognition. Despite such concerted efforts, the likely neural substrates of these mirror regions have remained controversial, and indeed the very existence of a distinct subcategory of human neurons with mirroring properties has been questioned. Here we used activation likelihood estimation (ALE), to provide a quantitative index of the consistency of patterns of fMRI activity measured in human studies of action observation and action execution. From an initial sample of more than 300 published works, data from 125 papers met our strict inclusion and exclusion criteria. The analysis revealed 14 separate clusters in which activation has been consistently attributed to brain regions with mirror properties, encompassing 9 different Brodmann areas. These clusters were located in areas purported to show mirroring properties in the macaque, such as the inferior parietal lobule, inferior frontal gyrus and the adjacent ventral premotor cortex, but surprisingly also in regions such as the primary visual cortex, cerebellum and parts of the limbic system. Our findings suggest a core network of human brain regions that possess mirror properties associated with action observation and execution, with additional areas recruited during tasks that engage non-motor functions, such as auditory, somatosensory and affective components."}
{"_id":"fb4dcbd818e5839f025a6bc247b3bc5632be502f","title":"Immersion and Emotion: Their Impact on the Sense of Presence","text":"The present study is designed to test the role of immersion and media content in the sense of presence. Specifically, we are interested in the affective valence of the virtual environments. This paper describes an experiment that compares three immersive systems (a PC monitor, a rear projected video wall, and a head-mounted display) and two virtual environments, one involving emotional content and the other not. The purpose of the experiment was to test the interactive role of these two media characteristics (form and content). Scores on two self-report presence measurements were compared among six groups of 10 people each. The results suggest that both immersion and affective content have an impact on presence. However, immersion was more relevant for non-emotional environments than for emotional ones."}
{"_id":"1f315e1ba65cdce34d372ae445b737c0bcc4dac7","title":"Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders","text":"To examine mirror neuron abnormalities in autism, high-functioning children with autism and matched controls underwent fMRI while imitating and observing emotional expressions. Although both groups performed the tasks equally well, children with autism showed no mirror neuron activity in the inferior frontal gyrus (pars opercularis). Notably, activity in this area was inversely related to symptom severity in the social domain, suggesting that a dysfunctional 'mirror neuron system' may underlie the social deficits observed in autism."}
{"_id":"0ae944eb32cdce405125f948b2eef2e7c0512fd3","title":"HF , VHF , and UHF Systems and Technology","text":"A wide variety of unique systems and components inhabits the HF, VHF, and UHF bands. Many communication systems (ionospheric, meteor-burst, and troposcatter) provide beyond-line-of-sight coverage and operate independently of external infrastructure. Broadcasting and over-the-horizon radar also operate in these bands. Magnetic-resonance imaging uses HF\/VHF signals to see the interior of a human body, and RF heating is used in a variety of medical and industrial applications. Receivers typically employ a mix of analog and digital-signal-processing techniques. Systems for these frequencies make use of RF-power MOSFETs, p-i-n diodes, and ferrite-loaded transmission-line transformers."}
{"_id":"722e2f7894a1b62e0ab09913ce9b98654733d98e","title":"Information overload and the message dynamics of online interaction spaces: a theoretical model and empirical exploration","text":"This publication contains reprint articles for which IEEE does not hold copyright. Full text is not available on IEEE Xplore for these articles."}
{"_id":"368f3dea4f12c77dfc9b7203f3ab2b9efaecb635","title":"Statistical Phrase-Based Translation","text":"We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems."}
{"_id":"6ebdb88c39787f4242e92504b6d2c60b8421193a","title":"The association between psychological distance and construal level: evidence from an implicit association test.","text":"According to construal level theory (N. Liberman, Y. Trope, & E. Stephan, in press; Y. Trope & N. Liberman, 2003), people use a more abstract, high construal level when judging, perceiving, and predicting more psychologically distal targets, and they judge more abstract targets as being more psychologically distal. The present research demonstrated that associations between more distance and higher level of construal also exist on a pure conceptual level. Eight experiments used the Implicit Association Test (IAT; A. G. Greenwald, D. E. McGhee, & J. L. K. Schwartz, 1998) to demonstrate an association between words related to construal level (low vs. high) and words related to four dimensions of distance (proximal vs. distal): temporal distance, spatial distance, social distance, and hypotheticality. In addition to demonstrating an association between level of construal and psychological distance, these findings also corroborate the assumption that all 4 dimensions of psychological distance are related to level of construal in a similar way and support the notion that they all are forms of psychological distance."}
{"_id":"1f4412f8c0d2e491b2b4bf486d47d448d8f46858","title":"Implicit Association Test 1 The Implicit Association Test at Age 7 : A Methodological and Conceptual Review","text":"A mong earthly organisms, humans have a unique propensity to introspect or look inward into the contents of their own minds, and to share those observations with others. With the ability to introspect comes the palpable feeling of \" knowing, \" of being objective or certain, of being mentally in control of one's thoughts, aware of the causes of one's thoughts, feelings, and actions, and of making decisions deliberately and rationally. Among the noteworthy discoveries of 20th century psychology was a challenge posed to this assumption of rationality. From the groundbreaking theorizing of Herbert Simon (1955) and the mind-boggling problems posed by Kahneman, Slovik, and Tversky (1982) to striking demonstrations of illusions of control (Wegner, 2002), the paucity of introspection (Nisbett and Wilson, 1977), and the automaticity of everyday thought (Bargh, 1997), psychologists have shown the frailties of the minds of their species. As psychologists have come to grips with the limits of the mind, there has been an increased interest in measuring aspects of thinking and feeling that may not be easily accessed or available to consciousness. Innovations in measurement have been undertaken with the purpose of bringing under scrutiny new forms of cogni-tion and emotion that were previously undiscovered and especially by asking if traditional concepts such as attitude and preference, belief and stereotype, self-concept and self-esteem can be rethought based on what the new measures reveal. These newer measures do not require introspection on the part of the subject. For many constructs this is considered a valuable, if not essential, feature of measurement; for others, avoiding introspection is greeted with suspicion and skepticism. For example, one approach to measuring math ability would be to ask \" how good are you at math? \" whereas an alternative approach is to infer math ability via a performance on a math skills test. The former requires introspection to assess the relevant construct, the latter does not. And yet, the latter is accepted"}
{"_id":"2698f74468c49b29ac69e193d5aeaa09bb33faea","title":"Can language restructure cognition? The case for space","text":"Frames of reference are coordinate systems used to compute and specify the location of objects with respect to other objects. These have long been thought of as innate concepts, built into our neurocognition. However, recent work shows that the use of such frames in language, cognition and gesture varies cross-culturally, and that children can acquire different systems with comparable ease. We argue that language can play a significant role in structuring, or restructuring, a domain as fundamental as spatial cognition. This suggests we need to rethink the relation between the neurocognitive underpinnings of spatial cognition and the concepts we use in everyday thinking, and, more generally, to work out how to account for cross-cultural cognitive diversity in core cognitive domains."}
{"_id":"755b94b766dee3a34536f6b481a60f0d9f68aa0c","title":"The Role of Feasibility and Desirability Considerations in Near and Distant Future Decisions : A Test of Temporal Construal Theory","text":"Temporal construal theory states that distant future situations are construed on a higher level (i.e., using more abstract and central features) than near future situations. Accordingly, the theory suggests that the value associated with the high-level construal is enhanced over delay and that the value associated with the low-level construal is discounted over delay. In goal-directed activities, desirability of the activity's end state represents a high-level construal, whereas the feasibility of attaining this end state represents a low-level construal. Study 1 found that distant future activities were construed on a higher level than near future activities. Studies 2 and 3 showed that decisions regarding distant future activities, compared with decisions regarding near future activities, were more influenced by the desirability of the end state and less influenced by the feasibility of attaining the end state. Study 4 presented students with a real-life choice of academic assignments varying in difficulty (feasibility) and interest (desirability). In choosing a distant future assignment, students placed relatively more weight on the assignment's interest, whereas in choosing a near future assignment, they placed relatively more weight on difficulty. Study 5 found that distant future plans, compared with near future plans, were related to desirability of activities rather than to time constraints."}
{"_id":"12a376e621d690f3e94bce14cd03c2798a626a38","title":"Rapid Object Detection using a Boosted Cascade of Simple Features","text":"This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection."}
{"_id":"682c434becc69b9dc70a4c18305f9d733d03f581","title":"Users of the world , unite ! The challenges and opportunities of Social Media","text":"As of January 2009, the online social networking application Facebook registered more than 175 million active users. To put that number in perspective, this is only slightly less than the population of Brazil (190 million) and over twice the population of Germany (80 million)! At the same time, every minute, 10 hours of content were uploaded to the video sharing platform YouTube. And, the image hosting site Flickr provided access to over 3 billion photographs, making the world-famous Louvre Museum\u2019s collection of 300,000 objects seem tiny in comparison. According to Forrester Research, 75% of Internet surfers used \u2018\u2018Social Media\u2019\u2019 in the second quarter of 2008 by joining social networks, reading blogs, or contributing reviews to shopping sites; this represents a significant rise from 56% in 2007. The growth is not limited to teenagers, either; members of Generation X, now 35\u201444 years old, increasingly populate the ranks of joiners, spectators, and critics. It is therefore reasonable to say that Social Media represent a revolutionary new trend that should be of interest to companies operating in online space\u2013\u2014or any space, for that matter. Yet, not overly many firms seem to act comfortably in a world where consumers can speak so freely Business Horizons (2010) 53, 59\u201468"}
{"_id":"178631e0f0e624b1607c7a7a2507ed30d4e83a42","title":"Speech recognition with deep recurrent neural networks","text":"Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."}
{"_id":"00a7370518a6174e078df1c22ad366a2188313b5","title":"Determining Optical Flow","text":"Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image."}
{"_id":"2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","title":"ImageNet Classification with Deep Convolutional Neural Networks","text":"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."}
{"_id":"254ded254065f2d26ca24ec024cefd7604bd74e7","title":"Efficient Parallel Graph Exploration on Multi-Core CPU and GPU","text":"Graphs are a fundamental data representation that has been used extensively in various domains. In graph-based applications, a systematic exploration of the graph such as a breadth-first search (BFS) often serves as a key component in the processing of their massive data sets. In this paper, we present a new method for implementing the parallel BFS algorithm on multi-core CPUs which exploits a fundamental property of randomly shaped real-world graph instances. By utilizing memory bandwidth more efficiently, our method shows improved performance over the current state-of-the-art implementation and increases its advantage as the size of the graph increases. We then propose a hybrid method which, for each level of the BFS algorithm, dynamically chooses the best implementation from: a sequential execution, two different methods of multicore execution, and a GPU execution. Such a hybrid approach provides the best performance for each graph size while avoiding poor worst-case performance on high-diameter graphs. Finally, we study the effects of the underlying architecture on BFS performance by comparing multiple CPU and GPU systems, a high-end GPU system performed as well as a quad-socket high-end CPU system."}
{"_id":"01f187c3f0390123e70e01f824101bf771e76b8f","title":"Bitcoin and Beyond: A Technical Survey on Decentralized Digital Currencies","text":"Besides attracting a billion dollar economy, Bitcoin revolutionized the field of digital currencies and influenced many adjacent areas. This also induced significant scientific interest. In this survey, we unroll and structure the manyfold results and research directions. We start by introducing the Bitcoin protocol and its building blocks. From there we continue to explore the design space by discussing existing contributions and results. In the process, we deduce the fundamental structures and insights at the core of the Bitcoin protocol and its applications. As we show and discuss, many key ideas are likewise applicable in various other fields, so that their impact reaches far beyond Bitcoin itself."}
{"_id":"12f7b71324ee8e1796a9ef07af05b66674fe6af0","title":"Collective annotation of Wikipedia entities in web text","text":"To take the first step beyond keyword-based search toward entity-based search, suitable token spans (\"spots\") on documents must be identified as references to real-world entities from an entity catalog. Several systems have been proposed to link spots on Web pages to entities in Wikipedia. They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity. Two recent systems exploit inter-label dependencies, but in limited ways. We propose a general collective disambiguation approach. Our premise is that coherent documents refer to entities from one or a few related topics or domains. We give formulations for the trade-off between local spot-to-entity compatibility and measures of global coherence between entities. Optimizing the overall entity assignment is NP-hard. We investigate practical solutions based on local hill-climbing, rounding integer linear programs, and pre-clustering entities followed by local optimization within clusters. In experiments involving over a hundred manually-annotated Web pages and tens of thousands of spots, our approaches significantly outperform recently-proposed algorithms."}
{"_id":"77d2698e8efadda698b0edb457cd8de75224bfa0","title":"Knowledge Base Population: Successful Approaches and Challenges","text":"In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking \u2013 linking names in context to entities in the KB \u2013 and Slot Filling \u2013 adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes (\u201cslots\u201d) derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges."}
{"_id":"0638d1f7d37f6bda49f6ec951de37aca0e53b98a","title":"Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation","text":"Conversation between two people is usually of Mixed-Initiative, with Control over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles."}
{"_id":"1c909ac1c331c0c246a88da047cbdcca9ec9b7e7","title":"Large-Scale Named Entity Disambiguation Based on Wikipedia Data","text":"This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles."}
{"_id":"2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2","title":"DBpedia: A Nucleus for a Web of Open Data","text":"DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for humanand machineconsumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data."}
{"_id":"1cc920998208f988a873dbbfa0315274d0b51b57","title":"Introduction to Robotics: Mechanics and Control","text":"How can you change your mind to be more open? There many sources that can help you to improve your thoughts. It can be from the other experiences and also story from some people. Book is one of the trusted sources to get. You can find so many books that we share here in this website. And now, we show you one of the best, the introduction to robotics mechanics and control john j craig solution manual."}
{"_id":"5238190eb598fb3352c51ee07b7ee8ec714f3c38","title":"OPEM: A Static-Dynamic Approach for Machine-Learning-Based Malware Detection","text":"Malware is any computer software potentially harmful to both computers and networks. The amount of malware is growing every year and poses a serious global security threat. Signature-based detection is the most extended method in commercial antivirus software, however, it consistently fails to detect new malware. Supervised machine learning has been adopted to solve this issue. There are two types of features that supervised malware detectors use: (i) static features and (ii) dynamic features. Static features are extracted without executing the sample whereas dynamic ones requires an execution. Both approaches have their advantages and disadvantages. In this paper, we propose for the first time, OPEM, an hybrid unknown malware detector which combines the frequency of occurrence of operational codes (statically obtained) with the information of the execution trace of an executable (dynamically obtained). We show that this hybrid approach enhances the performance of both approaches when run separately."}
{"_id":"e5acdb0246b33d33c2a34a4a23faaf21e2f9b924","title":"An Efficient Non-Negative Matrix-Factorization-Based Approach to Collaborative Filtering for Recommender Systems","text":"Matrix-factorization (MF)-based approaches prove to be highly accurate and scalable in addressing collaborative filtering (CF) problems. During the MF process, the non-negativity, which ensures good representativeness of the learnt model, is critically important. However, current non-negative MF (NMF) models are mostly designed for problems in computer vision, while CF problems differ from them due to their extreme sparsity of the target rating-matrix. Currently available NMF-based CF models are based on matrix manipulation and lack practicability for industrial use. In this work, we focus on developing an NMF-based CF model with a single-element-based approach. The idea is to investigate the non-negative update process depending on each involved feature rather than on the whole feature matrices. With the non-negative single-element-based update rules, we subsequently integrate the Tikhonov regularizing terms, and propose the regularized single-element-based NMF (RSNMF) model. RSNMF is especially suitable for solving CF problems subject to the constraint of non-negativity. The experiments on large industrial datasets show high accuracy and low-computational complexity achieved by RSNMF."}
{"_id":"11efa6998c2cfd3de59cf0ec0321a9e17418915d","title":"Toward Automated Dynamic Malware Analysis Using CWSandbox","text":"Malware is notoriously difficult to combat because it appears and spreads so quickly. In this article, we describe the design and implementation of CWSandbox, a malware analysis tool that fulfills our three design criteria of automation, effectiveness, and correctness for the Win32 family of operating systems"}
{"_id":"129ed742b496b23efdf745aaf0c48958ef64d2c6","title":"Exploring Multiple Execution Paths for Malware Analysis","text":"Malicious code (or Malware) is defined as software that fulfills the deliberately harmful intent of an attacker. Malware analysis is the process of determining the behavior and purpose of a given Malware sample (such as a virus, worm, or Trojan horse). This process is a necessary step to be able to develop effective detection techniques and removal tools. Currently, Malware analysis is mostly a manual process that is tedious and time-intensive. To mitigate this problem, a number of analysis tools have been proposed that automatically extract the behavior of an unknown program by executing it in a restricted environment and recording the operating system calls that are invoked. The problem of dynamic analysis tools is that only a single program execution is observed. Unfortunately, however, it is possible that certain malicious actions are only triggered under specific circumstances (e.g., on a particular day, when a certain file is present, or when a certain command is received). In this paper, we propose a system that allows us to explore multiple execution paths and identify malicious actions that are executed only when certain conditions are met. This enables us to automatically extract a more complete view of the program under analysis and identify under which circumstances suspicious actions are carried out. Our experimental results demonstrate that many Malware samples show different behavior depending on input read from the environment. Thus, by exploring multiple execution paths, we can obtain a more complete picture of their actions."}
{"_id":"2327ad6f237b37150e84f0d745a05565ebf0b24d","title":"Zerocash: Decentralized Anonymous Payments from Bitcoin","text":"Bit coin is the first digital currency to see widespread adoption. While payments are conducted between pseudonyms, Bit coin cannot offer strong privacy guarantees: payment transactions are recorded in a public decentralized ledger, from which much information can be deduced. Zero coin (Miers et al., IEEE S&P 2013) tackles some of these privacy issues by unlinking transactions from the payment's origin. Yet, it still reveals payments' destinations and amounts, and is limited in functionality. In this paper, we construct a full-fledged ledger-based digital currency with strong privacy guarantees. Our results leverage recent advances in zero-knowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARKs). First, we formulate and construct decentralized anonymous payment schemes (DAP schemes). A DAP scheme enables users to directly pay each other privately: the corresponding transaction hides the payment's origin, destination, and transferred amount. We provide formal definitions and proofs of the construction's security. Second, we build Zero cash, a practical instantiation of our DAP scheme construction. In Zero cash, transactions are less than 1 kB and take under 6 ms to verify - orders of magnitude more efficient than the less-anonymous Zero coin and competitive with plain Bit coin."}
{"_id":"2abf2c3e7ebed04e8c09e478157372dda5cb8bc5","title":"Real-time rigid-body visual tracking in a scanning electron microscope","text":"Robotics continues to provide researchers with an increasing ability to interact with objects at the nano scale. As micro- and nanorobotic technologies mature, more interest is given to computer-assisted or automated approaches to manipulation at these scales. Although actuators are currently available that enable displacements resolutions in the subnanometer range, improvements in feedback technologies have not kept pace. Thus, many actuators that are capable of performing nanometer displacements are limited in automated tasks by the lack of suitable feedback mechanisms. This paper proposes the use of a rigid-model-based method for end effector tracking in a scanning electron microscope to aid in enabling more precise automated manipulations and measurements. These models allow the system to leverage domain-specific knowledge to increase performance in a challenging tracking environment."}
{"_id":"2c6c6d3c94322e9ff75ff2143f7028bfab2b3c5f","title":"Extension of phase correlation to subpixel registration","text":"In this paper, we have derived analytic expressions for the phase correlation of downsampled images. We have shown that for downsampled images the signal power in the phase correlation is not concentrated in a single peak, but rather in several coherent peaks mostly adjacent to each other. These coherent peaks correspond to the polyphase transform of a filtered unit impulse centered at the point of registration. The analytic results provide a closed-form solution to subpixel translation estimation, and are used for detailed error analysis. Excellent results have been obtained for subpixel translation estimation of images of different nature and across different spectral bands."}
{"_id":"42d60f7faaa2f6fdd2b928c352d65eb57b4791aa","title":"Improving resolution by image registration","text":"Image resolution can be improved when the relative displacements in image sequences are known accurately, and some knowledge of the imaging process is available. The proposed approach is similar to back-projection used in tomography. Examples of improved image resolution are given for gray-level and color images, when the unknown image displacements are computed from the image sequence."}
{"_id":"90614cea8c2ab2bff0343231a26d6d0c9315d6c7","title":"A Comparison of Affine Region Detectors","text":"The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris\u00a0 (Mikolajczyk and \u00a0Schmid, 2002; Schaffalitzky and \u00a0Zisserman, 2002) and Hessian points\u00a0 (Mikolajczyk and \u00a0Schmid, 2002), a detector of \u2018maximally stable extremal regions', proposed by Matas et al.\u00a0(2002); an edge-based region detector\u00a0 (Tuytelaars and Van\u00a0Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van\u00a0Gool, 2000), and a detector of \u2018salient regions', proposed by Kadir, Zisserman and Brady\u00a0(2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework."}
{"_id":"bae5575284776cabed101750ac41848c700af431","title":"The social structure of free and open source software development","text":"Metaphors, such as the Cathedral and Bazaar, used to describe the organization of FLOSS projects typically place them in sharp contrast to proprietary development by emphasizing FLOSS\u2019s distinctive social and communications structures. But what do we really know about the communication patterns of FLOSS projects? How generalizable are the projects that have been studied? Is there consistency across FLOSS projects? Questioning the assumption of distinctiveness is important because practitioner-advocates from within the FLOSS community rely on features of social structure to describe and account for some of the advantages of FLOSS production. To address this question, we examined 120 project teams from SourceForge, representing a wide range of FLOSS project types, for their communications centralization as revealed in the interactions in the bug tracking system. We found that FLOSS development teams vary widely in their communications centralization, from projects completely centered on one developer to projects that are highly decentralized and exhibit a distributed pattern of conversation between developers and active users. We suggest, therefore, that it is wrong to assume that FLOSS projects are distinguished by a particular social structure merely because they are FLOSS. Our findings suggest that FLOSS projects might have to work hard to achieve the expected development advantages which have been assumed to flow from \u201cgoing open.\u201d In addition, the variation in communications structure across projects means that communications centralization is useful for comparisons between FLOSS teams. We"}
{"_id":"231878207a8641e605dc255f2c557fa4e8bb99bf","title":"The Cathedral and the Bazaar","text":"Permission is granted to copy, distribute and\/or modify this document under the terms of the Open Publication License, version 2.0. $Date: 2002\/08\/02 09:02:14 $ Revision History Revision 1.57 11 September 2000 esr New major section \u201cHow Many Eyeballs Tame Complexity\u201d. Revision 1.52 28 August 2000 esr MATLAB is a reinforcing parallel to Emacs. Corbato\u00f3 & Vyssotsky got it in 1965. Revision 1.51 24 August 2000 esr First DocBook version. Minor updates to Fall 2000 on the time-sensitive material. Revision 1.49 5 May 2000 esr Added the HBS note on deadlines and scheduling. Revision 1.51 31 August 1999 esr This the version that O\u2019Reilly printed in the first edition of the book. Revision 1.45 8 August 1999 esr Added the endnotes on the Snafu Principle, (pre)historical examples of bazaar development, and originality in the bazaar. Revision 1.44 29 July 1999 esr Added the \u201cOn Management and the Maginot Line\u201d section, some insights about the usefulness of bazaars for exploring design space, and substantially improved the Epilog. Revision 1.40 20 Nov 1998 esr Added a correction of Brooks based on the Halloween Documents. Revision 1.39 28 July 1998 esr I removed Paul Eggert\u2019s \u2019graph on GPL vs. bazaar in response to cogent aguments from RMS on Revision 1.31 February 1"}
{"_id":"2469fd136aaf16c49bbe6814d6153da1dc6c7c23","title":"Social translucence: an approach to designing systems that support social processes","text":"We are interested in desiging systems that support communication and collaboration among large groups of people over computing networks. We begin by asking what properties of the physical world support graceful human-human communication in face-to-face situations, and argue that it is possible to design digital systems that support coherent behavior by making participants and their activites visible to one another. We call such systems \u201csocially translucent systems\u201d and suggest that they have three characteristics\u2014visbility, awareness, and accountability\u2014which enable people to draw upon their experience and expertise to structure their interactions with one another. To motivate and focus our ideas we develop a vision of knowledge communities, conversationally based systems that support the creation, management and reuse of knowledge in a social context. We describe our experience in designing and deploying one layer of functionality for knowledge communities, embodied in a working system called \u201cBarbie\u201d and discuss research issues raised by a socially translucent approach to design."}
{"_id":"2fc0516f700b490b7e13db0f0d73d05afa5e346c","title":"Cave or Community? An Empirical Examination of 100 Mature Open Source Projects","text":"Starting with Eric Raymond\u2019s groundbreaking work, The Cathedral and the Bazaar, open-source software (OSS) has commonly been regarded as work produced by a community of developers. Yet, given the nature of software programs, one also hears of developers with no lives that work very hard to achieve great product results. In this paper, I sought empirical evidence that would help us understand which is more commonthe cave (i.e., lone producer) or the community. Based on a study of the top 100 mature products on Sourceforge, I find a few surprising things. First, most OSS programs are developed by individuals, rather than communities. The median number of developers in the 100 projects I looked at was 4 and the mode was 1numbers much lower than previous ones reported for highly successful projects! Second, most OSS programs do not generate a lot of discussion. Third, products with more developers tend to be viewed and downloaded more often. Fourth, the number of developers associated with a project is unrelated to the age of the project. Fifth, the larger the project, the smaller the percent of project administrators."}
{"_id":"280cd4a04cf7ecc36f84a7172483916a41403f5e","title":"Multi-class AdaBoost \u2217","text":"Boosting has been a very successful technique for solving the two-class classification problem. In going from two-class to multi-class classification, most algorithms have been restricted to reducing the multi-class classification problem to multiple two-class problems. In this paper, we develop a new algorithm that directly extends the AdaBoost algorithm to the multi-class case without reducing it to multiple two-class problems. We show that the proposed multi-class AdaBoost algorithm is equivalent to a forward stagewise additive modeling algorithm that minimizes a novel exponential loss for multi-class classification. Furthermore, we show that the exponential loss is a member of a class of Fisher-consistent loss functions for multi-class classification. As shown in the paper, the new algorithm is extremely easy to implement and is highly competitive in terms of misclassification error rate."}
{"_id":"3215a900e4fb9499c8904bfe662c59de042da67d","title":"Predicting Movie Sales from Blogger Sentiment","text":"The volume of discussion about a product in weblogs has recently been shown to correlate with the product\u2019s financial performance. In this paper, we study whether applying sentiment analysis methods to weblog data results in better correlation than volume only, in the domain of movies. Our main finding is that positive sentiment is indeed a better predictor for movie success when applied to a limited context around references to the movie in weblogs, posted prior to its release. If my film makes one more person miserable, I\u2019ve done my job."}
{"_id":"46a7464a8926241c8ed78b243ca0bf24253f8786","title":"Early Prediction of Movie Box Office Success Based on Wikipedia Activity Big Data","text":"Use of socially generated \"big data\" to access information about collective states of the minds in human societies has become a new paradigm in the emerging field of computational social science. A natural application of this would be the prediction of the society's reaction to a new product in the sense of popularity and adoption rate. However, bridging the gap between \"real time monitoring\" and \"early predicting\" remains a big challenge. Here we report on an endeavor to build a minimalistic predictive model for the financial success of movies based on collective activity data of online users. We show that the popularity of a movie can be predicted much before its release by measuring and analyzing the activity level of editors and viewers of the corresponding entry to the movie in Wikipedia, the well-known online encyclopedia."}
{"_id":"070096ce36bba240b39b5ddb7bc6071311478843","title":"Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments","text":"In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers."}
{"_id":"21dd2790b76a57b42191b19a54505837f3969141","title":"Tuned Models of Peer Assessment in MOOCs","text":"In massive open-access online courses (MOOCs), peer grading serves as a critical tool for scaling the grading of complex, open-ended assignments to courses with tens or hundreds of thousands of students. But despite promising initial trials, it does not always deliver accurate results compared to human experts. In this paper, we develop algorithms for estimating and correcting for grader biases and reliabilities, showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from Coursera\u2019s HCI course offerings \u2014 the largest peer grading networks analysed to date. We relate grader biases and reliabilities to other student factors such as engagement, performance as well as commenting style. We also show that our model can lead to more intelligent assignment of graders to gradees."}
{"_id":"3b073bf632aa91628d134a828911ff82706b8a32","title":"The critical importance of retrieval for learning.","text":"Learning is often considered complete when a student can produce the correct answer to a question. In our research, students in one condition learned foreign language vocabulary words in the standard paradigm of repeated study-test trials. In three other conditions, once a student had correctly produced the vocabulary item, it was repeatedly studied but dropped from further testing, repeatedly tested but dropped from further study, or dropped from both study and test. Repeated studying after learning had no effect on delayed recall, but repeated testing produced a large positive effect. In addition, students' predictions of their performance were uncorrelated with actual performance. The results demonstrate the critical role of retrieval practice in consolidating learning and show that even university students seem unaware of this fact."}
{"_id":"50fcb0e5f921357b2ec96be9a75bfd3169e8f8da","title":"Personalized Online Education - A Crowdsourcing Challenge","text":"Interest in online education is surging, as dramatized by the success of Khan Academy and recent Stanford online courses, but the technology for online education is in its infancy. Crowdsourcing mechanisms will likely be essential in order to reach the full potential of this medium. This paper sketches some of the challenges and directions we hope HCOMP researchers will ad-"}
{"_id":"7545f90299a10dae1968681f6bd268b9b5ab2c37","title":"Powergrading: a Clustering Approach to Amplify Human Effort for Short Answer Grading","text":"We introduce a new approach to the machine-assisted grading of short answer questions. We follow past work in automated grading by first training a similarity metric between student responses, but then go on to use this metric to group responses into clusters and subclusters. The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as \u201cpowergrading.\u201d We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available. We show results in terms of grading progress with a small \u201cbudget\u201d of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents."}
{"_id":"831ed2a5f40861866b4ebfe60257b997701e38e2","title":"ESPRIT-estimation of signal parameters via rotational invariance techniques","text":"High-resolution signal parameter estimation is a problem of significance in many signal processing applications. Such applications include direction-of-arrival (DOA) estimation, system identification, and time series analysis. A novel approach to the general problem of signal parameter estimation is described. Although discussed in the context of direction-of-arrival estimation, ESPRIT can be applied to a wide variety of problems including accurate detection and estimation of sinusoids in noise. It exploits an underlying rotational invariance among signal subspaces induced by an array of sensors with a translational invariance structure. The technique, when applicable, manifests significant performance and computational advantages over previous algorithms such as MEM, Capon's MLM, and MUSIC."}
{"_id":"f8c90c6549b97934da4fcdafe0012cea95cc443c","title":"State-of-the-Art Predictive Maintenance Techniques*","text":"This paper discusses the limitations of time-based equipment maintenance methods and the advantages of predictive or online maintenance techniques in identifying the onset of equipment failure. The three major predictive maintenance techniques, defined in terms of their source of data, are described as follows: 1) the existing sensor-based technique; 2) the test-sensor-based technique (including wireless sensors); and 3) the test-signal-based technique (including the loop current step response method, the time-domain reflectrometry test, and the inductance-capacitance-resistance test). Examples of detecting blockages in pressure sensing lines using existing sensor-based techniques and of verifying calibration using existing-sensor direct current output are given. Three Department of Energy (DOE)-sponsored projects, whose aim is to develop online and wireless hardware and software systems for performing predictive maintenance on critical equipment in nuclear power plants, DOE research reactors, and general industrial applications, are described."}
{"_id":"0bc46478051356455facc79f216a00b896c2dc5f","title":"ORTHONORMAL BASES OF COMPACTLY SUPPORTED WAVELETS","text":"We construct orthonormal bases of compactly supported wavelets, with arbitrarily high regularity. The order of regularity increases linearly with the support width. We start by reviewing the concept of multiresolution analysis as well as several algorithms in vision decomposition and reconstruction. The construction then follows from a synthesis of these different approaches."}
{"_id":"090d25f94cb021bdd3400a2f547f989a6a5e07ec","title":"Direct least squares fitting of ellipses","text":"This work presents a new efficient method for fitting ellipses to scattered data. Previous algorithms either fitted general conics or were computationally expensive. By minimizing the algebraic distance subject to the constraint 4 2 1 the new method incorporates the ellipticity constraint into the normalization factor. The new method combines several advantages: (i) It is ellipse-specific so that even bad data will always return an ellipse; (ii) It can be solved naturally by a generalized eigensystem and (iii) it is extremely robust, efficient and easy to implement. We compare the proposed method to other approaches and show its robustness on several examples in which other non-ellipse-specific approaches would fail or require computationally expensive iterative refinements. Source code for the algorithm is supplied and a demonstration is available on ! \" ! $#% '& () \"*) & +, .\/10 0 32, . 4) \"*) \"*) % 5* 0"}
{"_id":"4c4387afaeadda64d8183d7aba19574a9b757a6a","title":"SUITOR: an attentive information system","text":"Attentive systems pay attention to what users do so that they can attend to what users need. Such systems track user behavior, model user interests, and anticipate user desires and actions. Because the general class of attentive systems is broad \u2014 ranging from human butlers to web sites that profile users \u2014 we have focused specifically on attentive information systems, which observe user actions with information resources, model user information states, and suggest information that might be helpful to users. In particular, we describe an implemented system, Simple User Interest Tracker (Suitor), that tracks computer users through multiple channels \u2014 gaze, web browsing, application focus \u2014 to determine their interests and to satisfy their information needs. By observing behavior and modeling users, Suitor finds and displays potentially relevant information that is both timely and non-disruptive to the users' ongoing activities."}
{"_id":"002aaf4412f91d0828b79511f35c0863a1a32c47","title":"A real-time face tracker","text":"We present a real-time face tracker in this paper The system has achieved a rate of 30% frameshecond using an HP-9000 workstation with a framegrabber and a Canon VC-CI camera. It can track a person 'sface while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, we present a stochastic model to characterize skin-color distributions of human faces. The information provided by the model is sufJicient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second, a motion model e's used to estimate image motion and to predict search window. Third, a camera model is used toprediet and to compensate for camera motion. The system can be applied to tele-conferencing and many HCI applications including lip-reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand."}
{"_id":"69296a15df81fd853e648d160a329cbd9c0050c8","title":"Integrating perceived playfulness into expectation-confirmation model for web portal context","text":"This paper investigated the value of including \"playfulness\" in expectation-confirmation theory (ECT) when studying continued use of a web site. Original models examined cognitive beliefs and effects that influence a person's intention to continue to use an information system. Here, an extended ECT model (with an additional relationship between perceived playfulness and satisfaction) was shown to provide a better fit than a simple path from perceived usefulness to satisfaction. The results indicated that perceived playfulness, confirmation to satisfaction, and perceived usefulness all contributed significantly to the users' intent to reuse a web site. Thus, we believe that the extended ECT model is an appropriate tool for the study of web site effects."}
{"_id":"6eedf0a4fe861335f7f7664c14de7f71c00b7932","title":"Neural Turing Machines","text":"We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."}
{"_id":"096e07ced8d32fc9a3617ff1f725efe45507ede8","title":"Learning methods for generic object recognition with invariance to pose and lighting","text":"We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation\/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16\/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."}
{"_id":"073daaf4f6fa972d3bdee3c4e4510d21dc934dfb","title":"Machine learning - a probabilistic perspective","text":"\u201cKevin Murphy excels at unraveling the complexities of machine learning methods while motivating the reader with a stream of illustrated examples and real-world case studies. The accompanying software package includes source code for many of the figures, making it both easy and very tempting to dive in and explore these methods for yourself. A must-buy for anyone interested in machine learning or curious about how to extract useful knowledge from big data.\u201d John Winn, Microsoft Research"}
{"_id":"0cbc08b2e318133653448214d2b4fbbd7f812136","title":"Model-Driven Data Acquisition in Sensor Networks","text":"Declarative queries are proving to be an attractive paradigm for interacting with networks of wireless sensors. The metaphor that \u201cthe sensornet is a database\u201d is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a modelof that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques."}
{"_id":"8ecc044d920df247fbd455b752fd7cc0f7363ad7","title":"On the importance of initialization and momentum in deep learning","text":"Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods."}
{"_id":"9ed8e2f6c338f4e0d1ab0d8e6ab8b836ea66ae95","title":"A Fully Convolutional Neural Network for Speech Enhancement","text":"In hearing aids, the presence of babble noise degrades hearing intelligibility of human speech greatly. However, removing the babble without creating artifacts in human speech is a challenging task in a low SNR environment. Here, we sought to solve the problem by finding a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra via supervised learning. Specifically, we propose using fully Convolutional Neural Networks, which consist of lesser number of parameters than fully connected networks. The proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates that a convolutional network can be 12 times smaller than a recurrent network and yet achieves better performance, which shows its applicability for an embedded system: the hearing aids."}
{"_id":"0e400ee7aa7e53777596eaae63c4062a657d60f5","title":"Suppression of acoustic noise in speech using spectral subtraction","text":"A new technique for the modelling of perceptual systems called formal modelling is developed. This technique begins with qualitative observations about the perceptual system, the so-called perceptual symmetries, to obtain through mathematical analysis certain model structures which may then be calibrated by experiment. The analysis proceeds in two different ways depending upon the choice of linear or nonlinear models. For the linear case, the analysis proceeds through the methods of unitary representation theory. It begins with a unitary group representation on the image space and produces what we have called the fundamental structure theorem. For the nonlinear case, the analysis makes essential use of infinite-dimensional manifold theory. It ber ins with a Lie group action on an image manifold and produces the fundamental structure formula. These techniques will be used to study the brightness perception mechanism of the human visual system. Several visual groups are defined and their corresponding structures for visual system models are obtained. A new transform called the Mandala transform will be deduced from a certain visual group and its implications for image processing will be discussed. Several new phenomena of brightness perception will be presented. New facts about the Mach band illusion along with new adaptation phenomena will be presented. Also a new visual illusion will be presented. A visual model based on the above techniques will be presented. It will also be shown how use of statistical estimation theory can be made in the study of contrast adaptation."}
{"_id":"158fd3c7537d9d0af4c828cc0e3948e157287f83","title":"Dictionary Learning Algorithms for Sparse Representation","text":"Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave\/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial 25 words or less), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error)."}
{"_id":"02f872de0dc3f1d54ba68f9d751b7828f64d189c","title":"KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera","text":"KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. Uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions are shown. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch."}
{"_id":"849eaeeef1e11280bb7812239d34712b30023165","title":"RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments","text":"RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either structured light patterns combined with stereo sensing [6,10] or time-of-flight laser sensing [1] to generate depth estimates that can be associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below $100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3D maps of unprecedented richness, they have drawbacks that make their application to 3D mapping difficult: They provide depth only up to a limited distance (typically less than 5m), depth values are much noisier than those provided by laser scanners, and their field of view (\u223c 60\u25e6) is far more constrained than that of specialized cameras or laser scanners typically used for 3D mapping (\u223c 180\u25e6). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras."}
{"_id":"8d7eb0d250feebe1179532392e0dfdb9e2f2e80a","title":"Time-of-Flight sensor calibration for accurate range sensing","text":"Over the past years Time-of-Flight (ToF) sensors have become a considerable alternative to conventional distance sensing techniques like laser scanners or image based stereo-vision. Due to the ability to provide full-range distance information at high frame-rates, ToF sensors achieve a significant impact onto current research areas like online object recognition, collision prevention or scene and object reconstruction. Nevertheless, ToF-cameras like the Photonic Mixer Device (PMD) still exhibit a number of error sources that affect the accuracy of measured distance information. For this reason, major error sources for ToFcameras will be discussed, along with a new calibration approach that combines intrinsic, distance as well as a reflectivity related error calibration in an overall, easy to use system and thus significantly reduces the number of necessary reference images. The main contribution, in this context, is a new intensity-based calibration model that requires less input data compared to other models and thus significantly contributes to the reduction of calibration data. 2010 Elsevier Inc. All rights reserved."}
{"_id":"3e9517494eff0a2375348714d5eb6ecfd9b6cf60","title":"CALIBRATION OF A PMD-CAMERA USING A PLANAR CALIBRATION PATTERN TOGETHER WITH A MULTI-CAMERA SETUP","text":"We discuss the joint calibration of novel 3D range cameras based on the time-of-flight principle with the Photonic Mixing Device (PMD) and standard 2D CCD cameras. Due to the small field-of-view (fov) and low pixel resolution, PMD-cameras are difficult to calibrate with traditional calibration methods. In addition, the 3D range data contains systematic errors that need to be compensated. Therefore, a calibration method is developed that can estimate full intrinsic calibration of the PMD-camera including optical lens distortions and systematic range errors, and is able to calibrate the external orientation together with multiple 2D cameras that are rigidly coupled to the PMD-camera. The calibration approach is based on a planar checkerboard pattern as calibration reference, viewed from multiple angles and distances. By combining the PMD-camera with standard CCD-cameras the internal camera parameters can be estimated more precisely and the limitations of the small fov can be overcome. Furthermore we use the additional cameras to calibrate the systematic depth measurement error of the PMD-camera. We show that the correlation between rotation and translation estimation is significantly reduced with our method."}
{"_id":"7a953aaf29ef67ee094943d4be50d753b3744573","title":"\"GrabCut\": interactive foreground extraction using iterated graph cuts","text":""}
{"_id":"24171780855cb31e5f7ea74908485e2a0142b620","title":"Challenges of design and practical application of LTCC chip antennas","text":"In this paper key challenges related to design, manufacturing and practical application of small LTCC antennas are described and discussed. A summary of the state of the art in LTCC antenna technology is presented and then the focus is put on limitations that have to be taken into account in the course of antenna design and manufacturing. Finally, some aspects related to practical application of LTCC antennas are discussed."}
{"_id":"ba2ceb8f6c9bf49da1b366e4757d202724c3bff4","title":"The radiation properties of electrically small folded spherical helix antennas","text":"The radiation properties of several electrically small, folded spherical helix antennas are presented. The primary variables considered in the design of these antennas are the number of helical turns and the number of helical arms. The principle design objectives are to achieve self resonance, a low quality factor (Q), and a practical radiation resistance for small values of ka. Designs are presented for ka less than 0.5, where the antennas are self resonant, exhibiting an efficiency in excess of 95%, a Q within 1.5 times the fundamental limit, and a radiation resistance near 50 \/spl Omega\/. The relationship between the number of helical turns, the number of helical arms, and achieving self resonance at low frequencies is discussed."}
{"_id":"cfd913e1edd1a15b7456ef6d222c1319e056eddc","title":"Small Spherical Antennas Using Arrays of Electromagnetically Coupled Planar Elements","text":"This letter presents the design, fabrication, and experimental characterization of small spherical antennas fabricated using arrays of non-interconnected planar conductor elements. The antennas are based upon spherical resonator structures with radiation Q-factors approaching $1.5\\times$ the fundamental lower limit. The antennas are formed by coupling these resonators to an impedance-matched coplanar strip transmission line. Direct electrical connection between the feed and the antenna are made only to conductor elements coplanar with the transmission line, simplifying the fabrication process. The incident energy excites a collective resonant mode of the entire sphere (an electric dipole resonance), thereby inducing currents in each of the rings of the structure. The presence of the conductor elements outside of the feed plane is critical towards achieving the excellent bandwidth behavior observed here. The fabricated antennas have a normalized size $ka=0.54$ (where $k$ is the wavenumber and $a$ is the radius of the sphere) and exhibit high radiative efficiencies ($>$ 90%) and bandwidth performance near the fundamental limit for their size."}
{"_id":"ae5d79dc7abd6b649a7d0bb9e108f79e2696e128","title":"The Spherical Coil as an Inductor, Shield, or Antenna","text":"The spherical coil is an idealized form of inductor having, on a spherical surface, a single-layer winding of constant axial pitch. Its magnetic field inside is uniform and outside is that of a small coil or magnetic dipole. Its properties exemplify exactly some of the rules that are approximately applicable to practical inductors. Simple formulas are given for self-inductance, mutual inductance, coupling coefficient, effect of iron core, and radiation power factor in free space or sea water. This coil is the basis for evaluating the shielding effect of a closed conductive (nonmagnetic) metal shell. A special winding is described which enables simple and exact computation of self-resonance (the length of wire being just 1\/2 wavelength in some cases)."}
{"_id":"f23ecb25c3250fc6e2d3401dc2f54ffd6135ae2e","title":"Substrate-Integrated Millimeter-Wave and Terahertz Antenna Technology","text":"Significant advances in the development of millimeter-wave and terahertz (30-10 000 GHz) technologies have been made to cope with the increasing interest in this still not fully explored electromagnetic spectrum. The nature of electromagnetic waves over this frequency range is well suited for the development of high-resolution imaging applications, molecular-sensitive spectroscopic devices, and ultrabroadband wireless communications. In this paper, millimeter-wave and terahertz antenna technologies are overviewed including the conventional and nonconventional planar\/nonplanar antenna structures based on different platforms. As a promising technological platform, substrate-integrated circuits (SICs) attract more and more attention. Various substrate-integrated waveguide (SIW) schemes and other synthesized guide techniques have been widely employed in the design of antennas and arrays. Different types of substrate-integrated antennas and beamforming networks are discussed with respect to theoretical and experimental results in connection with electrical and mechanical performances."}
{"_id":"1ff3ebd402e29c3af7226ece7f1d716daf1eb4a9","title":"A 64 GHz 2 Gbps transmit\/receive phased-array communication link in SiGe with 300 meter coverage","text":"This paper presents a 64 GHz transmit\/receive communication link between two 32-element SiGe-based phased arrays. The antenna element is a series-fed patch array, which provides directivity in the elevation plane. The transmit array results in an EIRP of 42 dBm, while the receive array provides an electronic gain of 33 dB and a system NF < 8 dB including the T\/R switch and antenna losses. The arrays can be scanned to +\/\u221250\u00b0 in the azimuth using a 5-bit phase shifter on the SiGe chip, while keeping very low sidelobes and a near-ideal pattern. The communication link uses one array on the transmit side and another array on the receive side, together with external mixers and IF amplifiers. A Keysight M8195A arbitrary waveform generator is used to generate the modulated waveforms on the transmit side and a Keysight DSO804A oscilloscope is used to demodulate the received IF signal. The link performance was measured for different scan angles and modulation formats. Data rates of 1 Gbps using 16-QAM and 2 Gbps using QPSK are demonstrated at 300 m. The system also results in > 4 Gbps data rate at 100 meters, and \u223c 500 Mbps data rate at 800 meters."}
{"_id":"2fb03a66f250a2c51eb2eb30344a13a5e4d8a265","title":"Fabrication and measurement of a large, monolithic, PCB-based AESA","text":"This paper discusses a fabrication approach and experimental validation of a very large, planar active electronically scanned array (AESA). The planar AESA architecture employs a monolithic printed circuit board (PCB) with 768 active antenna elements at X-Band. Manufacturing physically large arrays with high element counts is discussed in relation to construction, assembly and yield considerations. Measured active array patterns of the ESA are also presented."}
{"_id":"5ec3ee90bbc5b23e748d82cb1914d1c45d85bdd9","title":"A Millimeter-Wave (40\u201345 GHz) 16-Element Phased-Array Transmitter in 0.18-$\\mu$ m SiGe BiCMOS Technology","text":"This paper demonstrates a 16-element phased-array transmitter in a standard 0.18-mum SiGe BiCMOS technology for Q-band satellite applications. The transmitter array is based on the all-RF architecture with 4-bit RF phase shifters and a corporate-feed network. A 1:2 active divider and two 1:8 passive tee-junction dividers constitute the corporate-feed network, and three-dimensional shielded transmission-lines are used for the passive divider to minimize area. All signals are processed differentially inside the chip except for the input and output interfaces. The phased-array transmitter results in a 12.5 dB of average power gain per channel at 42.5 GHz with a 3-dB gain bandwidth of 39.9-45.6 GHz. The RMS gain variation is < 1.3 dB and the RMS phase variation is < for all 4-bit phase states at 35-50 GHz. The measured input and output return losses are < -10 dB at 36.6-50 GHz, and <-10 dB at 37.6-50 GHz, respectively. The measured peak-to-peak group delay variation is plusmn 20 ps at 40-45 GHz. The output P-1dB is -5plusmn1.5 dBm and the maximum saturated output power is - 2.5plusmn1.5 dBm per channel at 42.5 GHz. The transmitter shows <1.8 dB of RMS gain mismatch and < 7deg of RMS phase mismatch between the 16 different channels over all phase states. A - 30 dB worst-case port-to-port coupling is measured between adjacent channels at 30-50 GHz, and the measured RMS gain and phase disturbances due to the inter-channel coupling are < 0.15 dB and < 1deg, respectively, at 35-50 GHz. All measurements are obtained without any on-chip calibration. The chip consumes 720 mA from a 5 V supply voltage and the chip size is 2.6times3.2 mm2."}
{"_id":"a1b40af260487c00a2031df1ffb850d3bc368cee","title":"A 28GHz Bulk-CMOS dual-polarization phased-array transceiver with 24 channels for 5G user and basestation equipment","text":"Developing next-generation cellular technology (5G) in the mm-wave bands will require low-cost phased-array transceivers [1]. Even with the benefit of beamforming, due to space constraints in the mobile form-factor, increasing TX output power while maintaining acceptable PA PAE, LNA NF, and overall transceiver power consumption is important to maximizing link budget allowable path loss and minimizing handset case temperature. Further, the phased-array transceiver will need to be able to support dual-polarization communication. An IF interface to the analog baseband is desired for low power consumption in the handset or user equipment (UE) active antenna and to enable use of arrays of transceivers for customer premises equipment (CPE) or basestation (BS) antenna arrays with a low-loss IF power-combining\/splitting network implemented on an antenna backplane carrying multiple tiled antenna modules."}
{"_id":"0515ff7de41fd349b4bff34f7fe4e9c12a7fff47","title":"7.2 A 28GHz 32-element phased-array transceiver IC with concurrent dual polarized beams and 1.4 degree beam-steering resolution for 5G communication","text":"Next-generation mobile technology (5G) aims to provide an improved experience through higher data-rates, lower latency, and improved link robustness. Millimeter-wave phased arrays offer a path to support multiple users at high data-rates using high-bandwidth directional links between the base station and mobile devices. To realize this vision, a phased-array-based pico-cell must support a large number of precisely controlled beams, yet be compact and power efficient. These system goals have significant mm-wave radio interface implications, including scalability of the RFIC+antenna-array solution, increase in the number of concurrent beams by supporting dual polarization, precise beam steering, and high output power without sacrificing TX power efficiency. Packaged Si-based phased arrays [1\u20133] with nonconcurrent dual-polarized TX and RX operation [2,3], concurrent dual-polarized RX operation [3] and multi-IC scaling [3,4] have been demonstrated. However, support for concurrent dual-polarized operation in both RX and TX remains unaddressed, and high output power comes at the cost of power consumption, cooling complexity and increased size. The RFIC reported here addresses these challenges. It supports concurrent and independent dual-polarized operation in TX and RX modes, and is compatible with a volume-efficient, scaled, antenna-in-package array. A new TX\/RX switch at the shared antenna interface enables high output power without sacrificing TX efficiency, and a t-line-based phase shifter achieves <1\u00b0 RMS error and <5\u00b0 phase steps for precise beam control."}
{"_id":"31e4725e74bf623aeaf86782f52d9f140b2af153","title":"A MINI UNMANNED AERIAL VEHICLE ( UAV ) : SYSTEM OVERVIEW AND IMAGE ACQUISITION","text":"In the last years UAV (Unmanned Aerial Vehicle)-systems became relevant for applications in precision farming and in infrastructure maintenance, like road maintenance and dam surveillance. This paper gives an overview about UAV (Unmanned Aerial Vehicle) systems and their application for photogrammetric recording and documentation of cultural heritage. First the historical development of UAV systems and the definition of UAV-helicopters will be given. The advantages of a photogrammetric system on-board a model helicopter will be briefly discussed and compared to standard aerial and terrestrial photogrammetry. UAVs are mostly low cost systems and flexible and therefore a suitable alternative solution compared to other mobile mapping systems. A mini UAV-system was used for photogrammetric image data acquisition near Palpa in Peru. A settlement from the 13 century AD, which was presumably used as a mine, was flown with a model helicopter. Based on the image data, an accurate 3D-model will be generated in the future. With an orthophoto and a DEM derived from aerial images in a scale of 1:7 000, a flight planning was build up. The determined flying positions were implemented in the flight control system. Thus, the helicopter is able to fly to predefined pathpoints automatically. Tests in Switzerland and the flights in Pinchango Alto showed that using the built-in GPS\/INSand stabilization units of the flight control system, predefined positions could be reached exactly to acquire the images. The predicted strip crossings and flying height were kept accurately in the autonomous flying mode."}
{"_id":"81e0f458a894322baf170fa4d6fa8099bd055c39","title":"Statistical Decision Theory and Bayesian Analysis, 2nd Edition","text":""}
{"_id":"3505447904364877605aabaa450c09568c8db1ec","title":"Smart irrigation using low-cost moisture sensors and XBee-based communication","text":"Deficiency in fresh water resources globally has raised serious alarms in the last decade. Efficient management of water resources play an important role in the agriculture sector. Unfortunately, this is not given prime importance in the third world countries because of adhering to traditional practices. This paper presents a smart system that uses a bespoke, low cost soil moisture sensor to control water supply in water deficient areas. The sensor, which works on the principle of moisture dependent resistance change between two points in the soil, is fabricated using affordable materials and methods. Moisture data acquired from a sensor node is sent through XBEE wireless communication modules to a centralized server that controls water supply. A user-friendly interface is developed to visualize the daily moisture data. The low-cost and wireless nature of the sensing hardware presents the possibility to monitor the moisture levels of large agricultural fields. Moreover, the proposed moisture sensing method has the ability to be incorporated into an automated drip-irrigation scheme and perform automated, precision agriculture in conjunction with de-centralized water control."}
{"_id":"0d7256ac0119d01acbb2ff6e124c4d60635fae1f","title":"Managing Organizational Knowledge By Diagnosing Intellectual Capital : Framing and Advancing the State of the Field","text":"Copyright \u00a9 2001, Idea Group Publishing. Since organizational knowledge is at the crux of sustainable competitive advantage, the burgeoning field of intellectual capital is an exciting area for both researchers and practitioners. Intellectual capital is conceptualized from numerous disciplines making the field a mosaic of perspectives. Accountants are interested in how to measure it on the balance sheet, information technologists want to codify it on systems, sociologists want to balance power with it, psychologists want to develop minds because of it, human resource managers want to calculate an ROI on it, and training and development officers want to make sure that they can build it. The following article represents a comprehensive literature review from a variety of managerial disciplines. In addition to highlighting the research to date, avenues for future pursuit are also offered."}
{"_id":"21d699e1cba89f8e3b40522530ea86b3253c111e","title":"Intellectual capital ROI : a causal map of human capital antecedents and consequents","text":"This report describes the results of a ground-breaking research study that measured the antecedents and consequents of effective human capital management. The research sample consisted of 76 senior executives from 25 companies in the financial services industry. The results of the study yielded a holistic causal map that integrated constructs from the fields of intellectual capital, knowledge management, human resources, organizational behaviour, information technology and accounting. The integration of both quantitative and qualitative measures in an overall conceptual model yielded several research implications. The resulting structural equation model allows participating organizations and researchers to gauge the effectiveness of an organization\u2019s human capital capabilities. This will allow practitioners and researchers to more efficiently allocate resources with regard to human capital management. The potential outcomes of the study are limitless, since a program of consistent re-evaluation can lead to the establishment of causal relationships between human capital management and economic and business results. Introduction Today\u2019s knowledge-based world consists of universal dynamic change and massive information bombardment. By the year 2010, the codified information base of the world is expected to `\u0300 double every 11 hours\u2019\u2019 (Bontis, 1999, p. 435). Information storage capacities continue to expand enormously. In 1950, IBM\u2019s Rama C tape contained 4.4 megabytes and they were able to store as many as 50 of these tapes together. At that time, 220 megabytes represented the frontiers of information storage. Many of today\u2019s standard desktop computers are being sold with 40 gigabytes of hard disk space. It is sobering to remember that full motion video in uncompressed form requires 1 gigabyte per minute and that the 83 minutes of Snow White digitized in full colour amount to 15 terabytes of space. Unfortunately, the conscious mind is only capable of processing somewhere between 16 and 40 bits of information (ones and zeros) per second. How do we reconcile this information bombardment conundrum when it seems that human beings are the bottle-neck? The current issue and full text archive of this journal is available at http:\/\/www.emeraldinsight.com\/1469-1930.htm The authors would like to acknowledge the following organizations for their financial support: Accenture, Saratoga Institute and the Institute for Intellectual Capital Research. The authors would also like to highlight the contribution of Vanessa Yeh, who administered the data collection phase of this research."}
{"_id":"5a46da9aec9238b5acf5f83b2bb9e453be37367b","title":"Producing sustainable competitive advantage through the effective management of people *","text":"Executive Overview Achieving competitive success through people involves fundamentally altering how we think about the workforce and the employment relationship. It means achieving success by working with people, not by replacing them or limiting the scope of their activities. It entails seeing the workforce as a source of strategic advantage, not just as a cost to be minimized or avoided. Firms that take this different perspective are often able to successfully outmaneuver and outperform their rivals. ........................................................................................................................................................................"}
{"_id":"4ddabe9893c8e2db7d4870b1aefbae4d20d22e43","title":"HOW MUCH DOES INDUSTRY MATTER , REALLY ?","text":"In this paper, we examine the importance of year, industry, corporate-parent, and businessspecific effects on the profitability of U.S. public corporations within specific 4-digit SIC categories. Our results indicate that year, industry, corporate-parent, and business-specific effects account for 2 percent, 19 percent, 4 percent, and 32 percent, respectively, of the aggregate variance in profitability. We also find that the importance of the effects differs substantially across broad economic sectors. Industry effects account for a smaller portion of profit variance in manufacturing but a larger portion in lodging\/entertainment, services, wholesale\/retail trade, and transportation. Across all sectors we find a negative covariance between corporate-parent and industry effects. A detailed analysis suggests that industry, corporate-parent, and business-specific effects are related in complex ways. \uf6d9 1997 by John Wiley & Sons, Ltd."}
{"_id":"2e6a52d23dba71c974adeabee506aee72df7b3bc","title":"Characterizing debate performance via aggregated twitter sentiment","text":"Television broadcasters are beginning to combine social micro-blogging systems such as Twitter with television to create social video experiences around events. We looked at one such event, the first U.S. presidential debate in 2008, in conjunction with aggregated ratings of message sentiment from Twitter. We begin to develop an analytical methodology and visual representations that could help a journalist or public affairs person better understand the temporal dynamics of sentiment in reaction to the debate video. We demonstrate visuals and metrics that can be used to detect sentiment pulse, anomalies in that pulse, and indications of controversial topics that can be used to inform the design of visual analytic systems for social media events."}
{"_id":"ec4a94637ecd115219869e9df8902cb7282481e0","title":"Semantic Sentiment Analysis of Twitter","text":"Sentiment analysis over Twitter offer organisations a fast and effective way to monitor the publics\u2019 feelings towards their brand, business, directors, etc. A wide range of features and methods for training sentiment classifiers for Twitter datasets have been researched in recent years with varying results. In this paper, we introduce a novel approach of adding semantics as additional features into the training set for sentiment analysis. For each extracted entity (e.g. iPhone) from tweets, we add its semantic concept (e.g. \u201cApple product\u201d) as an additional feature, and measure the correlation of the representative concept with negative\/positive sentiment. We apply this approach to predict sentiment for three different Twitter datasets. Our results show an average increase of F harmonic accuracy score for identifying both negative and positive sentiment of around 6.5% and 4.8% over the baselines of unigrams and part-of-speech features respectively. We also compare against an approach based on sentiment-bearing topic analysis, and find that semantic features produce better Recall and F score when classifying negative sentiment, and better Precision with lower Recall and F score in positive sentiment classification."}
{"_id":"07f33ce4159c1188ad20b864661731e246512239","title":"From bias to opinion: a transfer-learning approach to real-time sentiment analysis","text":"Real-time interaction, which enables live discussions, has become a key feature of most Web applications. In such an environment, the ability to automatically analyze user opinions and sentiments as discussions develop is a powerful resource known as real time sentiment analysis. However, this task comes with several challenges, including the need to deal with highly dynamic textual content that is characterized by changes in vocabulary and its subjective meaning and the lack of labeled data needed to support supervised classifiers. In this paper, we propose a transfer learning strategy to perform real time sentiment analysis. We identify a task - opinion holder bias prediction - which is strongly related to the sentiment analysis task; however, in constrast to sentiment analysis, it builds accurate models since the underlying relational data follows a stationary distribution.\n Instead of learning textual models to predict content polarity (i.e., the traditional sentiment analysis approach), we first measure the bias of social media users toward a topic, by solving a relational learning task over a network of users connected by endorsements (e.g., retweets in Twitter). We then analyze sentiments by transferring user biases to textual features. This approach works because while new terms may arise and old terms may change their meaning, user bias tends to be more consistent over time as a basic property of human behavior. Thus, we adopted user bias as the basis for building accurate classification models. We applied our model to posts collected from Twitter on two topics: the 2010 Brazilian Presidential Elections and the 2010 season of Brazilian Soccer League. Our results show that knowing the bias of only 10% of users generates an F1 accuracy level ranging from 80% to 90% in predicting user sentiment in tweets."}
{"_id":"1737177098539e295235678d664fcdb833568b94","title":"Cloud Task Scheduling Based on Load Balancing Ant Colony Optimization","text":"The cloud computing is the development of distributed computing, parallel computing and grid computing, or defined as the commercial implementation of these computer science concepts. One of the fundamental issues in this environment is related to task scheduling. Cloud task scheduling is an NP-hard optimization problem, and many meta-heuristic algorithms have been proposed to solve it. A good task scheduler should adapt its scheduling strategy to the changing environment and the types of tasks. This paper proposes a cloud task scheduling policy based on Load Balancing Ant Colony Optimization (LBACO) algorithm. The main contribution of our work is to balance the entire system load while trying to minimizing the make span of a given tasks set. The new scheduling strategy was simulated using the CloudSim toolkit package. Experiments results showed the proposed LBACO algorithm outperformed FCFS (First Come First Serve) and the basic ACO (Ant Colony Optimization)."}
{"_id":"591bc36fbfa094495c01951026f275292b87b92c","title":"A Particle Swarm Optimization-Based Heuristic for Scheduling Workflow Applications in Cloud Computing Environments","text":"Cloud computing environments facilitate applications by providing virtualized resources that can be provisioned dynamically. However, users are charged on a pay-per-use basis. User applications may incur large data retrieval and execution costs when they are scheduled taking into account only the \u2018execution time\u2019. In addition to optimizing execution time, the cost arising from data transfers between resources as well as execution costs must also be taken into account. In this paper, we present a particle swarm optimization (PSO) based heuristic to schedule applications to cloud resources that takes into account both computation cost and data transmission cost. We experiment with a workflow application by varying its computation and communication costs. We compare the cost savings when using PSO and existing \u2018Best Resource Selection\u2019 (BRS) algorithm. Our results show that PSO can achieve: a) as much as 3 times cost savings as compared to BRS, and b) good distribution of workload onto resources."}
{"_id":"8186763a49f36c32af6144f789ca3a5b03a3c01e","title":"QRSF: QoS-aware resource scheduling framework in cloud computing","text":"Cloud computing harmonizes and delivers the ability of resource sharing over different geographical sites. Cloud resource scheduling is a tedious task due to the problem of finding the best match of resource-workload pair. The efficient management of dynamic nature of resource can be done with the help of cloud workloads. Till cloud workload is deliberated as a central capability, the resources cannot be utilized in an effective way. In literature, very few efficient resource scheduling policies for energy, cost and time constraint cloud workloads are reported. This paper presents an efficient cloud workload management framework in which cloud workloads have been identified, analyzed and clustered through K-means on the basis of weights assigned and their QoS requirements. Further scheduling has been done based on different scheduling policies and their corresponding algorithms. The performance of the proposed algorithms has been evaluated with existing scheduling policies through CloudSim toolkit. The experimental results show that the proposed framework gives better results in terms of energy consumption, execution cost and time of different cloud workloads as compared to existing algorithms."}
{"_id":"bb59dd3db07e4e82b0fc734e37531417d6834f86","title":"Systematic literature reviews in software engineering - A systematic literature review","text":"Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. ! 2008 Elsevier B.V. All rights reserved."}
{"_id":"3969e582e68e418a2b79c604cd35d5d81de9b35d","title":"Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology","text":"Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both selfreported current usage (r=.63, Study 1) and self-predicted future usage (r =.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r= .45, Study 1) and future usage (r =.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance."}
{"_id":"37fd2fc9ae5baebe2f7ddb5456bc68f993d7bd66","title":"Error-Related EEG Potentials Generated During Simulated Brain\u2013Computer Interaction","text":"Brain-computer interfaces (BCIs) are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the electroencephalogram (EEG) recorded right after the occurrence of an error. Several studies show the presence of ErrP in typical choice reaction tasks. However, in the context of a BCI, the central question is: ldquoAre ErrP also elicited when the error is made by the interface during the recognition of the subject's intent?rdquo We have thus explored whether ErrP also follow a feedback indicating incorrect responses of the simulated BCI interface. Five healthy volunteer subjects participated in a new human-robot interaction experiment, which seem to confirm the previously reported presence of a new kind of ErrP. However, in order to exploit these ErrP, we need to detect them in each single trial using a short window following the feedback associated to the response of the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 83.5% and 79.2%, respectively, using a classifier built with data recorded up to three months earlier."}
{"_id":"714ed82f4f3d38677f823cafb1f037b10d32cc3d","title":"ERP components on reaction errors and their functional significance: a tutorial","text":"Some years ago we described a negative (Ne) and a later positive (Pe) deflection in the event-related brain potentials (ERPs) of incorrect choice reactions [Falkenstein, M., Hohnsbein, J., Hoormann, J., Blanke, L., 1990. In: Brunia, C.H.M., Gaillard, A.W.K., Kok, A. (Eds.), Psychophysiological Brain Research. Tilburg Univesity Press, Tilburg, pp. 192-195. Falkenstein, M., Hohnsbein, J., Hoormann, J., 1991. Electroencephalography and Clinical Neurophysiology, 78, 447-455]. Originally we assumed the Ne to represent a correlate of error detection in the sense of a mismatch signal when representations of the actual response and the required response are compared. This hypothesis was supported by the results of a variety of experiments from our own laboratory and that of Coles [Gehring, W. J., Goss, B., Coles, M.G.H., Meyer, D.E., Donchin, E., 1993. Psychological Science 4, 385-390. Bernstein, P.S., Scheffers, M.K., Coles, M.G.H., 1995. Journal of Experimental Psychology: Human Perception and Performance 21, 1312-1322. Scheffers, M.K., Coles, M. G.H., Bernstein, P., Gehring, W.J., Donchin, E., 1996. Psychophysiology 33, 42-54]. However, new data from our laboratory and that of Vidal et al. [Vidal, F., Hasbroucq, T., Bonnet, M., 1999. Biological Psychology, 2000] revealed a small negativity similar to the Ne also after correct responses. Since the above mentioned comparison process is also required after correct responses it is conceivable that the Ne reflects this comparison process itself rather than its outcome. As to the Pe, our results suggest that this is a further error-specific component, which is independent of the Ne, and hence associated with a later aspect of error processing or post-error processing. Our new results with different age groups argue against the hypotheses that the Pe reflects conscious error processing or the post-error adjustment of response strategies. Further research is necessary to specify the functional significance of the Pe."}
{"_id":"f2c4082faeff5d63b0144ef371c8964621ee33bf","title":"Clinical Applications of Brain-Computer Interfaces: Current State and Future Prospects","text":"Braincomputer interfaces (BCIs) allow their users to communicate or control external devices using brain signals rather than the brain's normal output pathways of peripheral nerves and muscles. Motivated by the hope of restoring independence to severely disabled individuals and by interest in further extending human control of external systems, researchers from many fields are engaged in this challenging new work. BCI research and development has grown explosively over the past two decades. Efforts have begun recently to provide laboratory-validated BCI systems to severely disabled individuals for real-world applications. In this paper, we discuss the current status and future prospects of BCI technology and its clinical applications. We will define BCI, review the BCI-relevant signals from the human brain, and describe the functional components of BCIs. We will also review current clinical applications of BCI technology and identify potential users and potential applications. Lastly, we will discuss current limitations of BCI technology, impediments to its widespread clinical use, and expectations for the future."}
{"_id":"13a8c7c30ca67cb88b4c5b78d3109d1c5a0f6025","title":"Anterior cingulate cortex, error detection, and the online monitoring of performance.","text":"An unresolved question in neuroscience and psychology is how the brain monitors performance to regulate behavior. It has been proposed that the anterior cingulate cortex (ACC), on the medial surface of the frontal lobe, contributes to performance monitoring by detecting errors. In this study, event-related functional magnetic resonance imaging was used to examine ACC function. Results confirm that this region shows activity during erroneous responses. However, activity was also observed in the same region during correct responses under conditions of increased response competition. This suggests that the ACC detects conditions under which errors are likely to occur rather than errors themselves."}
{"_id":"0651b333c2669227b0cc42de403268a4546ece70","title":"A Critical Review of Recurrent Neural Networks for Sequence Learning","text":"Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a selfcontained explication of the state of the art together with a historical perspective and references to primary research."}
{"_id":"56b228ad5d1efba154eec2e63f41011f563d6469","title":"A Passive RFID Information Grid for Location and Proximity Sensing for the Blind User","text":"We describe a navigation and location determination system for the blind using an RFID tag grid. Each RFID tag is programmed upon installation with spatial coordinates and information describing the surroundings. This allows for a self-describing, localized information system with no dependency on a centralized database or wireless infrastructure for communications. The system could be integrated into building code requirements as part of ADA (Americans with Disabilities Act) at a cost of less than $1 per square foot. With an established RFID grid infrastructure blind children and adults will gain the independence and freedom to explore and participate in activities without external assistance. An established RFID grid infrastructure will also enable advances in robotics which will benefit from knowing precise location. In this paper, we present an RFID based information grid system with a reader integrated into the user\u2019s shoe, which is connected to the user PDA or cell phone via a Bluetooth. An emphasis is placed on the architecture and design to allow for a truly integrated pervasive environment."}
{"_id":"b1397c9085361f308bd70793fc2427a4416973d7","title":"FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance","text":"This paper describes a probabilistic approach to the problem of recognizing places based on their appearance. The system we present is not limited to localization, but can determine that a new observation comes from a previously unseen place, and so augment its map. Effectively this is a SLAM system in the space of appearance. Our probabilistic approach allows us to explicitly account for perceptual aliasing in the environment\u2014identical but indistinctive observations receive a low probability of having come from the same place. We achieve this by learning a generative model of place appearance. By partitioning the learning problem into two parts, new place models can be learned online from only a single observation of a place. The algorithm complexity is linear in the number of places in the map, and is particularly suitable for online loop closure detection in mo-"}
{"_id":"e20aae4ce14009f689b55ebaf9dac541b88fb18d","title":"Multi-modal Semantic Place Classification","text":"The ability to represent knowledge about space and its position therein is crucial for a mobile robot. To this end, topological and semantic descriptions are gaining popularity for augmenting purely metric space representations. In this paper we present a multi-modal place classification system that allows a mobile robot to identify places and recognize semantic categories in an indoor environment. The system effectively utilizes information from different robotic sensors by fusing multiple visual cues and laser range data. This is achieved using a high-level cue integration scheme based on a Support Vector Machine (SVM) that learns how to optimally combine and weight each cue. Our multi-modal place classification approach can be used to obtain a real-time semantic space labeling system which integrates information over time and space. We perform an extensive experimental evaluation of the method for two different platforms and environments, on a realistic off-line database and in a live experiment on an autonomous robot. The results clearly demonstrate the effecThe International Journal of Robotics Research Vol. 00, No. 00, Xxxxxxxx 2009, pp. 000\u2013000 DOI: 10.1177\/0278364909356483 c The Author(s), 2009. Reprints and permissions: http:\/\/www.sagepub.co.uk\/journalsPermissions.nav Figures 1\u201315, 17, 18 appear in color online: http:\/\/ijr.sagepub.com tiveness of our cue integration scheme and its value for robust place classification under varying conditions. KEY WORDS\u2014recognition, sensor fusion, localization, multi-modal place classification, sensor and cue integration, semantic annotation of space"}
{"_id":"16ccb8d307d3f33ebb395b32db23279b409f1228","title":"RADAR: An In-Building RF-Based User Location and Tracking System","text":"The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF) based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It combines empirical measurements with signal propagation modeling to determine user location and thereby enable locationaware services and applications. We present experimental results that demonstrate the ability of RADAR to estimate user location with a high degree of accuracy."}
{"_id":"2c361ef5db3231d34656dd86d9b288397f0b929e","title":"No free lunch theorems for optimization","text":"A framework is developed to explore the connection between e ective optimization algorithms and the problems they are solving A number of no free lunch NFL theorems are presented that establish that for any algorithm any elevated performance over one class of problems is exactly paid for in performance over another class These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem Applications of the NFL theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented Other issues addressed are time varying optimization problems and a priori head to head minimax distinctions between optimization algorithms distinctions that can obtain despite the NFL theorems enforcing of a type of uniformity over all algorithms"}
{"_id":"d6647910bfdddeba029448221e33b508b25735a4","title":"Coordinated Scheduling of Residential Distributed Energy Resources to Optimize Smart Home Energy Services","text":"We describe algorithmic enhancements to a decision-support tool that residential consumers can utilize to optimize their acquisition of electrical energy services. The decision-support tool optimizes energy services provision by enabling end users to first assign values to desired energy services, and then scheduling their available distributed energy resources (DER) to maximize net benefits. We chose particle swarm optimization (PSO) to solve the corresponding optimization problem because of its straightforward implementation and demonstrated ability to generate near-optimal schedules within manageable computation times. We improve the basic formulation of cooperative PSO by introducing stochastic repulsion among the particles. The improved DER schedules are then used to investigate the potential consumer value added by coordinated DER scheduling. This is computed by comparing the end-user costs obtained with the enhanced algorithm simultaneously scheduling all DER, against the costs when each DER schedule is solved separately. This comparison enables the end users to determine whether their mix of energy service needs, available DER and electricity tariff arrangements might warrant solving the more complex coordinated scheduling problem, or instead, decomposing the problem into multiple simpler optimizations."}
{"_id":"2d8d5c7d02a7a54b99f1dc2499593a9289888831","title":"Detection and Localization of Curbs and Stairways Using Stereo Vision","text":"We present algorithms to detect and precisely localize curbs and stairways for autonomous navigation. These algorithms combine brightness information (in the form of edgels) with 3-D data from a commercial stereo system. The overall system (including stereo computation) runs at about 4 Hz on a 1 GHz laptop. We show experimental results and discuss advantages and shortcomings of our approach."}
{"_id":"63623c63ffddd08d266d884680d3493e8b7705f1","title":"Stereo Processing by Semiglobal Matching and Mutual Information","text":"This paper describes the semiglobal matching (SGM) stereo method. It uses a pixelwise, mutual information (Ml)-based matching cost for compensating radiometric differences of input images. Pixelwise matching is supported by a smoothness constraint that is usually expressed as a global cost function. SGM performs a fast approximation by pathwise optimizations from all directions. The discussion also addresses occlusion detection, subpixel refinement, and multibaseline matching. Additionally, postprocessing steps for removing outliers, recovering from specific problems of structured environments, and the interpolation of gaps are presented. Finally, strategies for processing almost arbitrarily large images and fusion of disparity images using orthographic projection are proposed. A comparison on standard stereo images shows that SGM is among the currently top-ranked algorithms and is best, if subpixel accuracy is considered. The complexity is linear to the number of pixels and disparity range, which results in a runtime of just 1-2 seconds on typical test images. An in depth evaluation of the Ml-based matching cost demonstrates a tolerance against a wide range of radiometric transformations. Finally, examples of reconstructions from huge aerial frame and pushbroom images demonstrate that the presented ideas are working well on practical problems."}
{"_id":"734e4341a2507c9b6039869aeb4c138bb86ade00","title":"Adaptive neighborhood selection for real-time surface normal estimation from organized point cloud data using integral images","text":"In this paper we present two real-time methods for estimating surface normals from organized point cloud data. The proposed algorithms use integral images to perform highly efficient border- and depth-dependent smoothing and covariance estimation. We show that this approach makes it possible to obtain robust surface normals from large point clouds at high frame rates and therefore, can be used in real-time computer vision algorithms that make use of Kinect-like data."}
{"_id":"154898f34460e95aef932bec5615bbd995824cad","title":"A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms","text":"Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms."}
{"_id":"0c253bb9aee9aa1ae7909700eda845bd3124197f","title":"Neural Program Meta-Induction","text":"Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input\/output (I\/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a k-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language [17]. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I\/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance."}
{"_id":"19bdbf4925551a8e10579dc1ea6004c0ff9e2081","title":"Neural Programmer-Interpreters","text":"We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms."}
{"_id":"61d226578cf4ca7d434c498891aaf1d4086a2986","title":"Making Neural Programming Architectures Generalize via Recursion","text":"Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion."}
{"_id":"b5a369ebcb2e9a8169b71791d77e7a3ad992870f","title":"Synthesizing Programs for Images using Reinforced Adversarial Learning","text":"Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator\u2019s output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, OMNIGLOT, CELEBA) and synthetic 3D datasets. A video of the agent can be found at https:\/\/youtu.be\/iSyvwAwa7vk."}
{"_id":"7628d73c73b8ae20c6cf866ce0865a1fb64612a3","title":"Image Texture Feature Extraction Using GLCM Approach","text":"Feature Extraction is a method of capturing visual content of images for indexing & retrieval. Primitive or low level image features can be either general features, such as extraction of color, texture and shape or domain specific features. This paper presents an application of gray level co-occurrence matrix (GLCM) to extract second order statistical texture features for motion estimation of images. The Four features namely, Angular Second Moment, Correlation, Inverse Difference Moment, and Entropy are computed using Xilinx FPGA. The results show that these texture features have high discrimination accuracy, requires less computation time and hence efficiently used for real time Pattern recognition applications."}
{"_id":"a1521e7108979598baecde9c5ef28ed0ca78cd7e","title":"Plant Leaf Recognition using Shape Based Features and Neural Network Classifiers","text":"This paper proposes an automated system for recognizing plant species based on leaf images. Plant leaf images corresponding to three plant types, are analyzed using two different shape modeling techniques, the first based on the Moments-Invariant (M-I) model and the second on the CentroidRadii (C-R) model. For the M-I model the first four normalized central moments have been considered and studied in various combinations viz. individually, in joint 2-D and 3-D feature spaces for producing optimum results. For the C-R model an edge detector has been used to identify the boundary of the leaf shape and 36 radii at 10 degree angular separation have been used to build the feature vector. To further improve the accuracy, a hybrid set of features involving both the M-I and C-R models has been generated and explored to find whether the combination feature vector can lead to better performance. Neural networks are used as classifiers for discrimination. The data set consists of 180 images divided into three classes with 60 images each. Accuracies ranging from 90%-100% are obtained which are comparable to the best figures reported in extant literature. Keywords-plant recognition; moment invariants; centroid-radii model; neural network; computer vision."}
{"_id":"a2e0aabcf0447c5960d5e624855cc8347bef723c","title":"Images Features Extraction of Tobacco Leaves","text":"Images features extraction is very important for the grading process of flue-cured tobacco leaves. In this paper, a machine vision techniques base system is proposed for the automatic inspection of flue-cured tobacco leaves. Machine vision techniques are used in this system to solve problems of features extraction and analysis of tobacco leaves, which include features of color, size, shape andsurface texture. The experimental results show that this system is a viable way for the features extraction of tobacco leaves, and can be used for the automatic classification of tobacco leaves."}
{"_id":"ddf1f27694928a729aefa6ffd6faabfd8ebf2842","title":"PLANT LEAF RECOGNITION","text":"This paper proposes an automated system for recognizing plant species based on leaf images. Plant leaf images corresponding to three plant types, are analyzed using three different shape modelling techniques, the first two based on the Moments-Invariant (M-I) model and the Centroid-Radii (C-R) model and the third based on a proposed technique of Binary-Superposition (B-S). For the M-I model the first four central normalized moments have been considered. For the C-R model an edge detector has been used to identify the boundary of the leaf shape and 36 radii at 10 degree angular separation have been used to build the shape vector. The proposed approach consists of comparing binary versions of the leaf images through superposition and using the sum of non-zero pixel values of the resultant as the feature vector. The data set for experimentations consists of 180 images divided into training and testing sets and comparison between them is done using Manhattan, Euclidean and intersection norms. Accuracies obtained using the proposed technique is seen to be an improvement over the M-I and C-R based techniques, and comparable to the best figures reported in extant literature."}
{"_id":"13754d470d8b5bf060ff42aa5a93b743f8be74a3","title":"COLOUR AND SHAPE ANALYSIS TECHNIQUES FOR WEED DETECTION IN CEREAL FIELDS","text":"Information on weed distribution within the field is necessary to implement spatially variable herbicide application. This paper deals with the development of near-ground image capture and processing techniques in order to detect broad leaf weeds in cereal crops, under actual field conditions. The proposed methods use both colour and shape analysis techniques for discriminating crop, weeds and soil. The performance of algorithms was assessed by comparing the results with a human classification, providing a good success rate. The study shows the potential of using image processing techniques to generate weed maps."}
{"_id":"daa63f57c3fbe994c4356f8d986a22e696e776d2","title":"Efficient Global Optimization of Expensive Black-Box Functions","text":"In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome."}
{"_id":"0e1ebbf27b303b8ccf62176e6e9370261963e2c0","title":"Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter","text":"There is a widespread intuitive sense that different kinds of information spread differently on-line, but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment. Here we study this issue on Twitter, analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users. We find significant variation in the ways that widely-used hashtags on different topics spread.\n Our results show that this variation is not attributable simply to differences in \"stickiness,\" the probability of adoption based on one or more exposures, but also to a quantity that could be viewed as a kind of \"persistence\" - the relative extent to which repeated exposures to a hashtag continue to have significant marginal effects. We find that hashtags on politically controversial topics are particularly persistent, with repeated exposures continuing to have unusually large marginal effects on adoption; this provides, to our knowledge, the first large-scale validation of the \"complex contagion\" principle from sociology, which posits that repeated exposures to an idea are particularly crucial when the idea is in some way controversial or contentious. Among other findings, we discover that hashtags representing the natural analogues of Twitter idioms and neologisms are particularly non-persistent, with the effect of multiple exposures decaying rapidly relative to the first exposure.\n We also study the subgraph structure of the initial adopters for different widely-adopted hashtags, again finding structural differences across topics. We develop simulation-based and generative models to analyze how the adoption dynamics interact with the network structure of the early adopters on which a hashtag spreads."}
{"_id":"3145fc2e5cbdf877ef07f7408dcaee5e44ba6d4f","title":"Meme-tracking and the dynamics of the news cycle","text":"Tracking new topics, ideas, and \"memes\" across the Web has been an issue of considerable interest. Recent work has developed methods for tracking topic shifts over long time scales, as well as abrupt spikes in the appearance of particular named entities. However, these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events.\n We develop a framework for tracking short, distinctive phrases that travel relatively intact through on-line text; developing scalable algorithms for clustering textual variants of such phrases, we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis. As our principal domain of study, we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis. We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle. In particular, we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively, with divergent behavior around the overall peak and a \"heartbeat\"-like pattern in the handoff between news and blogs. We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits."}
{"_id":"09031aa6d6743bebebc695955cd77c032cd9192f","title":"Group formation in large social networks: membership, growth, and evolution","text":"The processes by which communities come together, attract new members, and develop over time is a central research issue in the social sciences - political movements, professional organizations, and religious denominations all provide fundamental examples of such communities. In the digital domain, on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as MySpace and LiveJournal. However, the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved: what are the structural features that influence whether individuals will join communities, which communities will grow rapidly, and how do the overlaps among pairs of communities change over time.Here we address these questions using two large sources of data: friendship links and community membership on LiveJournal, and co-authorship and conference publications in DBLP. Both of these datasets provide explicit user-defined communities, where conferences serve as proxies for communities in DBLP. We study how the evolution of these communities relates to properties such as the structure of the underlying social networks. We find that the propensity of individuals to join communities, and of communities to grow rapidly, depends in subtle ways on the underlying network structure. For example, the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community, but also crucially by how those friends are connected to one another. We use decision-tree techniques to identify the most significant structural determinants of these properties. We also develop a novel methodology for measuring movement of individuals between communities, and show how such movements are closely aligned with changes in the topics of interest within the communities."}
{"_id":"9f675dbc30a6cb282e40f9929bd3defa96189de6","title":"IOT based crop-field monitoring and irrigation automation","text":"Internet Of Things (IoT)is a shared network of objects or things which can interact with each other provided the Internet connection. IoT plays an important role in agriculture industry which can feed 9.6 billion people on the Earth by 2050. Smart Agriculture helps to reduce wastage, effective usage of fertilizer and thereby increase the crop yield. In this work, a system is developed to monitor crop-field using sensors (soil moisture, temperature, humidity, Light) and automate the irrigation system. The data from sensors are sent to Web server database using wireless transmission. In server database the data are encoded in JSON format. The irrigation is automated if the moisture and temperature of the field falls below the brink. In greenhouses light intensity control can also be automated in addition to irrigation. The notifications are sent to farmers' mobile periodically. The farmers' can able to monitor the field conditions from anywhere. This system will be more useful in areas where water is in scarce. This system is 92% more efficient than the conventional approach."}
{"_id":"49a970d478146a43a9b0224ea5d881511c23c110","title":"Urban-Area and Building Detection Using SIFT Keypoints and Graph Theory","text":"Very high resolution satellite images provide valuable information to researchers. Among these, urban-area boundaries and building locations play crucial roles. For a human expert, manually extracting this valuable information is tedious. One possible solution to extract this information is using automated techniques. Unfortunately, the solution is not straightforward if standard image processing and pattern recognition techniques are used. Therefore, to detect the urban area and buildings in satellite images, we propose the use of scale invariant feature transform (SIFT) and graph theoretical tools. SIFT keypoints are powerful in detecting objects under various imaging conditions. However, SIFT is not sufficient for detecting urban areas and buildings alone. Therefore, we formalize the problem in terms of graph theory. In forming the graph, we represent each keypoint as a vertex of the graph. The unary and binary relationships between these vertices (such as spatial distance and intensity values) lead to the edges of the graph. Based on this formalism, we extract the urban area using a novel multiple subgraph matching method. Then, we extract separate buildings in the urban area using a novel graph cut method. We form a diverse and representative test set using panchromatic 1-m-resolution Ikonos imagery. By extensive testings, we report very promising results on automatically detecting urban areas and buildings."}
{"_id":"7d1e9bd95b07a0faec777e715f46ce95906d63d7","title":"Automated Detection of Arbitrarily Shaped Buildings in Complex Environments From Monocular VHR Optical Satellite Imagery","text":"This paper introduces a new approach for the automated detection of buildings from monocular very high resolution (VHR) optical satellite images. First, we investigate the shadow evidence to focus on building regions. To do that, we propose a new fuzzy landscape generation approach to model the directional spatial relationship between buildings and their shadows. Once all landscapes are collected, a pruning process is developed to eliminate the landscapes that may occur due to non-building objects. The final building regions are detected by GrabCut partitioning approach. In this paper, the input requirements of the GrabCut partitioning are automatically extracted from the previously determined shadow and landscape regions, so that the approach gained an efficient fully automated behavior for the detection of buildings. Extensive experiments performed on 20 test sites selected from a set of QuickBird and Geoeye-1 VHR images showed that the proposed approach accurately detects buildings with arbitrary shapes and sizes in complex environments. The tests also revealed that even under challenging environmental and illumination conditions, reasonable building detection performances could be achieved by the proposed approach."}
{"_id":"596cf71c1695d4eb914482ded1ebcf8f1333e0db","title":"Fast and extensible building modeling from airborne LiDAR data","text":"This paper presents an automatic algorithm which reconstructs building models from airborne LiDAR (light detection and ranging) data of urban areas. While our algorithm inherits the typical building reconstruction pipeline, several major distinct features are developed to enhance efficiency and robustness: 1) we design a novel vegetation detection algorithm based on differential geometry properties and unbalanced SVM; 2) after roof patch segmentation, a fast boundary extraction method is introduced to produce topology-correct water tight boundaries; 3) instead of making assumptions on the angles between roof boundary lines, we propose a data-driven algorithm which automatically learns the principal directions of roof boundaries and uses them in footprint production. Furthermore, we show the extendability of our algorithm by supporting non-flat object patterns with the help of only a few user interactions. We demonstrate the efficiency and accuracy of our algorithm by showing experiment results on urban area data of several different data sets."}
{"_id":"8f86f0276a40081841de28535cbf4ba87c1127f2","title":"Detection of small objects from high-resolution panchromatic satellite imagery based on supervised image segmentation","text":"A new concept for the detection of small objects from modular optoelectronic multispectral scanner (MOMS-02) high spatial resolution panchromatic satellite imagery is presented. We combine supervised shape classification with unsupervised image segmentation in an iterative procedure which allows a target-oriented search for specific object shapes."}
{"_id":"84ed097a35ab8e3c2c404a84ed8e22d40b5b9db4","title":"Development of robots for rehabilitation therapy: the Palo Alto VA\/Stanford experience.","text":"For over 25 years, personal assistant robots for severely disabled individuals have been in development. More recently, using robots to deliver rehabilitation therapy has been proposed. This paper summarizes the development and clinical testing of three mechatronic systems for post-stroke therapy conducted at the VA Palo Alto in collaboration with Stanford University. We describe the philosophy and experiences that guided their evolution. Unique to the Palo Alto approach is provision for bimanual, mirror-image, patient-controlled therapeutic exercise. Proof-of-concept was established with a 2-degree-of-freedom (DOF) elbow\/forearm manipulator. Tests of a second-generation therapy robot producing planar forearm movements in 19 hemiplegic and control subjects confirmed the validity and reliability of interaction forces during mechanically assisted upper-limb movements. Clinical trials comparing 3-D robot-assisted therapy to traditional therapy in 21 chronic stroke subjects showed significant improvement in the Fugl-Meyer (FM) measure of motor recovery in the robot group, which exceeded improvements in the control group."}
{"_id":"1c32125cc9fb052b881a6dec812b62ed998915d7","title":"Lessons from applying the systematic literature review process within the software engineering domain","text":"A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone. 2006 Elsevier Inc. All rights reserved."}
{"_id":"90089d3ec1c73857e7fde0d8a234d0e147c5a477","title":"SLR-Tool - A Tool for Performing Systematic Literature Reviews","text":"Systematic literature reviews (SLRs) have been gaining a significant amount of attention from Software Engineering researchers since 2004. SLRs are considered to be a new research methodology in Software Engineering, which allow evidence to be gathered with regard to the usefulness or effectiveness of the technology proposed in Software Engineering for the development and maintenance of software products. This is demonstrated by the growing number of publications related to SLRs that have appeared in recent years. While some tools exist that can support some or all of the activities of the SLR processes defined in (Kitchenham & Charters, 2007), these are not free. The objective of this paper is to present the SLR-Tool, which is a free tool and is available on the following website: http:\/\/alarcosj.esi.uclm.es\/SLRTool\/, to be used by researchers from any discipline, and not only Software Engineering. SLR-Tool not only supports the process of performing SLRs proposed in (Kitchenham & Charters, 2007), but also provides additional functionalities such as: refining searches within the documents by applying text mining techniques; defining a classification schema in order to facilitate data synthesis; exporting the results obtained to the format of tables and charts; and exporting the references from the primary studies to the formats used in bibliographic packages such as EndNote, BibTeX or Ris. This tool has, to date, been used by members of the Alarcos Research Group and PhD students, and their perception of it is that it is both highly necessary and useful. Our purpose now is to circulate the use of SLR-Tool throughout the entire research community in order to obtain feedback from other users."}
{"_id":"444b9f2fff2132251a43dc4a4f8bd213e7763634","title":"Evidence-based software engineering","text":"Objective: Our objective is to describe how softwareengineering might benefit from an evidence-basedapproach and to identify the potential difficultiesassociated with the approach.Method: We compared the organisation and technicalinfrastructure supporting evidence-based medicine (EBM)with the situation in software engineering. We consideredthe impact that factors peculiar to software engineering(i.e. the skill factor and the lifecycle factor) would haveon our ability to practice evidence-based softwareengineering (EBSE).Results: EBSE promises a number of benefits byencouraging integration of research results with a view tosupporting the needs of many different stakeholdergroups. However, we do not currently have theinfrastructure needed for widespread adoption of EBSE.The skill factor means software engineering experimentsare vulnerable to subject and experimenter bias. Thelifecycle factor means it is difficult to determine howtechnologies will behave once deployed.Conclusions: Software engineering would benefit fromadopting what it can of the evidence approach providedthat it deals with the specific problems that arise from thenature of software engineering."}
{"_id":"66392ca9b0fc8bb8c8d27312ddd90ca3b418516a","title":"Factors Influencing the Usage of Websites: The Case of a Generic Portal in the Netherlands","text":"In this paper, we empirically investigate an extension of the Technology Acceptance Model (TAM, Davis, 1989) to explain the individual acceptance and usage of websites. Conceptually, we examine perceived ease-of-use, usefulness, enjoyment, and their impact on attitude towards using, intention to use and actual use. The paper also introduces a new construct, \u201cperceived visual attractiveness\u201d of the website and suggest that it influences usefulness, enjoyment, and ease-of-use. For our empirical research we partnered with a Dutch generic portal site with over 300 000 subscribers at the time the research was conducted. The websurvey resulted in sample size of 825 respondents. The results confirmed all of the 12 hypotheses formulated. Three findings are worth mentioning in particular: (1) intention is most dominantly influenced by attitude (\u03b2 = 0.51), (2) ease-of-use, enjoyment, and usefulness contribute equally to attitude towards using (\u03b2 = 0.23, 0.23, and 0.17 respectively) and (3) visual attractiveness contributes remarkably well to both ease-of-use, enjoyment, and usefulness (\u03b2 = 0.41, 0.35, and 0.21). Although this is not the first research to apply TAM to an internet context, we claim three major contributions: (1) a single website as the unit of analysis, (2) the introduction of visual attractiveness, and (3) the use of \u201creal\u201d website visitors rather than student samples. Promising future research lies in the conceptual connection between actual website features and website use, a connection for which the TAM framework provides a meaningful bridge. Factors Influencing the Usage of Websites: The Case of a Generic Portal in the Netherlands"}
{"_id":"c8965cc5c62a245593dbc679aebdf3338bb945fc","title":"Visual odometry for ground vehicle applications","text":"We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual estimates can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results obtained with a stereo-head mounted on an autonomous ground vehicle. We give examples of camera trajectories estimated in real-time purely from images over previously unseen distances (600 meters) and periods of time ."}
{"_id":"46319a2732e38172d17a3a2f0bb218729a76e4ec","title":"Activity Recognition in the Home Using Simple and Ubiquitous Sensors","text":"In this work, a system for recognizing activities in the home setting using a set of small and simple state-change sensors is introduced. The sensors are designed to be \u201ctape on and forget\u201d devices that can be quickly and ubiquitously installed in home environments. The proposed sensing system presents an alternative to sensors that are sometimes perceived as invasive, such as cameras and microphones. Unlike prior work, the system has been deployed in multiple residential environments with non-researcher occupants. Preliminary results on a small dataset show that it is possible to recognize activities of interest to medical professionals such as toileting, bathing, and grooming with detection accuracies ranging from 25% to 89% depending on the evaluation criteria used ."}
{"_id":"18ad2478014dd61d38e8197ff7060e9997e7a989","title":"Evolving neural networks to play checkers without relying on expert knowledge","text":"An experiment was conducted where neural networks compete for survival in an evolving population based on their ability to play checkers. More specifically, multilayer feedforward neural networks were used to evaluate alternative board positions and games were played using a minimax search strategy. At each generation, the extant neural networks were paired in competitions and selection was used to eliminate those that performed poorly relative to other networks. Offspring neural networks were created from the survivors using random variation of all weights and bias terms. After a series of 250 generations, the best-evolved neural network was played against human opponents in a series of 90 games on an internet website. The neural network was able to defeat two expert-level players and played to a draw against a master. The final rating of the neural network placed it in the \"Class A\" category using a standard rating system. Of particular importance in the design of the experiment was the fact that no features beyond the piece differential were given to the neural networks as a priori knowledge. The process of evolution was able to extract all of the additional information required to play at this level of competency. It accomplished this based almost solely on the feedback offered in the final aggregated outcome of each game played (i.e., win, lose, or draw). This procedure stands in marked contrast to the typical artifice of explicitly injecting expert knowledge into a game-playing program."}
{"_id":"40fe245fd4ccdb9eb6b61528d7e88f564784caf3","title":"Verifying Anaconda's expert rating by competing against Chinook: experiments in co-evolving a neural checkers player","text":"Since the early days of arti5cial intelligence, there has been interest in having a computer teach itself how to play a game of skill, like checkers, at a level that is competitive with human experts. To be truly noteworthy, such e7orts should minimize the amount of human intervention in the learning process. Recently, co-evolution has been used to evolve a neural network (called Anaconda) that, when coupled with a minimax search, can evaluate checker-boards and play to the level of a human expert, as indicated by its rating of 2045 on an international web site for playing checkers. The neural network uses only the location, type, and number of pieces on the board as input. No other features that would require human expertise are included. Experiments were conducted to verify the neural network\u2019s expert rating by competing it in 10 games against a \u201cnovice-level\u201d version of Chinook, a world-champion checkers program. The neural network had 2 wins, 4 losses, and 4 draws in the 10-game match. Based on an estimated rating of Chinook at the novice level, the results corroborate Anaconda\u2019s expert rating. c \u00a9 2002 Elsevier Science B.V. All rights reserved."}
{"_id":"61b9de524afa0b134052c50b40a4b48ee60bb410","title":"Decentralized Decision Making in the Game of Tic-tac-toe","text":"Traditionally, the game of Tic-tac-toe is a pencil and paper game played by two people who take turn to place their pieces on a 3times3 grid with the objective of being the first player to fill a horizontal, vertical, or diagonal row with their pieces. What if instead of having one person playing against another, one person plays against a team of nine players, each of whom is responsible for one cell in the 3times3 grid? In this new way of playing the game, the team has to coordinate its players, who are acting independently based on their limited information. In this paper, we present a solution that can be extended to the case where two such teams play against each other, and also to other board games. Essentially, the solution uses a decentralized decision making, which at first seems to complicate the solution. However, surprisingly, we show that in this mode, an equivalent level of decision making ability comes from simple components that reduce system complexity"}
{"_id":"82d02f4782bd208b65fd4e3dea06abe95cc48b04","title":"A self-learning evolutionary chess program","text":"A central challenge of artificial intelligence is to create machines that can learn from their own experience and perform at the level of human experts. Using an evolutionary algorithm, a computer program has learned to play chess by playing games against itself. The program learned to evaluate chessboard configurations by using the positions of pieces, material and positional values, and neural networks to assess specific sections of the chessboard. During evolution, the program improved its play by almost 400 rating points. Testing under simulated tournament conditions against Pocket Fritz 2.0 indicated that the evolved program performs above the master level."}
{"_id":"b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e","title":"Machine learning - an artificial intelligence approach","text":"Research in the area of learning structural descriptions from examples is reviewed, giving primary attention to methods of learning characteristic descrip\u00ad tions of single concepts. In particular, we examine methods for finding the maximally-specific conjunctive generalizations (MSC-generalizations) that cover all of the training examples of a given concept. Various important aspects of structural learning in general are examined, and several criteria for evaluating structural learning methods are presented. Briefly, these criteria include (i) ade\u00ad quacy of the representation language, (ii) generalization rules employed, computational efficiency, and (iv) flexibility and extensibility. Selected learning methods developed by Buchanan, et al., Hayes-Roth, Vere, Winston, and the authors are analyzed according to these criteria. Finally, some goals are sug\u00ad gested for future research."}
{"_id":"6421e02baee72c531cb044760338b314c7161406","title":"Contextual Phrase-Level Polarity Analysis Using Lexical Affect Scoring and Syntactic N-Grams","text":"We present a classifier to predict contextual polarity of subjective phrases in a sentence. Our approach features lexical scoring derived from the Dictionary of Affect in Language (DAL) and extended through WordNet, allowing us to automatically score the vast majority of words in our input avoiding the need for manual labeling. We augment lexical scoring with n-gram analysis to capture the effect of context. We combine DAL scores with syntactic constituents and then extract ngrams of constituents from all sentences. We also use the polarity of all syntactic constituents within the sentence as features. Our results show significant improvement over a majority class baseline as well as a more difficult baseline consisting of lexical n-grams."}
{"_id":"708f339a2d7a5bb827149448fd3b37385ba9b873","title":"Exploring the factors associated with Web site success in the context of electronic commerce","text":"Web sites are being widely deployed commercially. As the widespread use and dependency on Web technology increases, so does the need to assess factors associated with Web site success. The objective is to explore these factors in the context of electronic commerce (EC). The research framework was derived from information systems and marketing literature. Webmasters from Fortune 1000 companies were used as the target group for a survey. Four factors that are critical to Web site success in EC were identi\u00aeed: (1) information and service quality, (2) system use, (3) playfulness, and (4) system design quality. An analysis of the data provides valuable managerial implications for Web site success in the context of electronic commerce. # 2000 Elsevier Science B.V. All rights reserved."}
{"_id":"df805da2bb2a7e830b615636ee7cd22368a63563","title":"Web Site Usability, Design, and Performance Metrics","text":"Websites provide the key interface for consumer use of the Internet. This research reports on a series of three studies that develop and validate Web site usability, design and performance metrics, including download delay, navigability, site content, interactivity, and responsiveness. The performance metric that was developed includes the subconstructs user satisfaction, the likelihood of return, and the frequency of use. Data was collected in 1997, 1999, and 2000 from corporate Web sites via three methods, namely, a jury, third-party ratings, and a software agent. Significant associations betweenWeb site design elements and Web site performance indicate that the constructs demonstrate good nomological validity. Together, the three studies provide a set of measures with acceptable validity and reliability. The findings also suggest lack of significant common methods biases across the jury-collected data, third-party data, and agent-collected data. Results suggest that Web site success is a first-order construct. Moreover, Web site success is significantly associated with Web site download delay (speed of access and display rate within the Web site), navigation (organization, arrangement, layout, and sequencing), content (amount and variety of product information), interactivity (customization and interactivity), and responsiveness (feedback options and FAQs). (e-Commerce, Web Metrics, or Measurement;Web Site Usability;Design and Performance Constructs; Construct Validity; Nomological Validity)"}
{"_id":"0b990a9c6000b80dc00b69b68f6091844b898215","title":"Marketing in hypermedia computer-mediated environment: Conceptual foundations","text":"This paper addresses the role of marketing in hypermedia computer-mediated environments (CMEs). Our approach considers hypermedia CMEs to be large-scale (i.e. national or global) networked environments, of which the World Wide Web on the Internet is the first and current global implementation. We introduce marketers to this revolutionary new medium, and propose two structural models of consumer behavior in a CME. Then we examine the set of consequent testable research propositions and marketing implications that flow from the models. Marketing in Hypermedia Computer-Mediated Environments: Conceptual Foundations 1) Introduction Firms communicate with their customers through various media. Traditionally, these media follow a passive one-to-many communication model whereby a firm reaches many current and potential customers, segmented or not, through marketing efforts that allow only limited forms of feedback on the part of the customer. For several years now, a revolution has been developing that is dramatically altering this traditional view of advertising and communication media. This revolution is the Internet, the massive global network of interconnected packet-switched computer networks, and as a new marketing medium, has the potential to radically change the way firms do business with their customers. The Internet operationalizes a model of distributed computing that facilitates interactive multimedia many-to-many communication. As such, the Internet supports discussion groups (e.g. USENET news and moderated and unmoderated mailing lists), multi-player games and communications systems (e.g. MUDs, irc, chat, MUSEs), file transfer, electronic mail, and global information access and retrieval systems (e.g. archie, gopher, and the World Wide Web). The business implications of this model \"[where] the engine of democratization sitting on so many desktops is already out of control, is already creating new players in a new game\" (Carroll 1994), will be played out in as yet unknown ways for years to come. This paper is concerned with the marketing implications of commercializing hypermedia computer-mediated environments (CMEs), of which the World Wide Web (Berners-Lee et. al. 1992, 1993) on the Internet is the first and current networked global implementation. While we provide a formal definition subsequently, at this point we informally define a hypermedia CME as a distributed computer network used to access and provide hypermedia content (i.e., multimedia content connected across the network with hypertext links). Though other CMEs are relevant to marketers, including private bulletin board systems (Bunch 1994); public conferencing systems such as the WELL (Figallo 1993; Rheingold 1992, 1993) and ECHO; and commercial online services such as America On-Line, Prodigy, and CompuServe, we restrict our current focus to marketing activities in hypermedia CMEs accessible via the \"Web\" on the Internet. The Internet is an important focus for marketers because consumers and firms are conducting business on the Internet in proportions that dwarf the commercial provider base of the other CMEs combined. There are over 21,700 commercial Internet addressess (Verity and Hof 1994), and an increasing percentage of these commercial addresses are providing Web services. As of December 28, 1994, 1465 firms were listed in Open Market\u2019s (1994) directory of \"Commercial Services on the Net,\" and there were 6370 entries in the \"Business\/Corporations\" directory of the Yahoo Guide to WWW (Filo and Yang 1994). The central thesis driving this research is that hypermedia CMEs, such as but not limited to the World Wide Web on the Internet, require the development and application of new marketing concepts and models. This is because hypermedia CMEs possess unique characteristics, including machine-interactivity, telepresence, hypermedia, and network navigation, which distinguish them from traditional media and some interactive multimedia, on which conventional concepts and models are based. Hoffman & Novak (1995), \"Marketing in Hypermedia CMEs: Conceptual Foundations\" page 1"}
{"_id":"8213dbed4db44e113af3ed17d6dad57471a0c048","title":"The Nature of Statistical Learning Theory","text":""}
{"_id":"033eb044ef6a865a53878397633876827b7a8f20","title":"Character-Aware Neural Language Models","text":"We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long shortterm memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-ofthe-art despite having 60% fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level\/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling."}
{"_id":"6e88d09b2adc7a3d9230d324387929ec54a9d886","title":"Data-intensive applications, challenges, techniques and technologies: A survey on Big Data","text":"It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore\u2019s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as dataintensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. 2014 Elsevier Inc. All rights reserved."}
{"_id":"f19983b3e9b7fe8106c0375ebbd9f73a53295a28","title":"Data quality management, data usage experience and acquisition intention of big data analytics","text":"Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics"}
{"_id":"478fbef8568a021c3d91c13128efa19ad719dd88","title":"The 8 requirements of real-time stream processing","text":"Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the \"sea change\" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get \"sensor-tagged\" and report its state or location in real time. This sensorization of the real world will lead to a \"green field\" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged---including off-the-shelf stream processing engines---specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being \"repurposed\" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned."}
{"_id":"6834913a76b686957c0b8c755d1ca6ef3bd76914","title":"Data privacy through optimal k-anonymization","text":"Data de-identification reconciles the demand for release of data for research purposes and the demand for privacy from individuals. This paper proposes and evaluates an optimization algorithm for the powerful de-identification procedure known as k-anonymization. A k-anonymized dataset has the property that each record is indistinguishable from at least k - 1 others. Even simple restrictions of optimized k-anonymity are NP-hard, leading to significant computational challenges. We present a new approach to exploring the space of possible anonymizations that tames the combinatorics of the problem, and develop data-management strategies to reduce reliance on expensive operations such as sorting. Through experiments on real census data, we show the resulting algorithm can find optimal k-anonymizations under two representative cost measures and a wide range of k. We also show that the algorithm can produce good anonymizations in circumstances where the input data or input parameters preclude finding an optimal solution in reasonable time. Finally, we use the algorithm to explore the effects of different coding approaches and problem variations on anonymization quality and performance. To our knowledge, this is the first result demonstrating optimal k-anonymization of a non-trivial dataset under a general model of the problem."}
{"_id":"76c5db9edf820433eae631383f08b4e89e90fffa","title":"Privacy-preserving trajectory data publishing by local suppression","text":"The pervasiveness of location-aware devices has spawned extensive research in trajectory data mining, resulting in many important real-life applications. Yet, the privacy issue in sharing trajectory data among different parties often creates an obstacle for effective data mining. In this paper, we study the challenges of anonymizing trajectory data: high dimensionality, sparseness, and sequentiality. Employing traditional privacy models and anonymization methods often leads to low data utility in the resulting data and ineffective data mining. In addressing these challenges, this is the first paper to introduce local suppression to achieve a tailored privacy model for trajectory data anonymization. The framework allows the adoption of various data utility metrics for different data mining tasks. As an illustration, we aim at preserving both instances of location-time doublets and frequent sequences in a trajectory database, both being the foundation of many trajectory data \u2217Corresponding author Email addresses: ru_che@encs.concordia.ca (Rui Chen), fung@ciise.concordia.ca (Benjamin C. M. Fung), no_moham@encs.concordia.ca (Noman Mohammed), bcdesai@cs.concordia.ca (Bipin C. Desai) Preprint submitted to Information Sciences April 10, 2011 mining tasks. Our experiments on both synthetic and real-life data sets suggest that the framework is effective and efficient to overcome the challenges in trajectory data anonymization. In particular, compared with the previous works in the literature, our proposed local suppression method can significantly improve the data utility in anonymous trajectory data."}
{"_id":"1b7690012a25bb33b429dbd72eca7459b9f50653","title":"PEGASUS: A Policy Search Method for Large MDPs and POMDPs","text":"We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \u201cequivalent\u201d POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng [7], but with \u201csample complexity\u201d bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state\/continuous action problem involving learning to ride a bicycle."}
{"_id":"b8737d6ec1b033f6185e1da0f40a14fa44808d3f","title":"Review of control algorithms for robotic ankle systems in lower-limb orthoses, prostheses, and exoskeletons.","text":"This review focuses on control strategies for robotic ankle systems in active and semiactive lower-limb orthoses, prostheses, and exoskeletons. Special attention is paid to algorithms for gait phase identification, adaptation to different walking conditions, and motion intention recognition. The relevant aspects of hardware configuration and hardware-level controllers are discussed as well. Control algorithms proposed for other actuated lower-limb joints (knee and\/or hip), with potential applicability to the development of ankle devices, are also included."}
{"_id":"d6c899b3cfc70d1d31f2d2cdf696ff7567ff0e02","title":"Exoskeleton robot for rehabilitation of elbow and forearm movements","text":"To perform essential daily activities the movement of shoulder, elbow, and wrist play a vital role and therefore proper functioning of upper-limb is very much essential. We therefore have been developing an exoskeleton robot (ExoRob) to rehabilitate and to ease upper limb motion. Toward to make a complete (i.e., 7DOF) upper-arm motion assisted robotic exoskeleton this paper focused on the development of a 2DOF exoskeleton robot to rehabilitate the elbow and forearm movements. The proposed 2DOF ExoRob is supposed to be worn on the lateral side of forearm and provide naturalistic range movements of elbow (flexion\/extension) and forearm (pronation\/supination) motions. This paper also focuses on the modeling and control of the proposed ExoRob. A kinematic model of the ExoRob has been developed based on modified Denavit-Hartenberg notations. Nonlinear sliding mode control technique is employed in dynamic simulation of the proposed ExoRob, where trajectory tracking that corresponds to typical rehab (passive) exercises has been carried out to evaluate the effectiveness of the developed model and controller. Simulated results show that the controller is able to maneuver the ExoRob efficiently to track the desired trajectories, which in this case consisted in passive arm movements. These movements are widely used in rehab therapy and could be performed efficiently with the developed ExoRob and the controller."}
{"_id":"6882dcb241f5aaefe85025bf754f8dd1c1502df1","title":"Robot-aided neurorehabilitation.","text":"Our goal is to apply robotics and automation technology to assist, enhance, quantify, and document neurorehabilitation. This paper reviews a clinical trial involving 20 stroke patients with a prototype robot-aided rehabilitation facility developed at the Massachusetts Institute of Technology, Cambridge, (MIT) and tested at Burke Rehabilitation Hospital, White Plains, NY. It also presents our approach to analyze kinematic data collected in the robot-aided assessment procedure. In particular, we present evidence 1) that robot-aided therapy does not have adverse effects, 2) that patients tolerate the procedure, and 3) that peripheral manipulation of the impaired limb may influence brain recovery. These results are based on standard clinical assessment procedures. We also present one approach using kinematic data in a robot-aided assessment procedure."}
{"_id":"04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac","title":"Boosting the margin: A new explanation for the effectiveness of voting methods","text":"One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik\u2019s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition."}
{"_id":"48327aaf21902c09a92b90b1122f5bf2de62f56e","title":"A Survey on Ambient-Assisted Living Tools for Older Adults","text":"In recent years, we have witnessed a rapid surge in assisted living technologies due to a rapidly aging society. The aging population, the increasing cost of formal health care, the caregiver burden, and the importance that the individuals place on living independently, all motivate development of innovative-assisted living technologies for safe and independent aging. In this survey, we will summarize the emergence of `ambient-assisted living\u201d (AAL) tools for older adults based on ambient intelligence paradigm. We will summarize the state-of-the-art AAL technologies, tools, and techniques, and we will look at current and future challenges."}
{"_id":"20faa2ef4bb4e84b1d68750cda28d0a45fb16075","title":"Clustering of time series data - a survey","text":"Time series clustering has been shown effective in providing useful information in various domains. There seems to be an increased interest in time series clustering as part of the effort in temporal data mining research. To provide an overview, this paper surveys and summarizes previous works that investigated the clustering of time series data in various application domains. The basics of time series clustering are presented, including general-purpose clustering algorithms commonly used in time series clustering studies, the criteria for evaluating the performance of the clustering results, and the measures to determine the similarity\/dissimilarity between two time series being compared, either in the forms of raw data, extracted features, or some model parameters. The past researchs are organized into three groups depending upon whether they work directly with the raw data either in the time or frequency domain, indirectly with features extracted from the raw data, or indirectly with models built from the raw data. The uniqueness and limitation of previous research are discussed and several possible topics for future research are identified. Moreover, the areas that time series clustering have been applied to are also summarized, including the sources of data used. It is hoped that this review will serve as the steppingstone for those interested in advancing this area of research. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved."}
{"_id":"0abb49fe138e8fb7332c26b148a48d0db39724fc","title":"Stochastic Pooling for Regularization of Deep Convolutional Neural Networks","text":"We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation."}
{"_id":"97862a468d375d6fbd83ed1baf2bd8d74ffefdee","title":"Wireless Sensor Network for Precise Agriculture Monitoring","text":"Precision Agriculture Monitor System (PAMS) is an intelligent system which can monitor the agricultural environments of crops and provides service to farmers. PAMS based on the wireless sensor network (WSN) technique attracts increasing attention in recent years. The purpose of such systems is to improve the outputs of crops by means of managing and monitoring the growth period. This paper presents the design of a WSN for PAMS, shares our real-world experience, and discusses the research and engineering challenges in implementation and deployments."}
{"_id":"114ef0ee39bfe835f7df778f36a8ad60571f5449","title":"A multiscale retinex for bridging the gap between color images and the human observation of scenes","text":"Direct observation and recorded color images of the same scenes are often strikingly different because human visual perception computes the conscious representation with vivid color and detail in shadows, and with resistance to spectral shifts in the scene illuminant. A computation for color images that approaches fidelity to scene observation must combine dynamic range compression, color consistency-a computational analog for human vision color constancy-and color and lightness tonal rendition. In this paper, we extend a previously designed single-scale center\/surround retinex to a multiscale version that achieves simultaneous dynamic range compression\/color consistency\/lightness rendition. This extension fails to produce good color rendition for a class of images that contain violations of the gray-world assumption implicit to the theoretical foundation of the retinex. Therefore, we define a method of color restoration that corrects for this deficiency at the cost of a modest dilution in color consistency. Extensive testing of the multiscale retinex with color restoration on several test scenes and over a hundred images did not reveal any pathological behaviour."}
{"_id":"336779e60b48443bfd5f45f24191616213cbaf81","title":"The business model concept: theoretical underpinnings and empirical illustrations","text":"Received: 13 December 2001 Revised: 27 March 2002 : 26 July 2002 Accepted:15 October 2002 Abstract The business model concept is becoming increasingly popular within IS, management and strategy literature. It is used within many fields of research, including both traditional strategy theory and in the emergent body of literature on e-business. However, the concept is often used independently from theory, meaning model components and their interrelations are relatively obscure. Nonetheless, we believe that the business model concept is useful in explaining the relation between IS and strategy. This paper offers an outline for a conceptual business model, and proposes that it should include customers and competitors, the offering, activities and organisation, resources and factor market interactions. The causal inter-relations and the longitudinal processes by which business models evolve should also be included. The model criticises yet draws on traditional strategy theory and on the literature that addresses business models directly. The business model is illustrated by an ERP implementation in a European multi-national company. European Journal of Information Systems (2003) 12, 49\u201359. doi:10.1057\/ palgrave.ejis.3000446"}
{"_id":"4623accb0524d3b000866709ec27f1692cc9b15a","title":"Design science in information systems research","text":""}
{"_id":"add1d5602e7aa2bae889ef554e2da7f589117d5b","title":"Balancing customer and network value in business models for mobile services","text":"Designing business models for mobile services is complex. A business model can be seen as a blueprint of four interrelated components: service offering, technical architecture, and organisational and financial arrangements. In this paper the connections among these components are explored by analysing the critical design issues in business models for mobile services, e.g., targeting and branding in the service domain, security and quality of service in the technology domain, network governance in the organisation domain, and revenue sharing in the finance domain. A causal framework is developed linking these critical design issues to expected customer value and expected network value, and hence, to business model viability."}
{"_id":"dd5df1147ae291917515a7956a6bc3d7b845f288","title":"Evaluation in Design-Oriented Research","text":"Design has been recognized for a long time both as art and as science. In the sixties of the previous century design-oriented research began to draw the attention of scientific researchers and methodologists, not only in technical engineering but also in the social sciences. However, a rather limited methodology for design-oriented research has been developed, especially as to the social sciences. In this article we introduce evaluation methodology and research methodology as a systematic input in the process of designing. A designing cycle is formulated with six stages, and for each of these stages operations, guidelines and criteria for evaluation are defined. All this may be used for a considerable improvement of the process and product of designing."}
{"_id":"26f9114b2fdc34696c483e0f29da3b3d89482741","title":"Business Models for Electronic Markets","text":"Introduction Electronic commerce can be defined loosely as \u201cdoing business electronically\u201d (European Commission 1997). Electronic commerce includes electronic trading of physical goods and of intangibles such as information. This encompasses all the trading steps such as online marketing, ordering, payment, and support for delivery. Electronic commerce includes the electronic provision of services, such as aftersales support or online legal advice. Finally it also includes electronic support for collaboration between companies, such as collaborative design."}
{"_id":"1af5293e7e270d35be5eca28cd904d1e8fc9219c","title":"CEDD: Color and Edge Directivity Descriptor: A Compact Descriptor for Image Indexing and Retrieval","text":"This paper deals with a new low level feature that is extracted from the images and can be used for indexing and retrieval. This feature is called \u201cColor and Edge Directivity Descriptor\u201d and incorporates color and texture information in a histogram. CEDD size is limited to 54 bytes per image, rendering this descriptor suitable for use in large image databases. One of the most important attribute of the CEDD is the low computational power needed for its extraction, in comparison with the needs of the most MPEG-7 descriptors. The objective measure called ANMRR is used to evaluate the performance of the proposed feature. An online demo that implements the proposed feature in an image retrieval system is available at: http:\/\/orpheus.ee.duth.gr\/image_retrieval."}
{"_id":"597edc3174dc9f26badd67c4e81d0e8a58f9dbb3","title":"Textural Features Corresponding to Visual Perception","text":"Textural features corresponding to human visual perception are very useful for optimum feature selection and texture analyzer design. We approximated in computational form six basic textural features, namely, coarseness, contrast, directionality, line-likeness, regularity, and roughness. In comparison with psychological measurements for human subjects, the computational measures gave good correspondences in rank correlation of 16 typical texture patterns. Similarity measurements using these features were attempted. The discrepancies between human vision and computerized techniques that we encountered in this study indicate fundamental problems in digital analysis of textures. Some of them could be overcome by analyzing their causes and using more sophisticated techniques."}
{"_id":"6646c3e932c69c6c2f317d615c55ccc22e21b4be","title":"FCTH: Fuzzy Color and Texture Histogram - A Low Level Feature for Accurate Image Retrieval","text":"This paper deals with the extraction of a new low level feature that combines, in one histogram, color and texture information. This feature is named FCTH - Fuzzy Color and Texture Histogram - and results from the combination of 3 fuzzy systems. FCTH size is limited to 72 bytes per image, rendering this descriptor suitable for use in large image databases. The proposed feature is appropriate for accurately retrieving images even in distortion cases such as deformations, noise and smoothing. It is tested on a large number of images selected from proprietary image databases or randomly retrieved from popular search engines. To evaluate the performance of the proposed feature, the averaged normalized modified retrieval rank was used. An online demo that implements the proposed feature in an image retrieval system is available at: http:\/\/orpheus.ee.duth.gr\/image_retrieval."}
{"_id":"1b8fb3367b2527b53eda74c7966db809172eed28","title":"M-tree: An Efficient Access Method for Similarity Search in Metric Spaces","text":"A new access method called M tree is pro posed to organize and search large data sets from a generic metric space i e where ob ject proximity is only de ned by a distance function satisfying the positivity symmetry and triangle inequality postulates We detail algorithms for insertion of objects and split management which keep the M tree always balanced several heuristic split alternatives are considered and experimentally evaluated Algorithms for similarity range and k nearest neighbors queries are also described Re sults from extensive experimentation with a prototype system are reported considering as the performance criteria the number of page I O s and the number of distance computa tions The results demonstrate that the M tree indeed extends the domain of applica bility beyond the traditional vector spaces performs reasonably well in high dimensional data spaces and scales well in case of growing les"}
{"_id":"562283bfb84f6b7a0b881a9dcf5b713e1c1f57bf","title":"Normalized cuts in 3-D for spinal MRI segmentation","text":"Segmentation of medical images has become an indispensable process to perform quantitative analysis of images of human organs and their functions. Normalized Cuts (NCut) is a spectral graph theoretic method that readily admits combinations of different features for image segmentation. The computational demand imposed by NCut has been successfully alleviated with the Nystro\/spl uml\/m approximation method for applications different than medical imaging. In this paper we discuss the application of NCut with the Nystro\/spl uml\/m approximation method to segment vertebral bodies from sagittal T1-weighted magnetic resonance images of the spine. The magnetic resonance images were preprocessed by the anisotropic diffusion algorithm, and three-dimensional local histograms of brightness was chosen as the segmentation feature. Results of the segmentation as well as limitations and challenges in this area are presented."}
{"_id":"0647fe48491f82a8314453ad79912c780badddba","title":"Sentence Simplification by Monolingual Machine Translation","text":"In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems."}
{"_id":"2dbd523c9cd754c9329e722a1c33e317c6c0d53b","title":"Motivations and Methods for Text Simplification","text":"Long and complicated sentences prove to be a stumbling block for current systems relying on NL input. These systems stand to gain from methods that syntactically simplify such sentences. To simplify a sentence, we need an idea of the structure of the sentence, to identify the components to be separated out. Obviously a parser could be used to obtain the complete structure of the sentence. However, full parsing is slow and prone to failure, especially on complex sentences. In this paper, we consider two alternatives to full parsing which could be used for simpli cation. The rst approach uses a Finite State Grammar (FSG) to produce noun and verb groups while the second uses a Supertagging model to produce dependency linkages. We discuss the impact of these two input representations on the simpli cation process. 1 Reasons for Text Simpli cation Long and complicated sentences prove to be a stumbling block for current systems which rely on natural language input. These systems stand to gain from methods that preprocess such sentences so as to make them simpler. Consider, for example, the following sentence: (1) The embattled Major government survived a crucial vote on coal pits closure as its last-minute concessions curbed the extent of Tory revolt over an issue that generated unusual heat in the House of Commons and brought the miners to London streets. Such sentences are not uncommon in newswire texts. Compare this with the multi-sentence version which has been manually simpli ed: (2) The embattled Major government survived a crucial vote on coal pits closure. Its last-minute concessions curbed the extent of On leave from the National Centre for Software Technology, Gulmohar Cross Road No. 9, Juhu, Bombay 400 049, India Tory revolt over the coal-mine issue. This issue generated unusual heat in the House of Commons. It also brought the miners to"}
{"_id":"911d79afaf3ca3e3b9634ec6ed16de450cce0c8c","title":"Improving Text Simplification Language Modeling Using Unsimplified Text Data","text":"In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams."}
{"_id":"0f34fcab599aabf0ab46d91c21703a9a86b5f048","title":"On computable numbers, with an application to the Entscheidungsproblem","text":"The \"computable\" numbers may be described briefly as the real numbers whose expressions as a decimal are calculable by finite means. Although the subject of this paper is ostensibly the computable numbers. it is almost equally easy to define and investigate computable functions of an integral variable or a real or computable variable, computable predicates, and so forth. The fundamental problems involved are, however, the same in each case, and I have chosen the computable numbers for explicit treatment as involving the least cumbrous technique. I hope shortly to give an account of the relations of the computable numbers, functions, and so forth to one another. This will include a development of the theory of functions of a real variable expressed in terms of computable numbers. According to my definition, a number is computable if its decimal can be written down by a machine."}
{"_id":"1dc697ae0d6a1e90dc8ff061e36441b6efdcff7e","title":"A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems","text":"We present an iterative linear-quadratic-Gaussian method for locally-optimal feedback control of nonlinear stochastic systems subject to control constraints. Previously, similar methods have been restricted to deterministic unconstrained problems with quadratic costs. The new method constructs an affine feedback control law, obtained by minimizing a novel quadratic approximation to the optimal cost-to-go function. Global convergence is guaranteed through a Levenberg-Marquardt method; convergence in the vicinity of a local minimum is quadratic. Performance is illustrated on a limited-torque inverted pendulum problem, as well as a complex biomechanical control problem involving a stochastic model of the human arm, with 10 state dimensions and 6 muscle actuators. A Matlab implementation of the new algorithm is availabe at www.cogsci.ucsd.edu\/\/spl sim\/todorov."}
{"_id":"26949ee96ec22e400d8b4b264ee23360f3df1f7b","title":"Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning","text":"Policy improvement methods seek to optimize the parameters of a policy with respect to a utility function. There are two main approaches to performing this optimization: reinforcement learning (RL) and black-box optimization (BBO). Whereas BBO algorithms are generic optimization methods that, due to there generality, may also be applied to optimizing policy parameters, RL algorithms are specifically tailored to leveraging the structure of policy improvement problems. In recent years, benchmark comparisons between RL and BBO have been made, and there has been several attempts to specify which approach works best for which types of problem classes. In this article, we make several contributions to this line of research: 1) We define four algorithmic properties that further clarify the relationship between RL and BBO: action-perturbation vs. parameter-perturbation, gradient estimation vs. rewardweighted averaging, use of only rewards vs. use of rewards and state information, actor-critic vs. direct policy search. 2) We show how the chronology of the derivation of ever more powerful algorithms displays a trend towards algorithms based on parameter-perturbation and reward-weighted averaging. A striking feature of this trend is that it has moved RL methods closer and closer to BBO. 3) We continue this trend by applying two modifications to the state-of-the-art \u201cPolicy Improvement with Path Integrals\u201d (PI), which yields an algorithm we denote PI. We show that PI is a BBO algorithm, and, more specifically, that it is a special case of the \u201cCovariance Matrix Adaptation \u2013 Evolutionary Strategy\u201d algorithm. Our empirical evaluation demonstrates that the simpler PI outperforms PI on simple evaluation tasks in terms of convergence speed and final cost. 4) Although our evaluation implies that, for these five tasks, BBO outperforms RL, we do not hold this to be a general statement, and provide an analysis of why these tasks are particularly well-suited for BBO. Thus, rather than making the case for BBO or RL, one of the main contributions of this article is rather to provide an algorithmic framework in which such cases may be made, as PI and PI use identical perturbation and parameter update methods, and differ only in being BBO and RL approaches respectively."}
{"_id":"2fd17ebb88c0748fe79f59da6d2fec51233c2dc0","title":"A dynamically stable single-wheeled mobile robot with inverse mouse-ball drive","text":"Multi-wheel statically-stable mobile robots tall enough to interact meaningfully with people must have low centers of gravity, wide bases of support, and low accelerations to avoid tipping over. These conditions present a number of performance limitations. Accordingly, we are developing an inverse of this type of mobile robot that is the height, width, and weight of a person, having a high center of gravity, that balances dynamically on a single spherical wheel. Unlike balancing 2-wheel platforms which must turn before driving in some direction, the single-wheel robot can move directly in any direction. We present the overall design, actuator mechanism based on an inverse mouse-ball drive, control system, and initial results including dynamic balancing, station keeping, and point-to-point motion"}
{"_id":"13566e7b702a156d2f0d6b2ccbcb58b95cdd41c5","title":"Evolutionary Function Approximation for Reinforcement Learning","text":"Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This thesis investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. I present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This thesis also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. I evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods."}
{"_id":"3b632a49509c0c46ab0e0c0780b2170524f7c0ac","title":"Demand Response as a Market Resource Under the Smart Grid Paradigm","text":"Demand response (DR), distributed generation (DG), and distributed energy storage (DES) are important ingredients of the emerging smart grid paradigm. For ease of reference we refer to these resources collectively as distributed energy resources (DER). Although much of the DER emerging under smart grid are targeted at the distribution level, DER, and more specifically DR resources, are considered important elements for reliable and economic operation of the transmission system and the wholesale markets. In fact, viewed from transmission and wholesale operations, sometimes the term \u00bfvirtual power plant\u00bf is used to refer to these resources. In the context of energy and ancillary service markets facilitated by the independent system operators (ISOs)\/regional transmission organizations (RTOs), the market products DER\/DR can offer may include energy, ancillary services, and\/or capacity, depending on the ISO\/RTO market design and applicable operational standards. In this paper we first explore the main industry drivers of smart grid and the different facets of DER under the smart grid paradigm. We then concentrate on DR and summarize the existing and evolving programs at different ISOs\/RTOs and the product markets they can participate in. We conclude by addressing some of the challenges and potential solutions for implementation of DR under smart grid and market paradigms."}
{"_id":"af954a7a097abaa828bc2b4080c2d8c1b6acb853","title":"A survey of communication\/networking in Smart Grids","text":"Smart Grid is designed to integrate advanced communication\/networking technologies into electrical power grids to make them \u2018\u2018smarter\u2019\u2019. Current situation is that most of the blackouts and voltage sags could be prevented if we have better and faster communication devices and technologies for the electrical grid. In order to make the current electrical power grid a Smart Grid, the design and implementation of a new communication infrastructure for the grid are two important fields of research. However, Smart Grid projects have only been proposed in recent years and only a few proposals for forwardlooking requirements and initial research work have been offered in this field. No any systematic reviews of communication\/networking in Smart Grids have been conducted yet. Therefore, we conduct a systematic review of communication\/networking technologies in Smart Grid in this paper, including communication\/networking architecture, different communication technologies thatwould be employed into this architecture, quality of service (QoS), optimizing utilization of assets, control and management, etc. \u00a9 2011 Elsevier B.V. All rights reserved."}
{"_id":"b746fc72c0bd3f0a94145e375cc267e1128ba32e","title":"An Optimal Power Scheduling Method for Demand Response in Home Energy Management System","text":"With the development of smart grid, residents have the opportunity to schedule their power usage in the home by themselves for the purpose of reducing electricity expense and alleviating the power peak-to-average ratio (PAR). In this paper, we first introduce a general architecture of energy management system (EMS) in a home area network (HAN) based on the smart grid and then propose an efficient scheduling method for home power usage. The home gateway (HG) receives the demand response (DR) information indicating the real-time electricity price that is transferred to an energy management controller (EMC). With the DR, the EMC achieves an optimal power scheduling scheme that can be delivered to each electric appliance by the HG. Accordingly, all appliances in the home operate automatically in the most cost-effective way. When only the real-time pricing (RTP) model is adopted, there is the possibility that most appliances would operate during the time with the lowest electricity price, and this may damage the entire electricity system due to the high PAR. In our research, we combine RTP with the inclining block rate (IBR) model. By adopting this combined pricing model, our proposed power scheduling method would effectively reduce both the electricity cost and PAR, thereby, strengthening the stability of the entire electricity system. Because these kinds of optimization problems are usually nonlinear, we use a genetic algorithm to solve this problem."}
{"_id":"d117ab7678e040d403f381505c22f72cb4c2b5ed","title":"Optimal Residential Load Control With Price Prediction in Real-Time Electricity Pricing Environments","text":"Real-time electricity pricing models can potentially lead to economic and environmental advantages compared to the current common flat rates. In particular, they can provide end users with the opportunity to reduce their electricity expenditures by responding to pricing that varies with different times of the day. However, recent studies have revealed that the lack of knowledge among users about how to respond to time-varying prices as well as the lack of effective building automation systems are two major barriers for fully utilizing the potential benefits of real-time pricing tariffs. We tackle these problems by proposing an optimal and automatic residential energy consumption scheduling framework which attempts to achieve a desired trade-off between minimizing the electricity payment and minimizing the waiting time for the operation of each appliance in household in presence of a real-time pricing tariff combined with inclining block rates. Our design requires minimum effort from the users and is based on simple linear programming computations. Moreover, we argue that any residential load control strategy in real-time electricity pricing environments requires price prediction capabilities. This is particularly true if the utility companies provide price information only one or two hours ahead of time. By applying a simple and efficient weighted average price prediction filter to the actual hourly-based price values used by the Illinois Power Company from January 2007 to December 2009, we obtain the optimal choices of the coefficients for each day of the week to be used by the price predictor filter. Simulation results show that the combination of the proposed energy consumption scheduling design and the price predictor filter leads to significant reduction not only in users' payments but also in the resulting peak-to-average ratio in load demand for various load scenarios. Therefore, the deployment of the proposed optimal energy consumption scheduling schemes is beneficial for both end users and utility companies."}
{"_id":"bbaac3ff0eb5b661faead2748313834c9cf771cf","title":"Outline for a Logical Theory of Adaptive Systems","text":"The purpose of this paper is to outline a theory of automata appropriate to the properties, requirements and questions of adaptation. The conditions that such a theory should satisfy come from not one but several fields: It should be possible to formulate, at least in an abstract version, some of the key hypotheses and problems from relevant parts of biology, particularly the areas concerned with molecular control and neurophysiology. The work in theoretical genetics initiated by R. A. Fisher [5] and Sewall Wright [24] should find a natural place in the theory. At the same time the rigorous methods of automata theory should be brought to bear (particularly those parts concerned with growing automata [1, 2, 3, 7, 8, 12, 15, 18, 23]). Finally the theory should include among its models abstract counterparts of artificial adaptive systems currently being studied, systems such as Newell-Shaw-Simon's \"General Problem Solver\" [13], Selfridge's \"Pandemonium\" [17], von Neumann's self-reproducing automata [22] and Turing's morphogenetic systems [19, 20]. The theory outlined here (which is intended as a theory and not the theory) is presented in four main parts. Section 2 discusses the study of adaptation via generation procedures and generated populations. Section 3 defines a continuum of generation procedures realizable in a reasonably direct fashion. Section 4 discusses the realization of generation procedures as populations of interacting programs in an iterative circuit computer. Section 5 discusses the process of adaptation in the context of the earlier sections. The paper concludes with a discussion of the nature of the theorems of this theory. Before entering upon the detailed discussion, one general feature of the theory should be noted. The interpretations or models of the theory divide into two broad categories: \"complete\" models and \"incomplete\" models. The \"complete\" models comprise the artificial systems--systems with properties and specifications completely delimited at the outset (cf. the rules of a game). One set of \"complete\" models for the theory consists of various programmed parallel computers. The \"incomplete\" models encompass natural systems. Any natural system involves an unlimited number of factors and, inevitably, the theory can handle only a selected few of these. Because there will always be variables which do not have explicit counterparts in the theory, the derived statements must be approximate relative to natural systems. For this reason it helps greatly that"}
{"_id":"0f3b8eba6e536215b6f6c727a009c8b44cda0a91","title":"A Convolutional Attention Network for Extreme Summarization of Source Code","text":"Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model\u2019s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network\u2019s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms."}
{"_id":"4fba12c75d28e46e3ea93102499fb0c27d360e17","title":"Low-Quality Product Review Detection in Opinion Summarization","text":"Product reviews posted at online shopping sites vary greatly in quality. This paper addresses the problem of detecting lowquality product reviews. Three types of biases in the existing evaluation standard of product reviews are discovered. To assess the quality of product reviews, a set of specifications for judging the quality of reviews is first defined. A classificationbased approach is proposed to detect the low-quality reviews. We apply the proposed approach to enhance opinion summarization in a two-stage framework. Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews."}
{"_id":"7752e0835506a6629c1b06e67f2afb1e5d2bb714","title":"Convergent and discriminant validation by the multitrait-multimethod matrix.","text":"Content Memory (Learning Ability) As Comprehension 82 Vocabulary Cs .30 ( ) .23 .31 ( ) .31 .31 .35 ( ) .29 .48 .35 .38 ( ) .30 .40 .47 .58 .48 ( ) As judged against these latter values, comprehension (.48) and vocabulary (.47), but not memory (.31), show some specific validity. This transmutability of the validation matrix argues for the comparisons within the heteromethod block as the most generally relevant validation data, and illustrates the potential interchangeability of trait and method components. Some of the correlations in Chi's (1937) prodigious study of halo effect in ratings are appropriate to a multitrait-multimethod matrix in which each rater might be regarded as representing a different method. While the published report does not make these available in detail because it employs averaged values, it is apparent from a comparison of his Tables IV and VIII that the ratings generally failed to meet the requirement that ratings of the same trait by different raters should correlate higher than ratings of different traits by the same rater. Validity is shown to the extent that of the correlations in the heteromethod block, those in the validity diagonal are higher than the average heteromethod-heterotrait values. A conspicuously unsuccessful multitrait-multimethod matrix is provided by Campbell (1953, 1956) for rating of the leadership behavior of officers by themselves and by their subordinates. Only one of 11 variables (Recognition Behavior) met the requirement of providing a validity diagonal value higher than any of the heterotrait-heteromethod values, that validity being .29. For none of the variables were the validities higher than heterotrait-monomethod values. A study of attitudes toward authority and nonauthority figures by Burwen and Campbell (1957) contains a complex multitrait-multimethod matrix, one symmetrical excerpt from which is shown in Table 6. Method variance was strong for most of the procedures in this study. Where validity was found, it was primarily at the level of validity diagonal values higher than heterotrait-heteromethod values. As illustrated in Table 6, attitude toward father showed this kind of validity, as did attitude toward peers to a lesser degree. Attitude toward boss showed no validity. There was no evidence of a generalized attitude toward authority which would include father and boss, although such values as the VALIDATION BY THE MULTITRAIT-MULTIMETHOD MATRIX"}
{"_id":"9a756fa7e7c8afa53ada2201bcea38a095425a8e","title":"The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation","text":""}
{"_id":"e0f22e9eee05566a4fdd9b5c0fae7e2f8b6802f4","title":"A Latent Semantic Indexing-based approach to multilingual document clustering","text":"The creation and deployment of knowledge repositories formanaging, sharing, and reusing tacit knowledgewithin an organization has emerged as a prevalent approach in current knowledge management practices. A knowledge repository typically contains vast amounts of formal knowledge elements, which generally are available as documents. To facilitate users' navigation of documents within a knowledge repository, knowledge maps, often created by document clustering techniques, represent an appealing and promising approach. Various document clustering techniques have been proposed in the literature, but most deal with monolingual documents (i.e., written in the same language). However, as a result of increased globalization and advances in Internet technology, an organization often maintains documents in different languages in its knowledge repositories, which necessitates multilingual document clustering (MLDC) to create organizational knowledge maps. Motivated by the significance of this demand, this study designs a Latent Semantic Indexing (LSI)-based MLDC technique capable of generating knowledge maps (i.e., document clusters) from multilingual documents. The empirical evaluation results show that the proposed LSI-based MLDC technique achieves satisfactory clustering effectiveness, measured by both cluster recall and cluster precision, and is capable of maintaining a good balance between monolingual and cross-lingual clustering effectiveness when clustering a multilingual document corpus. \u00a9 2007 Elsevier B.V. All rights reserved."}
{"_id":"7534ac4b42d39e2033e32cb4a38a9dc95070b50d","title":"Where the Action Is - The Foundations of Embodied Interaction","text":"Most textbooks in HCI and CSCW do not offer a coherent and over\u2013arching understanding of social and technological issues. They present a variety of techniques and technologies, and outline a little history, but offer little in terms of theory that addresses the complexity of collaborative systems\u2019 structure and use. The majority of practitioners and researchers do not see theory as one of the things that they do. Immersed in their craft, focusing on technological innovation or ethnomethodological detail, they do not engage in theoretical abstraction. Technologists contentedly explore new tools and devices, with little heed to older disciplines or deeper discussion about the limits and assumptions inherent in their craft. The area of sociology most influential in CSCW, ethnomethodology, deliberately keeps theorising and generalisation at a distance, seeing abstraction as brutish and creativity as foreign. In our field, theory is like the public library. If asked, most of us would say that we are glad that it is around\u2014but few of us actually go there. Most see it as a haven for the old, the unemployed and the eccentric. Paul Dourish is a card\u2013carrying member, however. He reads avidly and widely, but is also a skilful system designer and developer. The result is a book that is deep, accessible and useful, which is a rare thing nowadays. In the introduction, Dourish lays out the structure of the book. He describes and reflects on two current trends in system design: tangible and social computing. By looking at the usually hidden assumptions in system design, he makes his later theoretical discussion more relevant and accessible to a computer science audience. He does not develop new philosophy or social theory, but draws upon established 20th century philosophy of language and phenomenology. Much of this material is likely to be unfamiliar to \u2018the average programmer\u2019 and, although Dourish presents it well, it may still be challenging to the reader. However, its use is one of the book\u2019s main contributions. He uses it to ground a conceptual framework and a corresponding set of principles for system design practice. He aims to do justice to the sociality and heterogeneity of interactive media, and to avoid putting theory above practice\u2014or vice versa. His ideal is balance: \u201cthe ability to develop systemsthat resonate with, rather than restrict (or, worse, refute), the social organization of action\u201d. The term \u2018tangible computing\u2019 is used very broadly in this book. The chapter on this theme covers the designs of Hiroshi Ishii\u2019s group at MIT\u2019s Media Lab, but also ubiquitous computing work such as the badges, tabs and LiveBoards of Xerox PARC, and augmented reality systems such as Pierre Wellner\u2019s DigitalDesk. Although \u2018UbiComp\u2019 is more fashionable nowadays, Dourish chooses a term that helps him focus more on our perception and \u201cthe ways we experience the everyday world\u201d than on computation and technology. Much of this discussion centres on exemplary systems, and the way that an increasing number and variety of computational devices and sensors are distributed in our environment. This contrasts with older systems that used very few media and which were encapsulated in the beige box of the traditional PC. The following chapter is on social computing: \u201cthe application of sociological understanding to the design of interactive systems\u201d. This stands in contrast to the more traditional tendency for designers to treat people as isolated system users, with little account taken of organisational and social context. Dourish draws upon some influential studies of existing technology such as that of an air traffic control room by Hughes et al., and the ethnography of a print shop by Bowers, Button and Sharrock. Compared to the previous chapter, exemplary systems are scarce. There are none of the previous chapter\u2019s attractive images of exotic displays and devices, so beloved in undergraduate lectures and conference presentations, as this chapter addresses systems\u2019 internal structure rather than external interaction. Dourish has developed more systems in the \u2018social\u2019 category than the \u2018tangible\u2019, and here he offers as examples Mansfield et al.\u2019s Orbit and some of his own work: using a system design technique, computational reflection, to offer users an account of deep system structure. In describing this particular technique, Dourish touches on the essence of the entire book: What is radical is the relationship it proposes between technical design and social understandings. It argues that the most fruitful place to forge these relationships is at a foundational level, one that attempts to take sociological insights into the heart of the process and fabric of design. (p. 87)"}
{"_id":"00cf965e32f89e475e076aab5db97c8b3b36fa63","title":"A tutorial on support vector regression","text":"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention somemodifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective."}
{"_id":"385622a5862c989653a648ac8abc59ae3fe785f7","title":"An introduction to hidden Markov models","text":"The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition."}
{"_id":"cedeeccea19b851cdfa3cd8ce753226c2bf55dd8","title":"High Power Outphasing Modulation","text":""}
{"_id":"25fb5a6abcd88ee52bdb3165b844c941e90eb9bf","title":"Revisiting Distributed Synchronous SGD","text":"Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies."}
{"_id":"2d83f7bca2d6f8aa4d2fc8a95489a1e1dc8884f1","title":"Highway long short-term memory RNNS for distant speech recognition","text":"In this paper, we extend the deep long short-term memory (DL-STM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains 43.9\/47.7% WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with 15.7% and 5.3% relative improvement respectively."}
{"_id":"0e3e3c3d8ae5cb7c4636870d69967c197484d3bb","title":"Verb Semantics and Lexical Selection","text":"This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection."}
{"_id":"364c79d2d98819b27641c651cf6553142ef747bf","title":"Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition","text":"As visual recognition scales up to ever larger numbers of categories, maintaining high accuracy is increasingly difficult. In this work, we study the problem of optimizing accuracy-specificity trade-offs in large scale recognition, motivated by the observation that object categories form a semantic hierarchy consisting of many levels of abstraction. A classifier can select the appropriate level, trading off specificity for accuracy in case of uncertainty. By optimizing this trade-off, we obtain classifiers that try to be as specific as possible while guaranteeing an arbitrarily high accuracy. We formulate the problem as maximizing information gain while ensuring a fixed, arbitrarily small error rate with a semantic hierarchy. We propose the Dual Accuracy Reward Trade-off Search (DARTS) algorithm and prove that, under practical conditions, it converges to an optimal solution. Experiments demonstrate the effectiveness of our algorithm on datasets ranging from 65 to over 10,000 categories."}
{"_id":"3a0a839012575ba455f2b84c2d043a35133285f9","title":"Corpus-Guided Sentence Generation of Natural Images","text":"We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone."}
{"_id":"12a68626d06d860a85737b451c2dbbc77b6fa5a6","title":"Curlybot: Designing a New Class of Computational Toys","text":"We introduce an educational toy, called curlybot, as the basis for a new class of toys aimed at children in their early stages of development \u2014 ages four and up. curlybot is an autonomous two-wheeled vehicle with embedded electronics that can record how it has been moved on any flat surface and then play back that motion accurately and repeatedly. Children can use curlybot to develop intuitions for advanced mathematical and computational concepts, like differential geometry, through play away from a traditional computer.\nIn our preliminary studies, we found that children learn to use curlybot quickly. They readily establish an affective and body syntonic connection with curlybot, because of its ability to remember all of the intricacies of their original gesture; every pause, acceleration, and even the shaking in their hand is recorded. Programming by example in this context makes the educational ideas implicit in the design of curlybot accessible to young children."}
{"_id":"2329a46590b2036d508097143e65c1b77e571e8c","title":"Deep Speech: Scaling up end-to-end speech recognition","text":"We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \u201cphoneme.\u201d Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5\u201900, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems."}
{"_id":"5b5ddfc2a0bfc5a048461eb11c191831cd226014","title":"Finite Automata and Their Decision Problems","text":"Finite automata are considered in this paper a s instruments for classifying finite tapes. Each onetape automaton defines a set of tapes, a two-tape automaton defines a set of pairs of tapes, et cetera. The structure of the defined sets is studied. Various generalizations of the notion of an automaton are introduced and their relation to the classical automata is determined. Some decision problems concerning automata are shown to be solvable by effective algorithms; others turn out to be unsolvable by algorithms."}
{"_id":"f1613bc4a40a987f67eefe60c617a360c6d0d1e8","title":"Erasure coded storage systems for cloud storage \u2014 challenges and opportunities","text":"Erasure coded storage schemes offer a promising future for cloud storage. Highlights of erasure coded storage systems are that these offer the same level of fault tolerance as that of replication, at lower storage footprints. In the big data era, cloud storage systems based on data replication are of dubious usability due to 200% storage overhead in data replication systems. This has prompted storage service providers to use erasure coded storage as an alternative to replication. Refinements are required in various aspects of erasure coded storage systems to make it a real contender against data replication based storage systems. Streamlining huge bandwidth requirements during the recovery of failed nodes, inefficient update operations, effect of topology in recovery and consistency requirements of erasure coded storage systems, are some areas which need attention. This paper presents an in-depth study on the challenges faced, and research pursued in some of these areas. The survey shows that more research is required to improve erasure coded storage system from being bandwidth crunchers to efficient storage systems. Another challenge that has emerged from the study is the requirement of elaborate research for upgrading the erasure coded storage systems from being mere archival storage systems by providing better update methods. Provision of multiple level consistency in erasure coded storage is yet another research opportunity identified in this work. A brief introduction to open source libraries available for erasure coded storage is also presented in the paper."}
{"_id":"12005218a4b5841dd27d75ab88079e7421a56846","title":"Big data and cloud computing: current state and future opportunities","text":"Scalable database management systems (DBMS)---both for update intensive application workloads as well as decision support systems for descriptive and deep analytics---are a critical part of the cloud infrastructure and play an important role in ensuring the smooth transition of applications from the traditional enterprise infrastructures to next generation cloud infrastructures. Though scalable data management has been a vision for more than three decades and much research has focussed on large scale data management in traditional enterprise setting, cloud computing brings its own set of novel challenges that must be addressed to ensure the success of data management solutions in the cloud environment. This tutorial presents an organized picture of the challenges faced by application developers and DBMS designers in developing and deploying internet scale applications. Our background study encompasses both classes of systems: (i) for supporting update heavy applications, and (ii) for ad-hoc analytics and decision support. We then focus on providing an in-depth analysis of systems for supporting update intensive web-applications and provide a survey of the state-of-the-art in this domain. We crystallize the design choices made by some successful systems large scale database management systems, analyze the application demands and access patterns, and enumerate the desiderata for a cloud-bound DBMS."}
{"_id":"19a74d3c08f4fcdbcc367c80992178e93f1ee3bd","title":"Partial-parallel-repair (PPR): a distributed technique for repairing erasure coded storage","text":"With the explosion of data in applications all around us, erasure coded storage has emerged as an attractive alternative to replication because even with significantly lower storage overhead, they provide better reliability against data loss. Reed-Solomon code is the most widely used erasure code because it provides maximum reliability for a given storage overhead and is flexible in the choice of coding parameters that determine the achievable reliability. However, reconstruction time for unavailable data becomes prohibitively long mainly because of network bottlenecks. Some proposed solutions either use additional storage or limit the coding parameters that can be used. In this paper, we propose a novel distributed reconstruction technique, called Partial Parallel Repair (PPR), which divides the reconstruction operation to small partial operations and schedules them on multiple nodes already involved in the data reconstruction. Then a distributed protocol progressively combines these partial results to reconstruct the unavailable data blocks and this technique reduces the network pressure. Theoretically, our technique can complete the network transfer in \u2308(log2(k + 1))\u2309 time, compared to k time needed for a (k, m) Reed-Solomon code. Our experiments show that PPR reduces repair time and degraded read time significantly. Moreover, our technique is compatible with existing erasure codes and does not require any additional storage overhead. We demonstrate this by overlaying PPR on top of two prior schemes, Local Reconstruction Code and Rotated Reed-Solomon code, to gain additional savings in reconstruction time."}
{"_id":"1ea6b2f67a3a7f044209aae0d0fd1cb14a1e9e06","title":"Pixel Recurrent Neural Networks","text":"Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent."}
{"_id":"5ad0e283c4c2aa7b9985012979835d0131fe73d8","title":"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields","text":"We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency."}
{"_id":"9871aa511ca7e3c61c083c327063442bc2c411bf","title":"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks","text":""}
{"_id":"ba753286b9e2f32c5d5a7df08571262e257d2e53","title":"Conditional Generative Adversarial Nets","text":"Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels."}
{"_id":"599b7e1b4460c8ad77def2330ec76a2e0dfedb84","title":"Robust Subspace Clustering via Smoothed Rank Approximation","text":"Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this letter, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms."}
{"_id":"5facbc5bd594f4faa0524d871d49ba6a6e956e17","title":"Sparse subspace clustering","text":"We propose a method based on sparse representation (SR) to cluster data drawn from multiple low-dimensional linear or affine subspaces embedded in a high-dimensional space. Our method is based on the fact that each point in a union of subspaces has a SR with respect to a dictionary formed by all other data points. In general, finding such a SR is NP hard. Our key contribution is to show that, under mild assumptions, the SR can be obtained `exactly' by using l1 optimization. The segmentation of the data is obtained by applying spectral clustering to a similarity matrix built from this SR. Our method can handle noise, outliers as well as missing data. We apply our subspace clustering algorithm to the problem of segmenting multiple motions in video. Experiments on 167 video sequences show that our approach significantly outperforms state-of-the-art methods."}
{"_id":"726d2d44a739769e90bb7388ed120efe84ea8147","title":"Clustering and projected clustering with adaptive neighbors","text":"Many clustering methods partition the data groups based on the input data similarity matrix. Thus, the clustering results highly depend on the data similarity learning. Because the similarity measurement and data clustering are often conducted in two separated steps, the learned data similarity may not be the optimal one for data clustering and lead to the suboptimal results. In this paper, we propose a novel clustering model to learn the data similarity matrix and clustering structure simultaneously. Our new model learns the data similarity matrix by assigning the adaptive and optimal neighbors for each data point based on the local distances. Meanwhile, the new rank constraint is imposed to the Laplacian matrix of the data similarity matrix, such that the connected components in the resulted similarity matrix are exactly equal to the cluster number. We derive an efficient algorithm to optimize the proposed challenging problem, and show the theoretical analysis on the connections between our method and the K-means clustering, and spectral clustering. We also further extend the new clustering model for the projected clustering to handle the high-dimensional data. Extensive empirical results on both synthetic data and real-world benchmark data sets show that our new clustering methods consistently outperforms the related clustering approaches."}
{"_id":"c0b60d02b2d59123f6b336fe2e287bdb02a2a776","title":"Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study","text":"Visual attention is a process that enables biological and machine vision systems to select the most relevant regions from a scene. Relevance is determined by two components: 1) top-down factors driven by task and 2) bottom-up factors that highlight image regions that are different from their surroundings. The latter are often referred to as \u201cvisual saliency.\u201d Modeling bottom-up visual saliency has been the subject of numerous research efforts during the past 20 years, with many successful applications in computer vision and robotics. Available models have been tested with different datasets (e.g., synthetic psychological search arrays, natural images or videos) using different evaluation scores (e.g., search slopes, comparison to human eye tracking) and parameter settings. This has made direct comparison of models difficult. Here, we perform an exhaustive comparison of 35 state-of-the-art saliency models over 54 challenging synthetic patterns, three natural image datasets, and two video datasets, using three evaluation scores. We find that although model rankings vary, some models consistently perform better. Analysis of datasets reveals that existing datasets are highly center-biased, which influences some of the evaluation scores. Computational complexity analysis shows that some models are very fast, yet yield competitive eye movement prediction accuracy. Different models often have common easy\/difficult stimuli. Furthermore, several concerns in visual saliency modeling, eye movement datasets, and evaluation scores are discussed and insights for future work are provided. Our study allows one to assess the state-of-the-art, helps to organizing this rapidly growing field, and sets a unified comparison framework for gauging future efforts, similar to the PASCAL VOC challenge in the object recognition and detection domains."}
{"_id":"c18a0618ada97eba7e72dd9d4ccfa7b838718ad3","title":"Task and context determine where you look.","text":"The deployment of human gaze has been almost exclusively studied independent of any specific ongoing task and limited to two-dimensional picture viewing. This contrasts with its use in everyday life, which mostly consists of purposeful tasks where gaze is crucially involved. To better understand deployment of gaze under such circumstances, we devised a series of experiments, in which subjects navigated along a walkway in a virtual environment and executed combinations of approach and avoidance tasks. The position of the body and the gaze were monitored during the execution of the task combinations and dependence of gaze on the ongoing tasks as well as the visual features of the scene was analyzed. Gaze distributions were compared to a random gaze allocation strategy as well as a specific \"saliency model.\" Gaze distributions showed high similarity across subjects. Moreover, the precise fixation locations on the objects depended on the ongoing task to the point that the specific tasks could be predicted from the subject's fixation data. By contrast, gaze allocation according to a random or a saliency model did not predict the executed fixations or the observed dependence of fixation locations on the specific task."}
{"_id":"11190a466d1085c09a11e52cc63f112280ddce74","title":"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis","text":"A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail."}
{"_id":"494219f151071a752914c51024a307f0e22e9d2c","title":"An Integrated Model of Top-Down and Bottom-Up Attention for Optimizing Detection Speed","text":"Integration of goal-driven, top-down attention and image-driven, bottom-up attention is crucial for visual search. Yet, previous research has mostly focused on models that are purely top-down or bottom-up. Here, we propose a new model that combines both. The bottom-up component computes the visual salience of scene locations in different feature maps extracted at multiple spatial scales. The topdown component uses accumulated statistical knowledge of the visual features of the desired search target and background clutter, to optimally tune the bottom-up maps such that target detection speed is maximized. Testing on 750 artificial and natural scenes shows that the model\u2019s predictions are consistent with a large body of available literature on human psychophysics of visual search. These results suggest that our model may provide good approximation of how humans combine bottom-up and top-down cues such as to optimize target detection speed."}
{"_id":"2276067dcaa5fc9c240041a50a114a06a9636276","title":"Competing Memes Propagation on Networks: A Network Science Perspective","text":"In this paper, we study the intertwined propagation of two competing \"memes\" (or data, rumors, etc.) in a composite network. Within the constraints of this scenario, we ask two key questions: (a) which meme will prevail? and (b) can one influence the outcome of the propagations? Our model is underpinned by two key concepts, a structural graph model (composite network) and a viral propagation model (SI1I2S). Using this framework, we formulate a non-linear dynamic system and perform an eigenvalue analysis to identify the tipping point of the epidemic behavior. Based on insights gained from this analysis, we demonstrate an effective and accurate prediction method to determine viral dominance, which we call the EigenPredictor. Next, using a combination of synthetic and real composite networks, we evaluate the effectiveness of various viral suppression techniques by either a) concurrently suppressing both memes or b) unilaterally suppressing a single meme while leaving the other relatively unaffected."}
{"_id":"5b2bc4aaa63412ff1745a79d2f322b5ff67d0f9c","title":"Sentiment in short strength detection informal text","text":""}
{"_id":"0b65ecacd0025b4a0e31c4774a4925d3181631fa","title":"The dynamics of viral marketing","text":"We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We analyze how user behavior varies within user communities defined by a recommendation network. Product purchases follow a \u2018long tail\u2019 where a significant share of purchases belongs to rarely sold items. We establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies communities, product, and pricing categories for which viral marketing seems to be very effective."}
{"_id":"1a295aae9d78d54b329f5f83e1e3925988dadabd","title":"Cone Trees: animated 3D visualizations of hierarchical information","text":"The task of managing and accessing large information spaces is a problem in large scale cognition. Emerging technologies for 3D visualization and interactive animation offer potential solutions to this problem, especially when the structure of the information can be visualized. We describe one of these Information Visualization techniques, called the Cone Tree, which is used for visualizing hierarchical information structures. The hierarchy is presented in 3D to maximize effective use of available screen space and enable visualization of the whole structure. Interactive animation is used to shift some of the user's cognitive load to the human perceptual system."}
{"_id":"a4f38719e4e20c25ac1f6cd51f31b38d50264590","title":"Cloud-assisted Industrial Internet of Things (IIoT) - Enabled framework for health monitoring","text":"The promising potential of the emerging Internet of Things (IoT) technologies for interconnected medical devices and sensors has played an important role in the next-generation healthcare industry for quality patient care. Because of the increasing number of elderly and disabled people, there is an urgent need for a real-time health monitoring infrastructure for analyzing patients\u2019 healthcare data to avoid preventable deaths. Healthcare Industrial IoT (HealthIIoT) has significant potential for the realization of such monitoring. HealthIIoT is a combination of communication technologies, interconnected apps, Things (devices and sensors), and people that would function together as one smart system to monitor, track, and store patients\u2019 healthcare information for ongoing care. This paper presents a HealthIIoT-enabled monitoring framework, where ECG and other healthcare data are collected by mobile devices and sensors and securely sent to the cloud for seamless access by healthcare professionals. Signal enhancement, watermarking, and other related analytics will be used to avoid identity theft or clinical error by healthcare professionals. The suitability of this approach has been validated through both experimental evaluation, and simulation by deploying an IoT-driven ECG-based health monitoring service in the cloud. \u00a9 2016 Elsevier B.V. All rights reserved."}
{"_id":"0f4f5ba66a0b666c512c4f120c521cecc89e013f","title":"RFID Technology for IoT-Based Personal Healthcare in Smart Spaces","text":"The current evolution of the traditional medical model toward the participatory medicine can be boosted by the Internet of Things (IoT) paradigm involving sensors (environmental, wearable, and implanted) spread inside domestic environments with the purpose to monitor the user's health and activate remote assistance. RF identification (RFID) technology is now mature to provide part of the IoT physical layer for the personal healthcare in smart environments through low-cost, energy-autonomous, and disposable sensors. It is here presented a survey on the state-of-the-art of RFID for application to body centric systems and for gathering information (temperature, humidity, and other gases) about the user's living environment. Many available options are described up to the application level with some examples of RFID systems able to collect and process multichannel data about the human behavior in compliance with the power exposure and sanitary regulations. Open challenges and possible new research trends are finally discussed."}
{"_id":"c9bfed5fb8c6c7e57f65568f32a311fd9e6148fd","title":"Cloud-enabled wireless body area networks for pervasive healthcare","text":"With the support of mobile cloud computing, wireless body area networks can be significantly enhanced for massive deployment of pervasive healthcare applications. However, several technical issues and challenges are associated with the integration of WBANs and MCC. In this article, we study a cloud-enabled WBAN architecture and its applications in pervasive healthcare systems. We highlight the methodologies for transmitting vital sign data to the cloud by using energy-efficient routing, cloud resource allocation, semantic interactions, and data security mechanisms."}
{"_id":"ceae612aef2950a5b42009c25302079f891bf7e2","title":"An IoT-Aware Architecture for Smart Healthcare Systems","text":"Over the last few years, the convincing forward steps in the development of Internet of Things (IoT)-enabling solutions are spurring the advent of novel and fascinating applications. Among others, mainly radio frequency identification (RFID), wireless sensor network (WSN), and smart mobile technologies are leading this evolutionary trend. In the wake of this tendency, this paper proposes a novel, IoT-aware, smart architecture for automatic monitoring and tracking of patients, personnel, and biomedical devices within hospitals and nursing institutes. Staying true to the IoT vision, we propose a smart hospital system (SHS), which relies on different, yet complementary, technologies, specifically RFID, WSN, and smart mobile, interoperating with each other through a Constrained Application Protocol (CoAP)\/IPv6 over low-power wireless personal area network (6LoWPAN)\/representational state transfer (REST) network infrastructure. The SHS is able to collect, in real time, both environmental conditions and patients' physiological parameters via an ultra-low-power hybrid sensing network (HSN) composed of 6LoWPAN nodes integrating UHF RFID functionalities. Sensed data are delivered to a control center where an advanced monitoring application (MA) makes them easily accessible by both local and remote users via a REST web service. The simple proof of concept implemented to validate the proposed SHS has highlighted a number of key capabilities and aspects of novelty, which represent a significant step forward compared to the actual state of the art."}
{"_id":"21c2bd08b2111dcf957567b98e1c8dcad652e3dd","title":"Sample Size in Factor Analysis","text":"The factor analysis literature includes a range of recommendations regarding the minimum sample size necessary to obtain factor solutions that are adequately stable and that correspond closely to population factors. A fundamental misconception about this issue is that the minimum sample size, or the minimum ratio of sample size to the number of variables, is invariant across studies. In fact, necessary sample size is dependent on several aspects of any given study, including the level of communality of the variables and the level of overdetermination of the factors. The authors present a theoretical and mathematical framework that provides a basis for understanding and predicting these effects. The hypothesized effects are verified by a sampling study using artificial data. Results demonstrate the lack of validity of common rules of thumb and provide a basis for establishing guidelines for sample size in factor analysis."}
{"_id":"e99f72bc1d61bc7c8acd6af66880d9a815846653","title":"Automated Irrigation System-IoT Based Approach","text":"Agriculture is a major source of earning of Indians and agriculture has made a big impact on India's economy. The development of crops for a better yield and quality deliver is exceptionally required. So suitable conditions and suitable moisture in beds of crop can play a major role for production.. Mostly irrigation is done by tradition methods of stream flows from one end to other. Such supply may leave varied moisture levels in filed. The administration of the water system can be enhanced utilizing programmed watering framework This paper proposes a programmed water system with framework for the terrains which will reduce manual labour and optimizing water usage increasing productivity of crops. For formulating the setup, Arduino kit is used with moisture sensor with Wi-Fi module. Our experimental setup is connected with cloud framework and data is acquisition is done. Then data is analysed by cloud services and appropriate recommendations are given."}
{"_id":"43c9812beb3c36189c6a2c8299b5b5a7ae80c472","title":"Self-Autonomous Wireless Sensor Nodes With Wind Energy Harvesting for Remote Sensing of Wind-Driven Wildfire Spread","text":"The satellite-based remote sensing technique has been widely used in monitoring wildfire spread. There are two prominent drawbacks with this approach of using satellites located in space: (1) very low sampling rate (temporal resolution problem) and (2) lack of accuracy (spatial and spectral resolution problem). To address these challenges, a wireless sensor network deployed at ground level with high-fidelity and low-altitude atmospheric sensing for wind speed of local wildfire spread has been used. An indirect approach in sensing wind speed has been proposed in this paper as an alternative to the bulky conventional wind anemometer to save cost and space. The wind speed is sensed by measuring the equivalent electrical output voltage of the wind turbine generator (WTG). The percentage error in the wind speed measurement using the proposed indirect method is measured to be well within the \u00b14% limit with respect to wind anemometer accuracy. The same WTG also functions as a wind energy harvesting (WEH) system to convert the available wind energy into electrical energy to sustain the operation of the wireless sensor node. The experimental results show that the designed WEH system is able to harvest an average electrical power of 7.7 mW at an average wind speed of 3.62 m\/s for powering the operation of the wireless sensor node that consumes 3.5 mW for predicting the wildfire spread. Based on the sensed wind speed information, the fire control management system determines the spreading condition of the wildfire, and an adequate fire suppression action can be performed by the fire-fighting experts."}
{"_id":"4adef6d5951172dfce9d49e8672d960d11b6f8de","title":"Towards Text Knowledge Engineering","text":"We introduce a methodology for automating the maintenance of domain-specific taxonomies based on natural language text understanding. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual \u201cquality\u201d of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base."}
{"_id":"ccab8d39a252c43803d1e3273a48db162811ae5c","title":"A Single-Stage Single-Switch LED Driver Based on Class-E Converter","text":"This paper proposes a single-stage single-switch light-emitting diode (LED) driver that integrates a buck-boost circuit with a Class-E resonant converter by sharing single-switch device. The buck-boost circuit, working as a power-factor correction (PFC) stage, operates in the discontinuous conduction mode (DCM) to shape the input current. The Class-E converter steps down the voltage to drive the LED. For specific component parameters, the Class-E converter can achieve soft-switching on the power switch and two rectifier diodes, and reduce switch losses at high frequencies. Electrolytic capacitors are used in the proposed converter to achieve a lower cost, but the system reliability decreases. To overcome this disadvantage, film capacitors can be used, but the current ripple increases. Neglecting the cost, multilayered film capacitors are the best option, if higher reliability and lower current ripple are required. The proposed driver features high efficiency (90.8%), and a high power factor (PF) (0.995). In this paper, analytical results and design considerations at 100 kHz are presented, and a 100-W prototype with 110 VAC input was built to validate the theoretical analysis."}
{"_id":"135c89b491f82bd4fd7de175ac778207f598342b","title":"Not All Neural Embeddings are Born Equal","text":"Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and\/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status. It is well known that word representations can be learned from the distributional patterns in corpora. Originally, such representations were constructed by counting word co-occurrences, so that the features in one word\u2019s representation corresponded to other words [11, 17]. Neural language models, an alternative means to learn word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence [4, 14, 8], or simply a nearby word given a single cue word [13, 15]. The representations learned by neural models (sometimes called embeddings) generally outperform those acquired by co-occurrence counting models when applied to NLP tasks [3]. Despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings. Here, we explore this question by considering the embeddings learned by architectures with a very different objective function to monolingual language models: neural machine translation models. We show that translation-based embeddings outperform monolingual embeddings on two types of task: those that require knowledge of conceptual similarity (rather than simply association or relatedness), and those that require knowledge of syntactic role. We discuss what the findings indicate about the information content of different embeddings, and suggest how this content might emerge as a consequence of the translation objective. 1 Learning embeddings from language data Both neural language models and translation models learn real-valued embeddings (of specified dimension) for words in some pre-specified vocabulary, V , covering many or all words in their training corpus. At each training step, a \u2018score\u2019 for the current training example (or batch) is computed based on the embeddings in their current state. This score is compared to the model\u2019s objective function, and the error is backpropagated to update both the model weights (affecting how the score is computed from the embeddings) and the embedding features. At the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective. 1.1 Monolingual models In the original neural language model [4] and subsequent variants [8], each training example consists of n subsequent words, of which the model is trained to predict the n-th word given the first n \u2212 1 ar X iv :1 41 0. 07 18 v2 [ cs .C L ] 1 3 N ov 2 01 4 1 words. The model first represents the input as an ordered sequence of embeddings, which it transforms into a single fixed length \u2018hidden\u2019 representation by, e.g., concatenation and non-linear projection. Based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess at the next word. The model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus. More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer [13, 15]. Given a single word in the corpus, these models simply predict which other words will occur nearby. For each word w in V , a list of training cases (w, c) : c \u2208 V is extracted from the training corpus. For instance, in the skipgram approach [13], for each \u2018cue word\u2019 w the \u2018context words\u2019 c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).1 For each w in V , the model initialises both a cue-embedding, representing the w when it occurs as a cue-word, and a context-embedding, used when w occurs as a context-word. For a cue word w, the model can use the corresponding cueembedding and all context-embeddings to compute a probability distribution over V that reflects the probability of a word occurring in the context of w. When a training example (w, c) is observed, the model updates both the cue-word embedding of w and the context-word embeddings in order to increase the conditional probability of c. 1.2 Translation-based embeddings Neural translation models generate an appropriate sentence in their target language St given a sentence Ss in their source language [see, e.g., 16, 6]. In doing so, they learn distinct sets of embeddings for the vocabularies Vs and Vt in the source and target languages respectively. Observing a training case (Ss, St), such a model represents Ss as an ordered sequence of embeddings of words from Vs. The sequence for Ss is then encoded into a single representation RS .2 Finally, by referencing the embeddings in Vt, RS and a representation of what has been generated thus far, the model decodes a sentence in the target language word by word. If at any stage the decoded word does not match the corresponding word in the training target St, the error is recorded. The weights and embeddings in the model, which together parameterise the encoding and decoding process, are updated based on the accumulated error once the sentence decoding is complete. Although neural translation models can differ in low-level architecture [7, 2], the translation objective exerts similar pressure on the embeddings in all cases. The source language embeddings must be such that the model can combine them to form single representations for ordered sequences of multiple words (which in turn must enable the decoding process). The target language embeddings must facilitate the process of decoding these representations into correct target-language sentences. 2 Comparing Mono-lingual and Translation-based Embeddings To learn translation-based embeddings, we trained both the RNN encoder-decoder [RNNenc, 7] and the RNN Search architectures [2] on a 300m word corpus of English-French sentence pairs. We conducted all experiments with the resulting (English) source embeddings from these models. For comparison, we trained a monolingual skipgram model [13] and its Glove variant [15] for the same number of epochs on the English half of the bilingual corpus. We also extracted embeddings from a full-sentence language model [CW, 8] trained for several months on a larger 1bn word corpus. As in previous studies [1, 5, 3], we evaluate embeddings by calculating pairwise (cosine) distances and correlating these distances with (gold-standard) human judgements. Table 1 shows the correlations of different model embeddings with three such gold-standard resources, WordSim-353 [1], MEN [5] and SimLex-999 [10]. Interestingly, translation embeddings perform best on SimLex-999, while the two sets of monolingual embeddings perform better on modelling the MEN and WordSim353. To interpret these results, it should be noted that SimLex-999 evaluation quantifies conceptual similarity (dog wolf ), whereas MEN and WordSim-353 (despite its name) quantify more general relatedness (dog collar) [10]. The results seem to indicate that translation-based embeddings better capture similarity, while monolingual embeddings better capture relatedness. 1 Subsequent variants use different algorithms for selecting the (w, c) from the training corpus [9, 12] Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence [2]."}
{"_id":"1d2a4018b4fc6a5f498e65d68260615dbc9e7ec6","title":"Entity Linking meets Word Sense Disambiguation: a Unified Approach","text":"Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of-the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at http:\/\/babelfy.org"}
{"_id":"32cf9f4c97d74f8ee0d8a16e01cd4fd62d330650","title":"Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity","text":"Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening."}
{"_id":"4a87972b28143b61942a0eb011b60f76be0ebf2e","title":"Scalable Graph Exploration on Multicore Processors","text":"Many important problems in computational sciences, social network analysis, security, and business analytics, are data-intensive and lend themselves to graph-theoretical analyses. In this paper we investigate the challenges involved in exploring very large graphs by designing a breadth-first search (BFS) algorithm for advanced multi-core processors that are likely to become the building blocks of future exascale systems. Our new methodology for large-scale graph analytics combines a highlevel algorithmic design that captures the machine-independent aspects, to guarantee portability with performance to future processors, with an implementation that embeds processorspecific optimizations. We present an experimental study that uses state-of-the-art Intel Nehalem EP and EX processors and up to 64 threads in a single system. Our performance on several benchmark problems representative of the power-law graphs found in real-world problems reaches processing rates that are competitive with supercomputing results in the recent literature. In the experimental evaluation we prove that our graph exploration algorithm running on a 4-socket Nehalem EX is (1) 2.4 times faster than a Cray XMT with 128 processors when exploring a random graph with 64 million vertices and 512 millions edges, (2) capable of processing 550 million edges per second with an R-MAT graph with 200 million vertices and 1 billion edges, comparable to the performance of a similar graph on a Cray MTA-2 with 40 processors and (3) 5 times faster than 256 BlueGene\/L processors on a graph with average degree 50."}
{"_id":"138f8fc3e05509eb9d43d0446fcff21a73cf06ae","title":"Statistical Pattern Recognition","text":"This course will provide an introduction to statistical pattern recognition. The lectures will focus on different techniques including methods for feature extraction, dimensionality reduction, data clustering and pattern classification. State-of-art approaches such as ensemble learning and sparse modelling will be introduced. Selected real-world applications will illustrate how the techniques are applied in practice."}
{"_id":"0633015006fd8088c9089a94848ef3f21ac3881c","title":"QUASY: Quantitative Synthesis Tool","text":"We present the tool Q UASY, a quantitative synthesis tool. Q UASY takes qualitative and quantitative specifications and automatic ally onstructs a system that satisfies the qualitative specification and optimizes the qu antitative specification, if such a system exists. The user can choose between a system that satisfies and optimi zes the specifications (a) under all possible environment behaviors or (b) under th most-likely environment behaviors given as a probability distribution on the po ssible input sequences. QUASY solves these two quantitative synthesis problems by reduct ion to instances of 2-player games and Markov Decision Processes (MDPs) with qua ntitative winning objectives. QUASY can also be seen as a game solver for quantitative games. Most notable, it can solve lexicographic mean-payoff games with 2 players, MDPs with meanpayoff objectives, and ergodic MDPs with mean-payoff parit y objectives."}
{"_id":"1199c44a5834cce889e52ba128068b4d2224b067","title":"Gist: A Solver for Probabilistic Games","text":"Gist is a tool that (a) solves the qualitative analysis problem of turn-based probabilistic games with \u03c9-regular objectives; and (b) synthesizes reasonable environment assumptions for synthesis of unrealizable specifications. Our tool provides the first and efficient implementations of several reduction-based techniques to solve turn-based probabilistic games, and uses the analysis of turn-based probabilistic games for synthesizing environment assumptions for unrealizable specifications."}
{"_id":"38c6ff59aadeab427024b62f69c8f41d68e7cb37","title":"Playing Stochastic Games Precisely","text":"We study stochastic two-player games where the goal of one player is to achieve precisely a given expected value of the objective function, while the goal of the opponent is the opposite. Potential applications for such games include controller synthesis problems where the optimisation objective is to maximise or minimise a given payoff function while respecting a strict upper or lower bound, respectively. We consider a number of objective functions including reachability, \u03c9-regular, discounted reward, and total reward. We show that precise value games are not determined, and compare the memory requirements for winning strategies. For stopping games we establish necessary and sufficient conditions for the existence of a winning strategy of the controller for a large class of functions, as well as provide the constructions of compact strategies for the studied objectives."}
{"_id":"5b9631561a89a3e071d8ec386a616a120220bfd9","title":"PRISM 4.0: Verification of Probabilistic Real-Time Systems","text":"This paper describes a major new release of the PRISM probabilistic model checker, adding, in particular, quantitative verification of (priced) probabilistic timed automata. These model systems exhibiting probabilistic, nondeterministic and real-time characteristics. In many application domains, all three aspects are essential; this includes, for example, embedded controllers in automotive or avionic systems, wireless communication protocols such as Bluetooth or Zigbee, and randomised security protocols. PRISM, which is open-source, also contains several new components that are of independent use. These include: an extensible toolkit for building, verifying and refining abstractions of probabilistic models; an explicit-state probabilistic model checking library; a discrete-event simulation engine for statistical model checking; support for generation of optimal adversaries\/strategies; and a benchmark suite."}
{"_id":"8a6818f092710d5b03bd713bf74059346cff1678","title":"The Ins and Outs of the Probabilistic Model Checker MRMC","text":"The Markov Reward Model Checker (MRMC) is a software tool for verifying properties over probabilistic models. It supports PCTL and CSL model checking, and their reward extensions. Distinguishing features of MRMC are its support for computing timeand reward-bounded reachability probabilities, (property-driven) bisimulation minimization, and precise on-the-fly steady-state detection. Recent tool features include time-bounded reachability analysis for uniform CTMDPs and CSL model checking by discrete-event simulation. This paper presents the tool\u2019s current status and its implementation details."}
{"_id":"691d0a287a2515ebe5019cda498dcb6d24dfd5a4","title":"Least-Squares Fitting of Two 3-D Point Sets","text":"Two point sets {pi} and {p'i}; i = 1, 2,..., N are related by p'i = Rpi + T + Ni, where R is a rotation matrix, T a translation vector, and Ni a noise vector. Given {pi} and {p'i}, we present an algorithm for finding the least-squares solution of R and T, which is based on the singular value decomposition (SVD) of a 3 \u00d7 3 matrix. This new algorithm is compared to two earlier algorithms with respect to computer time requirements."}
{"_id":"280bb91e1f8f84eec3979c5d592396d3a4a2963c","title":"Analysis and Design of a Single-Stage Parallel AC-to-DC Converter","text":"In this paper, a single-stage (S2) parallel ac-to-dc converter based on single-switch two-output boost-flyback converter is presented. The converter contains two semistages. One is the boost-flyback semistage, which transfers partial input power transferred to load directly through one power flow path and has excellent self-power factor correction property when operating in discontinuous conduction mode even though the boost output is close to the peak value of the line voltage. The other one is the flyback dc-to-dc (dc\/dc) semistage that provides the output regulation on another parallel power flow path. With this design, the power conversion efficiency is improved and the current stress of control switch is reduced. Furthermore, the calculation process of power distribution and bulk capacitor voltage, design equations, and design procedure for key parameters are also presented. By following the procedure, an 80 W prototype converter has been built and tested. The experimental results show that the measured line harmonic current at the worst condition complies with the IEC61000-3-2 class D limits, the maximum bulk capacitor voltage is about 415.4 V, and the maximum efficiency is about 85.8%. Hence, the proposed S2 converter is suitable for universal input usage."}
{"_id":"3d7348c63309ddb68b4e69782bc6bf516bb1ced7","title":"A High Step-Down Transformerless Single-Stage Single-Switch AC\/DC Converter","text":"This paper presents a high step-down tranformerless single-stage single-switch ac\/dc converter suitable for universal line applications (90-270 Vrms) . The topology integrates a buck-type power-factor correction (PFC) cell with a buck-boost dc\/dc cell and part of the input power is coupled to the output directly after the first power processing. With this direct power transfer feature and sharing capacitor voltages, the converter is able to achieve efficient power conversion, high power factor, low voltage stress on intermediate bus (less than 130 V) and low output voltage without a high step-down transformer. The absence of transformer reduces the component counts and cost of the converter. Unlike most of the boost-type PFC cell, the main switch of the proposed converter only handles the peak inductor current of dc\/dc cell rather than the superposition of both inductor currents. Detailed analysis and design procedures of the proposed circuit are given and verified by experimental results."}
{"_id":"43d09c0ebcde0c8d06ab1d6ea50701cb15f437f8","title":"A Novel Single-Stage High-Power-Factor AC-to-DC LED Driving Circuit With Leakage Inductance Energy Recycling","text":"This paper proposes a novel single-stage ac-to-dc light-emitting-diode (LED) driver circuit. A buck-boost power factor (PF) corrector is integrated with a flyback converter. A recycling path is built to recover the inductive leakage energy. In this way, the presented circuit can provide not only high PF and low total harmonic distortion but also high conversion efficiency and low switching voltage spikes. Above 0.95 PF and 90% efficiency are obtained from an 8-W LED lamp driver prototype."}
{"_id":"beb38cf2b03e3c22f5fe4e75999bbea52ed3ceee","title":"Flicker-Free Electrolytic Capacitor-Less Universal Input Offline LED Driver With PFC","text":"Recent developments in improving lighting efficiency and cost reduction of LEDs have made them suitable alternatives to the current lighting systems. In this paper, a novel offline structure is proposed to drive LEDs. The proposed circuit has a high-input power factor, high efficiency, a long lifetime, and it produces no flicker. To increase the lifetime of the converter, the proposed circuit does not include any electrolytic capacitors in the power stage. The proposed circuit consists of a transition mode flyback converter in order to improve power factor. Additionally, a buck converter is added to the third winding of the flyback transformer in order to create two parallel paths for the electrical power to feed the output load. DC power reaches the load through one stage (flyback) and ac power reaches the load through two stages of conversion (flyback + buck). Therefore, in the proposed one-and-a-half stage circuit, the efficiency is improved compared to a regular two-stage circuit. Although the proposed structure has some output current ripple, it is low enough (less than 8%) that the structure can be rendered flicker free, as shall be discussed. Principles of operation and design equations are presented as well as experimental results for a 700 mA\/20 W universal input prototype."}
{"_id":"46e678c863fa00b178ed17b594da8cf1dab6d5a0","title":"Road Network Extraction and Intersection Detection From Aerial Images by Tracking Road Footprints","text":"In this paper, a new two-step approach (detecting and pruning) for automatic extraction of road networks from aerial images is presented. The road detection step is based on shape classification of a local homogeneous region around a pixel. The local homogeneous region is enclosed by a polygon, called the footprint of the pixel. This step involves detecting road footprints, tracking roads, and growing a road tree. We use a spoke wheel operator to obtain the road footprint. We propose an automatic road seeding method based on rectangular approximations to road footprints and a toe-finding algorithm to classify footprints for growing a road tree. The road tree pruning step makes use of a Bayes decision model based on the area-to-perimeter ratio (the A\/P ratio) of the footprint to prune the paths that leak into the surroundings. We introduce a lognormal distribution to characterize the conditional probability of A\/P ratios of the footprints in the road tree and present an automatic method to estimate the parameters that are related to the Bayes decision model. Results are presented for various aerial images. Evaluation of the extracted road networks using representative aerial images shows that the completeness of our road tracker ranges from 84% to 94%, correctness is above 81%, and quality is from 82% to 92%."}
{"_id":"d77ddbb81886d07941ec4679ffe83721e3f5ef2b","title":"UAV borne real-time road mapping system","text":"Road information is useful in many fields. In this paper, a real-time mapping system is presented to acquire the image and determine the geometry of the road. These include designs of platform and instruments, data transmission, processing and archiving. Compare with the traditional platforms, the UAV (Unmanned Aerial Vehicle) platforms offer greater flexibility, shorter response time and is able to generate very high resolution data. It is inexpensive to operate, and its operation is not limited by air traffic constraints. With autonomous flight control system, it is easy to navigate the aircraft along the planned lines strictly. In this system, an unmanned fixed-wing aircraft with double engine and double generator is used, which helps to improve the reliability and capability of the platform. A digital three-axis stabilized platform is developed, which performs an important role in camera attitude controlling. To ensure there is no coverage hole, a data manage system is adopted: after an instantaneous inspection of data integrity, the data will be archived as they are received. If any anomaly is detected, the mission planning will be adapted to re-image the concerned area. Vehicle information extraction from image sequence is also described in this paper. The experiment showed the presented system worked well in real-time road mapping and vehicle information extraction."}
{"_id":"e429e4874f4b3a397a754f28a919362d282b7966","title":"Road Extraction Using SVM and Image Segmentation","text":"In this paper, a unique approach for road extraction utilizing pixel spectral information for classification and image segmentation-derived object features was developed. In this approach, road extraction was performed in two steps. In the first step, support vector machine (SVM) was employed merely to classify the image into two groups of categories: a road group and a non-road group. For this classification, support vector machine (SVM) achieved higher accuracy than Gaussian maximum likelihood (GML). In the second step, the road group image was segmented into geometrically homogeneous objects using a region growing technique based on a similarity criterion, with higher weighting on shape factors over spectral criteria. A simple thresholding on the shape index and density features derived from these objects was performed to extract road features, which were further processed by thinning and vectorization to obtain road centerlines. The experiment showed the proposed approach worked well with images comprised by both rural and urban area features. Introduction Road information not only plays a central role in the transportation application, but also is an important data layer in Geographical Information Systems (GIS). Automated road extraction can save time and labor to a great degree in updating a road spatial database. Various road extraction approaches have been developed. Xiong (2001) grouped these methods into five categories: ridge finding, heuristic reasoning, dynamic programming, statistical inference, and map matching. In ridge finding, edge operators are performed on images to derive edge magnitude and direction, followed by a thresholding and thinning process to obtain ridge pixels (Nevatia and Babu, 1980; Treash and Amaratunga, 2000). Alternatively, gradient direction profile analysis can be performed to generate edge pixels (Gong and Wang, 1997). Ridge points are linked to produce the road segments. Heuristic reasoning is a knowledge-based method in which a series of pre-set rules on road characteristics such as shape index, the distance between image primitives, fragments trend, and contextual information are employed to detect and connect image primitives or antiparallel linear edges to road segments (McKeown, et al., 1985; Zhu and Yeh, 1986). In the dynamic programming method, roads are modeled with a set of mathematical equations on the derivatives of gray values and select characteristics of roads, such as smooth curves, homogeneous surface, narrow linear features, and relatively constant width. Dynamic programming is employed to solve the optimization problem Road Extraction Using SVM and Image Segmentation Mingjun Song and Daniel Civco (Gruen and Li, 1995). In the statistical inference method, linear features are modeled as a Markov point process or a geometric-stochastic model on the road width, direction, intensity and background intensity, and maximum a posteriori probability is used to estimate the road network (Barzohar and Cooper, 1996; Stoica, et al., 2000). In a map matching method, existing road maps are used as starting point to update the road network. In general, two steps are involved: first, a mapimage matching algorithm is employed to match the roads on the map to the image; second, new roads are searched based on the assumption that they are connected to existing roads (Stilla, 1995). Xiong\u2019s classification on road extraction methods is only a generalization, and some other methods may combine different techniques. Active contour models, known as snakes, are also used in road extraction (Gruen and Li, 1997; Agouris, et al., 2001). A snake is a spline with minimized energy driven by internal spline and external image forces (Park, et al., 2001). In general, external image forces are represented by the gradient magnitude of an image, which attracts snakes to contours with strong edges. Internal forces are given by a continuity term and a curvature term expressed by the differences of adjacent snaxels, which are vertex nodes of the snake, with weights coming from training data, which control the shape and smoothness of the snakes. Through the optimization, the snake evolves from its initial position to desired position with minimized energy. Park and Kim (2001) used template matching to extract road segments in which a road template was formed around the road seed, and an adaptive least squares matching algorithm was used to detect a target window with similar transformation. This method assumes a small difference in brightness values between template and target windows. Most of these road extraction methods require some road seeds as starting points, which are in general provided by users, and road segments evolve under a certain model. Sometimes control points are needed to correct the evolution of roads (Zhao, et al., 2002). Further, these methods use black-and-white aerial photographs or the panchromatic band of high-resolution satellite images and therefore the geometric characteristics of roads alone play a critical role. Boggess (1993) used a classification method incorporating texture and neural networks to extract roads by classifying roads and other features from Landsat TM imagery, but obtained numerous false-inclusions. Roberts, et al. (2001) developed a spectral mixture library using hyperspectral images to extract roads, but the use of spectral information alone does not capture the spatial properties of these curvilinear features. P H OTO G R A M M E T R I C E N G I N E E R I N G & R E M OT E S E N S I N G December 2004 1 3 6 5 Center for Land use Education and Research, Department of Natural Resources Management and Engineering, The University of Connecticut U-4087, 1376 Storrs Road, Storrs, CT 06269-4087 (mingjun.song@uconn.edu, daniel.civco@ uconn.edu). Photogrammetric Engineering & Remote Sensing Vol. 70, No. 12, December 2004, pp. 1365\u20131371. 0099-1112\/04\/7012\u20131365\/$3.00\/0 \u00a9 2004 American Society for Photogrammetry and Remote Sensing LFX-536.qxd 11\/9\/04 16:13 Page 1365"}
{"_id":"3813fade6b111f08636ad220ef32bd95b57d3e03","title":"PERFORMANCE LIMITS OF SWITCHED-CAPACITOR DC-DC CONVERTERS","text":"Theoretical performance limits of twophase switched-capacitor (SC) dc-dc converters are discussed in this paper. For a given number of capacitors k, the complete set of attainable dc conversion ratios is found. The maximum step-up or stepdown ratio is given by the k t h Fibonacci number, while the bound on the number of switches required in any SC circuit is 3k 2. Practical implications, illustrated by several SC converter examples, include savings in the number of components required for a given application, and the ability to construct SC converters that can maintain the output voltage regulation and high conversion efficiency over a wide range of input voltage variations. Limits found for the output resistance and efflciency can be used for selection and comparison of SC converters."}
{"_id":"533ea26a8af1f364b92d856f0aae2fc4e1539952","title":"Analysis and Optimization of Switched-Capacitor DC\u2013DC Converters","text":"Analysis methods are developed that fully determine a switched-capacitor (SC) dc-dc converter's steady-state performance through evaluation of its output impedance. This analysis method has been verified through simulation and experimentation. The simple formulation developed permits optimization of the capacitor sizes to meet a constraint such as a total capacitance or total energy storage limit, and also permits optimization of the switch sizes subject to constraints on total switch conductances or total switch volt-ampere (V-A) products. These optimizations then permit comparison among several switched-capacitor topologies, and comparisons of SC converters with conventional magnetic-based dc-dc converter circuits, in the context of various application settings. Significantly, the performance (based on conduction loss) of a ladder-type converter is found to be superior to that of a conventional magnetic-based converter for medium to high conversion ratios."}
{"_id":"0206bc94b05200094f32a7cf440551e4daebc618","title":"MOS charge pumps for low-voltage operation","text":"New MOS charge pumps utilizing the charge transfer switches (CTS\u2019s) to direct charge flow and generate boosted output voltage are described. Using the internal boosted voltage to backward control the CTS of a previous stage yields charge pumps that are suitable for low-voltage operation. Applying dynamic control to the CTS\u2019s can eliminate the reverse charge sharing phenomenon and further improve the voltage pumping gain. The limitation imposed by the diode-configured output stage can be mitigated by pumping it with a clock of enhanced voltage amplitude. Using the new circuit techniques, a 1.2-V-to-3.5-V charge pump and a 2-V-to-16-V charge pump are demonstrated."}
{"_id":"74e7b8023c09e12205745d9a10b95076a9b0f82a","title":"Analytical and Practical Analysis of Switched-Capacitor DC-DC Converters","text":"Switched-capacitor DC-DC converters are useful alternatives to inductor-based converters in many lowpower and medium-power applications. This work develops a straightforward analysis method to determine a switched-capacitor converter\u2019s output impedance (a measure of performance and power loss). This resistive impedance is a function of frequency and has two asymptotic limits, one corresponding to very high switching frequency where resistive paths dominate the impedance, and one corresponding to very low switching frequency where charge transfers among idealized capacitors dominate the impedance. An optimization method is developed to improve the performance of these converters through component sizing based on practical constraints. Several switched-capacitor converter topologies are compared in the two asymptotic limits. Switched-capacitor converter performance (based on conduction loss) is compared with that of two magnetics-based DC-DC converters. At moderate to high conversion ratios, the switchedcapacitor converter has significantly less conduction loss than an inductor-based buck converter. Some aspects of converter implementation are discussed, including the power loss due to device parasitics and methods for transistor control. Implementation using both integrated and discrete devices is discussed. With the correct analysis methods, switched-capacitor DC-DC converters can provide an attractive alternative to conventional power converters."}
{"_id":"7cde4cf792f2be12deb8d5410170a003375397d5","title":"SWITCHED-CAPACITOR DC-DC CONVERTERS FOR LOW-POWER ON-CHIP APPLICATIONS","text":"The paper describes switched-capacitor dc-dc converters (charge pumps) suitable for on-chip, low-power applications. The proposed configurations are based on connecting two identical but opposite-phase SC converters in parallel, thus eliminating the need for separate bootstrap gate drivers. We focus on emerging very low-power VLSI applications such as batterypowered or self-powered signal processors where high power conversion efficiency is important and where power levels are in the milliwatt range. Conduction and switching losses are considered to allow design optimization in terms of switching frequency and component sizes. Open-loop and closed-loop operation of an experimental, fully integrated, 10MHz voltage doubler is described. The doubler has 2V or 3V input and generates 3.3V or 5V output at up to 5mW load. The converter circuit fabricated in a standard 1.2\u03bc CMOS technology takes 0.7mm of the chip area."}
{"_id":"60afb1828de4efa1588401f87caa55ac3a0cd820","title":"Early-onset hidradenitis suppurativa.","text":"A 9-year-old girl developed hidradenitis suppurativa 3 months after the first signs of adrenarche. Such a close temporal relationship is consistent with the hypothesis that the disease is androgen dependent. Less than 2% of patients have onset of the disease before the age of 11 years. The exceptionally early age of onset in our patient may be partly explained by the fact that she had an early puberty."}
{"_id":"7f60b70dede16fe4d7b412674929b4805c9b5c95","title":"Prepubertal hidradenitis suppurativa: two case reports and review of the literature.","text":"Hidradenitis suppurativa (HS) is a chronic suppurative scarring disease of apocrine sweat gland-bearing skin in the axillary, anogenital, and, rarely, the breast and scalp regions. Females are more commonly affected than males and it is usually seen at puberty or later. We report two girls with prepubertal hidradenitis suppurativa whose initial presentation predated any signs of puberty. This early onset is very rare and its etiology remains unknown. Severe disease can be seen in prepubertal children and surgical intervention is effective in these cases."}
{"_id":"8b85f287c144aad5ff038ec0b140e0c4e210990b","title":"Finasteride for the treatment of hidradenitis suppurativa in children and adolescents.","text":"IMPORTANCE\nHidradenitis suppurativa (HS) is a chronic debilitating cutaneous disease for which there is no universally effective treatment. Patients typically present at puberty with tender subcutaneous nodules that can progress to dermal abscess formation. Antiandrogens have been used in the treatment of HS, and studies have primarily focused on adult patients.\n\n\nOBSERVATIONS\nWe present a case series of 3 pediatric patients with HS who were successfully treated with oral finasteride, resulting in decreased frequency and severity of disease flares with no significant adverse effects.\n\n\nCONCLUSIONS AND RELEVANCE\nFinasteride is a therapeutic option that provides benefit for pediatric patients with HS. Further prospective data and randomized controlled studies will provide helpful information in the management of this disease."}
{"_id":"e72e10ad6228bd3dcee792f6f571c5ffed37266f","title":"Hidradenitis suppurativa in children and adolescents: a review of treatment options.","text":"Hidradenitis suppurativa (HS) is a burdensome disease and has the potential to affect the life course of patients. It is a rare disease in children, and the recorded literature is correspondingly scarce. This article reviews the therapeutic options for HS in children and adolescents, and highlights particular differences or challenges with treating patients in this age group compared with adults. The work-up of paediatric patients with HS should include considerations of possible endocrine co-morbidities and obesity. Medical therapy of lesions may include topical clindamycin. Systemic therapy may include analgesics, clindamycin and rifampicin, finasteride, corticosteroids or tumour necrosis factor alpha (TNF\u03b1) blockers. Superinfections should be appropriately treated. Scarring lesions generally require surgery."}
{"_id":"f4f6ae0b10f2cc26e5cf75b4c39c70703f036e9b","title":"Morbidity in patients with hidradenitis suppurativa.","text":"BACKGROUND\nAlthough skin diseases are often immediately visible to both patients and society, the morbidity they cause is only poorly defined. It has been suggested that quality-of-life measures may be a relevant surrogate measure of skin disease. Hidradenitis suppurativa (HS) leads to painful eruptions and malodorous discharge and is assumed to cause a significant degree of morbidity. The resulting impairment of life quality has not previously been quantitatively assessed, although such an assessment may form a pertinent measure of disease severity in HS.\n\n\nOBJECTIVES\nTo measure the impairment of life quality in patients with HS.\n\n\nMETHODS\nIn total, 160 patients suffering from HS were approached. The following data were gathered: quality-of-life data (Dermatology Life Quality Index, DLQI questionnaire), basic demographic data, age at onset of the condition and the average number of painful lesions per month.\n\n\nRESULTS\nOne hundred and fourteen patients participated in the study. The mean +\/- SD age of the patients was 40.9 +\/- 11.7 years, the mean +\/- SD age at onset 21.8 +\/- 9.9 years and the mean +\/- SD duration of the disease 18.8 +\/- 11.4 years. Patients had a mean +\/- SD DLQI score of 8.9 +\/- 8.3 points. The highest mean score out of the 10 DLQI questions was recorded for question 1, which measures the level of pain, soreness, stinging or itching (mean 1.55 points, median 2 points). Patients experienced a mean of 5.1 lesions per month.\n\n\nCONCLUSIONS\nHS causes a high degree of morbidity, with the highest scores obtained for the level of pain caused by the disease. The mean DLQI score for HS was higher than for previously studied skin diseases, and correlated with disease intensity as expressed by lesions per month. This suggests that the DLQI may be a relevant outcome measure in future therapeutic trials in HS."}
{"_id":"3978e9f794174c7a2700b20193c071a7b1532b22","title":"Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods","text":"Direct search methods are best known as unconstrained optimization techniques that do not explicitly use derivatives. Direct search methods were formally proposed and widely applied in the 1960s but fell out of favor with the mathematical optimization community by the early 1970s because they lacked coherent mathematical analysis. Nonetheless, users remained loyal to these methods, most of which were easy to program, some of which were reliable. In the past fifteen years, these methods have seen a revival due, in part, to the appearance of mathematical analysis, as well as to interest in parallel and distributed computing. This review begins by briefly summarizing the history of direct search methods and considering the special properties of problems for which they are well suited. Our focus then turns to a broad class of methods for which we provide a unifying framework that lends itself to a variety of convergence results. The underlying principles allow generalization to handle bound constraints and linear constraints. We also discuss extensions to problems with nonlinear constraints."}
{"_id":"2a7a76cddd1d04aed660c62a0a879470bf98ca32","title":"On automatic differentiation","text":"During these last years, the environmental problems have acquired a growing place in our society. It becomes necessary to find a lasting way to the nuclear waste storage. The waste storage is in relation to the characteristics of wastes according to their activity and the life of the radionuclide. After many studies on the topic, led in particular by ANDRA, the storage in deep level is considered as the reference strategy for wastes with high or medium activity and long-lived. We have to make simulations of the radionucleide transport in underground in order to determine the impact of a possible propagation of radioelements. The modelling of the flow in porous media around the storage requires to know the physical parameters of the different geological layers. Those parameters (porosity and diffusion) are not directly accessible by measurements, hence we have to solve an inverse problem to recover them."}
{"_id":"4ac53a9341ae34d4651eff34e729e91806ab0c44","title":"Equation of State Calculations by Fast Computing Machines","text":"1087 instead, only water molecules with different amounts of excitation energy. These may follow any of three paths: (a) The excitation energy is lost without dissociation into radicals (by collision, or possibly radiation, as in aromatic hydrocarbons). (b) The molecules dissociate, but the resulting radicals recombine without escaping from the liquid cage. (c) The molecules dissociate and escape from the cage. In this case we would not expect them to move more than a few molecular diameters through the dense medium before being thermalized. paths (a) and (b) can be designated H 2 0* and those following path (c) can be designated H 2 0t. It seems reasonable to assume for the purpose of these calculations that the ionized H 2 0 molecules will become the H 20 t molecules, but this is not likely to be a complete correspondence. In conclusion we would like to emphasize that the qualitative result of this section is not critically dependent on the exact values of the physical parameters used. However, this treatment is classical, and a correct treatment must be wave mechanical; therefore the result of this section cannot be taken as an a priori theoretical prediction. The success of the radical diffusion model given above lends some plausibility to the occurrence of electron capture as described by this crude calculation. Further work is clearly needed. A general method, suitable for fast computing machines, for investigatiflg such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion."}
{"_id":"f514a883e2cb8901fc368078d8445da5aed18806","title":"A theoretical framework for simulated annealing","text":"Simulated Annealing has been a very successful general algorithm for the solution of large, complex combinatorial optimization problems. Since its introduction, several applications in different fields of engineering, such as integrated circuit placement, optimal encoding, resource allocation, logic synthesis, have been developed. In parallel, theoretical studies have been focusing on the reasons for the excellent behavior of the algorithm. This paper reviews most of the important results on the theory of Simulated Annealing, placing them in a unified framework. New results are reported as well."}
{"_id":"8da1dda34ecc96263102181448c94ec7d645d085","title":"Approximation by superpositions of a sigmoidal function","text":"Abstr,,ct. In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set ofaffine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single bidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."}
{"_id":"9e4291de6cdce8e6f247effa308d72e2ec3f6122","title":"Estimating the dimension of a linear model","text":""}
{"_id":"33ffc82ea3d8708ed9037debc3f1d4f9e9e49269","title":"Characterizing the software process: a maturity framework","text":"A description is given of a software-process maturity framework that has been developed to provide the US Department of Defense with a means to characterize the capabilities of software-development organizations. This software-development process-maturity model reasonably represents the actual ways in which software-development organizations improve. It provides a framework for assessing these organizations and identifying the priority areas for immediate improvement. It also helps identify those places where advanced technology can be most valuable in improving the software-development process. The framework can be used by any software organization to assess its own capabilities and identify the most important areas for improvement.<<ETX>>"}
{"_id":"7588c5f7cc699d7a071e85e08eb98d514e9c73f8","title":"Capability Maturity Model for Software","text":"This paper provides an overview of the latest version of the Capability Maturity Model for Software, CMM v1.1. Based on over six years of experience with software process improvement and the contributions of hundreds of reviewers, CMM v1.1 describes the software engineering and management practices that characterize organizations as they mature their processes for developing and maintaining software. This paper stresses the need for a process maturity framework to prioritize improvement actions, describes the process maturity framework of five maturity levels and the associated structural components, and discusses future directions for the CMM."}
{"_id":"f22a4acdd90388f386acaf0589f17731e0cf5cfa","title":"Climbing the \"Stairway to Heaven\" -- A Mulitiple-Case Study Exploring Barriers in the Transition from Agile Development towards Continuous Deployment of Software","text":"Agile software development is well-known for its focus on close customer collaboration and customer feedback. In emphasizing flexibility, efficiency and speed, agile practices have lead to a paradigm shift in how software is developed. However, while agile practices have succeeded in involving the customer in the development cycle, there is an urgent need to learn from customer usage of software also after delivering and deployment of the software product. The concept of continuous deployment, i.e. the ability to deliver software functionality frequently to customers and subsequently, the ability to continuously learn from real-time customer usage of software, has become attractive to companies realizing the potential in having even shorter feedback loops. However, the transition towards continuous deployment involves a number of barriers. This paper presents a multiple-case study in which we explore barriers associated with the transition towards continuous deployment. Based on interviews at four different software development companies we present key barriers in this transition as well as actions that need to be taken to address these."}
{"_id":"1bf2c4ce84b83b285f76a14dee459fd5353f2121","title":"Survey of semantic annotation platforms","text":"The realization of the Semantic Web requires the widespread availability of semantic annotations for existing and new documents on the Web. Semantic annotations are to tag ontology class instance data and map it into ontology classes. The fully automatic creation of semantic annotations is an unsolved problem. Instead, current systems focus on the semi-automatic creation of annotations. The Semantic Web also requires facilities for the storage of annotations and ontologies, user interfaces, access APIs, and other features to fully support annotation usage. This paper examines current Semantic Web annotation platforms that provide annotation and related services, and reviews their architecture, approaches and performance."}
{"_id":"455e1168304e0eb2909093d5ab9b5ec85cda5028","title":"The String-to-String Correction Problem","text":"The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings."}
{"_id":"3e2fd91786d37293af7f60a0d67b290faf5475f3","title":"Ontology Learning for the Semantic Web","text":"The Semantic Web relies heavily on the formal ontologies that structure underlying data for the purpose of comprehensive and transportable machine understanding. Therefore, the success of the Semantic Web depends strongly on the proliferation of ontologies, which requires fast and easy engineering of ontologies and avoidance of a knowledge acquisition bottleneck. Ontology Learning greatly facilitates the construction of ontologies by the ontology engineer. The vision of ontology learning that we propose here includes a number of complementary disciplines that feed on different types of unstructured, semi-structured and fully structured data in order to support a semi-automatic, cooperative ontology engineering process. Our ontology learning framework proceeds through ontology import, extraction, pruning, refinement, and evaluation giving the ontology engineer a wealth of coordinated tools for ontology modeling. Besides of the general framework and architecture, we show in this paper some exemplary techniques in the ontology learning cycle that we have implemented in our ontology learning environment, Text-ToOnto, such as ontology learning from free text, from dictionaries, or from legacy ontologies, and refer to some others that need to complement the complete architecture, such as reverse engineering of ontologies from database schemata or learning from XML documents. Ontologies for the Semantic Web Conceptual structures that define an underlying ontology are germane to the idea of machine processable data on the Semantic Web. Ontologies are (meta)data schemas, providing a controlled vocabulary of concepts, each with an explicitly defined and machine processable semantics. By defining shared and common domain theories, ontologies help both people and machines to communicate concisely,"}
{"_id":"152086bea7688c533794c0076bfa210ce1031bfc","title":"Semantic Annotation and Reasoning for Sensor Data","text":"Developments in (wireless) sensor and actuator networks and the capabilities to manufacture low cost and energy efficient networked embedded devices have lead to considerable interest in adding real world sense to the Internet and the Web. Recent work has raised the idea towards combining the Internet of Things (i.e. real world resources) with semantic Web technologies to design future service and applications for the Web. In this paper we focus on the current developments and discussions on designing Semantic Sensor Web, particularly, we advocate the idea of semantic annotation with the existing authoritative data published on the semantic Web. Through illustrative examples, we demonstrate how rule-based reasoning can be performed over the sensor observation and measurement data and linked data to derive additional or approximate knowledge. Furthermore, we discuss the association between sensor data, the semantic Web, and the social Web which enable construction of context-aware applications and services, and contribute to construction of a networked knowledge framework."}
{"_id":"158463840d4beed75e5a821e218526cd4d4d6801","title":"The SSN ontology of the W3C semantic sensor network incubator group","text":"The W3C Semantic Sensor Network Incubator group (the SSN-XG) produced an OWL 2 ontology to describe sensors and observations \u2014 the SSN ontology, available at http:\/\/purl.oclc.org\/NET\/ssnx\/ssn. The SSN ontology can describe sensors in terms of capabilities, measurement processes, observations and deployments. This article describes the SSN ontology. It further gives an example and describes the use of the ontology in recent research projects."}
{"_id":"5b081ed14184ca48da725032b1022c23669cc2be","title":"SemSOS: Semantic sensor Observation Service","text":"Sensor Observation Service (SOS) is a Web service specification defined by the Open Geospatial Consortium (OGC) Sensor Web Enablement (SWE) group in order to standardize the way sensors and sensor data are discovered and accessed on the Web. This standard goes a long way in providing interoperability between repositories of heterogeneous sensor data and applications that use this data. Many of these applications, however, are ill equipped at handling raw sensor data as provided by SOS and require actionable knowledge of the environment in order to be practically useful. There are two approaches to deal with this obstacle, make the applications smarter or make the data smarter. We propose the latter option and accomplish this by leveraging semantic technologies in order to provide and apply more meaningful representation of sensor data. More specifically, we are modeling the domain of sensors and sensor observations in a suite of ontologies, adding semantic annotations to the sensor data, using the ontology models to reason over sensor observations, and extending an open source SOS implementation with our semantic knowledge base. This semantically enabled SOS, or SemSOS, provides the ability to query high-level knowledge of the environment as well as low-level raw sensor data."}
{"_id":"961b8e95e4b360e5d95ef79a21958540d4e551ab","title":"New Generation Sensor Web Enablement","text":"Many sensor networks have been deployed to monitor Earth's environment, and more will follow in the future. Environmental sensors have improved continuously by becoming smaller, cheaper, and more intelligent. Due to the large number of sensor manufacturers and differing accompanying protocols, integrating diverse sensors into observation systems is not straightforward. A coherent infrastructure is needed to treat sensors in an interoperable, platform-independent and uniform way. The concept of the Sensor Web reflects such a kind of infrastructure for sharing, finding, and accessing sensors and their data across different applications. It hides the heterogeneous sensor hardware and communication protocols from the applications built on top of it. The Sensor Web Enablement initiative of the Open Geospatial Consortium standardizes web service interfaces and data encodings which can be used as building blocks for a Sensor Web. This article illustrates and analyzes the recent developments of the new generation of the Sensor Web Enablement specification framework. Further, we relate the Sensor Web to other emerging concepts such as the Web of Things and point out challenges and resulting future work topics for research on Sensor Web Enablement."}
{"_id":"a1cb4845c6cb1fa0ee2661f87ccf1a9ece93cc69","title":"An Internet of Things Platform for Real-World and Digital Objects","text":"The vision of the Internet of Things (IoT) relies on the provisioning of real-world services, which are provided by smart objects that are directly related to the physical world. A structured, machine-processible approach to provision such real-world services is needed to make heterogeneous physical objects accessible on a large scale and to integrate them with the digital world. The incorporation of observation and measurement data obtained from the physical objects with the Web data, using information processing and knowledge engineering methods, enables the construction of \u201dintelligent and interconnected things\u201d. The current research mostly focuses on the communication and networking aspects between the devices that are used for sensing amd measurement of the real world objects. There is, however, relatively less effort concentrated on creating dynamic infrastructures to support integration of the data into the Web and provide unified access to such data on service and application levels. This paper presents a semantic modelling and linked data approach to create an information framework for IoT. The paper describes a platform to publish instances of the IoT related resources and entities and to link them to existing resources on the Web. The developed platform supports publication of extensible and interoperable descriptions in the form of linked data."}
{"_id":"cc611049fa1029230858ec62399b571cb40bfb71","title":"Power Amplifiers and Transmitters for RF and Microwave","text":"The generation of RF\/microwave power is required not only in wireless communications, but also in applications such as jamming, imaging, RF heating, and miniature dc\/dc converters. Each application has its own unique requirements for frequency, bandwidth, load, power, efficiency, linearity, and cost. RF power is generated by a wide variety of techniques, implementations, and active devices. Power amplifiers are incorporated into transmitters in a similarly wide variety of architectures, including linear, Kahn, envelope tracking, outphasing, and Doherty. Linearity can be improved through techniques such as feedback, feedforward, and predistortion."}
{"_id":"e6901330f1b056578cf2584420cecd01168b1761","title":"CMOS Outphasing Class-D Amplifier With Chireix Combiner","text":"This letter presents a CMOS outphasing class-D power amplifier (PA) with a Chireix combiner. Two voltage-mode class-D amplifiers used in the outphasing system were designed and implemented with a 0.18-mum CMOS process. By applying the Chireix combiner technique, drain efficiency of the outphasing PA for CDMA signals was improved from 38.6% to 48% while output power was increased from 14.5 to 15.4 dBm with an adjacent channel power ratio of -45 dBc."}
{"_id":"32d31bdc147ee2ade8166b715bd381227c523f0a","title":"An extended Doherty amplifier with high efficiency over a wide power range","text":"An extension of the Doherty amplifier architecture which maintains high efficiency over a wide range of output power (>6 dB) is presented. This extended Doherty amplifier is demonstrated experimentally with InGaP-GaAs HBTs at a frequency of 950 MHz. P\/sub 1 dB\/ is measured at 27.5 dBm with PAE of 46%. PAE of at least 39% is maintained for over an output power range of 12 dB backed-off from P\/sub 1 dB\/. This is an improvement over the classical Doherty amplifier, where high efficiency is typically obtained up to 5-6 dB backed-off from P\/sub 1 dB\/. Generalized design equations for the Doherty amplifier are derived to show a careful choice of the output matching circuit and device scaling parameters can improve efficiencies at lower output power."}
{"_id":"03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","title":"On Using Very Large Target Vocabulary for Neural Machine Translation","text":"Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English\u2192German and English\u2192French translation tasks of WMT\u201914."}
{"_id":"060e380b28be29b7eda509981a50b4406ea6b21b","title":"Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation","text":"The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on ChineseEnglish and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training."}
{"_id":"1c23e1ad1a538416e8123f128a87c928b09be868","title":"Alignment by Agreement","text":"We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. Disciplines Computer Sciences Comments Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL '06). Association for Computational Linguistics, Stroudsburg, PA, USA, 104-111. DOI=10.3115\/1220835.1220849 http:\/\/dx.doi.org\/10.3115\/1220835.1220849 \u00a9 ACM, 2006. This is the author's version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, {(2006)} http:\/\/doi.acm.org\/10.3115\/1220835.1220849\" Email permissions@acm.org This conference paper is available at ScholarlyCommons: http:\/\/repository.upenn.edu\/cis_papers\/533 Alignment by Agreement Percy Liang UC Berkeley Berkeley, CA 94720 pliang@cs.berkeley.edu Ben Taskar UC Berkeley Berkeley, CA 94720 taskar@cs.berkeley.edu Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu"}
{"_id":"41b77840bf309358ecf45b16d00053ed12aea5c0","title":"Identifying careless responses in survey data.","text":"When data are collected via anonymous Internet surveys, particularly under conditions of obligatory participation (such as with student samples), data quality can be a concern. However, little guidance exists in the published literature regarding techniques for detecting careless responses. Previously several potential approaches have been suggested for identifying careless respondents via indices computed from the data, yet almost no prior work has examined the relationships among these indicators or the types of data patterns identified by each. In 2 studies, we examined several methods for identifying careless responses, including (a) special items designed to detect careless response, (b) response consistency indices formed from responses to typical survey items, (c) multivariate outlier analysis, (d) response time, and (e) self-reported diligence. Results indicated that there are two distinct patterns of careless response (random and nonrandom) and that different indices are needed to identify these different response patterns. We also found that approximately 10%-12% of undergraduates completing a lengthy survey for course credit were identified as careless responders. In Study 2, we simulated data with known random response patterns to determine the efficacy of several indicators of careless response. We found that the nature of the data strongly influenced the efficacy of the indices to identify careless responses. Recommendations include using identified rather than anonymous responses, incorporating instructed response items before data collection, as well as computing consistency indices and multivariate outlier analysis to ensure high-quality data."}
{"_id":"9c58c19b01b04ca7dbf122d59684bf05353cc77b","title":"A new scale of social desirability independent of psychopathology.","text":"It has long been recognized that personality test scores are influenced by non-test-relevant response determinants. Wiggins and Rumrill (1959) distinguish three approaches to this problem. Briefly, interest in the problem of response distortion has been concerned with attempts at statistical correction for \"faking good\" or \"faking bad\" (Meehl & Hathaway, 1946), the analysis of response sets (Cronbach, 1946,1950), and ratings of the social desirability of personality test items (Edwards, 19 5 7). A further distinction can be made, however, which results in a somewhat different division of approaches to the question of response distortion. Common to both the Meehl and Hathaway corrections for faking good and faking bad and Cronbach's notion of response sets is an interest in the test behavior of the subject(S). By social desirability, on the other hand, Edwards primarily means the \"scale value for any personality statement such that the scale value indicates the position of the statement on the social desirability continuum . . .\" (1957, p. 3). Social desirability, thus, has been used to refer to a characteristic of test items, i.e., their scale position on a social desirability scale. Whether the test behavior of 5s or the social desirability properties of items are the focus of interest, however, it now seems clear that underlying both these approaches is the concept of statistical deviance. In the construction of the MMPI K scale, for example, items were selected which differentiated between clinically normal persons producing abnormal te\u00a5Tpfpfiles~snd^cTinically abnormal individuals with abnormal test profiles, and between clinically abnormal persons with normal test profiles and abnormal 5s whose test records were abnormal. Keyed responses to the K scale items tend to be statistically deviant in the parent populations. Similarly, the development of the Edwards Social Desirability Scale (SDS) illustrates this procedure. Items were drawn from various MMPI scales (F, L, K, and the Manifest Anxiety Scale [Taylor, 1953]) and submitted to judges who categorized them as either socially desirable or socially undesirable. Only items on which there was unanimous agreement among the 10 judges were included in the SDS. It seems clear that the items in Edwards SDS would, of necessity, have extreme social desirability scale positions or, in other words, be statistically deviant. Some unfortunate consequences follow from the strict use of the statistical deviance model in the development of-sOcialTtesirSbTBty scales. With items drawn from the MMPI, it is apparent that in addition to their scalability for social desirability the items may also be characterized by their content which,^n a general sense, has pathological implications. When a social desrrabtltty^scale constructed according to this procedure is then applied to a college student population, the meaning of high social desirability scores is not at all clear. When 5s given the Edwards SDS deny, for example, that their sleep is fitful and disturbed (Item 6) or that they worry quite a bit over possible misfortunes (Item 35), it cannot be determined whether these responses are attributable to social desirability or to a genuine absence of such symptoms. The probability of occurrence of the symptoms represented in MMPI items (and incorportated in the SDS)"}
{"_id":"b65c9bac7a42ac4a52a7be4d8d58b152d9124d11","title":"Simultaneous administration of the Rosenberg Self-Esteem Scale in 53 nations: exploring the universal and culture-specific features of global self-esteem.","text":"The Rosenberg Self-Esteem Scale (RSES) was translated into 28 languages and administered to 16,998 participants across 53 nations. The RSES factor structure was largely invariant across nations. RSES scores correlated with neuroticism, extraversion, and romantic attachment styles within nearly all nations, providing additional support for cross-cultural equivalence of the RSES. All nations scored above the theoretical midpoint of the RSES, indicating generally positive self-evaluation may be culturally universal. Individual differences in self-esteem were variable across cultures, with a neutral response bias prevalent in more collectivist cultures. Self-competence and self-liking subscales of the RSES varied with cultural individualism. Although positively and negatively worded items of the RSES were correlated within cultures and were uniformly related to external personality variables, differences between aggregates of positive and negative items were smaller in developed nations. Because negatively worded items were interpreted differently across nations, direct cross-cultural comparisons using the RSES may have limited value."}
{"_id":"5d86a5dbcb22cee5b931d8e9d6a1a95d6d8f394d","title":"Assessing psychopathic attributes in a noninstitutionalized population.","text":"The present study examined antisocial dispositions in 487 university students. Primary and secondary psychopathy scales were developed to assess a protopsychopathic interpersonal philosophy. An antisocial action scale also was developed for purposes of validation. The primary, secondary, and antisocial action scales were correlated with each other and with boredom susceptibility and disinhibition but not with experience seeking and thrill and adventure seeking. Secondary psychopathy was associated with trait anxiety. Multiple regression analysis revealed that the strongest predictors of antisocial action were disinhibition, primary psychopathy, secondary psychopathy, and sex, whereas thrill and adventure seeking was a negative predictor. This argues against a singular behavioral inhibition system mediating both antisocial and risk-taking behavior. These findings are also consistent with the view that psychopathy is a continuous dimension."}
{"_id":"4ce94ff24cc238440a76598da13361ef4da9e5ed","title":"The Whale Optimization Algorithm","text":"This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at http:\/\/www.alimirjalili.com\/WOA.html \u00a9 2016 Elsevier Ltd. All rights reserved."}
{"_id":"8a7a9672b4981e72d6e9206024c758cc047db8cd","title":"Evolution strategies \u2013 A comprehensive introduction","text":"This article gives a comprehensive introduction into one of the main branches of evolutionary computation \u2013 the evolution strategies (ES) the history of which dates back to the 1960s in Germany. Starting from a survey of history the philosophical background is explained in order to make understandable why ES are realized in the way they are. Basic ES algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of ES research are discussed."}
{"_id":"59a3fea1f38c5dd661cc5bfec50add2c2f881454","title":"A Fast Elitist Non-dominated Sorting Genetic Algorithm for Multi-objective Optimisation: NSGA-II","text":"Multi-objectiveevolutionaryalgorithmswhichusenon-dominatedsorting andsharinghave beenmainly criticizedfor their (i) computational complexity (where is thenumberof objectivesand is thepopulationsize), (ii) non-elitismapproach, and(iii) theneedfor specifyingasharingparameter . In this paper , we suggesta non-dominatedsortingbasedmulti-objective evolutionaryalgorithm(wecalledit theNon-dominatedSortingGA-II or NSGA-II) which alleviatesall theabove threedifficulties.Specifically, a fastnon-dominatedsorting approachwith computationalcomplexity is presented. Second,a selectionoperatoris presentedwhich createsa mating pool by combiningthe parentandchild populationsandselectingthe best(with respectto fitnessand spread) solutions.Simulationresultson five difficult testproblemsshow that theproposedNSGA-II is ableto find muchbetterspreadof solutionsin all problemscomparedto PAES\u2014anotherelitist multi-objective EA which paysspecial attentiontowardscreatinga diversePareto-optimalfront. Becauseof NSGA-II\u2019s low computational requirements, elitist approach, andparameter -lesssharingapproach,NSGA-II shouldfind increasingapplicationsin theyearsto come."}
{"_id":"2bc27481fa57a1b247ab1fc5d23a07912480352a","title":"How computer games affect CS (and other) students' school performance","text":"Compulsive game playing, especially of the role-playing variety, risks failing grades and withdrawal of financial support from tuition-paying parents."}
{"_id":"3716c4896944c3461477f845319ac09e3dfe3a10","title":"eSports: collaborative and synchronous video annotation system in grid computing environment","text":"We designed eSports - a collaborative and synchronous video annotation platform, which is to be used in Internet scale cross-platform grid computing environment to facilitate computer supported cooperative work (CSCW) in education settings such as distance sport coaching, distance classroom etc. Different from traditional multimedia annotation systems, eSports provides the capabilities to collaboratively and synchronously play and archive real time live video, to take snapshots, to annotate video snapshots using whiteboard and to play back the video annotations synchronized with original video streams. eSports is designed based on the grid based collaboration paradigm $the shared event model using NaradaBrokering, which is a publish\/subscribe based distributed message passing and event notification system. In addition to elaborate the design and implementation of eSports, we analyze the potential use cases of eSports under different education settings. We believed that eSports is very useful to improve the online collaborative coaching and education."}
{"_id":"948876640d3ca519a2c625a4a52dc830fec26b29","title":"The Role of Flow Experience in Cyber-Game Addiction","text":"Consumer habit, an important key to repetitive consumption, is an interesting yet puzzling phenomenon. Sometimes this consumption becomes obsessive--consumers will continue to act a certain way even when they feel it is not in their best interests. However, not all consumers develop such addictions. This study uses cyber-game addiction syndrome as an analogue to trace the possible causes of consumer addiction. Results from structure equation modeling show that repetition of favorite activities has a moderate effect upon addiction, which is in line with the assertion of rational addiction theory. However, flow experience--the emotional state embracing perceptional distortion and enjoyment--shows a much stronger impact on addiction. This suggests that consumers who have experienced flow are more likely to be addicted."}
{"_id":"c1b66422b1dab3eeee6d6c760f4bd227a8bb16c5","title":"Being There: The Subjective Experience of Presence","text":""}
{"_id":"7a11cd3434c7f1eabbf76f03182255f62fd27fa3","title":"Application of CFD in building performance simulation for the outdoor environment : an overview","text":"This paper provides an overview of the application of CFD in building performance simulation for the outdoor environment, focused on four topics: (1) pedestrian wind environment around buildings, (2) wind-driven rain on building facades, (3) convective heat transfer coefficients at exterior building surfaces, and (4) air pollutant dispersion around buildings. For each topic, its background, the need for CFD, an overview of some past CFD studies, a discussion about accuracy and some perspectives for practical application are provided. The paper indicates that for all four topics, CFD offers considerable advantages compared to wind tunnel modelling or (semi-)empirical formulae because it can provide detailed whole-flow field data under fully controlled conditions and without similarity constraints. The main limitations are the deficiencies of steady RANS modelling, the increased complexity and computational expense of LES and the requirement of systematic and time-consuming CFD solution verification and validation studies."}
{"_id":"b17dd35f5e884823fd292a6d72d8124d0758173a","title":"Numerical Study of Urban Canyon Microclimate Related to Geometrical Parameters","text":"In this study a microclimate analysis on a particular urban configuration: the\u2014street canyon\u2014has been carried out. The analysis, conducted by performing numerical simulations using the finite volumes commercial code ANSYS-Fluent, shows the flow field in an urban environment, taking into account three different aspect ratios (H\/W). This analysis can be helpful in the study on urban microclimate and on the heat exchanges with the buildings. Fluid-dynamic fields on vertical planes within the canyon, have been evaluated. The results show the importance of the geometrical configuration, in relation to the ratio between the height (H) of the buildings and the width (W) of the road. This is a very important subject from the point of view of \u201cSmart Cities\u201d, considering the urban canyon as a subsystem of a larger one (the city), which is affected by climate changes."}
{"_id":"bc39516aba1e74f7d8ed7391b4ff9e5a9ceeecf2","title":"BEST PRACTICE GUIDELINE FOR THE CFD SIMULATION OF FLOWS IN THE URBAN ENVIRONMENT","text":"Legal notice by the COST Office Neither the COST Office nor any person acting on its behalf is responsible for the use which might be made of the information contained in the present publication. The COST Office is not responsible for the external web sites referred to in the present publication. No permission to reproduce or utilize the contents of this book by any means is necessary, other than in the case of images, diagrams or other material from other copyright holders. In such cases permission of the copyright holders is required. This book may be cited as: Title of the book and Action Number"}
{"_id":"f5e717d62ee75465deb3c3495b2b867bdc17560e","title":"Urban Physics : Effect of the micro-climate on comfort , health and energy demand","text":"Y-NC-ND license. Abstract The global trend towards urbanisation explains the growing interest in the study of the modification of the urban climate due to the heat island effect and global warming, and its impact on energy use of buildings. Also urban comfort, health and durability, referring respectively to pedestrian wind\/ thermal comfort, pollutant dispersion and wind-driven rain are of interest. Urban Physics is a wellestablished discipline, incorporating relevant branches of physics, environmental chemistry, aerodynamics, meteorology and statistics. Therefore, Urban Physics is well positioned to provide keycontributions to the current urban problems and challenges. The present paper addresses the role of Urban Physics in the study of wind comfort, thermal comfort, energy demand, pollutant dispersion and wind-driven rain. Furthermore, the three major research methods applied in Urban Physics, namely field experiments, wind tunnel experiments and numerical simulations are discussed. Case studies illustrate the current challenges and the relevant contributions of Urban Physics. & 2012. Higher Education Press Limited Company. Production and hosting by Elsevier B.V. Open access under CC BY-NC-ND license."}
{"_id":"7046b8930c262c1841a7fad461dfc37eeb466771","title":"Verification , Validation , and Predictive Capability in Computational Engineering and Physics","text":"Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V&V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, i.e., experimental data, is the issue."}
{"_id":"8efe3b64ee8a936e583b18b415d0071a1cd65a7c","title":"System Design of a 77 GHz Automotive Radar Sensor with Superresolution DOA Estimation","text":"This paper introduces a novel 77 GHz FMCW automotive long range radar (LRR) system concept. High resolution direction of arrival (DOA) estimation is an important requirement for the application in automotive safety systems. The challenges in system design regarding low cost and superresolution signal processing are discussed. Dominant interferences to the MUSIC DOA estimator are amplitude and phase mismatches due to inhomogeneous antenna patterns. System simulation results deliver design guidelines for the required signal-to-noise ratio (SNR) and the antenna design. Road traffic measurements with a demonstrator system show superior DOA resolution and demonstrate the feasibility of the design goals."}
{"_id":"908f48cf72e0724a80baf87913f1b8534ed5a380","title":"Automotive Radar \u2013 Status and Trends","text":"The paper gives a brief overview of automotive radar. The status of the frequency regulation for short and long range radar is summarized because of its importance for car manufacturers and their sensor suppliers. Front end concepts and antenna techniques of 24 GHz and 77 GHz sensors are briefly described. Their impact on the sensor\u2019s field of view and on the angular measurement capability is discussed. Esp. digital beamforming concepts are considered and promising results are presented."}
{"_id":"408a8e250316863da94ffb3eab077175d08c01bf","title":"Multiple Emitter Location and Signal Parameter-- Estimation","text":""}
{"_id":"ad570ceaffa4a012ff3e0157df80c6e1229f0a73","title":"Proposal of millimeter-wave holographic radar with antenna switching","text":"This paper proposes a millimeter-wave holographic radar with a simple structure for automotive applications. The simplicity can be realized by switching both transmitting antennas and receiving antennas. Also, a super resolution technique is introduced for the detection of angular positions in the proposed radar. The radar developed experimentally has accomplished an azimuthal angular resolution of less than 2 degrees and an azimuthal field of view (FoV) of more than 20 degrees simultaneously."}
{"_id":"0bb4401b9a1b064c513bda3001f43f8f2f3e28de","title":"Learning with Noisy Labels","text":"In this paper, we theoretically study the problem of binary classification in the presence of random classification noise \u2014 the learner, instead of seeing the true labels, sees labels that have independently been flipped with some small probability. Moreover, random label noise is class-conditional \u2014 the flip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence \u2014 methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88% accuracy even when 40% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets."}
{"_id":"165ef2b5f86b9b2c68b652391db5ece8c5a0bc7e","title":"Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation","text":"Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information, specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78:0 on the challenging PASCAL VOC 2012 dataset."}
{"_id":"32cde90437ab5a70cf003ea36f66f2de0e24b3ab","title":"The Cityscapes Dataset for Semantic Urban Scene Understanding","text":"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."}
{"_id":"3cdb1364c3e66443e1c2182474d44b2fb01cd584","title":"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation","text":"We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http:\/\/mi.eng.cam.ac.uk\/projects\/segnet\/."}
{"_id":"568a0de980b9773fe96abf3c75ac0891d1df9c2b","title":"Is e-Learning the Solution for Individual Learning?.","text":"Despite the fact that e-Learning exists for a relatively long time, it is still in its infancy. Current eLearning systems on the market are limited to technical gadgets and organizational aspects of teaching, instead of supporting the learning. As a result the learner has become deindividualized and demoted to a noncritical homogenous user. One way out of this drawback is the creation of individual e-Learning materials. For this purpose a flexible multidimensional data model and the generation of individual content are the solution. It is necessary to enable the interaction between the learners and the content in e-Learning systems in the same manner."}
{"_id":"56c16d9e2a5270ba6b1d83271e2c10916591968d","title":"Human memory ; A proposed system and its control processes","text":""}
{"_id":"595e5b8d9e08d56dfcec464cfc2854e562cd7089","title":"Heart rate variability and its relation to prefrontal cognitive function: the effects of training and detraining","text":"The aim of the present study was to investigate the relationship between physical fitness, heart rate variability (HRV) and cognitive function in 37 male sailors from the Royal Norwegian Navy. All subjects participated in an 8-week training program, after which the subjects completed the initial cognitive testing (pre-test). The subjects were assigned into a detrained group (DG) and a trained group (TG) based on their application for further duty. The DG withdrew from the training program for 4 weeks after which all subjects then completed the cognitive testing again (post-test). Physical fitness, measured as maximum oxygen consumption (V\u0307O2max), resting HRV, and cognitive function, measured using a continuous performance task (CPT) and a working memory test (WMT), were recorded during the pre-test and the post-test, and the data presented as the means and standard deviations. The results showed no between-group differences in V\u0307O2max or HRV at the pre-test. The DG showed a significant decrease in V\u0307O2max from the pre- to the post-test and a lower resting HRV than the TG on the post-test. Whereas there were no between-group differences on the CPT or WMT at the pre-test, the TG had faster reaction times and more true positive responses on tests of executive function at the post-test compared to the pre-test. The DG showed faster reaction times on non-executive tasks at the post-test compared to the pre-test. The results are discussed within a neurovisceral integration framework linking parasympathetic outflow to the heart to prefrontal neural functions."}
{"_id":"46739eed6aefecd4591beed0d45b783cc0052a94","title":"Power spectrum analysis of heart rate fluctuation: a quantitative probe of beat-to-beat cardiovascular control.","text":"Power spectrum analysis of heart rate fluctuations provides a quantitative noninvasive means of assessing the functioning of the short-term cardiovascular control systems. We show that sympathetic and parasympathetic nervous activity make frequency-specific contributions to the heart rate power spectrum, and that renin-angiotensin system activity strongly modulates the amplitude of the spectral peak located at 0.04 hertz. Our data therefore provide evidence that the renin-angiotensin system plays a significant role in short-term cardiovascular control in the time scale of seconds to minutes."}
{"_id":"9684797643d13be86002683f60caa1cb69832e74","title":"Vagal influence on working memory and attention.","text":"The aim of the present study was to investigate the effect of vagal tone on performance during executive and non-executive tasks, using a working memory and a sustained attention test. Reactivity to cognitive tasks was also investigated using heart rate (HR) and heart rate variability (HRV). Fifty-three male sailors from the Royal Norwegian Navy participated in this study. Inter-beat-intervals were recorded continuously for 5 min of baseline, followed by randomized presentation of a working memory test (WMT) based on Baddeley and Hitch's research (1974) and a continuous performance test (CPT). The session ended with a 5-min recovery period. High HRV and low HRV groups were formed based on a median split of the root mean squared successive differences during baseline. The results showed that the high HRV group showed more correct responses than the low HRV group on the WMT. Furthermore, the high HRV group showed faster mean reaction time (mRT), more correct responses and less error, than the low HRV group on the CPT. Follow-up analysis revealed that this was evident only for components of the CPT where executive functions were involved. The analyses of reactivity showed a suppression of HRV and an increase in HR during presentation of cognitive tasks compared to recovery. This was evident for both groups. The present results indicated that high HRV was associated with better performance on tasks involving executive function."}
{"_id":"52047f4929bf616ca6dfad6acf7d5da2a0c15aa8","title":"Three-dimensional menus: A survey and taxonomy","text":"Various interaction techniques have been developed in the field of virtual and augmented reality. Whereas techniques for object selection, manipulation, travel, and wayfinding have already been covered in existing taxonomies in some detail, application control techniques have not yet been sufficiently considered. However, they are needed by almost every mixed reality application, e.g. for choosing from alternative objects or options. For this purpose a great variety of distinct three-dimensional (3D) menu selection techniques is available. This paper surveys existing 3D menus from the corpus of literature and classifies them according to various criteria. The taxonomy introduced here assists developers of interactive 3D applications to better evaluate their options when choosing, optimizing, and implementing a 3D menu technique. Since the taxonomy spans the design space for 3D menu solutions, it also aids researchers in identifying opportunities to improve or create novel virtual menu techniques. r 2006 Elsevier Ltd. All rights reserved."}
{"_id":"661ce19f315aafbf5a3916684e0e7c10e642d5f1","title":"ShoeSoleSense: proof of concept for a wearable foot interface for virtual and real environments","text":"ShoeSoleSense is a proof of concept, novel body worn interface - an insole that enables location independent hands-free interaction through the feet. Forgoing hand or finger interaction is especially beneficial when the user is engaged in real world tasks. In virtual environments as moving through safety training applications is often conducted via finger input, which is not very suitable. To enable a more intuitive interaction, alternative control concepts utilize gesture control, which is usually tracked by statically installed cameras in CAVE-like-installations. Since tracking coverage is limited, problems may also occur. The introduced prototype provides a novel control concept for virtual reality as well as real life applications. Demonstrated functions include movement control in a virtual reality installation such as moving straight, turning and jumping. Furthermore the prototype provides additional feedback by heating up the feet and vibrating in dedicated areas on the surface of the insole."}
{"_id":"9810a7976242774b4d7878fd121f54094412ae40","title":"A discussion of cybersickness in virtual environments","text":"An important and troublesome problem with current virtual environment (VE) technology is the tendency for some users to exhibit symptoms that parallel symptoms of classical motion sickness both during and after the VE experience. This type of sickness, cybersickness, is distinct from motion sickness in that the user is often stationary but has a compelling sense of self motion through moving visual imagery. Unfortunately, there are many factors that can cause cybersickness and there is no foolproof method for eliminating the problem. In this paper, I discuss a number of the primary factors that contribute to the cause of cybersickness, describe three conflicting cybersickness theories that have been postulated, and discuss some possible methods for reducing cybersickness in VEs."}
{"_id":"16af753e94919ca257957cee7ab6c1b30407bb91","title":"ChairIO--the Chair-Based Interface","text":""}
{"_id":"58bb338cab40d084321f6d1dd7f4512896249566","title":"Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices: Design Considerations","text":"In recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We evaluate the effect of various RPU device features\/non-idealities and system parameters on performance in order to derive the device and system level specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30, 000 \u00d7 compared to state-of-the-art microprocessors while providing power efficiency of 84, 000 GigaOps\u2215s\u2215W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisting of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration, and analysis of multimodal sensory data flows from a massive number of IoT (Internet of Things) sensors."}
{"_id":"00af4fba4bc85262d381881848c3ad67536fcb6b","title":"A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse, Heterogeneous Clinical Data","text":"The ability to determine patient acuity (or severity of illness) has immediate practical use for clinicians. We evaluate the use of multivariate timeseries modeling with the multi-task Gaussian process (GP) models using noisy, incomplete, sparse, heterogeneous and unevenly-sampled clinical data, including both physiological signals and clinical notes. The learned multi-task GP (MTGP) hyperparameters are then used to assess and forecast patient acuity. Experiments were conducted with two real clinical data sets acquired from ICU patients: firstly, estimating cerebrovascular pressure reactivity, an important indicator of secondary damage for traumatic brain injury patients, by learning the interactions between intracranial pressure and mean arterial blood pressure signals, and secondly, mortality prediction using clinical progress notes. In both cases, MTGPs provided improved results: an MTGP model provided better results than single-task GP models for signal interpolation and forecasting (0.91 vs 0.69 RMSE), and the use of MTGP hyperparameters obtained improved results when used as additional classification features (0.812 vs 0.788 AUC)."}
{"_id":"23c3953fb45536c9129e86ac7a23098bd9f1381d","title":"Machine Learning for Sequential Data: A Review","text":"Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues."}
{"_id":"604a82697d874c4da2aa07797c4b9f24c3dd272a","title":"Lung cancer cell identification based on artificial neural network ensembles","text":"An artificial neural network ensemble is a learning paradigm where several artificial neural networks are jointly used to solve a problem. In this paper, an automatic pathological diagnosis procedure named Neural Ensemble-based Detection (NED) is proposed, which utilizes an artificial neural network ensemble to identify lung cancer cells in the images of the specimens of needle biopsies obtained from the bodies of the subjects to be diagnosed. The ensemble is built on a two-level ensemble architecture. The first-level ensemble is used to judge whether a cell is normal with high confidence where each individual network has only two outputs respectively normal cell or cancer cell. The predictions of those individual networks are combined by a novel method presented in this paper, i.e. full voting which judges a cell to be normal only when all the individual networks judge it is normal. The second-level ensemble is used to deal with the cells that are judged as cancer cells by the first-level ensemble, where each individual network has five outputs respectively adenocarcinoma, squamous cell carcinoma, small cell carcinoma, large cell carcinoma, and normal, among which the former four are different types of lung cancer cells. The predictions of those individual networks are combined by a prevailing method, i.e. plurality voting. Through adopting those techniques, NED achieves not only a high rate of overall identification, but also a low rate of false negative identification, i.e. a low rate of judging cancer cells to be normal ones, which is important in saving lives due to reducing missing diagnoses of cancer patients."}
{"_id":"16a39222c0297c55401b94aa3bed09ca825be732","title":"Graph Drawing by Stress Majorization","text":"One of the most popular graph drawing methods is based of achieving graphtheoretic target ditsances. This method was used by Kamada and Kawai [15], who formulated it as an energy optimization problem. Their energy is known in the multidimensional scaling (MDS) community as the stress function. In this work, we show how to draw graphs by stress majorization, adapting a technique known in the MDS community for more than two decades. It appears that majorization has advantages over the technique of Kamada and Kawai in running time and stability. We also present a few extensions to the basic energy model which can improve layout quality and computation speed in practice. Majorization-based optimization is essential to these extensions."}
{"_id":"198a8507c7b26f89419430ed51f1c7675e5fa6c7","title":"Eigensolver Methods for Progressive Multidimensional Scaling of Large Data","text":"We present a novel sampling-based approximation technique for classical multidimensional scaling that yields an extremely fast layout algorithm suitable even for very large graphs. It produces layouts that compare favorably with other methods for drawing large graphs, and it is among the fastest methods available. In addition, our approach allows for progressive computation, i.e. a rough approximation of the layout can be produced even faster, and then be refined until satisfaction."}
{"_id":"4c77e5650e2328390995f3219ec44a4efd803b84","title":"Accelerating Large Graph Algorithms on the GPU Using CUDA","text":"Large graphs involving millions of vertices are common in many practical applications and are challenging to process. Practical-time implementations using high-end computers are reported but are accessible only to a few. Graphics Processing Units (GPUs) of today have high computation power and low price. They have a restrictive programming model and are tricky to use. The G80 line of Nvidia GPUs can be treated as a SIMD processor array using the CUDA programming model. We present a few fundamental algorithms \u2013 including breadth first search, single source shortest path, and all-pairs shortest path \u2013 using CUDA on large graphs. We can compute the single source shortest path on a 10 million vertex graph in 1.5 seconds using the Nvidia 8800GTX GPU costing $600. In some cases optimal sequential algorithm is not the fastest on the GPU architecture. GPUs have great potential as high-performance co-processors."}
{"_id":"30f11d456739d8d83d8cbf240dea46a26bca6509","title":"A power line communication network infrastructure for the smart home","text":"Low voltage electrical wiring in homes has largely been dismissed as too noisy and unpredictable to support high speed communication signals. However, recent advances in communication and modulation methodologies as well as in adaptive digital signal processing and error detection and correction have spawned novel media access control (MAC) and physical layer (PHY) protocols, capable of supporting power line communication networks at speeds comparable to wired local area networks (LANs). In this paper we motivate the use of power line LAN\u2019s as a basic infrastructure for building integrated \u201dsmart homes,\u201d wherein information appliances (IA)\u2014ranging from simple control or monitoring devices to multimedia entertainment systems\u2014are seamlessly interconnected by the very wires which provide them electricity. By simulation and actual measurements using \u201dreference design\u201d prototype commercial powerline products, we show that the HomePlugMAC and PHY layers can guarantee QoS for real-time communications, supporting delay sensitive data streams for \u201dSmart Home\u201d applications."}
{"_id":"6032773345c73957f87178fd5d0556870299c4e1","title":"Learning Deep Boltzmann Machines using Adaptive MCMC","text":"When modeling high-dimensional richly structured data, it is often the case that the distribution defined by the Deep Boltzmann Machine (DBM) has a rough energy landscape with many local minima separated by high energy barriers. The commonly used Gibbs sampler tends to get trapped in one local mode, which often results in unstable learning dynamics and leads to poor parameter estimates. In this paper, we concentrate on learning DBM\u2019s using adaptive MCMC algorithms. We first show a close connection between Fast PCD and adaptive MCMC. We then develop a Coupled Adaptive Simulated Tempering algorithm that can be used to better explore a highly multimodal energy landscape. Finally, we demonstrate that the proposed algorithm considerably improves parameter estimates, particularly when learning large-scale DBM\u2019s."}
{"_id":"6e7dadd63455c194e3472bb181aaf509f89b9166","title":"Classification using discriminative restricted Boltzmann machines","text":"Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting."}
{"_id":"a0117ec4cd582974d06159644d12f65862a8daa3","title":"Deep Belief Networks Are Compact Universal Approximators","text":"Deep belief networks (DBN) are generative models with many layers of hidden causal variables, recently introduced by Hinton, Osindero, and Teh (2006), along with a greedy layer-wise unsupervised learning algorithm. Building on Le Roux and Bengio (2008) and Sutskever and Hinton (2008), we show that deep but narrow generative networks do not require more parameters than shallow ones to achieve universal approximation. Exploiting the proof technique, we prove that deep but narrow feedforward neural networks with sigmoidal units can represent any Boolean expression."}
{"_id":"2fb19f33df18e6975653b7574ab4c897d9b6ba06","title":"Multi-scale retinex for color image enhancement","text":"The retinex is a human perception-based image processing algorithm which provides color constancy and dynamic range compression. We have previously reported on a single-scale retinex (SSR) and shown that it can either achieve color\/lightness rendition or dynamic range compression, but not both simultaneously. We now present a multi-scale retinex (MSR) which overcomes this limitation for most scenes. Both color rendition and dynamic range compression are successfully accomplished except for some \\patho-logical\" scenes that have very strong spectral characteristics in a single band."}
{"_id":"916823fe0525f8db8bf5a6863bcab1c7077ff59e","title":"Properties and performance of a center\/surround retinex","text":"The last version of Land's (1986) retinex model for human vision's lightness and color constancy has been implemented and tested in image processing experiments. Previous research has established the mathematical foundations of Land's retinex but has not subjected his lightness theory to extensive image processing experiments. We have sought to define a practical implementation of the retinex without particular concern for its validity as a model for human lightness and color perception. We describe the trade-off between rendition and dynamic range compression that is governed by the surround space constant. Further, unlike previous results, we find that the placement of the logarithmic function is important and produces best results when placed after the surround formation. Also unlike previous results, we find the best rendition for a \"canonical\" gain\/offset applied after the retinex operation. Various functional forms for the retinex surround are evaluated, and a Gaussian form is found to perform better than the inverse square suggested by Land. Images that violate the gray world assumptions (implicit to this retinex) are investigated to provide insight into cases where this retinex fails to produce a good rendition."}
{"_id":"9fa3c3f1fb6f1566638f97fcb993fe121646433e","title":"Real-time single image dehazing using block-to-pixel interpolation and adaptive dark channel prior","text":""}
{"_id":"2d6d5cfa8e99dd53e50bf870e24e72b0be7f7aeb","title":"Decision Combination in Multiple Classifier Systems","text":"A multiple classifier system is a powerful solution to difficult pattern recognition problems involving large class sets and noisy input because it allows simultaneous use of arbitrary feature descriptors and classification procedures. Decisions by the classifiers can be represented as rankings of classes so that they are comparable across different types of classifiers and different instances of a problem. The rankings can be combined by methods that either reduce or rerank a given set of classes. An intersection method and a union method are proposed for class set reduction. Three methods based on the highest rank, the Borda count, and logistic regression are proposed for class set reranking. These methods have been tested in applications on degraded machine-printed characters and words from large lexicons, resulting in substantial improvement in overall correctness."}
{"_id":"141e6c1dd532504611266d08458dbe2a0dbb4e98","title":"Multiple kernel learning, conic duality, and the SMO algorithm","text":"While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes."}
{"_id":"e0cf9c51192f63c3a9e3b23aa07ee5654fc97b68","title":"Large Margin Methods for Structured and Interdependent Output Variables","text":"Learning general functional dependencies between arbitra ry input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representa tions, this paper addresses the complementary issue of designing classification algorithms that c an deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider pr oblems involving multiple dependent output variables, structured output spaces, and classifica tion problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulat ion. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem i n polynomial time for a large class of problems. The proposed method has important applications i n areas such as computational biology, natural language processing, information retrieval\/extr action, and optical character recognition. Experiments from various domains involving different types o f output spaces emphasize the breadth and generality of our approach."}
{"_id":"0948365ef39ef153e61e9569ade541cf881c7c2a","title":"Learning the Kernel Matrix with Semi-Definite Programming","text":"Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive definite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space\u2014classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semi-definite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm\u2014 using the labelled part of the data one can learn an embedding also for the unlabelled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method to learn the 2-norm soft margin parameter in support vector machines, solving another important open problem. Finally, the novel approach presented in the paper is supported by positive empirical results."}
{"_id":"76d259940fe57f399a992ea0e2bb6cd5304b6a23","title":"Why did my car just do that ? Explaining semi-autonomous driving actions to improve driver understanding , trust , and performance","text":"This study explores, in the context of semiautonomous driving, how the content of the verbalized message accompanying the car\u2019s autonomous action affects the driver\u2019s attitude and safety performance. Using a driving simulator with an auto-braking function, we tested different messages that provided advance explanation of the car\u2019s imminent autonomous action. Messages providing only \u201chow\u201d information describing actions (e.g., \u201cThe car is braking\u201d) led to poor driving performance, whereas \u201cwhy\u201d information describing reasoning for actions (e.g., \u201cObstacle ahead\u201d) was preferred by drivers and led to better driving performance. Providing both \u201chow and why\u201d resulted in the safest driving performance but increased negative feelings in drivers. These results suggest that, to increase overall safety, car makers need to attend not only to the design of autonomous actions but also to the right way to explain these actions to the drivers."}
{"_id":"8eea0da60738a54c0fc6a092aecf0daf0c51cee3","title":"Public opinion on automated driving : Results of an international questionnaire among 5000 respondents","text":"This study investigated user acceptance, concerns, and willingness to buy partially, highly, and fully automated vehicles. By means of a 63-question Internet-based survey, we collected 5000 responses from 109 countries (40 countries with at least 25 respondents). We determined cross-national differences, and assessed correlations with personal variables, such as age, gender, and personality traits as measured with a short version of the Big Five Inventory. Results showed that respondents, on average, found manual driving the most enjoyable mode of driving. Responses were diverse: 22% of the respondents did not want to pay more than $0 for a fully automated driving system, whereas 5% indicated they would be willing to pay more than $30,000, and 33% indicated that fully automated driving would be highly enjoyable. 69% of respondents estimated that fully automated driving will reach a 50% market share between now and 2050. Respondents were found to be most concerned about software hacking\/misuse, and were also concerned about legal issues and safety. Respondents scoring higher on neuroticism were slightly less comfortable about data transmitting, whereas respondents scoring higher on agreeableness were slightly more comfortable with this. Respondents from more developed countries (in terms of lower accident statistics, higher education, and higher income) were less comfortable with their vehicle transmitting data, with cross-national correlations between q = 0.80 and q = 0.90. The present results indicate the major areas of promise and concern among the international public, and could be useful for vehicle developers and other stakeholders. 2015 Elsevier Ltd. All rights reserved."}
{"_id":"eab0415ebbf5a2e163737e34df4d008405c2be5f","title":"Effects of adaptive cruise control and highly automated driving on workload and situation awareness : A review of the empirical evidence","text":"Department of BioMechanical Engineering, Faculty of Mechanical, Maritime and Materials Engineering, Delft University of Technology, Mekelweg 2, 2628 CD Delft, The Netherlands Centre for Transport Studies, University of Twente, Drienerlolaan 5, 7522 NB Enschede, The Netherlands c TNO Human Factors, Kampweg 5, 3769 DE Soesterberg, The Netherlands d Transportation Research Group, Civil, Maritime, Environmental Engineering and Science, Engineering and the Environment, University of Southampton, United Kingdom"}
{"_id":"2160b8f38538320ff6bbd1c35de7b441541b8007","title":"The expansion of Google Scholar versus Web of Science: a longitudinal study","text":"Web of Science (WoS) and Google Scholar (GS) are prominent citation services with distinct indexing mechanisms. Comprehensive knowledge about the growth patterns of these two citation services is lacking. We analyzed the development of citation counts in WoS and GS for two classic articles and 56 articles from diverse research fields, making a distinction between retroactive growth (i.e., the relative difference between citation counts up to mid-2005 measured in mid-2005 and citation counts up to mid-2005 measured in April 2013) and actual growth (i.e., the relative difference between citation counts up to mid-2005 measured in April 2013 and citation counts up to April 2013 measured in April 2013). One of the classic articles was used for a citation-by-citation analysis. Results showed that GS has substantially grown in a retroactive manner (median of 170\u00a0% across articles), especially for articles that initially had low citations counts in GS as compared to WoS. Retroactive growth of WoS was small, with a median of 2\u00a0% across articles. Actual growth percentages were moderately higher for GS than for WoS (medians of 54 vs. 41\u00a0%). The citation-by-citation analysis showed that the percentage of citations being unique in WoS was lower for more recent citations (6.8\u00a0% for citations from 1995 and later vs. 41\u00a0% for citations from before 1995), whereas the opposite was noted for GS (57 vs. 33\u00a0%). It is concluded that, since its inception, GS has shown substantial expansion, and that the majority of recent works indexed in WoS are now also retrievable via GS. A discussion is provided on quantity versus quality of citations, threats for WoS, weaknesses of GS, and implications for literature research and research evaluation."}
{"_id":"2d4f10ccd2503c37ec32aa0033d3e5b3559f4404","title":"Toward a Theory of Situation Awareness in Dynamic Systems","text":"Situational awareness has become an increasingly salient factor contributing to flight safety and operational performance, and the research has burgeoned to cope with the human performance challenges associated with the installation of advanced avionics systems in modern aircraft. The systematic study and application of situational awareness has also extended beyond the cockpit to include air traffic controllers and personnel operating within other complex, high consequence work domains. This volume offers a collection of essays that have made important contributions to situational awareness research and practice. To this end, it provides unique access to key readings that address the conceptual development of situational awareness, methods for its assessment, and applications to enhance situational awareness through training and design."}
{"_id":"48b38420f9c39c601dcf81621609d131b8035f94","title":"Smart Health Monitoring Systems: An Overview of Design and Modeling","text":"Health monitoring systems have rapidly evolved during the past two decades and have the potential to change the way health care is currently delivered. Although smart health monitoring systems automate patient monitoring tasks and, thereby improve the patient workflow management, their efficiency in clinical settings is still debatable. This paper presents a review of smart health monitoring systems and an overview of their design and modeling. Furthermore, a critical analysis of the efficiency, clinical acceptability, strategies and recommendations on improving current health monitoring systems will be presented. The main aim is to review current state of the art monitoring systems and to perform extensive and an in-depth analysis of the findings in the area of smart health monitoring systems. In order to achieve this, over fifty different monitoring systems have been selected, categorized, classified and compared. Finally, major advances in the system design level have been discussed, current issues facing health care providers, as well as the potential challenges to health monitoring field will be identified and compared to other similar systems."}
{"_id":"24e56a1b8e1d4f32efc64a220da19f948a2942e6","title":"Leveraging the Crowd to Detect and Reduce the Spread of Fake News and Misinformation","text":"Online social networking sites are experimenting with the following crowd-powered procedure to reduce the spread of fake news and misinformation: whenever a user is exposed to a story through her feed, she can flag the story as misinformation and, if the story receives enough flags, it is sent to a trusted third party for fact checking. If this party identifies the story as misinformation, it is marked as disputed. However, given the uncertain number of exposures, the high cost of fact checking, and the trade-off between flags and exposures, the above mentioned procedure requires careful reasoning and smart algorithms which, to the best of our knowledge, do not exist to date. In this paper, we first introduce a flexible representation of the above procedure using the framework of marked temporal point processes. Then, we develop a scalable online algorithm, CURB, to select which stories to send for fact checking and when to do so to efficiently reduce the spread of misinformation with provable guarantees. In doing so, we need to solve a novel stochastic optimal control problem for stochastic differential equations with jumps, which is of independent interest. Experiments on two real-world datasets gathered from Twitter and Weibo show that our algorithm may be able to effectively reduce the spread of fake news and misinformation."}
{"_id":"7cfab12f2258c12ef68901b1f966803a575847f2","title":"The wisdom of the few: a collaborative filtering approach based on expert opinions from the web","text":"Nearest-neighbor collaborative filtering provides a successful means of generating recommendations for web users. However, this approach suffers from several shortcomings, including data sparsity and noise, the cold-start problem, and scalability. In this work, we present a novel method for recommending items to users based on expert opinions. Our method is a variation of traditional collaborative filtering: rather than applying a nearest neighbor algorithm to the user-rating data, predictions are computed using a set of expert neighbors from an independent dataset, whose opinions are weighted according to their similarity to the user. This method promises to address some of the weaknesses in traditional collaborative filtering, while maintaining comparable accuracy. We validate our approach by predicting a subset of the Netflix data set. We use ratings crawled from a web portal of expert reviews, measuring results both in terms of prediction accuracy and recommendation list precision. Finally, we explore the ability of our method to generate useful recommendations, by reporting the results of a user-study where users prefer the recommendations generated by our approach."}
{"_id":"94a62f470aeea69af436e2dd0b54cd50eaaa4b23","title":"A Survey of Collaborative Filtering Techniques","text":"As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, modelbased, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area."}
{"_id":"ced981c28215dd218f05ecbba6512671b22d1cc6","title":"Features for Measuring Credibility on Facebook Information","text":"Abstract\u2014Nowadays social media information, such as news, links, images, or VDOs, is shared extensively. However, the effectiveness of disseminating information through social media lacks in quality: less fact checking, more biases, and several rumors. Many researchers have investigated about credibility on Twitter, but there is no the research report about credibility information on Facebook. This paper proposes features for measuring credibility on Facebook information. We developed the system for credibility on Facebook. First, we have developed FB credibility evaluator for measuring credibility of each post by manual human\u2019s labelling. We then collected the training data for creating a model using Support Vector Machine (SVM). Secondly, we developed a chrome extension of FB credibility for Facebook users to evaluate the credibility of each post. Based on the usage analysis of our FB credibility chrome extension, about 81% of users\u2019 responses agree with suggested credibility automatically computed by the proposed system."}
