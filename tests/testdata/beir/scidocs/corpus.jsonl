{"_id":"033b62167e7358c429738092109311af696e9137","title":"Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews","text":"This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \u201csubtle nuances\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201cvery cavalier\u201d). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \u201cexcellent\u201d minus the mutual information between the given phrase and the word \u201cpoor\u201d. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews."}
{"_id":"105a0b3826710356e218685f87b20fe39c64c706","title":"Opinion observer: analyzing and comparing opinions on the Web","text":"The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he\/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him\/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly."}
{"_id":"2ae40898406df0a3732acc54f147c1d377f54e2a","title":"Query by Committee","text":"We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms."}
{"_id":"49e85869fa2cbb31e2fd761951d0cdfa741d95f3","title":"Adaptive Manifold Learning","text":"Manifold learning algorithms seek to find a low-dimensional parameterization of high-dimensional data. They heavily rely on the notion of what can be considered as local, how accurately the manifold can be approximated locally, and, last but not least, how the local structures can be patched together to produce the global parameterization. In this paper, we develop algorithms that address two key issues in manifold learning: 1) the adaptive selection of the local neighborhood sizes when imposing a connectivity structure on the given set of high-dimensional data points and 2) the adaptive bias reduction in the local low-dimensional embedding by accounting for the variations in the curvature of the manifold as well as its interplay with the sampling density of the data set. We demonstrate the effectiveness of our methods for improving the performance of manifold learning algorithms using both synthetic and real-world data sets."}
{"_id":"bf07d60ba6d6c6b8cabab72dfce06f203782df8f","title":"Manifold-Learning-Based Feature Extraction for Classification of Hyperspectral Data: A Review of Advances in Manifold Learning","text":"Advances in hyperspectral sensing provide new capability for characterizing spectral signatures in a wide range of physical and biological systems, while inspiring new methods for extracting information from these data. HSI data often lie on sparse, nonlinear manifolds whose geometric and topological structures can be exploited via manifold-learning techniques. In this article, we focused on demonstrating the opportunities provided by manifold learning for classification of remotely sensed data. However, limitations and opportunities remain both for research and applications. Although these methods have been demonstrated to mitigate the impact of physical effects that affect electromagnetic energy traversing the atmosphere and reflecting from a target, nonlinearities are not always exhibited in the data, particularly at lower spatial resolutions, so users should always evaluate the inherent nonlinearity in the data. Manifold learning is data driven, and as such, results are strongly dependent on the characteristics of the data, and one method will not consistently provide the best results. Nonlinear manifold-learning methods require parameter tuning, although experimental results are typically stable over a range of values, and have higher computational overhead than linear methods, which is particularly relevant for large-scale remote sensing data sets. Opportunities for advancing manifold learning also exist for analysis of hyperspectral and multisource remotely sensed data. Manifolds are assumed to be inherently smooth, an assumption that some data sets may violate, and data often contain classes whose spectra are distinctly different, resulting in multiple manifolds or submanifolds that cannot be readily integrated with a single manifold representation. Developing appropriate characterizations that exploit the unique characteristics of these submanifolds for a particular data set is an open research problem for which hierarchical manifold structures appear to have merit. To date, most work in manifold learning has focused on feature extraction from single images, assuming stationarity across the scene. Research is also needed in joint exploitation of global and local embedding methods in dynamic, multitemporal environments and integration with semisupervised and active learning."}
{"_id":"01996726f44253807537cec68393f1fce6a9cafa","title":"Stochastic Neighbor Embedding","text":"We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional \u201cimages\u201d of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word \u201cbank\u201d, to have versions close to the images of both \u201criver\u201d and \u201cfinance\u201d without forcing the images of outdoor concepts to be located close to those of corporate concepts."}
{"_id":"0e1431fa42d76c44911b07078610d4b9254bd4ce","title":"Nonlinear Component Analysis as a Kernel Eigenvalue Problem","text":"A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."}
{"_id":"40cfac582cafeadb0e09e5f020e2febf5cbd4986","title":"Leveraging graph topology and semantic context for pharmacovigilance through twitter-streams","text":"Adverse drug events (ADEs) constitute one of the leading causes of post-therapeutic death and their identification constitutes an important challenge of modern precision medicine. Unfortunately, the onset and effects of ADEs are often underreported complicating timely intervention. At over 500 million posts per day, Twitter is a commonly used social media platform. The ubiquity of day-to-day personal information exchange on Twitter makes it a promising target for data mining for ADE identification and intervention. Three technical challenges are central to this problem: (1) identification of salient medical keywords in (noisy) tweets, (2) mapping drug-effect relationships, and (3) classification of such relationships as adverse or non-adverse. We use a bipartite graph-theoretic representation called a drug-effect graph (DEG) for modeling drug and side effect relationships by representing the drugs and side effects as vertices. We construct individual DEGs on two data sources. The first DEG is constructed from the drug-effect relationships found in FDA package inserts as recorded in the SIDER database. The second DEG is constructed by mining the history of Twitter users. We use dictionary-based information extraction to identify medically-relevant concepts in tweets. Drugs, along with co-occurring symptoms are connected with edges weighted by temporal distance and frequency. Finally, information from the SIDER DEG is integrate with the Twitter DEG and edges are classified as either adverse or non-adverse using supervised machine learning. We examine both graph-theoretic and semantic features for the classification task. The proposed approach can identify adverse drug effects with high accuracy with precision exceeding 85\u00a0% and F1 exceeding 81\u00a0%. When compared with leading methods at the state-of-the-art, which employ un-enriched graph-theoretic analysis alone, our method leads to improvements ranging between 5 and 8\u00a0% in terms of the aforementioned measures. Additionally, we employ our method to discover several ADEs which, though present in medical literature and Twitter-streams, are not represented in the SIDER databases. We present a DEG integration model as a powerful formalism for the analysis of drug-effect relationships that is general enough to accommodate diverse data sources, yet rigorous enough to provide a strong mechanism for ADE identification."}
{"_id":"292eee24017356768f1f50b72701ea636dba7982","title":"IMPLICIT SHAPE MODELS FOR OBJECT DETECTION IN 3D POINT CLOUDS","text":"We present a method for automatic object localization and recognition in 3D point clouds representing outdoor urban scenes. The method is based on the implicit shape models (ISM) framework, which recognizes objects by voting for their center locations. It requires only few training examples per class, which is an important property for practical use. We also introduce and evaluate an improved version of the spin image descriptor, more robust to point density variation and uncertainty in normal direction estimation. Our experiments reveal a significant impact of these modifications on the recognition performance. We compare our results against the state-of-the-art method and get significant improvement in both precision and recall on the Ohio dataset, consisting of combined aerial and terrestrial LiDAR scans of 150,000 m of urban area in total."}
{"_id":"ffd7ac9b4fff641d461091d5237321f83bae5216","title":"Multi-task Learning for Maritime Traffic Surveillance from AIS Data Streams","text":"In a world of global trading, maritime safety, security and efficiency are crucial issues. We propose a multi-task deep learning framework for vessel monitoring using Automatic Identification System (AIS) data streams. We combine recurrent neural networks with latent variable modeling and an embedding of AIS messages to a new representation space to jointly address key issues to be dealt with when considering AIS data streams: massive amount of streaming data, noisy data and irregular timesampling. We demonstrate the relevance of the proposed deep learning framework on real AIS datasets for a three-task setting, namely trajectory reconstruction, anomaly detection and vessel type identification."}
{"_id":"6385cd92746386c82a69ffdc3bc0a9da9f64f721","title":"The Dysphagia Outcome and Severity Scale","text":"The Dysphagia Outcome and Severity Scale (DOSS) is a simple, easy-to-use, 7-point scale developed to systematically rate the functional severity of dysphagia based on objective assessment and make recommendations for diet level, independence level, and type of nutrition. Intra- and interjudge reliabilities of the DOSS was established by four clinicians on 135 consecutive patients who underwent a modified barium swallow procedure at a large teaching hospital. Patients were assigned a severity level, independence level, and nutritional level based on three areas most associated with final recommendations: oral stage bolus transfer, pharyngeal stage retention, and airway protection. Results indicate high interrater (90%) and intrarater (93%) agreement with this scale. Implications are suggested for use of the DOSS in documenting functional outcomes of swallowing and diet status based on objective assessment."}
{"_id":"1d18fba47004a4cf2643c41ca82f6b04904bb134","title":"Depth Map Super-Resolution Considering View Synthesis Quality","text":"Accurate and high-quality depth maps are required in lots of 3D applications, such as multi-view rendering, 3D reconstruction and 3DTV. However, the resolution of captured depth image is much lower than that of its corresponding color image, which affects its application performance. In this paper, we propose a novel depth map super-resolution (SR) method by taking view synthesis quality into account. The proposed approach mainly includes two technical contributions. First, since the captured low-resolution (LR) depth map may be corrupted by noise and occlusion, we propose a credibility based multi-view depth maps fusion strategy, which considers the view synthesis quality and interview correlation, to refine the LR depth map. Second, we propose a view synthesis quality based trilateral depth-map up-sampling method, which considers depth smoothness, texture similarity and view synthesis quality in the up-sampling filter. Experimental results demonstrate that the proposed method outperforms state-of-the-art depth SR methods for both super-resolved depth maps and synthesized views. Furthermore, the proposed method is robust to noise and achieves promising results under noise-corruption conditions."}
{"_id":"922b5eaa5ca03b12d9842b7b84e0e420ccd2feee","title":"A New Approach to Linear Filtering and Prediction Problems","text":"AN IMPORTANT class of theoretical and practical problems in communication and control is of a statistical nature. Such problems are: (i) Prediction of random signals; (ii) separation of random signals from random noise; (iii) detection of signals of known form (pulses, sinusoids) in the presence of random noise. In his pioneering work, Wiener [1]3 showed that problems (i) and (ii) lead to the so-called Wiener-Hopf integral equation; he also gave a method (spectral factorization) for the solution of this integral equation in the practically important special case of stationary statistics and rational spectra. Many extensions and generalizations followed Wiener\u2019s basic work. Zadeh and Ragazzini solved the finite-memory case [2]. Concurrently and independently of Bode and Shannon [3], they also gave a simplified method [2) of solution. Booton discussed the nonstationary Wiener-Hopf equation [4]. These results are now in standard texts [5-6]. A somewhat different approach along these main lines has been given recently by Darlington [7]. For extensions to sampled signals, see, e.g., Franklin [8], Lees [9]. Another approach based on the eigenfunctions of the WienerHopf equation (which applies also to nonstationary problems whereas the preceding methods in general don\u2019t), has been pioneered by Davis [10] and applied by many others, e.g., Shinbrot [11], Blum [12], Pugachev [13], Solodovnikov [14]. In all these works, the objective is to obtain the specification of a linear dynamic system (Wiener filter) which accomplishes the prediction, separation, or detection of a random signal.4 \u2014\u2014\u2014 1 This research was supported in part by the U. S. Air Force Office of Scientific Research under Contract AF 49 (638)-382. 2 7212 Bellona Ave. 3 Numbers in brackets designate References at end of paper. 4 Of course, in general these tasks may be done better by nonlinear filters. At present, however, little or nothing is known about how to obtain (both theoretically and practically) these nonlinear filters. Contributed by the Instruments and Regulators Division and presented at the Instruments and Regulators Conference, March 29\u2013 Apri1 2, 1959, of THE AMERICAN SOCIETY OF MECHANICAL ENGINEERS. NOTE: Statements and opinions advanced in papers are to be understood as individual expressions of their authors and not those of the Society. Manuscript received at ASME Headquarters, February 24, 1959. Paper No. 59-IRD\u201411. A New Approach to Linear Filtering and Prediction Problems"}
{"_id":"9ee859bec32fa8240de0273faff6f20e16e21cc6","title":"Detection of exudates in fundus photographs using convolutional neural networks","text":"Diabetic retinopathy is one of the leading causes of preventable blindness in the developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into screening programs and especially into automated screening programs. Detection of exudates is very important for early diagnosis of diabetic retinopathy. Deep neural networks have proven to be a very promising machine learning technique, and have shown excellent results in different compute vision problems. In this paper we show that convolutional neural networks can be effectively used in order to detect exudates in color fundus photographs."}
{"_id":"13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986","title":"Random Forests","text":"Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression."}
{"_id":"aeb7eaf29e16c82d9c0038a10d5b12d32b26ab60","title":"Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic Speech Recognition","text":"Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM\/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM\/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AVASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7% relative improvement in WER is reported at -3 SNR dB1."}
{"_id":"45fe9fac928c9e64c96b5feda318a09f1b0228dd","title":"Ethnicity estimation with facial images","text":"We have advanced an effort to develop vision based human understanding technologies for realizing human-friendly machine interfaces. Visual information, such as gender, age ethnicity, and facial expression play an important role in face-to-face communication. This paper addresses a novel approach for ethnicity classification with facial images. In this approach, the Gabor wavelets transformation and retina sampling are combined to extract key facial features, and support vector machines that are used for ethnicity classification. Our system, based on this approach, has achieved approximately 94% for ethnicity estimation under various lighting conditions."}
{"_id":"2dbaedea8ac09b11d9da8767eb73b6b821890661","title":"Bullying and victimization in adolescence: concurrent and stable roles and psychological health symptoms.","text":"From an initial sample of 1278 Italian students, the authors selected 537 on the basis of their responses to a self-report bully and victim questionnaire. Participants' ages ranged from 13 to 20 years (M = 15.12 years, SD = 1.08 years). The authors compared the concurrent psychological symptoms of 4 participant groups (bullies, victims, bully\/victims [i.e., bullies who were also victims of bullying], and uninvolved students). Of participants, 157 were in the bullies group, 140 were in the victims group, 81 were in the bully\/victims group, and 159 were in the uninvolved students group. The results show that bullies reported a higher level of externalizing problems, victims reported more internalizing symptoms, and bully\/victims reported both a higher level of externalizing problems and more internalizing symptoms. The authors divided the sample into 8 groups on the basis of the students' recollection of their earlier school experiences and of their present role. The authors classified the participants as stable versus late bullies, victims, bully\/victims, or uninvolved students. The authors compared each stable group with its corresponding late group and found that stable victims and stable bully\/victims reported higher degrees of anxiety, depression, and withdrawal than did the other groups. The authors focus their discussion on the role of chronic peer difficulties in relation to adolescents' symptoms and well-being."}
{"_id":"d26ce29a109f8ccb42d7b0d312c70a6a750018ce","title":"Side gate HiGT with low dv\/dt noise and low loss","text":"This paper presents a novel side gate HiGT (High-conductivity IGBT) that incorporates historical changes of gate structures for planar and trench gate IGBTs. Side gate HiGT has a side-wall gate, and the opposite side of channel region for side-wall gate is covered by a thick oxide layer to reduce Miller capacitance (Cres). In addition, side gate HiGT has no floating p-layer, which causes the excess Vge overshoot. The proposed side gate HiGT has 75% smaller Cres than the conventional trench gate IGBT. The excess Vge overshoot during turn-on is effectively suppressed, and Eon + Err can be reduced by 34% at the same diode's recovery dv\/dt. Furthermore, side gate HiGT has sufficiently rugged RBSOA and SCSOA."}
{"_id":"0db78d548f914135ad16c0d6618890de52c0c065","title":"Incremental Dialogue System Faster than and Preferred to its Nonincremental Counterpart","text":"Current dialogue systems generally operate in a pipelined, modular fashion on one complete utterance at a time. Evidence from human language understanding shows that human understanding operates incrementally and makes use of multiple sources of information during the parsing process, including traditionally \u201clater\u201d components such as pragmatics. In this paper we describe a spoken dialogue system that understands language incrementally, provides visual feedback on possible referents during the course of the user\u2019s utterance, and allows for overlapping speech and actions. We further present findings from an empirical study showing that the resulting dialogue system is faster overall than its nonincremental counterpart. Furthermore, the incremental system is preferred to its nonincremental counterpart \u2013 beyond what is accounted for by factors such as speed and accuracy. These results indicate that successful incremental understanding systems will improve both performance and usability."}
{"_id":"eec44862b2d58434ca7706224bc0e9437a2bc791","title":"The balanced scorecard: a foundation for the strategic management of information systems","text":"\u017d . The balanced scorecard BSC has emerged as a decision support tool at the strategic management level. Many business leaders now evaluate corporate performance by supplementing financial accounting data with goal-related measures from the following perspectives: customer, internal business process, and learning and growth. It is argued that the BSC concept can be adapted to assist those managing business functions, organizational units and individual projects. This article develops a \u017d . balanced scorecard for information systems IS that measures and evaluates IS activities from the following perspectives: business value, user orientation, internal process, and future readiness. Case study evidence suggests that a balanced IS scorecard can be the foundation for a strategic IS management system provided that certain development guidelines are followed, appropriate metrics are identified, and key implementation obstacles are overcome. q 1999 Elsevier Science B.V. All rights reserved."}
{"_id":"4828a00f623651a9780b945980530fb6b3cb199a","title":"Adversarial Machine Learning at Scale","text":"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model\u2019s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \u201clabel leaking\u201d effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process."}
{"_id":"f7d3b7986255f2e5e2402e84d7d7c5e583d7cb05","title":"A Combined Model- and Learning-Based Framework for Interaction-Aware Maneuver Prediction","text":"This paper presents a novel online-capable interaction-aware intention and maneuver prediction framework for dynamic environments. The main contribution is the combination of model-based interaction-aware intention estimation with maneuver-based motion prediction based on supervised learning. The advantages of this framework are twofold. On one hand, expert knowledge in the form of heuristics is integrated, which simplifies the modeling of the interaction. On the other hand, the difficulties associated with the scalability and data sparsity of the algorithm due to the so-called curse of dimensionality can be reduced, as a reduced feature space is sufficient for supervised learning. The proposed algorithm can be used for highly automated driving or as a prediction module for advanced driver assistance systems without the need of intervehicle communication. At the start of the algorithm, the motion intention of each driver in a traffic scene is predicted in an iterative manner using the game-theoretic idea of stochastic multiagent simulation. This approach provides an interpretation of what other drivers intend to do and how they interact with surrounding traffic. By incorporating this information into a Bayesian network classifier, the developed framework achieves a significant improvement in terms of reliable prediction time and precision compared with other state-of-the-art approaches. By means of experimental results in real traffic on highways, the validity of the proposed concept and its online capability is demonstrated. Furthermore, its performance is quantitatively evaluated using appropriate statistical measures."}
{"_id":"7616624dd230c42f6397a9a48094cf4611c02ab8","title":"Frequency Tracking Control for a Capacitor-Charging Parallel Resonant Converter with Phase-Locked Loop","text":"This study investigates a phase-locked loop (PLL) controlled parallel resonant converter (PRC) for a pulse power capacitor charging application. The dynamic nature of the capacitor charging is such that it causes a shift in the resonant frequency of the PRC. Using the proposed control method, the PRC can be optimized to operate with its maximum power capability and guarantee ZVS operation, even when the input voltage and resonant tank parameters vary. The detailed implementation of the PLL controller, as well as the determination of dead-time and leading time, is presented in this paper. Simulation and experimental results verify the performance of the proposed control method."}
{"_id":"4902805fe1e2f292f6beed7593154e686d7f6dc2","title":"A Novel Connectionist System for Unconstrained Handwriting Recognition","text":"Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance."}
{"_id":"e48df18774fbaff8b70b0231a02c3ccf1ebdf784","title":"Enhancing computer vision to detect face spoofing attack utilizing a single frame from a replay video attack using deep learning","text":"Recently, automatic face recognition has been applied in many web and mobile applications. Developers integrate and implement face recognition as an access control into these applications. However, face recognition authentication is vulnerable to several attacks especially when an attacker presents a 2-D printed image or recorded video frames in front of the face sensor system to gain access as a legitimate user. This paper introduces a non-intrusive method to detect face spoofing attacks that utilize a single frame of sequenced frames. We propose a specialized deep convolution neural network to extract complex and high features of the input diffused frame. We tested our method on the Replay Attack dataset which consists of 1200 short videos of both real-access and spoofing attacks. An extensive experimental analysis was conducted that demonstrated better results when compared to previous static algorithms results."}
{"_id":"1663f1b811c0ea542c1d128ff129cdf5cd7f9c44","title":"ISAR - radar imaging of targets with complicated motion","text":"ISAR imaging is described for general motion of a radar target. ISAR imaging may be seen as a 3D to 2D projection, and the importance of the ISAR image projection plane is stated. For general motion, ISAR images are often smeared when using FFT processing. Time frequency methods are used to analyze such images, and to form sharp images. A given smeared image is shown to be the result of changes both in scale and in the projection plane orientation."}
{"_id":"792e9c588e3426ec55f630fffefa439fc17e0406","title":"Closing the Loop: Evaluating a Measurement Instrument for Maturity Model Design","text":"To support the systematic improvement of business intelligence (BI) in organizations, we have designed and refined a BI maturity model (BIMM) and a respective measurement instrument (MI) in prior research. In this study, we devise an evaluation strategy, and evaluate the validity of the designed measurement artifact. Through cluster analysis of maturity assessments of 92 organizations, we identify characteristic BI maturity scenarios and representative cases for the relevant scenarios. For evaluating the designed instrument, we compare its results with insights obtained from in-depth interviews in the respective companies. A close match between our model's quantitative maturity assessments and the maturity levels from the qualitative analyses indicates that the MI correctly assesses BI maturity. The applied evaluation approach has the potential to be re-used in other design research studies where the validity of utility claims is often hard to prove."}
{"_id":"2907cde029f349948680a3690500d4cf09b5be96","title":"An architecture for scalable, universal speech recognition","text":"This thesis describes MultiSphinx, a concurrent architecture for scalable, low-latency automatic speech recognition. We first consider the problem of constructing a universal \u201ccore\u201d speech recognizer on top of which domain and task specific adaptation layers can be constructed. We then show that when this problem is restricted to that of expanding the search space from a \u201ccore\u201d vocabulary to a superset of this vocabulary across multiple passes of search, it allows us to effectively \u201cfactor\u201d a recognizer into components of roughly equal complexity. We present simple but effective algorithms for constructing the reduced vocabulary and associated statistical language model from an existing system. Finally, we describe the MultiSphinx decoder architecture, which allows multiple passes of recognition to operate concurrently and incrementally, either in multiple threads in the same process, or across multiple processes on separate machines, and which allows the best possible partial results, including confidence scores, to be obtained at any time during the recognition process."}
{"_id":"86436e9d0c98e7133c7d00d8875bcf0720ad3882","title":"' SMART \u2019 CANE FOR THE VISUALLY IMPAIRED : DESIGN AND CONTROLLED FIELD TESTING OF AN AFFORDABLE OBSTACLE DETECTION SYSTEM","text":"(USA) respectively. Since graduation, they have been associated with this project in voluntary and individual capacity. Sandeep Singh Gujral, an occupational therapist was an intern at the IIT Delhi from August-November 2009 and conducted user training for the trials."}
{"_id":"ee548bd6b96cf1d748def335c1517c2deea1b3f5","title":"Forecasting Nike's sales using Facebook data","text":"This paper tests whether accurate sales forecasts for Nike are possible from Facebook data and how events related to Nike affect the activity on Nike's Facebook pages. The paper draws from the AIDA sales framework (Awareness, Interest, Desire, and Action) from the domain of marketing and employs the method of social set analysis from the domain of computational social science to model sales from Big Social Data. The dataset consists of (a) selection of Nike's Facebook pages with the number of likes, comments, posts etc. that have been registered for each page per day and (b) business data in terms of quarterly global sales figures published in Nike's financial reports. An event study is also conducted using the Social Set Visualizer (SoSeVi). The findings suggest that Facebook data does have informational value. Some of the simple regression models have a high forecasting accuracy. The multiple regressions have a lower forecasting accuracy and cause analysis barriers due to data set characteristics such as perfect multicollinearity. The event study found abnormal activity around several Nike specific events but inferences about those activity spikes, whether they are purely event-related or coincidences, can only be determined after detailed case-by-case text analysis. Our findings help assess the informational value of Big Social Data for a company's marketing strategy, sales operations and supply chain."}
{"_id":"dcbebb8fbd3ebef2816ebe0f7da12340a5725a8b","title":"Wiktionary-Based Word Embeddings","text":"Vectorial representations of words have grown remarkably popular in natural language processing and machine translation. The recent surge in deep learning-inspired methods for producing distributed representations has been widely noted even outside these fields. Existing representations are typically trained on large monolingual corpora using context-based prediction models. In this paper, we propose extending pre-existing word representations by exploiting Wiktionary. This process results in a substantial extension of the original word vector representations, yielding a large multilingual dictionary of word embeddings. We believe that this resource can enable numerous monolingual and cross-lingual applications, as evidenced in a series of monolingual and cross-lingual semantic experiments that we have conducted."}
{"_id":"10d710c01acb10c4aea702926d21697935656c3d","title":"Infrared Colorization Using Deep Convolutional Neural Networks","text":"This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR\/RGB camera, which ensures a perfect pixel to pixel registration."}
{"_id":"325d145af5f38943e469da6369ab26883a3fd69e","title":"Colorful Image Colorization","text":"Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a \u201ccolorization Turing test,\u201d asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks."}
{"_id":"326a0914dcdf7f42b5e1c2887174476728ca1b9d","title":"Wasserstein GAN","text":"The problem this paper is concerned with is that of unsupervised learning. Mainly, what does it mean to learn a probability distribution? The classical answer to this is to learn a probability density. This is often done by defining a parametric family of densities (P\u03b8)\u03b8\u2208Rd and finding the one that maximized the likelihood on our data: if we have real data examples {x}i=1, we would solve the problem"}
{"_id":"5287d8fef49b80b8d500583c07e935c7f9798933","title":"Generative Adversarial Text to Image Synthesis","text":"Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions."}
{"_id":"57bbbfea63019a57ef658a27622c357978400a50","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation","text":""}
{"_id":"6ec02fb5bfc307911c26741fb3804f16d8ad299c","title":"Active learning for on-road vehicle detection: a comparative study","text":"In recent years, active learning has emerged as a powerful tool in building robust systems for object detection using computer vision. Indeed, active learning approaches to on-road vehicle detection have achieved impressive results. While active learning approaches for object detection have been explored and presented in the literature, few studies have been performed to comparatively assess costs and merits. In this study, we provide a cost-sensitive analysis of three popular active learning methods for on-road vehicle detection. The generality of active learning findings is demonstrated via learning experiments performed with detectors based on histogram of oriented gradient features and SVM classification (HOG\u2013SVM), and Haar-like features and Adaboost classification (Haar\u2013Adaboost). Experimental evaluation has been performed on static images and real-world on-road vehicle datasets. Learning approaches are assessed in terms of the time spent annotating, data required, recall, and precision."}
{"_id":"c2b6755543c3f7c71adb3e14eb06179f27b6ad5d","title":"HyFlex nickel-titanium rotary instruments after clinical use: metallurgical properties.","text":"AIM\nTo analyse the type and location of defects in HyFlex CM instruments after clinical use in a graduate endodontic programme and to examine the impact of clinical use on their metallurgical properties.\n\n\nMETHODOLOGY\nA total of 468 HyFlex CM instruments discarded from a graduate endodontic programme were collected after use in three teeth. The incidence and type of instrument defects were analysed. The lateral surfaces of the defect instruments were examined by scanning electron microscopy. New and clinically used instruments were examined by differential scanning calorimetry (DSC) and x-ray diffraction (XRD). Vickers hardness was measured with a 200-g load near the flutes for new and clinically used axially sectioned instruments. Data were analysed using one-way anova or Tukey's multiple comparison test.\n\n\nRESULTS\nOf the 468 HyFlex instruments collected, no fractures were observed and 16 (3.4%) revealed deformation. Of all the unwound instruments, size 20, .04 taper unwound the most often (n\u00a0=\u00a05) followed by size 25, .08 taper (n\u00a0=\u00a04). The trend of DSC plots of new instruments and clinically used (with and without defects) instruments groups were very similar. The DSC analyses showed that HyFlex instruments had an austenite transformation completion or austenite-finish (Af ) temperature exceeding 37\u00a0\u00b0C. The Af temperatures of HyFlex instruments (with or without defects) after multiple clinical use were much lower than in new instruments (P\u00a0<\u00a00.05). The enthalpy values for the transformation from martensitic to austenitic on deformed instruments were smaller than in the new instruments at the tip region (P\u00a0<\u00a00.05). XRD results showed that NiTi instruments had austenite and martensite structure on both new and used HyFlex instruments at room temperature. No significant difference in microhardness was detected amongst new and used instruments (with and without defects).\n\n\nCONCLUSIONS\nThe risk of HyFlex instruments fracture in the canal is very low when instruments are discarded after three cases of clinical use. New HyFlex instruments were a mixture of martensite and austenite structure at body temperature. Multiple clinical use caused significant changes in the microstructural properties of HyFlex instruments. Smaller instruments should be considered as single-use."}
{"_id":"a25f6d05c8191be01f736073fa2bc20c03ad7ad8","title":"Integrated control of a multi-fingered hand and arm using proximity sensors on the fingertips","text":"In this study, we propose integrated control of a robotic hand and arm using only proximity sensing from the fingertips. An integrated control scheme for the fingers and for the arm enables quick control of the position and posture of the arm by placing the fingertips adjacent to the surface of an object to be grasped. The arm control scheme enables adjustments based on errors in hand position and posture that would be impossible to achieve by finger motions alone, thus allowing the fingers to grasp an object in a laterally symmetric grasp. This can prevent grasp failures such as a finger pushing the object out of the hand or knocking the object over. Proposed control of the arm and hand allowed correction of position errors on the order of several centimeters. For example, an object on a workbench that is in an uncertain positional relation with the robot, with an inexpensive optical sensor such as a Kinect, which only provides coarse image data, would be sufficient for grasping an object."}
{"_id":"13317a497f4dc5f62a15dbdc135dd3ea293474df","title":"Do Multi-Sense Embeddings Improve Natural Language Understanding?","text":"Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while \u2018multi-sense\u2019 methods have been proposed and tested on artificial wordsimilarity tasks, we don\u2019t know if they improve real natural language understanding tasks. In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language un-"}
{"_id":"7ec6b06b0f421b80ca25994c7aa106106c7bfb50","title":"Design and Simulation of Bridgeless PFC Boost Rectifiers","text":"This work presents new three-level unidirectional single-phase PFC rectifier topologies well-suited for applications targeting high efficiency and\/or high power density. The characteristics of a selected novel rectifier topology, including its principles of operation, modulation strategy, PID control scheme, and a power circuit design related analysis are presented. Finally, a 220-V\/3-kW laboratory prototype is constructed and used in order to verify the characteristics of the new converter, which include remarkably low switching losses and single ac-side boost inductor, that allow for a 98.6% peak efficiency with a switching frequency of 140 kHz."}
{"_id":"c62ba57869099f20c8bcefd9b38ce5d8b4b3db56","title":"Computational models of trust and reputation: agents, evolutionary games, and social networks","text":"Many recent studies of trust and reputation are made in the context of commercial reputation or rating systems for online communities. Most of these systems have been constructed without a formal rating model or much regard for our sociological understanding of these concepts. We first provide a critical overview of the state of research on trust and reputation. We then propose a formal quantitative model for the rating process. Based on this model, we formulate two personalized rating schemes and demonstrate their effectiveness at inferring trust experimentally using a simulated dataset and a real world movie-rating dataset. Our experiments show that the popular global rating scheme widely used in commercial electronic communities is inferior to our personalized rating schemes when sufficient ratings among members are available. The level of sufficiency is then discussed. In comparison with other models of reputation, we quantitatively show that our framework provides significantly better estimations of reputation. \"Better\" is discussed with respect to a rating process and specific games as defined in this work. Secondly, we propose a mathematical framework for modeling trust and reputation that is rooted in findings from the social sciences. In particular, our framework makes explicit the importance of social information (i.e., indirect channels of inference) in aiding members of a social network choose whom they want to partner with or to avoid. Rating systems that make use of such indirect channels of inference are necessarily personalized in nature, catering to the individual context of the rater. Finally, we have extended our trust and reputation framework toward addressing a fundamental problem for social science and biology: evolution of cooperation. We show that by providing an indirect inference mechanism for the propagation of trust and reputation, cooperation among selfish agents can be explained for a set of game theoretic simulations. For these simulations in particular, our proposal is shown to have provided more cooperative agent communities than existing schemes are able to. Thesis Supervisor: Peter Szolovits Title: Professor of Electrical Engineering and Computer Science"}
{"_id":"61df37b2c1f731e2b6bcb1ae2c2b7670b917284c","title":"Surface Management System Field Trial Results","text":"NASA Ames Research Center, in cooperation with the FAA, has completed research and development of a proof-ofconcept Surface Management System (SMS). This paper reports on two recent SMS field tests as well as final performance and benefits analyses. Field tests and analysis support the conclusion that substantial portions of SMS technology are ready for transfer to the FAA and deployment throughout the National Airspace System (NAS). Other SMS capabilities were accepted in concept but require additional refinement for inclusion in subsequent development spirals. SMS is a decision support tool that helps operational specialists at Air Traffic Control (ATC) and NAS user facilities to collaboratively manage the movements of aircraft on the surface of busy airports, thereby improving capacity, efficiency, and flexibility. SMS provides accurate predictions of the future demand and how that demand will affect airport resources \u2013 information that is not currently available. The resulting shared awareness enables the Air Traffic Control Tower (ATCT), Terminal Radar Approach Control (TRACON), Air Route Traffic Control Center (ARTCC), and air carriers to coordinate traffic management decisions. Furthermore, SMS uses its ability to predict how future demand will play out on the surface to evaluate the effect of various traffic management decisions in advance of implementing them, to plan and advise surface operations. The SMS concept, displays, and algorithms were evaluated through a series of field tests at Memphis International Airport (MEM). An operational trial in September, 2003 evaluated SMS traffic management components, such as runway configuration change planning; shadow testing in January, 2004 tested tactical components (e.g., Approval Request (APREQ) coordination, sequencing for departure, and Expected Departure Clearance Time (EDCT) compliance). Participants in these evaluations rated the SMS concept and many of the traffic management displays very positively. Local and Ground controller displays will require integration with other automation systems. Feedback from FAA and NAS user participants support the conclusion that SMS algorithms currently provide information that has acceptable and beneficial accuracy for traffic management applications. Performance analysis results document the current accuracy of SMS algorithms. Benefits\/cost analysis of delay cost reduction due to SMS provides the business case for SMS deployment."}
{"_id":"dcff311940942dcf81db5073e551a87e1710e52a","title":"Recognizing Malicious Intention in an Intrusion Detection Process","text":"Generally, theintrudermustperformseveralactions,organizedin anintrusionscenario, to achieve hisor hermaliciousobjective.Wearguethatintrusionscenarioscan bemodelledasa planningprocessandwesuggestmodellinga maliciousobjectiveas anattemptto violatea givensecurityrequirement. Our proposalis thento extendthe definitionof attackcorrelationpresentedin [CM02] to correlateattackswith intrusion objectivesThis notionis usefulto decideif a sequenceof correlatedactionscanlead to a securityrequirementviolation.This approachprovidesthesecurityadministrator with aglobalview of whathappensin thesystem.In particular, it controlsunobserved actionsthroughhypothesisgeneration,clustersrepeatedactionsin a singlescenario, recognizesintrudersthatarechangingtheir intrusionobjectivesandis efficient to detectvariationsof anintrusionscenario.Thisapproachcanalsobeusedto eliminatea category of falsepositivesthatcorrespondto falseattacks,that is actionsthatarenot furthercorrelatedto anintrusionobjective."}
{"_id":"7ffdf4d92b4bc5690249ed98e51e1699f39d0e71","title":"Reconfigurable RF MEMS Phased Array Antenna Integrated Within a Liquid Crystal Polymer (LCP) System-on-Package","text":"For the first time, a fully integrated phased array antenna with radio frequency microelectromechanical systems (RF MEMS) switches on a flexible, organic substrate is demonstrated above 10 GHz. A low noise amplifier (LNA), MEMS phase shifter, and 2 times 2 patch antenna array are integrated into a system-on-package (SOP) on a liquid crystal polymer substrate. Two antenna arrays are compared; one implemented using a single-layer SOP and the second with a multilayer SOP. Both implementations are low-loss and capable of 12deg of beam steering. The design frequency is 14 GHz and the measured return loss is greater than 12 dB for both implementations. The use of an LNA allows for a much higher radiated power level. These antennas can be customized to meet almost any size, frequency, and performance needed. This research furthers the state-of-the-art for organic SOP devices."}
{"_id":"3aa41f8fdb6a4523e2cd95365bb6c7499ad29708","title":"iCanTrace : Avatar Personalization through Selfie Sketches","text":"This paper introduces a novel system that allows users to generate customized cartoon avatars through a sketching interface. The rise of social media and personalized gaming has given a need for personalized virtual appearances. Avatars, self-curated and customized images to represent oneself, have become a common means of expressing oneself in these new media. Avatar creation platforms face the challenge of granting user significant control over the avatar creation, and the challenge of encumbering the user with too many choices in their avatar customization. This paper demonstrates a sketch-guided avatar customization system and its potential to simplify the avatar creation process. Author"}
{"_id":"ba02b6125ba47ff3629f1d09d1bada28169c2b32","title":"Teaching Syntax by Adversarial Distraction","text":"Existing entailment datasets mainly pose problems which can be answered without attention to grammar or word order. Learning syntax requires comparing examples where different grammar and word order change the desired classification. We introduce several datasets based on synthetic transformations of natural entailment examples in SNLI or FEVER, to teach aspects of grammar and word order. We show that without retraining, popular entailment models are unaware that these syntactic differences change meaning. With retraining, some but not all popular entailment models can learn to compare the syntax properly."}
{"_id":"cb25c33ba56db92b7da4d5080f73fba07cb914a3","title":"A large-stroke flexure fast tool servo with new displacement amplifier","text":"As the rapid progress of science and technology, the free-form surface optical component has played an important role in spaceflight, aviation, national defense, and other areas of the technology. While the technology of fast tool servo (FTS) is the most promising method for the machining of free-form surface optical component. However, the shortcomings of short-stroke of fast tool servo device have constrained the development of free-form surface optical component. To address this problem, a new large-stroke flexible FTS device is proposed in this paper. A series of mechanism modeling and optimal designs are carried out via compliance matrix theory, pseudo-rigid body theory, and Particle Swarm Optimization (PSO) algorithm, respectively. The mechanism performance of the large-stroke FTS device is verified by the Finite Element Analysis (FEA) method. For this study, a piezoelectric (PZT) actuator P-840.60 that can travel to 90 \u00b5m under open-loop control is employed, the results of experiment indicate that the maximum of output displacement can achieve 258.3\u00b5m, and the bandwidth can achieve around 316.84 Hz. Both theoretical analysis and the test results of prototype uniformly verify that the presented FTS device can meet the demand of the actual microstructure processing."}
{"_id":"3d4bae33c2ccc0a6597f80e27cbeed64990b95bd","title":"Mindfulness practice leads to increases in regional brain gray matter density","text":"Therapeutic interventions that incorporate training in mindfulness meditation have become increasingly popular, but to date little is known about neural mechanisms associated with these interventions. Mindfulness-Based Stress Reduction (MBSR), one of the most widely used mindfulness training programs, has been reported to produce positive effects on psychological well-being and to ameliorate symptoms of a number of disorders. Here, we report a controlled longitudinal study to investigate pre-post changes in brain gray matter concentration attributable to participation in an MBSR program. Anatomical magnetic resonance (MR) images from 16 healthy, meditation-na\u00efve participants were obtained before and after they underwent the 8-week program. Changes in gray matter concentration were investigated using voxel-based morphometry, and compared with a waiting list control group of 17 individuals. Analyses in a priori regions of interest confirmed increases in gray matter concentration within the left hippocampus. Whole brain analyses identified increases in the posterior cingulate cortex, the temporo-parietal junction, and the cerebellum in the MBSR group compared with the controls. The results suggest that participation in MBSR is associated with changes in gray matter concentration in brain regions involved in learning and memory processes, emotion regulation, self-referential processing, and perspective taking."}
{"_id":"217af49622a4e51b6d1b9b6c75726eaf1355a903","title":"Animating pictures with stochastic motion textures","text":"In this paper, we explore the problem of enhancing still pictures with subtly animated motions. We limit our domain to scenes containing passive elements that respond to natural forces in some fashion. We use a semi-automatic approach, in which a human user segments the scene into a series of layers to be individually animated. Then, a \"stochastic motion texture\" is automatically synthesized using a spectral method, i.e., the inverse Fourier transform of a filtered noise spectrum. The motion texture is a time-varying 2D displacement map, which is applied to each layer. The resulting warped layers are then recomposited to form the animated frames. The result is a looping video texture created from a single still image, which has the advantages of being more controllable and of generally higher image quality and resolution than a video texture created from a video source. We demonstrate the technique on a variety of photographs and paintings."}
{"_id":"10d79507f0f2e2d2968bf3a962e1daffc8bd44f0","title":"Modeling the statistical time and angle of arrival characteristics of an indoor multipath channel","text":"Most previously proposed statistical models for the indoor multipath channel include only time of arrival characteristics. However, in order to use statistical models in simulating or analyzing the performance of systems employing spatial diversity combining, information about angle of arrival statistics is also required. Ideally, it would be desirable to characterize the full spare-time nature of the channel. In this paper, a system is described that was used to collect simultaneous time and angle of arrival data at 7 GHz. Data processing methods are outlined, and results obtained from data taken in two different buildings are presented. Based on the results, a model is proposed that employs the clustered \"double Poisson\" time-of-arrival model proposed by Saleh and Valenzuela (1987). The observed angular distribution is also clustered with uniformly distributed clusters and arrivals within clusters that have a Laplacian distribution."}
{"_id":"d00ef607a10e5be00a9e05504ab9771c0b05d4ea","title":"Analysis and Comparison of a Fast Turn-On Series IGBT Stack and High-Voltage-Rated Commercial IGBTS","text":"High-voltage-rated solid-state switches such as insulated-gate bipolar transistors (IGBTs) are commercially available up to 6.5 kV. Such voltage ratings are attractive for pulsed power and high-voltage switch-mode converter applications. However, as the IGBT voltage ratings increase, the rate of current rise and fall are generally reduced. This tradeoff is difficult to avoid as IGBTs must maintain a low resistance in the epitaxial or drift region layer. For high-voltage-rated IGBTs with thick drift regions to support the reverse voltage, the required high carrier concentrations are injected at turn on and removed at turn off, which slows the switching speed. An option for faster switching is to series multiple, lower voltage-rated IGBTs. An IGBT-stack prototype with six, 1200 V rated IGBTs in series has been experimentally tested. The six-series IGBT stack consists of individual, optically isolated, gate drivers and aluminum cooling plates for forced air cooling which results in a compact package. Each IGBT is overvoltage protected by transient voltage suppressors. The turn-on current rise time of the six-series IGBT stack and a single 6.5 kV rated IGBT has been experimentally measured in a pulsed resistive-load, capacitor discharge circuit. The IGBT stack has also been compared to two IGBT modules in series, each rated at 3.3 kV, in a boost circuit application switching at 9 kHz and producing an output of 5 kV. The six-series IGBT stack results in improved turn-on switching speed, and significantly higher power boost converter efficiency due to a reduced current tail during turn off. The experimental test parameters and the results of the comparison tests are discussed in the following paper"}
{"_id":"b63a60e666c4c0335d8de4581eaaa3f71e8e0e54","title":"A Nonlinear-Disturbance-Observer-Based DC-Bus Voltage Control for a Hybrid AC\/DC Microgrid","text":"DC-bus voltage control is an important task in the operation of a dc or a hybrid ac\/dc microgrid system. To improve the dc-bus voltage control dynamics, traditional approaches attempt to measure and feedforward the load or source power in the dc-bus control scheme. However, in a microgrid system with distributed dc sources and loads, the traditional feedforward-based methods need remote measurement with communications. In this paper, a nonlinear disturbance observer (NDO) based dc-bus voltage control is proposed, which does not need the remote measurement and enables the important \u201cplug-and-play\u201d feature. Based on this observer, a novel dc-bus voltage control scheme is developed to suppress the transient fluctuations of dc-bus voltage and improve the power quality in such a microgrid system. Details on the design of the observer, the dc-bus controller and the pulsewidth-modulation (PWM) dead-time compensation are provided in this paper. The effects of possible dc-bus capacitance variation are also considered. The performance of the proposed control strategy has been successfully verified in a 30 kVA hybrid microgrid including ac\/dc buses, battery energy storage system, and photovoltaic (PV) power generation system."}
{"_id":"6b557c35514d4b6bd75cebdaa2151517f5e820e2","title":"Prediction , operations , and condition monitoring in wind energy","text":"Recent developments in wind energy research including wind speed prediction, wind turbine control, operations of hybrid power systems, as well as condition monitoring and fault detection are surveyed. Approaches based on statistics, physics, and data mining for wind speed prediction at different time scales are reviewed. Comparative analysis of prediction results reported in the literature is presented. Studies of classical and intelligent control of wind turbines involving different objectives and strategies are reported. Models for planning operations of different hybrid power systems including wind generation for various objectives are addressed. Methodologies for condition monitoring and fault detection are discussed. Future research directions in wind energy are proposed. 2013 Elsevier Ltd. All rights reserved."}
{"_id":"e81f115f2ac725f27ea6549f4de0a71b3a3f6a5c","title":"NEUROPSI: a brief neuropsychological test battery in Spanish with norms by age and educational level.","text":"The purpose of this research was to develop, standardize, and test the reliability of a short neuropsychological test battery in the Spanish language. This neuropsychological battery was named \"NEUROPSI,\" and was developed to assess briefly a wide spectrum of cognitive functions, including orientation, attention, memory, language, visuoperceptual abilities, and executive functions. The NEUROPSI includes items that are relevant for Spanish-speaking communities. It can be applied to illiterates and low educational groups. Administration time is 25 to 30 min. Normative data were collected from 800 monolingual Spanish-speaking individuals, ages 16 to 85 years. Four age groups were used: (1) 16 to 30 years, (2) 31 to 50 years, (3) 51 to 65 years, and (4) 66 to 85 years. Data also are analyzed and presented within 4 different educational levels that were represented in this sample; (1) illiterates (zero years of school); (2) 1 to 4 years of school; (2) 5 to 9 years of school; and (3) 10 or more years of formal education. The effects of age and education, as well as the factor structure of the NEUROPSI are analyzed. The NEUROPSI may fulfill the need for brief, reliable, and objective evaluation of a broad range of cognitive functions in Spanish-speaking populations."}
{"_id":"382c057c0be037340e7d6494fc3a580b9d6b958c","title":"Should TED talks be teaching us something?","text":"The nonprofit phenomenon \u201cTED,\u201d the brand name for the concepts of Technology Education and Design, was born in 1984. It launched into pop culture stardom in 2006 when the organization\u2019s curators began offering short, free, unrestricted, and educational video segments. Known as \u201cTEDTalks,\u201d these informational segments are designed to be no longer than 18 minutes in length and provide succinct, targeted enlightenment on various topics or ideas that are deemed \u201cworth spreading.\u201d TED Talks, often delivered in sophisticated studios with trendy backdrops, follow a format that focuses learners on the presenter and limited, extremely purposeful visual aids. Topics range from global warming to running to the developing world. Popular TED Talks, such as Sir Ken Robinson\u2019s \u201cSchools Kill Creatively\u201d or Dan Gilbert\u2019s \u201cWhy Are We Happy?\u201d can easily garner well over a million views. TED Talks are a curious phenomenon for educators to observe. They are in many ways the antithesis of traditional lectures, which are typically 60-120 minutes in length and delivered in cavernous halls by faculty members engaged in everyday academic lives. Perhaps the formality of the lecture is the biggest superficial difference in comparison to casual TEDTalks (Table 1). However, TED Talks are not as unstructured as they may appear. Presenters are well coached and instructed to follow a specific presentation formula, whichmaximizes storyboarding and highlights passion for the subject. While learning is not formally assessed, TED Talks do seem to accomplish their goals of spreading ideas while sparking curiosity within the learner. The fact that some presentations have been viewed more than 16 million times points to the effectiveness of the platform in at least reaching learners and stimulating a desire to click, listen, and learn.Moreover, the TEDTalks website is the fourth most popular technology website and the single most popular conference and events website in the world. The TED phenomenon may have both direct and subliminal messages for academia. Perhaps an initial question to ponder is whether the TED phenomenon is a logical grassroots educational evolution or a reaction to the digital generation and their preference for learning that occurs \u201cwherever, whenever.\u201d The diverse cross-section of TED devotees ranging in background and age would seem to provide evidence that the platform does not solely appeal to younger generations of learners. Instead, it suggests that adult learners are either more drawn to digital learning than they think they are or than they are likely to admit. The perceived efficacy of TED once again calls into question the continued reliance of academia on the lecture as the primary currency of learning. TED Talks do not convey large chunks of information but rather present grander ideas. Would TED-like educational modules or blocks of 18-20 minutes be more likely to pique student curiosity across a variety of pharmacy topics, maintain attention span, and improve retention? Many faculty members who are recognized as outstanding teachers or lecturers might confess that they already teach through a TED-like lens. Collaterally, TED Talks or TED-formatted learning experiences might be ideal springboards for incorporation into inverted or flipped classroom environments where information is gathered and learned at home, while ideas are analyzed, debated, and assimilated within the classroom. Unarguably, TED Talks have given scientists and other researchers a real-time, mass media driven opportunity to disseminate their research, ideas, and theories that might otherwise have gone unnoticed. Similar platforms or approaches may be able to provide opportunities for the academy to further transmit research to the general public. The TED approach to idea dissemination is not without its critics. Several authors have criticized TED for flattening or dumbing down ideas so they fit into a preconceived, convenient format that is primarily designed to entertain. Consequently, the oversimplified ideas and conceptsmay provoke little effort from the learner to analyze data, theory, or controversy. Some Corresponding Author: Frank Romanelli, PharmD, MPH, 789 South Limestone Road, University of Kentucky College of Pharmacy, Lexington, KY 40536. E-mail: froma2@email. uky.edu American Journal of Pharmaceutical Education 2014; 78 (6) Article 113."}
{"_id":"36eff99a7f23cec395e4efc80ff7f937934c7be6","title":"Geometry and Meaning","text":"Geometry and Meaning is an interesting book about a relationship between geometry and logic defined on certain types of abstract spaces and how that intimate relationship might be exploited when applied in computational linguistics. It is also about an approach to information retrieval, because the analysis of natural language, especially negation, is applied to problems in IR, and indeed illustrated throughout the book by simple examples using search engines. It is refreshing to see IR issues tackled from a different point of view than the standard vector space (Salton, 1968). It is an enjoyable read, as intended by the author, and succeeds as a sort of tourist guide to the subject in hand. The early part of the book concentrates on the introduction of a number of elementary concepts from mathematics: graph theory, linear algebra (especially vector spaces), lattice theory, and logic. These concepts are well motivated and illustrated with good examples, mostly of a classificatory or taxonomic kind. One of the major goals of the book is to argue that non-classical logic, in the form of a quantum logic, is a candidate for analyzing language and its underlying logic, with a promise that such an approach could lead to improved search engines. The argument for this is aided by copious references to early philosophers, scientists, and mathematicians, creating the impression that when Aristotle, Descartes, Boole, and Grassmann were laying the foundations for taxonomy, analytical geometry, logic, and vector spaces, they had a more flexible and broader view of these subjects than is current. This is especially true of logic. Thus the historical approach taken to introducing quantum logic (chapter 7) is to show that this particular kind of logic and its interpretation in vector space were inherent in some of the ideas of these earlier thinkers. Widdows claims that Aristotle was never respected for his mathematics and that Grassmann\u2019s Ausdehnungslehre was largely ignored and left in obscurity. Whether Aristotle was never admired for his mathematics I am unable to judge, but certainly Alfred North Whitehead (1925) was not complimentary when he said:"}
{"_id":"f0d82cbac15c4379677d815c9d32f7044b19d869","title":"Emerging Frontiers of Neuroengineering: A Network Science of Brain Connectivity.","text":"Neuroengineering is faced with unique challenges in repairing or replacing complex neural systems that are composed of many interacting parts. These interactions form intricate patterns over large spatiotemporal scales and produce emergent behaviors that are difficult to predict from individual elements. Network science provides a particularly appropriate framework in which to study and intervene in such systems by treating neural elements (cells, volumes) as nodes in a graph and neural interactions (synapses, white matter tracts) as edges in that graph. Here, we review the emerging discipline of network neuroscience, which uses and develops tools from graph theory to better understand and manipulate neural systems from micro- to macroscales. We present examples of how human brain imaging data are being modeled with network analysis and underscore potential pitfalls. We then highlight current computational and theoretical frontiers and emphasize their utility in informing diagnosis and monitoring, brain-machine interfaces, and brain stimulation. A flexible and rapidly evolving enterprise, network neuroscience provides a set of powerful approaches and fundamental insights that are critical for the neuroengineer's tool kit."}
{"_id":"f7d5f8c60972c18812925715f685ce8ae5d5659d","title":"A new exact method for the two-dimensional orthogonal packing problem","text":"The two-dimensional orthogonal packing problem (2OPP ) consists of determining if a set of rectangles (items) can be packed into one rectangle of fixed size (bin). In this paper we propose two exact algorithms for solving this problem. The first algorithm is an improvement on a classical branch&bound method, whereas the second algorithm is based on a two-step enumerative method. We also describe reduction procedures and lower bounds which can be used within the branch&bound method. We report computational experiments for randomly generated benchmarks, which demonstrate the efficiency of both methods."}
{"_id":"90c1104142203c8ead18882d49bfea8aec23e758","title":"Sensitivity and diagnosticity of NASA-TLX and simplified SWAT to assess the mental workload associated with operating an agricultural sprayer.","text":"The objectives of the present study were: a) to investigate three continuous variants of the NASA-Task Load Index (TLX) (standard NASA (CNASA), average NASA (C1NASA) and principal component NASA (PCNASA)) and five different variants of the simplified subjective workload assessment technique (SSWAT) (continuous standard SSWAT (CSSWAT), continuous average SSWAT (C1SSWAT), continuous principal component SSWAT (PCSSWAT), discrete event-based SSWAT (D1SSWAT) and discrete standard SSWAT (DSSWAT)) in terms of their sensitivity and diagnosticity to assess the mental workload associated with agricultural spraying; b) to compare and select the best variants of NASA-TLX and SSWAT for future mental workload research in the agricultural domain. A total of 16 male university students (mean 30.4 +\/- 12.5 years) participated in this study. All the participants were trained to drive an agricultural spraying simulator. Sensitivity was assessed by the ability of the scales to report the maximum change in workload ratings due to the change in illumination and difficulty levels. In addition, the factor loading method was used to quantify sensitivity. The diagnosticity was assessed by the ability of the scale to diagnose the change in task levels from single to dual. Among all the variants of NASA-TLX and SSWAT, PCNASA and discrete variants of SSWAT showed the highest sensitivity and diagnosticity. Moreover, among all the variants of NASA and SSWAT, the discrete variants of SSWAT showed the highest sensitivity and diagnosticity but also high between-subject variability. The continuous variants of both scales had relatively low sensitivity and diagnosticity and also low between-subject variability. Hence, when selecting a scale for future mental workload research in the agricultural domain, a researcher should decide what to compromise: 1) between-subject variability or 2) sensitivity and diagnosticity. STATEMENT OF RELEVANCE: The use of subjective workload scales is very popular in mental workload research. The present study investigated the different variants of two popular workload rating scales (i.e. NASA-TLX and SSWAT) in terms of their sensitivity and diagnositicity and selected the best variants of each scale for future mental workload research."}
{"_id":"b1cbfd6c1e7f8a77e6c1e6db6cd0625e3bd785ef","title":"Stadium Hashing: Scalable and Flexible Hashing on GPUs","text":"Hashing is one of the most fundamental operations that provides a means for a program to obtain fast access to large amounts of data. Despite the emergence of GPUs as many-threaded general purpose processors, high performance parallel data hashing solutions for GPUs are yet to receive adequate attention. Existing hashing solutions for GPUs not only impose restrictions (e.g., inability to concurrently execute insertion and retrieval operations, limitation on the size of key-value data pairs) that limit their applicability, their performance does not scale to large hash tables that must be kept out-of-core in the host memory. In this paper we present Stadium Hashing (Stash) that is scalable to large hash tables and practical as it does not impose the aforementioned restrictions. To support large out-of-core hash tables, Stash uses a compact data structure named ticket-board that is separate from hash table buckets and is held inside GPU global memory. Ticket-board locally resolves significant portion of insertion and lookup operations and hence, by reducing accesses to the host memory, it accelerates the execution of these operations. Split design of the ticket-board also enables arbitrarily large keys and values. Unlike existing methods, Stash naturally supports concurrent insertions and retrievals due to its use of double hashing as the collision resolution strategy. Furthermore, we propose Stash with collaborative lanes (clStash) that enhances GPU's SIMD resource utilization for batched insertions during hash table creation. For concurrent insertion and retrieval streams, Stadium hashing can be up to 2 and 3 times faster than GPU Cuckoo hashing for in-core and out-of-core tables respectively."}
{"_id":"20f5b475effb8fd0bf26bc72b4490b033ac25129","title":"Real time detection of lane markers in urban streets","text":"We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques."}
{"_id":"27edbcf8c6023905db4de18a4189c2093ab39b23","title":"Robust Lane Detection and Tracking in Challenging Scenarios","text":"A lane-detection system is an important component of many intelligent transportation systems. We present a robust lane-detection-and-tracking algorithm to deal with challenging scenarios such as a lane curvature, worn lane markings, lane changes, and emerging, ending, merging, and splitting lanes. We first present a comparative study to find a good real-time lane-marking classifier. Once detection is done, the lane markings are grouped into lane-boundary hypotheses. We group left and right lane boundaries separately to effectively handle merging and splitting lanes. A fast and robust algorithm, based on random-sample consensus and particle filtering, is proposed to generate a large number of hypotheses in real time. The generated hypotheses are evaluated and grouped based on a probabilistic framework. The suggested framework effectively combines a likelihood-based object-recognition algorithm with a Markov-style process (tracking) and can also be applied to general-part-based object-tracking problems. An experimental result on local streets and highways shows that the suggested algorithm is very reliable."}
{"_id":"4d2cd0b25c5b0f69b6976752ebca43ec5f04a461","title":"Lane detection and tracking using B-Snake","text":"In this paper, we proposed a B-Snake based lane detection and tracking algorithm without any cameras\u2019 parameters. Compared with other lane models, the B-Snake based lane model is able to describe a wider range of lane structures since B-Spline can form any arbitrary shape by a set of control points. The problems of detecting both sides of lane markings (or boundaries) have been merged here as the problem of detecting the mid-line of the lane, by using the knowledge of the perspective parallel lines. Furthermore, a robust algorithm, called CHEVP, is presented for providing a good initial position for the B-Snake. Also, a minimum error method by Minimum Mean Square Error (MMSE) is proposed to determine the control points of the B-Snake model by the overall image forces on two sides of lane. Experimental results show that the proposed method is robust against noise, shadows, and illumination variations in the captured road images. It is also applicable to the marked and the unmarked roads, as well as the dash and the solid paint line roads. q 2003 Elsevier B.V. All rights reserved."}
{"_id":"1c0f7854c14debcc34368e210568696a01c40573","title":"Using vanishing points for camera calibration","text":"In this article a new method for the calibration of a vision system which consists of two (or more) cameras is presented. The proposed method, which uses simple properties of vanishing points, is divided into two steps. In the first step, the intrinsic parameters of each camera, that is, the focal length and the location of the intersection between the optical axis and the image plane, are recovered from a single image of a cube. In the second step, the extrinsic parameters of a pair of cameras, that is, the rotation matrix and the translation vector which describe the rigid motion between the coordinate systems fixed in the two cameras are estimated from an image stereo pair of a suitable planar pattern. Firstly, by matching the corresponding vanishing points in the two images the rotation matrix can be computed, then the translation vector is estimated by means of a simple triangulation. The robustness of the method against noise is discussed, and the conditions for optimal estimation of the rotation matrix are derived. Extensive experimentation shows that the precision that can be achieved with the proposed method is sufficient to efficiently perform machine vision tasks that require camera calibration, like depth from stereo and motion from image sequence."}
{"_id":"235aff8bdb65654163110b35f268de6933814c49","title":"Realtime lane tracking of curved local road","text":"A lane detection system is an important component of many intelligent transportation systems. We present a robust realtime lane tracking algorithm for a curved local road. First, we present a comparative study to find a good realtime lane marking classifier. Once lane markings are detected, they are grouped into many lane boundary hypotheses represented by constrained cubic spline curves. We present a robust hypothesis generation algorithm using a particle filtering technique and a RANSAC (random sample concensus) algorithm. We introduce a probabilistic approach to group lane boundary hypotheses into left and right lane boundaries. The proposed grouping approach can be applied to general part-based object tracking problems. It incorporates a likelihood-based object recognition technique into a Markov-style process. An experimental result on local streets shows that the suggested algorithm is very reliable"}
{"_id":"514ee2a4d6dec51d726012bd74b32b1e05f13271","title":"The Ontological Foundation of REA Enterprise Information Systems","text":"Philosophers have studied ontologies for centuries in their search for a systematic explanation of existence: \u201cWhat kind of things exist?\u201d Recently, ontologies have emerged as a major research topic in the fields of artificial intelligence and knowledge management where they address the content issue: \u201cWhat kind of things should we represent?\u201d The answer to that question differs with the scope of the ontology. Ontologies that are subject-independent are called upper-level ontologies, and they attempt to define concepts that are shared by all domains, such as time and space. Domain ontologies, on the other hand, attempt to define the things that are relevant to a specific application domain. Both types of ontologies are becoming increasingly important in the era of the Internet where consistent and machine-readable semantic definitions of economic phenomena become the language of e-commerce. In this paper, we propose the conceptual accounting framework of the Resource-Event-Agent (REA) model of McCarthy (1982) as an enterprise domain ontology, and we build upon the initial ontology work of Geerts and McCarthy (2000) which explored REA with respect to the ontological categorizations of John Sowa (1999). Because of its conceptual modeling heritage, REA already resembles an established ontology in many declarative (categories) and procedural (axioms) respects, and we also propose here to extend formally that framework both (1) vertically in terms of entrepreneurial logic (value chains) and workflow detail, and (2) horizontally in terms of type and commitment images of enterprise economic phenomena. A strong emphasis throughout the paper is given to the microeconomic foundations of the category definitions."}
{"_id":"944692d5d33fbc5f42294a8310380e0b057a1320","title":"Dual- and Multiband U-Slot Patch Antennas","text":"A wide band patch antenna fed by an L-probe can be designed for dual- and multi-band application by cutting U-slots on the patch. Simulation and measurement results are presented to illustrate this design."}
{"_id":"6800fbe3314be9f638fb075e15b489d1aadb3030","title":"Advances in Collaborative Filtering","text":"The collaborative filtering (CF) approach to recommenders has recently enjoyed much interest and progress. The fact that it played a central role within the recently completed Netflix competition has contributed to its popularity. This chapter surveys the recent progress in the field. Matrix factorization techniques, which became a first choice for implementing CF, are described together with recent innovations. We also describe several extensions that bring competitive accuracy into neighborhood methods, which used to dominate the field. The chapter demonstrates how to utilize temporal models and implicit feedback to extend models accuracy. In passing, we include detailed descriptions of some the central methods developed for tackling the challenge of the Netflix Prize competition."}
{"_id":"12bbec48c8fde83ea276402ffedd2e241e978a12","title":"VirtualTable: a projection augmented reality game","text":"VirtualTable is a projection augmented reality installation where users are engaged in an interactive tower defense game. The installation runs continuously and is designed to attract people to a table, which the game is projected onto. Any number of players can join the game for an optional period of time. The goal is to prevent the virtual stylized soot balls, spawning on one side of the table, from reaching the cheese. To stop them, the players can place any kind of object on the table, that then will become part of the game. Depending on the object, it will become either a wall, an obstacle for the soot balls, or a tower, that eliminates them within a physical range. The number of enemies is dependent on the number of objects in the field, forcing the players to use strategy and collaboration and not the sheer number of objects to win the game."}
{"_id":"ffd76d49439c078a6afc246e6d0638a01ad563f8","title":"A Context-Aware Usability Model for Mobile Health Applications","text":"Mobile healthcare is a fast growing area of research that capitalizes on mobile technologies and wearables to provide realtime and continuous monitoring and analysis of vital signs of users. Yet, most of the current applications are developed for general population without taking into consideration the context and needs of different user groups. Designing and developing mobile health applications and diaries according to the user context can significantly improve the quality of user interaction and encourage the application use. In this paper, we propose a user context model and a set of usability attributes for developing mobile applications in healthcare. The proposed model and the selected attributes are integrated into a mobile application development framework to provide user-centered and context-aware guidelines. To validate our framework, a mobile diary was implemented for patients undergoing Peritoneal Dialysis (PD) and tested with real users."}
{"_id":"8deafc34941a79b9cfc348ab63ec51752c7b1cde","title":"New approach for clustering of big data: DisK-means","text":"The exponential growth in the amount of data gathered from various sources has resulted in the need for more efficient algorithms to quickly analyze large datasets. Clustering techniques, like K-Means are useful in analyzing data in a parallel fashion. K-Means largely depends upon a proper initialization to produce optimal results. K-means++ initialization algorithm provides solution based on providing an initial set of centres to the K-Means algorithm. However, its inherent sequential nature makes it suffer from various limitations when applied to large datasets. For instance, it makes k iterations to find k centres. In this paper, we present an algorithm that attempts to overcome the drawbacks of previous algorithms. Our work provides a method to select a good initial seeding in less time, facilitating fast and accurate cluster analysis over large datasets."}
{"_id":"455d562bf02dcb5161c98668a5f5e470d02b70b8","title":"A probabilistic constrained clustering for transfer learning and image category discovery","text":"Neural network-based clustering has recently gained popularity, and in particular a constrained clustering formulation has been proposed to perform transfer learning and image category discovery using deep learning. The core idea is to formulate a clustering objective with pairwise constraints that can be used to train a deep clustering network; therefore the cluster assignments and their underlying feature representations are jointly optimized end-toend. In this work, we provide a novel clustering formulation to address scalability issues of previous work in terms of optimizing deeper networks and larger amounts of categories. The proposed objective directly minimizes the negative log-likelihood of cluster assignment with respect to the pairwise constraints, has no hyper-parameters, and demonstrates improved scalability and performance on both supervised learning and unsupervised transfer learning."}
{"_id":"e6bef595cb78bcad4880aea6a3a73ecd32fbfe06","title":"Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach","text":"The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classifiers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains."}
{"_id":"d77d2ab03f891d8f0822083020486a6de1f2900f","title":"EEG Classification of Different Imaginary Movements within the Same Limb","text":"The task of discriminating the motor imagery of different movements within the same limb using electroencephalography (EEG) signals is challenging because these imaginary movements have close spatial representations on the motor cortex area. There is, however, a pressing need to succeed in this task. The reason is that the ability to classify different same-limb imaginary movements could increase the number of control dimensions of a brain-computer interface (BCI). In this paper, we propose a 3-class BCI system that discriminates EEG signals corresponding to rest, imaginary grasp movements, and imaginary elbow movements. Besides, the differences between simple motor imagery and goal-oriented motor imagery in terms of their topographical distributions and classification accuracies are also being investigated. To the best of our knowledge, both problems have not been explored in the literature. Based on the EEG data recorded from 12 able-bodied individuals, we have demonstrated that same-limb motor imagery classification is possible. For the binary classification of imaginary grasp and elbow (goal-oriented) movements, the average accuracy achieved is 66.9%. For the 3-class problem of discriminating rest against imaginary grasp and elbow movements, the average classification accuracy achieved is 60.7%, which is greater than the random classification accuracy of 33.3%. Our results also show that goal-oriented imaginary elbow movements lead to a better classification performance compared to simple imaginary elbow movements. This proposed BCI system could potentially be used in controlling a robotic rehabilitation system, which can assist stroke patients in performing task-specific exercises."}
{"_id":"de0f84359078ec9ba79f4d0061fe73f6cac6591c","title":"Single-Stage Single-Switch Four-Output Resonant LED Driver With High Power Factor and Passive Current Balancing","text":"A resonant single-stage single-switch four-output LED driver with high power factor and passive current balancing is proposed. By controlling one output current, the other output currents of four-output LED driver can be controlled via passive current balancing, which makes its control simple. When magnetizing inductor current operates in critical conduction mode, unity power factor is achieved. The proposed LED driver uses only one active switch and one magnetic component, thus it benefits from low cost, small volume, and light weight. Moreover, high-efficiency performance is achieved due to single-stage power conversion and soft-switching characteristics. The characteristics of the proposed LED driver are studied in this paper and experimental results of two 110-W four-output isolated LED drivers are provided to verify the studied results."}
{"_id":"1924ae6773f09efcfc791454d42a3ec53207a815","title":"Flexible Ambiguity Resolution and Incompleteness Detection in Requirements Descriptions via an Indicator-Based Configuration of Text Analysis Pipelines","text":"Natural language software requirements descriptions enable end users to formulate their wishes and expectations for a future software product without much prior knowledge in requirements engineering. However, these descriptions are susceptible to linguistic inaccuracies such as ambiguities and incompleteness that can harm the development process. There is a number of software solutions that can detect deficits in requirements descriptions and partially solve them, but they are often hard to use and not suitable for end users. For this reason, we develop a software system that helps end-users to create unambiguous and complete requirements descriptions by combining existing expert tools and controlling them using automatic compensation strategies. In order to recognize the necessity of individual compensation methods in the descriptions, we have developed linguistic indicators, which we present in this paper. Based on these indicators, the whole text analysis pipeline is ad-hoc configured and thus adapted to the individual circumstances of a requirements description."}
{"_id":"727774c3a911d45ea6fe2d4ad66fd3b453a18c99","title":"Correlating low-level image statistics with users - rapid aesthetic and affective judgments of web pages","text":"In this paper, we report a study that examines the relationship between image-based computational analyses of web pages and users' aesthetic judgments about the same image material. Web pages were iteratively decomposed into quadrants of minimum entropy (quadtree decomposition) based on low-level image statistics, to permit a characterization of these pages in terms of their respective organizational symmetry, balance and equilibrium. These attributes were then evaluated for their correlation with human participants' subjective ratings of the same web pages on four aesthetic and affective dimensions. Several of these correlations were quite large and revealed interesting patterns in the relationship between low-level (i.e., pixel-level) image statistics and design-relevant dimensions."}
{"_id":"21c76cc8ebfb9c112c2594ce490b47e458b50e31","title":"American Sign Language Recognition Using Leap Motion Sensor","text":"In this paper, we present an American Sign Language recognition system using a compact and affordable 3D motion sensor. The palm-sized Leap Motion sensor provides a much more portable and economical solution than Cyblerglove or Microsoft kinect used in existing studies. We apply k-nearest neighbor and support vector machine to classify the 26 letters of the English alphabet in American Sign Language using the derived features from the sensory data. The experiment result shows that the highest average classification rate of 72.78% and 79.83% was achieved by k-nearest neighbor and support vector machine respectively. We also provide detailed discussions on the parameter setting in machine learning methods and accuracy of specific alphabet letters in this paper."}
{"_id":"519f5892938d4423cecc999b6e489b72fc0d0ca7","title":"Cognitive, emotional, and behavioral considerations for chronic pain management in the Ehlers-Danlos syndrome hypermobility-type: a narrative review.","text":"BACKGROUND\nEhlers-Danlos syndrome (EDS) hypermobility-type is the most common hereditary disorder of the connective tissue. The tissue fragility characteristic of this condition leads to multi-systemic symptoms in which pain, often severe, chronic, and disabling, is the most experienced. Clinical observations suggest that the complex patient with EDS hypermobility-type is refractory toward several biomedical and physical approaches. In this context and in accordance with the contemporary conceptualization of pain (biopsychosocial perspective), the identification of psychological aspects involved in the pain experience can be useful to improve interventions for this under-recognized pathology.\n\n\nPURPOSE\nReview of the literature on joint hypermobility and EDS hypermobility-type concerning psychological factors linked to pain chronicity and disability.\n\n\nMETHODS\nA comprehensive search was performed using scientific online databases and references lists, encompassing publications reporting quantitative and qualitative research as well as unpublished literature.\n\n\nRESULTS\nDespite scarce research, psychological factors associated with EDS hypermobility-type that potentially affect pain chronicity and disability were identified. These are cognitive problems and attention to body sensations, negative emotions, and unhealthy patterns of activity (hypo\/hyperactivity).\n\n\nCONCLUSIONS\nAs in other chronic pain conditions, these aspects should be more explored in EDS hypermobility-type, and integrated into chronic pain prevention and management programs. Implications for Rehabilitation Clinicians should be aware that joint hypermobility may be associated with other health problems, and in its presence suspect a heritable disorder of connective tissue such as the Ehlers-Danlos syndrome (EDS) hypermobility-type, in which chronic pain is one of the most frequent and invalidating symptoms. It is necessary to explore the psychosocial functioning of patients as part of the overall chronic pain management in the EDS hypermobility-type, especially when they do not respond to biomedical approaches as psychological factors may be operating against rehabilitation. Further research on the psychological factors linked to pain chronicity and disability in the EDS hypermobility-type is needed."}
{"_id":"7d2fda30e52c39431dbb90ae065da036a55acdc7","title":"A brief review: factors affecting the length of the rest interval between resistance exercise sets.","text":"Research has indicated that multiple sets are superior to single sets for maximal strength development. However, whether maximal strength gains are achieved may depend on the ability to sustain a consistent number of repetitions over consecutive sets. A key factor that determines the ability to sustain repetitions is the length of rest interval between sets. The length of the rest interval is commonly prescribed based on the training goal, but may vary based on several other factors. The purpose of this review was to discuss these factors in the context of different training goals. When training for muscular strength, the magnitude of the load lifted is a key determinant of the rest interval prescribed between sets. For loads less than 90% of 1 repetition maximum, 3-5 minutes rest between sets allows for greater strength increases through the maintenance of training intensity. However, when testing for maximal strength, 1-2 minutes rest between sets might be sufficient between repeated attempts. When training for muscular power, a minimum of 3 minutes rest should be prescribed between sets of repeated maximal effort movements (e.g., plyometric jumps). When training for muscular hypertrophy, consecutive sets should be performed prior to when full recovery has taken place. Shorter rest intervals of 30-60 seconds between sets have been associated with higher acute increases in growth hormone, which may contribute to the hypertrophic effect. When training for muscular endurance, an ideal strategy might be to perform resistance exercises in a circuit, with shorter rest intervals (e.g., 30 seconds) between exercises that involve dissimilar muscle groups, and longer rest intervals (e.g., 3 minutes) between exercises that involve similar muscle groups. In summary, the length of the rest interval between sets is only 1 component of a resistance exercise program directed toward different training goals. Prescribing the appropriate rest interval does not ensure a desired outcome if other components such as intensity and volume are not prescribed appropriately."}
{"_id":"fe0643f3405c22fe7ca0b7d1274a812d6e3e5a11","title":"Silicon carbide power MOSFETs: Breakthrough performance from 900 V up to 15 kV","text":"Since Cree, Inc.'s 2<sup>nd<\/sup> generation 4H-SiC MOSFETs were commercially released with a specific on-resistance (R<sub>ON, SP<\/sub>) of 5 m\u03a9\u00b7cm<sup>2<\/sup> for a 1200 V-rating in early 2013, we have further optimized the device design and fabrication processes as well as greatly expanded the voltage ratings from 900 V up to 15 kV for a much wider range of high-power, high-frequency, and high-voltage energy-conversion and transmission applications. Using these next-generation SiC MOSFETs, we have now achieved new breakthrough performance for voltage ratings from 900 V up to 15 kV with a R<sub>ON, SP<\/sub> as low as 2.3 m\u03a9\u00b7cm<sup>2<\/sup> for a breakdown voltage (BV) of 1230 V and 900 V-rating, 2.7 m\u03a9\u00b7cm<sup>2<\/sup> for a BV of 1620 V and 1200 V-rating, 3.38 m\u03a9\u00b7cm<sup>2<\/sup> for a BV of 1830 V and 1700 V-rating, 10.6 m\u03a9\u00b7cm<sup>2<\/sup> for a BV of 4160 V and 3300 V-rating, 123 m\u03a9\u00b7cm<sup>2<\/sup> for a BV of 12 kV and 10 kV-rating, and 208 m\u03a9\u00b7cm<sup>2<\/sup> for a BV of 15.5 kV and 15 kV-rating. In addition, due to the lack of current tailing during the bipolar device switching turn-off, the SiC MOSFETs reported in this work exhibit incredibly high frequency switching performance over their silicon counter parts."}
{"_id":"011d4ccb74f32f597df54ac8037a7903bd95038b","title":"The evolution of human skin coloration.","text":"Skin color is one of the most conspicuous ways in which humans vary and has been widely used to define human races. Here we present new evidence indicating that variations in skin color are adaptive, and are related to the regulation of ultraviolet (UV) radiation penetration in the integument and its direct and indirect effects on fitness. Using remotely sensed data on UV radiation levels, hypotheses concerning the distribution of the skin colors of indigenous peoples relative to UV levels were tested quantitatively in this study for the first time. The major results of this study are: (1) skin reflectance is strongly correlated with absolute latitude and UV radiation levels. The highest correlation between skin reflectance and UV levels was observed at 545 nm, near the absorption maximum for oxyhemoglobin, suggesting that the main role of melanin pigmentation in humans is regulation of the effects of UV radiation on the contents of cutaneous blood vessels located in the dermis. (2) Predicted skin reflectances deviated little from observed values. (3) In all populations for which skin reflectance data were available for males and females, females were found to be lighter skinned than males. (4) The clinal gradation of skin coloration observed among indigenous peoples is correlated with UV radiation levels and represents a compromise solution to the conflicting physiological requirements of photoprotection and vitamin D synthesis. The earliest members of the hominid lineage probably had a mostly unpigmented or lightly pigmented integument covered with dark black hair, similar to that of the modern chimpanzee. The evolution of a naked, darkly pigmented integument occurred early in the evolution of the genus Homo. A dark epidermis protected sweat glands from UV-induced injury, thus insuring the integrity of somatic thermoregulation. Of greater significance to individual reproductive success was that highly melanized skin protected against UV-induced photolysis of folate (Branda & Eaton, 1978, Science201, 625-626; Jablonski, 1992, Proc. Australas. Soc. Hum. Biol.5, 455-462, 1999, Med. Hypotheses52, 581-582), a metabolite essential for normal development of the embryonic neural tube (Bower & Stanley, 1989, The Medical Journal of Australia150, 613-619; Medical Research Council Vitamin Research Group, 1991, The Lancet338, 31-37) and spermatogenesis (Cosentino et al., 1990, Proc. Natn. Acad. Sci. U.S.A.87, 1431-1435; Mathur et al., 1977, Fertility Sterility28, 1356-1360).As hominids migrated outside of the tropics, varying degrees of depigmentation evolved in order to permit UVB-induced synthesis of previtamin D(3). The lighter color of female skin may be required to permit synthesis of the relatively higher amounts of vitamin D(3)necessary during pregnancy and lactation. Skin coloration in humans is adaptive and labile. Skin pigmentation levels have changed more than once in human evolution. Because of this, skin coloration is of no value in determining phylogenetic relationships among modern human groups."}
{"_id":"d87d70ecd0fdf0976cebbeaeacf25ad9872ffde1","title":"Robust and false positive free watermarking in IWT domain using SVD and ABC","text":"Watermarking is used to protect the copyrighted materials from being misused and help us to know the lawful ownership. The security of any watermarking scheme is always a prime concern for the developer. In this work, the robustness and security issue of IWT (integer wavelet transform) and SVD (singular value decomposition) based watermarking is explored. Generally, SVD based watermarking techniques suffer with an issue of false positive problem. This leads to even authenticating the wrong owner. We are proposing a novel solution to this false positive problem; that arises in SVD based approach. Firstly, IWT is employed on the host image and then SVD is performed on this transformed host. The properties of IWT and SVD help in achieving high value of robustness. Singular values are used for the watermark embedding. In order to further improve the quality of watermarking, the optimization of scaling factor (mixing ratio) is performed with the help of artificial bee colony (ABC) algorithm. A comparison with other schemes is performed to show the superiority of proposed scheme. & 2015 Elsevier Ltd. All rights reserved."}
{"_id":"ae3ebe6c69fdb19e12d3218a5127788fae269c10","title":"A Literature Survey of Benchmark Functions For Global Optimization Problems","text":"Test functions are important to validate and compare the performance of optimization algorithms. There have been many test or benchmark functions reported in the literature; however, there is no standard list or set of benchmark functions. Ideally, test functions should have diverse properties so that can be truly useful to test new algorithms in an unbiased way. For this purpose, we have reviewed and compiled a rich set of 175 benchmark functions for unconstrained optimization problems with diverse properties in terms of modality, separability, and valley landscape. This is by far the most complete set of functions so far in the literature, and tt can be expected this complete set of functions can be used for validation of new optimization in the future."}
{"_id":"d28235adc2c8c6fdfaa474bc2bab931129149fd6","title":"Approaches to Measuring the Difficulty of Games in Dynamic Difficulty Adjustment Systems","text":"In this article, three approaches are proposed for measuring difficulty that can be useful in developing Dynamic Difficulty Adjustment (DDA) systems in different game genres. Our analysis of the existing DDA systems shows that there are three ways to measure the difficulty of the game: using the formal model of gameplay, using the features of the game, and direct examination of the player. These approaches are described in this article and supplemented by appropriate examples of DDA implementations. In addition, the article describes the distinction between task complexity and task difficulty in DDA systems. Separating task complexity (especially the structural one) is suggested, which is an objective characteristic of the task, and task difficulty, which is related to the interaction between the task and the task performer."}
{"_id":"5c881260bcc64070b2b33c10d28f23f793b8344f","title":"A low-voltage, low quiescent current, low drop-out regulator","text":"The demand for low voltage, low drop-out (LDO) regulators is increasing because of the growing demand for portable electronics, i.e., cellular phones, pagers, laptops, etc. LDOs are used coherently with dc-dc converters as well as standalone parts. In power supply systems, they are typically cascaded onto switching regulators to suppress noise and provide a low noise output. The need for low voltage is innate to portable low power devices and corroborated by lower breakdown voltages resulting from reductions in feature size. Low quiescent current in a battery operated system is an intrinsic performance parameter because it partially determines battery life. This paper discusses some techniques that enable the practical realizations of low quiescent current LDOs at low voltages and in existing technologies. The proposed circuit exploits the frequency response dependence on load-current to minimize quiescent current flow. Moreover, the output current capabilities of MOS power transistors are enhanced and drop-out voltages are decreased for a given device size. Other applications, like dc-dc converters, can also reap the benefits of these enhanced MOS devices. An LDO prototype incorporating the aforementioned techniques was fabricated. The circuit was operable down to input voltages of 1 V with a zero-load quiescent current flow of 23 \u03bcA. Moreover, the regulator provided 18 and 50 mA of output current at input voltages of 1 and 1.2 V respectively."}
{"_id":"950ff860dbc8a24fc638ac942ce9c1f51fb24899","title":"Where to Go Next: A Spatio-temporal LSTM model for Next POI Recommendation","text":"Next Point-of-Interest (POI) recommendation is of great value for both location-based service providers and users. Recently Recurrent Neural Networks (RNNs) have been proved to be effective on sequential recommendation tasks. However, existing RNN solutions rarely consider the spatio-temporal intervals between neighbor checkins, which are essential for modeling user check-in behaviors in next POI recommendation. In this paper, we propose a new variant of LSTM, named STLSTM, which implements time gates and distance gates into LSTM to capture the spatio-temporal relation between successive check-ins. Specifically, one time gate and one distance gate are designed to control short-term interest update, and another time gate and distance gate are designed to control long-term interest update. Furthermore, to reduce the number of parameters and improve efficiency, we further integrate coupled input and forget gates with our proposed model. Finally, we evaluate the proposed model using four real-world datasets from various location-based social networks. Our experimental results show that our model significantly outperforms the state-of-the-art approaches for next POI recommendation."}
{"_id":"f99a50ce62845c62d9fcdec277e0857350534cc9","title":"Absorptive Frequency-Selective Transmission Structure With Square-Loop Hybrid Resonator","text":"A novel design of an absorptive frequency-selective transmission structure (AFST) is proposed. This structure is based on the design of a frequency-dependent lossy layer with square-loop hybrid resonator (SLHR). The parallel resonance provided by the hybrid resonator is utilized to bypass the lossy path and improve the insertion loss. Meanwhile, the series resonance of the hybrid resonator is used for expanding the upper absorption bandwidth. Furthermore, the absorption for out-of-band frequencies is achieved by using four metallic strips with lumped resistors, which are connected with the SLHR. The quantity of lumped elements required in a unit cell can be reduced by at least 50% compared to previous structures. The design guidelines are explained with the aid of an equivalent circuit model. Both simulation and experiment results are presented to demonstrate the performance of our AFST. It is shown that an insertion loss of 0.29 dB at 6.1 GHz and a 112.4% 10 dB reflection reduction bandwidth are obtained under the normal incidence."}
{"_id":"26f70336acf7247a35d3c0be6308fe29f25d2872","title":"Implementation of AES-GCM encryption algorithm for high performance and low power architecture Using FPGA","text":"Evaluation of the Advanced Encryption Standard (AES) algorithm in FPGA is proposed here. This Evaluation is compared with other works to show the efficiency. Here we are concerned about two major purposes. The first is to define some of the terms and concepts behind basic cryptographic methods, and to offer a way to compare the myriad cryptographic schemes in use today. The second is to provide some real examples of cryptography in use today. The design uses an iterative looping approach with block and key size of 128 bits, lookup table implementation of S-box. This gives low complexity architecture and easily achieves low latency as well as high throughput. Simulation results, performance results are presented and compared with previous reported designs. Since its acceptance as the adopted symmetric-key algorithm, the Advanced Encryption Standard (AES) and its recently standardized authentication Galois\/Counter Mode (GCM) have been utilized in various security-constrained applications. Many of the AES-GCM applications are power and resource constrained and requires efficient hardware implementations. In this project, AES-GCM algorithms are evaluated and optimized to identify the high-performance and low-power architectures. The Advanced Encryption Standard (AES) is a specification for the encryption of electronic data. The Cipher Block Chaining (CBC) mode is a confidentiality mode whose encryption process features the combining (\u201cchaining\u201d) of the plaintext blocks with the previous Cipher text blocks. The CBC mode requires an IV to combine with the first plaintext block. The IV need not be secret, but it must be unpredictable. Also, the integrity of the IV should be protected. Galois\/Counter Mode (GCM) is a block cipher mode of operation that uses universal hashing over a binary Galois field to provide authenticated encryption. Galois Hash is used for authentication, and the Advanced Encryption Standard (AES) block cipher is used for encryption in counter mode of operation. To obtain the least-complexity S-box, the formulations for the Galois Field (GF) sub-field inversions in GF (24) are optimized By conducting exhaustive simulations for the input transitions, we analyze the synthesis of the AES S-boxes considering the switching activities, gate-level net lists, and parasitic information. Finally, by implementation of AES-GCM the high-performance GF (2128) multiplier architectures, gives the detailed information of its performance. An optimized coding for the implementation of Advanced Encryption Standard-Galois Counter Mode has been developed. The speed factor of the algorithm implementation has been targeted and a software code in Verilog HDL has been developed. This implementation is useful in wireless security like military communication and mobile telephony where there is a grayer emphasis on the speed of communication."}
{"_id":"03f64a5989e4d2ecab989d9724ad4cc58f976daf","title":"Multi-Document Summarization using Sentence-based Topic Models","text":"Most of the existing multi-document summarization methods decompose the documents into sentences and work directly in the sentence space using a term-sentence matrix. However, the knowledge on the document side, i.e. the topics embedded in the documents, can help the context understanding and guide the sentence selection in the summarization procedure. In this paper, we propose a new Bayesian sentence-based topic model for summarization by making use of both the term-document and term-sentence associations. An efficient variational Bayesian algorithm is derived for model parameter estimation. Experimental results on benchmark data sets show the effectiveness of the proposed model for the multi-document summarization task."}
{"_id":"26d4ab9b60b91bb610202b58fa1766951fedb9e9","title":"DRAW: A Recurrent Neural Network For Image Generation","text":"This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye."}
{"_id":"c79ddcef4bdf56c5467143b32e53b23825c17eff","title":"A Framework based on SDN and Containers for Dynamic Service Chains on IoT Gateways","text":"In this paper, we describe a new approach for managing service function chains in scenarios where data from Internet of Things (IoT) devices is partially processed at the network edge. Our framework is enabled by two emerging technologies, Software-Defined Networking (SDN) and container based virtualization, which ensure several benefits in terms of flexibility, easy programmability, and versatility. These features are well suitable with the increasingly stringent requirements of IoT applications, and allow a dynamic and automated network service chaining. An extensive performance evaluation, which has been carried out by means of a testbed, seeks to understand how our proposed framework performs in terms of computational overhead, network bandwidth, and energy consumption. By accounting for the constraints of typical IoT gateways, our evaluation tries to shed light on the actual deployability of the framework on low-power nodes."}
{"_id":"cbcd9f32b526397f88d18163875d04255e72137f","title":"Gradient-based learning applied to document recognition","text":""}
{"_id":"14829636fee5a1cf8dee9737849a8e2bdaf9a91f","title":"Bitter to Better - How to Make Bitcoin a Better Currency","text":"Bitcoin is a distributed digital currency which has attracted a substantial number of users. We perform an in-depth investigation to understand what made Bitcoin so successful, while decades of research on cryptographic e-cash has not lead to a large-scale deployment. We ask also how Bitcoin could become a good candidate for a long-lived stable currency. In doing so, we identify several issues and attacks of Bitcoin, and propose suitable techniques to address them."}
{"_id":"35fe18606529d82ce3fc90961dd6813c92713b3c","title":"SoK: Research Perspectives and Challenges for Bitcoin and Cryptocurrencies","text":"Bit coin has emerged as the most successful cryptographic currency in history. Within two years of its quiet launch in 2009, Bit coin grew to comprise billions of dollars of economic value despite only cursory analysis of the system's design. Since then a growing literature has identified hidden-but-important properties of the system, discovered attacks, proposed promising alternatives, and singled out difficult future challenges. Meanwhile a large and vibrant open-source community has proposed and deployed numerous modifications and extensions. We provide the first systematic exposition Bit coin and the many related crypto currencies or 'altcoins.' Drawing from a scattered body of knowledge, we identify three key components of Bit coin's design that can be decoupled. This enables a more insightful analysis of Bit coin's properties and future stability. We map the design space for numerous proposed modifications, providing comparative analyses for alternative consensus mechanisms, currency allocation mechanisms, computational puzzles, and key management tools. We survey anonymity issues in Bit coin and provide an evaluation framework for analyzing a variety of privacy-enhancing proposals. Finally we provide new insights on what we term disinter mediation protocols, which absolve the need for trusted intermediaries in an interesting set of applications. We identify three general disinter mediation strategies and provide a detailed comparison."}
{"_id":"3d16ed355757fc13b7c6d7d6d04e6e9c5c9c0b78","title":"Majority Is Not Enough: Bitcoin Mining Is Vulnerable","text":""}
{"_id":"991891e3aa226766dcb4ad7221045599f8607685","title":"Review of axial flux induction motor for automotive applications","text":"Hybrid and electric vehicles have been the focus of many academic and industrial studies to reduce transport pollution; they are now established products. In hybrid and electric vehicles, the drive motor should have high torque density, high power density, high efficiency, strong physical structure and variable speed range. An axial flux induction motor is an interesting solution, where the motor is a double sided axial flux machine. This can significantly increase torque density. In this paper a review of the axial flux motor for automotive applications, and the different possible topologies for the axial field motor, are presented."}
{"_id":"34feeafb5ff7757b67cf5c46da0869ffb9655310","title":"Perpetual environmentally powered sensor networks","text":"Environmental energy is an attractive power source for low power wireless sensor networks. We present Prometheus, a system that intelligently manages energy transfer for perpetual operation without human intervention or servicing. Combining positive attributes of different energy storage elements and leveraging the intelligence of the microprocessor, we introduce an efficient multi-stage energy transfer system that reduces the common limitations of single energy storage systems to achieve near perpetual operation. We present our design choices, tradeoffs, circuit evaluations, performance analysis, and models. We discuss the relationships between system components and identify optimal hardware choices to meet an application's needs. Finally we present our implementation of a real system that uses solar energy to power Berkeley's Telos Mote. Our analysis predicts the system will operate for 43 years under 1% load, 4 years under 10% load, and 1 year under 100% load. Our implementation uses a two stage storage system consisting of supercapacitors (primary buffer) and a lithium rechargeable battery (secondary buffer). The mote has full knowledge of power levels and intelligently manages energy transfer to maximize lifetime."}
{"_id":"3689220c58f89e9e19cc0df51c0a573884486708","title":"AmbiMax: Autonomous Energy Harvesting Platform for Multi-Supply Wireless Sensor Nodes","text":"AmbiMax is an energy harvesting circuit and a supercapacitor based energy storage system for wireless sensor nodes (WSN). Previous WSNs attempt to harvest energy from various sources, and some also use supercapacitors instead of batteries to address the battery aging problem. However, they either waste much available energy due to impedance mismatch, or they require active digital control that incurs overhead, or they work with only one specific type of source. AmbiMax addresses these problems by first performing maximum power point tracking (MPPT) autonomously, and then charges supercapacitors at maximum efficiency. Furthermore, AmbiMax is modular and enables composition of multiple energy harvesting sources including solar, wind, thermal, and vibration, each with a different optimal size. Experimental results on a real WSN platform, Eco, show that AmbiMax successfully manages multiple power sources simultaneously and autonomously at several times the efficiency of the current state-of-the-art for WSNs"}
{"_id":"4833d690f7e0a4020ef48c1a537dbb5b8b9b04c6","title":"Integrated photovoltaic maximum power point tracking converter","text":"A low-power low-cost highly efficient maximum power point tracker (MPPT) to be integrated into a photovoltaic (PV) panel is proposed. This can result in a 25% energy enhancement compared to a standard photovoltaic panel, while performing functions like battery voltage regulation and matching of the PV array with the load. Instead of using an externally connected MPPT, it is proposed to use an integrated MPPT converter as part of the PV panel. It is proposed that this integrated MPPT uses a simple controller in order to be cost effective. Furthermore, the converter has to be very efficient, in order to transfer more energy to the load than a directly coupled system. This is achieved by using a simple soft-switched topology. A much higher conversion efficiency at lower cost will then result, making the MPPT an affordable solution for small PV energy systems."}
{"_id":"61c1d66defb225eda47462d1bc393906772c9196","title":"Hardware design experiences in ZebraNet","text":"The enormous potential for wireless sensor networks to make a positive impact on our society has spawned a great deal of research on the topic, and this research is now producing environment-ready systems. Current technology limits coupled with widely-varying application requirements lead to a diversity of hardware platforms for different portions of the design space. In addition, the unique energy and reliability constraints of a system that must function for months at a time without human intervention mean that demands on sensor network hardware are different from the demands on standard integrated circuits. This paper describes our experiences designing sensor nodes and low level software to control them.\n In the ZebraNet system we use GPS technology to record fine-grained position data in order to track long term animal migrations [14]. The ZebraNet hardware is composed of a 16-bit TI microcontroller, 4 Mbits of off-chip flash memory, a 900 MHz radio, and a low-power GPS chip. In this paper, we discuss our techniques for devising efficient power supplies for sensor networks, methods of managing the energy consumption of the nodes, and methods of managing the peripheral devices including the radio, flash, and sensors. We conclude by evaluating the design of the ZebraNet nodes and discussing how it can be improved. Our lessons learned in developing this hardware can be useful both in designing future sensor nodes and in using them in real systems."}
{"_id":"576803b930ef44b79028048569e7ea321c1cecb0","title":"Adaptive Computer-Based Training Increases on the Job Performance of X-Ray Screeners","text":"Due to severe terrorist attacks in recent years, aviation security issues have moved into the focus of politicians as well as the general public. Effective screening of passenger bags using state-of-the-art X-ray screening systems is essential to prevent terrorist attacks. The performance of the screening process depends critically on the security personnel, because they decide whether bags are OK or whether they might contain a prohibited item. Screening X-ray images of passenger bags for dangerous and prohibited items effectively and efficiently is a demanding object recognition task. Effectiveness of computer-based training (CBT) on X-ray detection performance was assessed using computer-based tests and on the job performance measures using threat image projection (TIP). It was found that adaptive CBT is a powerful tool to increase detection performance and efficiency of screeners in X-ray image interpretation. Moreover, the results of training could be generalized to the real life situation as shown in the increased detection performance in TIP not only for trained items, but also for new (untrained) items. These results illustrate that CBT is a very useful tool to increase airport security from a human factors perspective."}
{"_id":"6c1ccc66420136488cf34c1ffe707afefd8b00b9","title":"Rotation-discriminating template matching based on Fourier coefficients of radial projections with robustness to scaling and partial occlusion","text":"We consider brightness\/contrast-invariant and rotation-discriminating template matching that searches an image to analyze A for a query image Q. We propose to use the complex coefficients of the discrete Fourier transform of the radial projections to compute new rotation-invariant local features. These coefficients can be efficiently obtained via FFT. We classify templates in \u201cstable\u201d and \u201cunstable\u201d ones and argue that any local feature-based template matching may fail to find unstable templates. We extract several stable sub-templates of Q and find them in A by comparing the features. The matchings of the sub-templates are combined using the Hough transform. As the features of A are computed only once, the algorithm can find quickly many different sub-templates in A, and it is suitable for: finding many query images in A; multi-scale searching and partial occlusion-robust template matching."}
{"_id":"3370784dacf9df1e54384190dad40b817520ba3a","title":"Haswell: The Fourth-Generation Intel Core Processor","text":"Haswell, Intel's fourth-generation core processor architecture, delivers a range of client parts, a converged core for the client and server, and technologies used across many products. It uses an optimized version of Intel 22-nm process technology. Haswell provides enhancements in power-performance efficiency, power management, form factor and cost, core and uncore microarchitecture, and the core's instruction set."}
{"_id":"146da74cd886acbd4a593a55f0caacefa99714a6","title":"Working model of Self-driving car using Convolutional Neural Network, Raspberry Pi and Arduino","text":"The evolution of Artificial Intelligence has served as the catalyst in the field of technology. We can now develop things which was once just an imagination. One of such creation is the birth of self-driving car. Days have come where one can do their work or even sleep in the car and without even touching the steering wheel, accelerator you will still be able to reach your target destination safely. This paper proposes a working model of self-driving car which is capable of driving from one location to the other or to say on different types of tracks such as curved tracks, straight tracks and straight followed by curved tracks. A camera module is mounted over the top of the car along with Raspberry Pi sends the images from real world to the Convolutional Neural Network which then predicts one of the following directions. i.e. right, left, forward or stop which is then followed by sending a signal from the Arduino to the controller of the remote controlled car and as a result of it the car moves in the desired direction without any human intervention."}
{"_id":"6fd62c67b281956c3f67eb53fafaea83b2f0b4fb","title":"Taking perspective into account in a communicative task","text":"Previous neuroimaging studies of spatial perspective taking have tended not to activate the brain's mentalising network. We predicted that a task that requires the use of perspective taking in a communicative context would lead to the activation of mentalising regions. In the current task, participants followed auditory instructions to move objects in a set of shelves. A 2x2 factorial design was employed. In the Director factor, two directors (one female and one male) either stood behind or next to the shelves, or were replaced by symbolic cues. In the Object factor, participants needed to use the cues (position of the directors or symbolic cues) to select one of three possible objects, or only one object could be selected. Mere presence of the Directors was associated with activity in the superior dorsal medial prefrontal cortex (MPFC) and the superior\/middle temporal sulci, extending into the extrastriate body area and the posterior superior temporal sulcus (pSTS), regions previously found to be responsive to human bodies and faces respectively. The interaction between the Director and Object factors, which requires participants to take into account the perspective of the director, led to additional recruitment of the superior dorsal MPFC, a region activated when thinking about dissimilar others' mental states, and the middle temporal gyri, extending into the left temporal pole. Our results show that using perspective taking in a communicative context, which requires participants to think not only about what the other person sees but also about his\/her intentions, leads to the recruitment of superior dorsal MPFC and parts of the social brain network."}
{"_id":"30b1447fbfdbd887a9c896a2b0d80177fc17c94e","title":"3-Axis Magnetic Sensor Array System for Tracking Magnet's Position and Orientation","text":"In medical diagnoses and treatments, e.g., the endoscopy, the dosage transition monitoring, it is often desirable to wirelessly track an object that moves through the human GI tract. In this paper, we present a magnetic localization and orientation system for such applications. This system uses a small magnet enclosed in the object to serve as excitation source. It does not require the connection wire and power supply for excitation signal. When the magnet moves, it establishes a static magnetic field around, whose intensity is related to the magnet's position and orientation. With the magnetic sensors, the magnetic intensities in some pre-determined spatial points can be detected, and the magnet's position and orientation parameters can be calculated based on an appropriate algorithm. Here, we propose a real-time tracking system built by Honeywell 3-axis magnetic sensors, HMC1053, as well as the computer sampling circuit. The results show that satisfactory tracking accuracy (average localization error is 3.3 mm) can be achieved using a sensor array with enough number of the 3-axis magnetic sensors"}
{"_id":"b551feaa696da1ba44c31e081555e50358c6eca9","title":"A Polymer-Based Capacitive Sensing Array for Normal and Shear Force Measurement","text":"In this work, we present the development of a polymer-based capacitive sensing array. The proposed device is capable of measuring normal and shear forces, and can be easily realized by using micromachining techniques and flexible printed circuit board (FPCB) technologies. The sensing array consists of a polydimethlysiloxane (PDMS) structure and a FPCB. Each shear sensing element comprises four capacitive sensing cells arranged in a 2 \u00d7 2 array, and each capacitive sensing cell has two sensing electrodes and a common floating electrode. The sensing electrodes as well as the metal interconnect for signal scanning are implemented on the FPCB, while the floating electrodes are patterned on the PDMS structure. This design can effectively reduce the complexity of the capacitive structures, and thus makes the device highly manufacturable. The characteristics of the devices with different dimensions were measured and discussed. A scanning circuit was also designed and implemented. The measured maximum sensitivity is 1.67%\/mN. The minimum resolvable force is 26 mN measured by the scanning circuit. The capacitance distributions induced by normal and shear forces were also successfully captured by the sensing array."}
{"_id":"bb17e8858b0d3a5eba2bb91f45f4443d3e10b7cd","title":"The Balanced Scorecard: Translating Strategy Into Action","text":""}
{"_id":"01cac0a7c2a3240cb77a1e090694a104785f78f5","title":"Workflow Automation: Overview and Research Issues","text":"Workflow management systems, a relatively recent technology, are designed to make work more efficient, integrate heterogeneous application systems, and support interorganizational processes in electronic commerce applications. In this paper, we introduce the field of workflow automation, the subject of this special issue of Information Systems Frontiers. In the first part of the paper, we provide basic definitions and frameworks to aid understanding of workflow management technologies. In the remainder of the paper, we discuss technical and management research opportunities in this field and discuss the other contributions to the special issue."}
{"_id":"4cdad9b059b5077fcce00fb8bcb4e381edd353bd","title":"A Novel Anomaly Detection Scheme Based on Principal Component Classifier","text":"This paper proposes a novel scheme that uses robust principal component classifier in intrusion detection problems where the training data may be unsupervised. Assuming that anomalies can be treated as outliers, an intrusion predictive model is constructed from the major and minor principal components of the normal instances. A measure of the difference of an anomaly from the normal instance is the distance in the principal component space. The distance based on the major components that account for 50% of the total variation and the minor components whose eigenvalues less than 0.20 is shown to work well. The experiments with KDD Cup 1999 data demonstrate that the proposed method achieves 98.94% in recall and 97.89% in precision with the false alarm rate 0.92% and outperforms the nearest neighbor method, density-based local outliers (LOF) approach, and the outlier detection algorithm based on Canberra metric."}
{"_id":"ca20f466791f4b051ef3b8d2bf63789d33c562c9","title":"CredFinder: A real-time tweets credibility assessing system","text":"Lately, Twitter has grown to be one of the most favored ways of disseminating information to people around the globe. However, the main challenge faced by the users is how to assess the credibility of information posted through this social network in real time. In this paper, we present a real-time content credibility assessment system named CredFinder, which is capable of measuring the trustworthiness of information through user analysis and content analysis. The proposed system is capable of providing a credibility score for each user's tweets. Hence, it provides users with the opportunity to judge the credibility of information faster. CredFinder consists of two parts: a frontend in the form of an extension to the Chrome browser that collects tweets in real time from a Twitter search or a user-timeline page and a backend that analyzes the collected tweets and assesses their credibility."}
{"_id":"9923edf7815c720aa0d6d58a28332806ae91b224","title":"Design of an Ultra-Wideband Pulse Generator Based on Avalanche Transistor","text":"Based on the avalanche effect of avalanche transistor, a kind of ultra-wideband nanosecond pulse circuit has been designed, whose frequency, pulse width and amplitude are tunable. In this paper, the principle, structure and selection of components' parameters in the circuit are analyzed in detail. The circuit generates periodic negative pulse, whose pulse full width is 890 ps and pulse amplitude is -11.2 V in simulation mode. By setting up circuit for experiment and changing parameters properly, a kind of ultra-wideband pulse with pulse width of 2.131 ns and pulse amplitude of -9.23 V is achieved. With the features such as simple structure, stable and reliable performance and low cost, this pulse generator is applicable to ultra-wideband wireless communication system."}
{"_id":"33b424698c2b7602dcb579513c34fe20cc3ae669","title":"A 0.5ps 1.4mW 50MS\/s Nyquist bandwidth time amplifier based two-step flash-\u0394\u03a3 time-to-digital converter","text":"We propose a 50-MS\/s two-step flash-\u0394\u03a3 time-to-digital converter (TDC) using stable time amplifiers (TAs). The TDC demonstrates low-levels of shaped quantization noise. The system is simulated in 40-nm CMOS and consumes 1.3 mA from a 1.1 V supply. The bandwidth is broadened to Nyquist rate. At frequencies below 25 MHz, the integrated TDC error is as low as 143 fsrms, which is equal to an equivalent TDC resolution of 0.5 ps."}
{"_id":"7fabd0639750563e0fb09df341e0e62ef4d6e1fb","title":"Brain-computer interfaces: communication and restoration of movement in paralysis.","text":"The review describes the status of brain-computer or brain-machine interface research. We focus on non-invasive brain-computer interfaces (BCIs) and their clinical utility for direct brain communication in paralysis and motor restoration in stroke. A large gap between the promises of invasive animal and human BCI preparations and the clinical reality characterizes the literature: while intact monkeys learn to execute more or less complex upper limb movements with spike patterns from motor brain regions alone without concomitant peripheral motor activity usually after extensive training, clinical applications in human diseases such as amyotrophic lateral sclerosis and paralysis from stroke or spinal cord lesions show only limited success, with the exception of verbal communication in paralysed and locked-in patients. BCIs based on electroencephalographic potentials or oscillations are ready to undergo large clinical studies and commercial production as an adjunct or a major assisted communication device for paralysed and locked-in patients. However, attempts to train completely locked-in patients with BCI communication after entering the complete locked-in state with no remaining eye movement failed. We propose that a lack of contingencies between goal directed thoughts and intentions may be at the heart of this problem. Experiments with chronically curarized rats support our hypothesis; operant conditioning and voluntary control of autonomic physiological functions turned out to be impossible in this preparation. In addition to assisted communication, BCIs consisting of operant learning of EEG slow cortical potentials and sensorimotor rhythm were demonstrated to be successful in drug resistant focal epilepsy and attention deficit disorder. First studies of non-invasive BCIs using sensorimotor rhythm of the EEG and MEG in restoration of paralysed hand movements in chronic stroke and single cases of high spinal cord lesions show some promise, but need extensive evaluation in well-controlled experiments. Invasive BMIs based on neuronal spike patterns, local field potentials or electrocorticogram may constitute the strategy of choice in severe cases of stroke and spinal cord paralysis. Future directions of BCI research should include the regulation of brain metabolism and blood flow and electrical and magnetic stimulation of the human brain (invasive and non-invasive). A series of studies using BOLD response regulation with functional magnetic resonance imaging (fMRI) and near infrared spectroscopy demonstrated a tight correlation between voluntary changes in brain metabolism and behaviour."}
{"_id":"a31b795f8defb59889df8f13321e057192d64f73","title":"iCONCUR: informed consent for clinical data and bio-sample use for research","text":"Background\nImplementation of patient preferences for use of electronic health records for research has been traditionally limited to identifiable data. Tiered e-consent for use of de-identified data has traditionally been deemed unnecessary or impractical for implementation in clinical settings.\n\n\nMethods\nWe developed a web-based tiered informed consent tool called informed consent for clinical data and bio-sample use for research (iCONCUR) that honors granular patient preferences for use of electronic health record data in research. We piloted this tool in 4 outpatient clinics of an academic medical center.\n\n\nResults\nOf patients offered access to iCONCUR, 394 agreed to participate in this study, among whom 126 patients accessed the website to modify their records according to data category and data recipient. The majority consented to share most of their data and specimens with researchers. Willingness to share was greater among participants from an Human Immunodeficiency Virus (HIV) clinic than those from internal medicine clinics. The number of items declined was higher for for-profit institution recipients. Overall, participants were most willing to share demographics and body measurements and least willing to share family history and financial data. Participants indicated that having granular choices for data sharing was appropriate, and that they liked being informed about who was using their data for what purposes, as well as about outcomes of the research.\n\n\nConclusion\nThis study suggests that a tiered electronic informed consent system is a workable solution that respects patient preferences, increases satisfaction, and does not significantly affect participation in research."}
{"_id":"7aa1866adbc2b4758c04d8484e5bf22e4cce9cc9","title":"Automotive radome design - fishnet structure for 79 GHz","text":"Metamaterials are considered as an option for quasi-optical matching of automotive radar radomes, lowering transmission loss and minimizing reflections. This paper shows a fishnet structure design for the 79 GHz band which is suitable for this type of matching and which exhibits a negative index of refraction. The measured transmission loss is 0.9 dB at 79 GHz. A tolerance study concerning copper plating, substrate permittivity, oblique incidence, and polarization is shown. Quasi-optical measurements were done in the range of 60 \u2013 90 GHz, which agree with simulated results."}
{"_id":"b95dd9e28f2126aac27da8b0378d3b9487d8b73d","title":"Avatar-independent scripting for real-time gesture animation","text":"When animation of a humanoid figure is to be generated at run-time, instead of by replaying precomposed motion clips, some method is required of specifying the avatar\u2019s movements in a form from which the required motion data can be automatically generated. This form must be of a more abstract nature than raw motion data: ideally, it should be independent of the particular avatar\u2019s proportions, and both writable by hand and suitable for automatic generation from higher-level descriptions of the required actions. We describe here the development and implementation of such a scripting language for the particular area of sign languages of the deaf, called SiGML (Signing Gesture Markup Language), based on the existing HamNoSys notation for sign languages. We conclude by suggesting how this work may be extended to more general animation for interactive virtual reality applications."}
{"_id":"267718d3b9399a5eab90a1b1701e78369696e8fe","title":"Analyzing multiple configurations of a C program","text":"Preprocessor conditionals are heavily used in C programs since they allow the source code to be configured for different platforms or capabilities. However, preprocessor conditionals, as well as other preprocessor directives, are not part of the C language. They need to be evaluated and removed, and so a single configuration selected, before parsing can take place. Most analysis and program understanding tools run on this preprocessed version of the code so their results are based on a single configuration. This paper describes the approach of CRefactory, a refactoring tool for C programs. A refactoring tool cannot consider only a single configuration: changing the code for one configuration may break the rest of the code. CRefactory analyses the program for all possible configurations simultaneously. CRefactory also preserves preprocessor directives and integrates them in the internal representations. The paper also presents metrics from two case studies to show that CRefactory's program representation is practical."}
{"_id":"63abfb7d2d35d60a5dc2cc884251f9fee5d46963","title":"The contribution of electrophysiology to functional connectivity mapping","text":"A powerful way to probe brain function is to assess the relationship between simultaneous changes in activity across different parts of the brain. In recent years, the temporal activity correlation between brain areas has frequently been taken as a measure of their functional connections. Evaluating 'functional connectivity' in this way is particularly popular in the fMRI community, but has also drawn interest among electrophysiologists. Like hemodynamic fluctuations observed with fMRI, electrophysiological signals display significant temporal fluctuations, even in the absence of a stimulus. These neural fluctuations exhibit a correlational structure over a wide range of spatial and temporal scales. Initial evidence suggests that certain aspects of this correlational structure bear a high correspondence to so-called functional networks defined using fMRI. The growing family of methods to study activity covariation, combined with the diverse neural mechanisms that contribute to the spontaneous fluctuations, has somewhat blurred the operational concept of functional connectivity. What is clear is that spontaneous activity is a conspicuous, energy-consuming feature of the brain. Given its prominence and its practical applications for the functional connectivity mapping of brain networks, it is of increasing importance that we understand its neural origins as well as its contribution to normal brain function."}
{"_id":"1a1f0d0abcbdaa2d487f0a46dba1ca097774012d","title":"Practical Backscatter Communication Systems for Battery-Free Internet of Things: A Tutorial and Survey of Recent Research","text":"Backscatter presents an emerging ultralow-power wireless communication paradigm. The ability to offer submilliwatt power consumption makes it a competitive core technology for Internet of Things (IoT) applications. In this article, we provide a tutorial of backscatter communication from the signal processing perspective as well as a survey of the recent research activities in this domain, primarily focusing on bistatic backscatter systems. We also discuss the unique real-world applications empowered by backscatter communication and identify open questions in this domain. We believe this article will shed light on the low-power wireless connectivity design toward building and deploying IoT services in the wild."}
{"_id":"6fd78d20e6f51d872f07cde9350f4d31078ff723","title":"OF A TUNABLE STIFFNESS COMPOSITE LEG FOR DYNAMIC LOCOMOTION","text":"Passively compliant legs have been instrumental in the development of dynamically running legged robots. Having properly tuned leg springs is essential for stable, robust and energetically efficient running at high speeds. Recent simulation studies indicate that having variable stiffness legs, as animals do, can significantly improve the speed and stability of these robots in changing environmental conditions. However, to date, the mechanical complexities of designing usefully robust tunable passive compliance into legs has precluded their implementation on practical running robots. This paper describes a new design of a \u201dstructurally controlled variable stiffness\u201d leg for a hexapedal running robot. This new leg improves on previous designs\u2019 performance and enables runtime modification of leg stiffness in a small, lightweight, and rugged package. Modeling and leg test experiments are presented that characterize the improvement in stiffness range, energy storage, and dynamic coupling properties of these legs. We conclude that this variable stiffness leg design is now ready for implementation and testing on a dynamical running robot."}
{"_id":"d1c4907b1b225f61059915a06a3726706860c71e","title":"An in-depth study of the promises and perils of mining GitHub","text":"With over 10 million git repositories, GitHub is becoming one of the most important sources of software artifacts on the Internet. Researchers mine the information stored in GitHub\u2019s event logs to understand how its users employ the site to collaborate on software, but so far there have been no studies describing the quality and properties of the available GitHub data. We document the results of an empirical study aimed at understanding the characteristics of the repositories and users in GitHub; we see how users take advantage of GitHub\u2019s main features and how their activity is tracked on GitHub and related datasets to point out misalignment between the real and mined data. Our results indicate that while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. For example, we show that the majority of the projects are personal and inactive, and that almost 40 % of all pull requests do not appear as merged even though they were. Also, approximately half of GitHub\u2019s registered users do not have public activity, while the activity of GitHub users in repositories is not always easy to pinpoint. We use our identified perils to see if they can pose validity threats; we review selected papers from the MSR 2014 Mining Challenge and see if there are potential impacts to consider. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub."}
{"_id":"9c1ebae0eea2aa27fed13c71dc98dc0f67dd52a0","title":"Unsupervised Segmentation of 3D Medical Images Based on Clustering and Deep Representation Learning","text":"This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation."}
{"_id":"27a693acee22752fa66f442b8d52b7f3c83134c7","title":"Optimal Multiserver Configuration for Profit Maximization in Cloud Computing","text":"As cloud computing becomes more and more popular, understanding the economics of cloud computing becomes critically important. To maximize the profit, a service provider should understand both service charges and business costs, and how they are determined by the characteristics of the applications and the configuration of a multiserver system. The problem of optimal multiserver configuration for profit maximization in a cloud computing environment is studied. Our pricing model takes such factors into considerations as the amount of a service, the workload of an application environment, the configuration of a multiserver system, the service-level agreement, the satisfaction of a consumer, the quality of a service, the penalty of a low-quality service, the cost of renting, the cost of energy consumption, and a service provider's margin and profit. Our approach is to treat a multiserver system as an M\/M\/m queuing model, such that our optimization problem can be formulated and solved analytically. Two server speed and power consumption models are considered, namely, the idle-speed model and the constant-speed model. The probability density function of the waiting time of a newly arrived service request is derived. The expected service charge to a service request is calculated. The expected net business gain in one unit of time is obtained. Numerical calculations of the optimal server size and the optimal server speed are demonstrated."}
{"_id":"e8217edd7376c26c714757a362724f81f3afbee0","title":"Overview on Additive Manufacturing Technologies","text":"This paper provides an overview on the main additive manufacturing\/3D printing technologies suitable for many satellite applications and, in particular, radio-frequency components. In fact, nowadays they have become capable of producing complex net-shaped or nearly net-shaped parts in materials that can be directly used as functional parts, including polymers, metals, ceramics, and composites. These technologies represent the solution for low-volume, high-value, and highly complex parts and products."}
{"_id":"bb6e6e3251bbb80587bdb5064e24b55d728529b1","title":"Mixed Methods Research : A Research Paradigm Whose Time Has Come","text":"14 The purposes of this article are to position mixed methods research (mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm \u201cwars\u201d and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research (mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it."}
{"_id":"0b3cfbf79d50dae4a16584533227bb728e3522aa","title":"Long Short-Term Memory","text":"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."}
{"_id":"69613390ca76bf103791ef251e1568deb5fe91dd","title":"Satellite Image Classification Methods and Techniques: A Review","text":"Satellite image classification process involves grouping the image pixel values into meaningful categories. Several satellite image classification methods and techniques are available. Satellite image classification methods can be broadly classified into three categories 1) automatic 2) manual and 3) hybrid. All three methods have their own advantages and disadvantages. Majority of the satellite image classification methods fall under first category. Satellite image classification needs selection of appropriate classification method based on the requirements. The current research work is a study on satellite image classification methods and techniques. The research work also compares various researcher&apos;s comparative results on satellite image classification methods."}
{"_id":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database","text":"WordNet is perhaps the most important and widely used lexical resource for natural language processing systems up to now. WordNet: An Electronic Lexical Database, edited by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive Sciences Laboratory of Princeton University, where WordNet was produced, and nine chapters contributed by scientists from elsewhere. Miller's foreword offers a fascinating account of the history of WordNet. He discusses the presuppositions of such a lexical database, how the top-level noun categories were determined, and the sources of the words in WordNet. He also writes about the evolution of WordNet from its original incarnation as a dictionary browser to a broad-coverage lexicon, and the involvement of different people during its various stages of development over a decade. It makes very interesting reading for casual and serious users of WordNet and anyone who is grateful for the existence of WordNet. The book is organized in three parts. Part I is about WordNet itself and consists of four chapters: \"Nouns in WordNet\" by George Miller, \"Modifiers in WordNet\" by Katherine Miller, \"A semantic network of English verbs\" by Christiane Fellbaum, and \"Design and implementation of the WordNet lexical database and search software\" by Randee Tengi. These chapters are essentially updated versions of four papers from Miller (1990). Compared with the earlier papers, the chapters in this book focus more on the underlying assumptions and rationales behind the design decisions. The description of the information contained in WordNet, however, is not as detailed as in Miller (1990). The main new additions in these chapters include an explanation of sense grouping in George Miller's chapter, a section about adverbs in Katherine Miller's chapter, observations about autohyponymy (one sense of a word being a hyponym of another sense of the same word) and autoantonymy (one sense of a word being an antonym of another sense of the same word) in Fellbaum's chapter, and Tengi's description of the Grinder, a program that converts the files the lexicographers work with to searchable lexical databases. The three papers in Part II are characterized as \"extensions, enhancements and"}
{"_id":"1528def1ddbd2deb261ebb873479f27f48251031","title":"Clustering WordNet word senses","text":"This paper presents the results of a set of methods to cluster WordNet word senses. The methods rely on different information sources: confusion matrixes from Senseval-2 Word Sense Disambiguation systems, translation similarities, hand-tagged examples of the target word senses and examples obtained automatically from the web for the target word senses. The clustering results have been evaluated using the coarsegrained word senses provided for the lexical sample in Senseval-2. We have used Cluto, a general clustering environment, in order to test different clustering algorithms. The best results are obtained for the automatically obtained examples, yielding purity values up to 84% on average over 20 nouns."}
{"_id":"b49af9c4ab31528d37122455e4caf5fdeefec81a","title":"Smart homes and their users: a systematic analysis and key challenges","text":"Published research on smart homes and their users is growing exponentially, yet a clear understanding of who these users are and how they might use smart home technologies is missing from a field being overwhelmingly pushed by technology developers. Through a systematic analysis of peer-reviewed literature on smart homes and their users, this paper takes stock of the dominant research themes and the linkages and disconnects between them. Key findings within each of nine themes are analysed, grouped into three: (1) views of the smart home\u2014functional, instrumental, socio-technical; (2) users and the use of the smart home\u2014prospective users, interactions and decisions, using technologies in the home; and (3) challenges for realising the smart home\u2014hardware and software, design, domestication. These themes are integrated into an organising framework for future research that identifies the presence or absence of cross-cutting relationships between different understandings of smart homes and their users. The usefulness of the organising framework is illustrated in relation to two major concerns\u2014privacy and control\u2014that have been narrowly interpreted to date, precluding deeper insights and potential solutions. Future research on smart homes and their users can benefit by exploring and developing cross-cutting relationships between the research themes identified."}
{"_id":"0414c4cc1974e6d3e69d9f2986e5bb9fb1af4701","title":"Natural Language Processing using NLTK and WordNet","text":"Natural Language Processing is a theoretically motivated range of computational techniques for analysing and representing naturally occurring texts at one or more levels of linguistic analysis for the purpose of achieving human-like language processing for a range of tasks or applications [1]. To perform natural language processing a variety of tools and platform have been developed, in our case we will discuss about NLTK for Python.The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language[2]. It provides easy-to-use interfaces to many corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. In this paper we discuss different approaches for natural language processing using NLTK."}
{"_id":"7d46c3648f76b5542d8ecd582f01f155e6b248d5","title":"On Formal and Cognitive Semantics for Semantic Computing","text":"Semantics is the meaning of symbols, notations, concepts, functions, and behaviors, as well as their relations that can be deduced onto a set of predefined entities and\/or known concepts. Semantic computing is an emerging computational methodology that models and implements computational structures and behaviors at semantic or knowledge level beyond that of symbolic data. In semantic computing, formal semantics can be classified into the categories of to be, to have, and to do semantics. This paper presents a comprehensive survey of formal and cognitive semantics for semantic computing in the fields of computational linguistics, software science, computational intelligence, cognitive computing, and denotational mathematics. A set of novel formal semantics, such as deductive semantics, concept-algebra-based semantics, and visual semantics, is introduced that forms a theoretical and cognitive foundation for semantic computing. Applications of formal semantics in semantic computing are presented in case studies on semantic cognition of natural languages, semantic analyses of computing behaviors, behavioral semantics of human cognitive processes, and visual semantic algebra for image and visual object manipulations."}
{"_id":"41f1abe566060e53ad93d8cfa8c39ac582256868","title":"Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial","text":"The state machine approach is a general method for implementing fault-tolerant services in distributed systems. This paper reviews the approach and describes protocols for two different failure models\u2014Byzantine and fail stop. Systems reconfiguration techniques for removing faulty components and integrating repaired components are also discussed."}
{"_id":"052b1d8ce63b07fec3de9dbb583772d860b7c769","title":"Learning representations by back-propagating errors","text":"We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."}
{"_id":"07f3f736d90125cb2b04e7408782af411c67dd5a","title":"Convolutional Neural Network Architectures for Matching Natural Language Sentences","text":"Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layerby-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models."}
{"_id":"0af737eae02032e66e035dfed7f853ccb095d6f5","title":"ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs","text":"How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence\u2019s representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks. We release code at: https:\/\/github.com\/yinwenpeng\/Answer_Selection."}
{"_id":"1c059493904b2244d2280b8b4c0c7d3ca115be73","title":"node2vec: Scalable Feature Learning for Networks","text":"Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.\n We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks."}
{"_id":"468b9055950c428b17f0bf2ff63fe48a6cb6c998","title":"A Neural Attention Model for Abstractive Sentence Summarization","text":"Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."}
{"_id":"81eb0a1ea90a6f6d5e7f14cb3397a4ee0f77824a","title":"Question\/Answer Matching for CQA System via Combining Lexical and Sequential Information","text":"Community-based Question Answering (CQA) has become popular in knowledge sharing sites since it allows users to get answers to complex, detailed, and personal questions directly from other users. Large archives of historical questions and associated answers have been accumulated. Retrieving relevant historical answers that best match a question is an essential component of a CQA service. Most state of the art approaches are based on bag-of-words models, which have been proven successful in a range of text matching tasks, but are insufficient for capturing the important word sequence information in short text matching. In this paper, a new architecture is proposed to more effectively model the complicated matching relations between questions and answers. It utilises a similarity matrix which contains both lexical and sequential information. Afterwards the information is put into a deep architecture to find potentially suitable answers. The experimental study shows its potential in improving matching accuracy of question and answer."}
{"_id":"81ff60a35e57e150875cfdde735fe69d19e9fdc4","title":"Development of attentional networks in childhood","text":"Recent research in attention has involved three networks of anatomical areas that carry out the functions of orienting, alerting and executive control (including conflict monitoring). There have been extensive cognitive and neuroimaging studies of these networks in adults. We developed an integrated Attention Network Test (ANT) to measure the efficiency of the three networks with adults. We have now adapted this test to study the development of these networks during childhood. The test is a child-friendly version of the flanker task with alerting and orienting cues. We studied the development of the attentional networks in a cross-sectional experiment with four age groups ranging from 6 through 9 (Experiment 1). In a second experiment, we compared children (age 10 years) and adult performance in both child and adults versions of the ANT. Reaction time and accuracy improved at each age interval and positive values were found for the average efficiency of each of the networks. Alertness showed evidence of change up to and beyond age 10, while conflict scores appear stable after age seven and orienting scores do not change in the age range studied. A final experiment with forty 7-year-old children suggested that children like adults showed independence between the three networks under some conditions."}
{"_id":"6c1cabe3f5980cbc50d290c2ed60b9aca624eab8","title":"Mathematical modelling of infectious diseases.","text":"INTRODUCTION\nMathematical models allow us to extrapolate from current information about the state and progress of an outbreak, to predict the future and, most importantly, to quantify the uncertainty in these predictions. Here, we illustrate these principles in relation to the current H1N1 epidemic.\n\n\nSOURCES OF DATA\nMany sources of data are used in mathematical modelling, with some forms of model requiring vastly more data than others. However, a good estimation of the number of cases is vitally important.\n\n\nAREAS OF AGREEMENT\nMathematical models, and the statistical tools that underpin them, are now a fundamental element in planning control and mitigation measures against any future epidemic of an infectious disease. Well-parameterized mathematical models allow us to test a variety of possible control strategies in computer simulations before applying them in reality.\n\n\nAREAS OF CONTROVERSY\nThe interaction between modellers and public-health practitioners and the level of detail needed for models to be of use.\n\n\nGROWING POINTS\nThe need for stronger statistical links between models and data.\n\n\nAREAS TIMELY FOR DEVELOPING RESEARCH\nGreater appreciation by the medical community of the uses and limitations of models and a greater appreciation by modellers of the constraints on public-health resources."}
{"_id":"5e22c4362df3b0accbe04517c41848a2b229efd1","title":"Predicting sports events from past results Towards effective betting on football matches","text":"A system for predicting the results of football matches that beats the bookmakers\u2019 odds is presented. The predictions for the matches are based on previous results of the teams involved."}
{"_id":"de93c4f886bdf55bfc1bcaefad648d5996ed3302","title":"Modern Intrusion Detection, Data Mining, and Degrees of Attack Guilt","text":"This chapter examines the state of modern intrusion detection, with a particular emphasis on the emerging approach of data mining. The discussion paralleIs two important aspects of intrusion detection: general detection strategy (misuse detection versus anomaly detection) and data source (individual hosts versus network trafik). Misuse detection attempts to match known patterns of intrusion , while anomaly detection searches for deviations from normal behavior . Between the two approaches, only anomaly detection has the ability to detect unknown attacks. A particularly promising approach to anomaly detection combines association mining with other forms of machine learning such as classification. Moreover, the data source that an intrusion detection system employs significantly impacts the types of attacks it can detect. There is a tradeoff in the level of detailed information available verD. Barbar\u00e1 et al. (ed .), Applications of Data Mining in Computer Security \u00a9 Kluwer Academic Publishers 2002 s"}
{"_id":"df25eaf576f55c09bb460d67134646fcb422b2ac","title":"AGA: Attribute-Guided Augmentation","text":"We consider the problem of data augmentation, i.e., generating artificial samples to extend a given corpus of training data. Specifically, we propose attributed-guided augmentation (AGA) which learns a mapping that allows to synthesize data such that an attribute of a synthesized sample is at a desired value or strength. This is particularly interesting in situations where little data with no attribute annotation is available for learning, but we have access to a large external corpus of heavily annotated samples. While prior works primarily augment in the space of images, we propose to perform augmentation in feature space instead. We implement our approach as a deep encoder-decoder architecture that learns the synthesis function in an end-to-end manner. We demonstrate the utility of our approach on the problems of (1) one-shot object recognition in a transfer-learning setting where we have no prior knowledge of the new classes, as well as (2) object-based one-shot scene recognition. As external data, we leverage 3D depth and pose information from the SUN RGB-D dataset. Our experiments show that attribute-guided augmentation of high-level CNN features considerably improves one-shot recognition performance on both problems."}
{"_id":"b52abc5f401a6dec62d650f5a2a500f469b9a7c0","title":"A Case Study on Barriers and Enhancements of the PET Bottle-to-Bottle Recycling Systems in Germany and Sweden","text":"Problem: The demand of beverages in PET bottles is constantly increasing. In this context, environmental, technological and regulatory aspects set a stronger focus on recycling. Generally, the reuse of recycled material from post-consumer PET bottles in bottle-to-bottle applications is seen as least environmentally harmful. However, closedloop systems are not widely implemented in Europe. Previous research mainly focuses on open-loop recycling systems and generally lacks discussion about the current German and Swedish systems and their challenges. Furthermore, previous studies lack theoretical and practical enhancements for bottle-to-bottle recycling from a managerial perspective. Purpose: The purpose of this study is to compare the PET bottle recycling systems in Germany and Sweden, analyse the main barriers and develop enhancements for closedloop systems. Method: This qualitative study employs a case study strategy about the two cases of Germany and Sweden. In total, 14 semi-structured interviews are conducted with respondents from different industry sectors within the PET bottle recycling systems. The empirical data is categorised and then analysed by pattern matching with the developed theoretical framework. Conclusion: Due to the theoretical and practical commitment to closed-loop recycling, the Swedish PET bottle recycling system outperforms the Germany system. In Germany, bottle-to-bottle recycling is currently performed on a smaller scale without a unified system. The main barriers for bottle-to-bottle recycling are distinguished into (1) quality and material factors, (2) regulatory and legal factors, (3) economic and market factors and (4) factors influenced by consumers. The enhancements for the systems are (1) quality and material factors, (2) regulatory and legal factors, (3) recollection factors and (4) expanding factors. Lastly, the authors provide further recommendations, which are (1) a recycling content symbol on bottle labels, (2) a council for bottle quality in Germany, (3) a quality seal for the holistic systems, (4) a reduction of transportation in Sweden and (5) an increase of consumer awareness on PET bottle consumption."}
{"_id":"9e00005045a23f3f6b2c9fca094930f8ce42f9f6","title":"Managing Portfolios of Development Projects in a Complex Environment How the UN assign priorities to Programs at the Country","text":""}
{"_id":"2ec2f8cd6cf1a393acbc7881b8c81a78269cf5f7","title":"Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics","text":"We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images."}
{"_id":"94d1a665c0c7fbd017c9f3c50d35992e1c0c1ed0","title":"Molecular and Morphological Characterization of Aphelenchoides fuchsi sp. n. (Nematoda: Aphelenchoididae) Isolated from Pinus eldarica in Western Iran.","text":"Aphelenchoides fuchsi sp. n. is described and illustrated from bark and wood samples of a weakened Mondell pine in Kermanshah Province, western Iran. The new species has body length of 332 to 400 \u00b5m (females) and 365 to 395 \u00b5m (males). Lip region set off from body contour. The cuticle is weakly annulated, and there are four lines in the lateral field. The stylet is 8 to 10 \u03bcm long and has small basal swellings. The excretory pore is located ca one body diam. posterior to metacorpus valve or 51 to 62 \u03bcm from the head. The postuterine sac well developed (60-90 \u00b5m). Spicules are relatively short (15-16 \u03bcm in dorsal limb) with apex and rostrum rounded, well developed, and the end of the dorsal limb clearly curved ventrad like a hook. The male tail has usual three pairs of caudal papillae (2+2+2) and a well-developed mucro. The female tail is conical, terminating in a complicated step-like projection, usually with many tiny nodular protuberances. The new species belongs to the Group 2 sensu Shahina, category of Aphelenchoides species. Phylogenetic analysis based on small subunit (SSU) and partial large subunit (LSU) sequences of rRNA supported the morphological results."}
{"_id":"4d40a715a51bcca554915ecc5d88005fd56dc1e5","title":"The future of seawater desalination: energy, technology, and the environment.","text":"In recent years, numerous large-scale seawater desalination plants have been built in water-stressed countries to augment available water resources, and construction of new desalination plants is expected to increase in the near future. Despite major advancements in desalination technologies, seawater desalination is still more energy intensive compared to conventional technologies for the treatment of fresh water. There are also concerns about the potential environmental impacts of large-scale seawater desalination plants. Here, we review the possible reductions in energy demand by state-of-the-art seawater desalination technologies, the potential role of advanced materials and innovative technologies in improving performance, and the sustainability of desalination as a technological solution to global water shortages."}
{"_id":"6180482e02eb79eca6fd2e9b1ee9111d749d5ca2","title":"A bidirectional soft pneumatic fabric-based actuator for grasping applications","text":"THIS paper presents the development of a bidirectional fabric-based soft pneumatic actuator requiring low fluid pressurization for actuation, which is incorporated into a soft robotic gripper to demonstrate its utility. The bidirectional soft fabric-based actuator is able to provide both flexion and extension. Fabrication of the fabric actuators is simple as compared to the steps involved in traditional silicone-based approach. In addition, the fabric actuators are able to generate comparably larger vertical grip resistive force at lower operating pressure than elastomeric actuators and 3D-printed actuators, being able to generate resistive grip force up to 20N at 120 kPa. Five of the bidirectional soft fabric-based actuators are deployed within a five-fingered soft robotic gripper, complete with five casings and a base. It is capable of grasping a variety of objects with maximum width or diameter closer to its bending curvature. A cutting task involved bimanual manipulation was demonstrated successfully with the gripper. To incorporate intelligent control for such a task, a soft force made completely of compliant material was attached to the gripper, which allows determination of whether the cutting task is completed. To the authors' knowledge, this work is the first study which incorporates two soft robotic grippers for bimanual manipulation with one of the grippers sensorized to provide closed loop control."}
{"_id":"3f0924241a7deba2b40b0c1ea57a2e3d10c57ae0","title":"Principles of GNSS, inertial, and multisensor integrated navigation systems, 2nd edition [Book review]","text":"This second edition of Dr. Grove's book (the original was published in 2008) could arguably be considered a new work. At just under 1,000 pages (including the 11 appendices on the DVD), the second edition is 80% longer than the original. Frankly, the word \"book\" hardly seems adequate, considering the wide range of topics covered. \"Mini-encyclopedia\" seems more appropriate. The hardcover portion of the book comprises 18 chapters, and the DVD includes the aforementioned appendices plus 20 fully worked examples, 125 problems or exercises (with answers), and MATLAB routines for the simulation of many of the algorithms discussed in the main text. Here is a brief overview of the contents: \u25b8 Chapters 1\u20133: an overview of the diversity of positioning techniques and navigation systems; fundamentals of coordinate frames, kinematics and earth models; introduction to Kaiman filtering \u25b8 Chapters 4\u20136: inertial sensors, inertial navigation, and lower-cost dead reckoning systems \u25b8 Chapters 7\u201312: principles of radio positioning, short-, medium-, and long-range radio navigation, as well as extensive coverage of global navigation satellite systems (GNSS) \u25b8 Chapter 13: environmental feature matching. \u25b8 Chapters 14\u201316: various integration topics, including inertial navigation system (INS)\/GNSS integration, alignment, zero-velocity updates, and multisensor integration \u25b8 Chapter 17: fault detection. \u25b8 Chapter 18: applications and trends. In summary, this book is an excellent reference (with numerous nuggets of wisdom) that should be readily handy on the shelf of every practicing navigation engineer. In the hands of an experienced instructor, the book will also serve students as a great textbook. However, the lack of examples integrated in the main text makes it difficult for the book to serve as a self-study guide for those that are new to the field."}
{"_id":"b0e7d36c94935fadf3c514903e4340eaa415e4ee","title":"True self-configuration for the IoT","text":"For the Internet of Things to finally become a reality, obstacles on different levels need to be overcome. This is especially true for the upcoming challenge of leaving the domain of technical experts and scientists. Devices need to connect to the Internet and be able to offer services. They have to announce and describe these services in machine-understandable ways so that user-facing systems are able to find and utilize them. They have to learn about their physical surroundings, so that they can serve sensing or acting purposes without explicit configuration or programming. Finally, it must be possible to include IoT devices in complex systems that combine local and remote data, from different sources, in novel and surprising ways. We show how all of that is possible today. Our solution uses open standards and state-of-the art protocols to achieve this. It is based on 6LowPAN and CoAP for the communications part, semantic web technologies for meaningful data exchange, autonomous sensor correlation to learn about the environment, and software built around the Linked Data principles to be open for novel and unforeseen applications."}
{"_id":"a8e656fe16825c47a41df9b28e0c97d4bc8fa58f","title":"From turtles to Tangible Programming Bricks: explorations in physical language design","text":"This article provides a historical overview of educational computing research at MIT from the mid-1960s to the present day, focusing on physical interfaces. It discusses some of the results of this research: electronic toys that help children develop advanced modes of thinking through free-form play. In this historical context, the article then describes and discusses the author\u2019s own research into tangible programming, culminating in the development of the Tangible Programming Bricks system\u2014a platform for creating microworlds for children to explore computation and scientific thinking."}
{"_id":"f83a207712fd4cf41aded79e9e6c4345ba879128","title":"Ray: A Distributed Framework for Emerging AI Applications","text":"The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray\u2014a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system\u2019s control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications."}
{"_id":"aa2213a9f39736f80ccc54b9096e414682afa082","title":"Wave-front Transformation with Gradient Metasurfaces","text":"Relying on abrupt phase discontinuities, metasurfaces characterized by a transversely inhomogeneous surface impedance profile have been recently explored as an ultrathin platform to generate arbitrary wave fronts over subwavelength thicknesses. Here, we outline fundamental limitations of passive gradient metasurfaces in molding the impinging wave and show that local phase compensation is essentially insufficient to realize arbitrary wave manipulation, but full-wave designs should be considered. These findings represent a critical step towards realistic and highly efficient conformal wave manipulation beyond the scope of ray optics, enabling unprecedented nanoscale light molding."}
{"_id":"8641be8daff5b24e98a0d68138a61456853aef82","title":"Adaptation impact and environment models for architecture-based self-adaptive systems","text":"Self-adaptive systems have the ability to adapt their behavior to dynamic operating conditions. In reaction to changes in the environment, these systems determine the appropriate corrective actions based in part on information about which action will have the best impact on the system. Existing models used to describe the impact of adaptations are either unable to capture the underlying uncertainty and variability of such dynamic environments, or are not compositional and described at a level of abstraction too low to scale in terms of specification effort required for non-trivial systems. In this paper, we address these shortcomings by describing an approach to the specification of impact models based on architectural system descriptions, which at the same time allows us to represent both variability and uncertainty in the outcome of adaptations, hence improving the selection of the best corrective action. The core of our approach is a language equipped with a formal semantics defined in terms of Discrete Time Markov Chains that enables us to describe both the impact of adaptation tactics, as well as the assumptions about the environment. To validate our approach, we show how employing our language can improve the accuracy of predictions used for decision-making in the Rainbow framework for architecture-based self-adaptation."}
{"_id":"a65e815895bed510c0549957ce6baa129c909813","title":"Induction of Root and Pattern Lexicon for Unsupervised Morphological Analysis of Arabic","text":"We propose an unsupervised approach to learning non-concatenative morphology, which we apply to induce a lexicon of Arabic roots and pattern templates. The approach is based on the idea that roots and patterns may be revealed through mutually recursive scoring based on hypothesized pattern and root frequencies. After a further iterative refinement stage, morphological analysis with the induced lexicon achieves a root identification accuracy of over 94%. Our approach differs from previous work on unsupervised learning of Arabic morphology in that it is applicable to naturally-written, unvowelled text."}
{"_id":"5da41b7d7b1963cd1e86d99b4d9b86ad6d7a227a","title":"An Unequal Wilkinson Power Divider for a Frequency and Its First Harmonic","text":"This letter presents a Wilkinson power divider operating at a frequency and its first harmonic with unequal power divider ratio. To obtain the unequal property, four groups of 1\/6 wavelength transmission lines with different characteristic impedances are needed to match all ports. Theoretically, closed-form equations for the design are derived based on transmission line theory. Experimental results have indicated that all the features of this novel power divider can be fulfilled at f 0 and 2f 0 simultaneously."}
{"_id":"6cd700af0b7953345d831c129a5a4e0d927bfa19","title":"Adaptive Haptic Feedback Steering Wheel for Driving Simulators","text":"Controlling a virtual vehicle is a sensory-motor activity with a specific rendering methodology that depends on the hardware technology and the software in use. We propose a method that computes haptic feedback for the steering wheel. It is best suited for low-cost, fixed-base driving simulators but can be ported to any driving simulator platform. The goal of our method is twofold. 1) It provides an efficient yet simple algorithm to model the steering mechanism using a quadri-polar representation. 2) This model is used to compute the haptic feedback on top of which a tunable haptic augmentation is adjusted to overcome the lack of presence and the unavoidable simulation loop latencies. This algorithm helps the driver to laterally control the virtual vehicle. We also discuss the experimental results that demonstrate the usefulness of our haptic feedback method."}
{"_id":"3f4e71d715fce70c89e4503d747aad11fcac8a43","title":"Competing Values in the Era of Digitalization","text":"This case study examines three different digital innovation projects within Auto Inc -- a large European automaker. By using the competing values framework as a theoretical lens we explore how dynamic capabilities occur in a firm trying to meet increasing demands in originating and innovating from digitalization. In this digitalization process, our study indicates that established socio-technical congruences are being challenged. More so, we pinpoint the need for organizations to find ways to embrace new experimental learning processes in the era of digitalization. While such a change requires long-term commitment and vision, this study presents three informal enablers for such experimental processes these enablers are timing, persistence, and contacts."}
{"_id":"215b4c25ad34557644b1a177bd5aeac8b2e66bc6","title":"Why Your Encrypted Database Is Not Secure","text":"Encrypted databases, a popular approach to protecting data from compromised database management systems (DBMS's), use abstract threat models that capture neither realistic databases, nor realistic attack scenarios. In particular, the \"snapshot attacker\" model used to support the security claims for many encrypted databases does not reflect the information about past queries available in any snapshot attack on an actual DBMS.\n We demonstrate how this gap between theory and reality causes encrypted databases to fail to achieve their \"provable security\" guarantees."}
{"_id":"84cf1178a7526355f323ce0442458de3b3744358","title":"A high performance parallel algorithm for 1-D FFT","text":"In this paper we propose a parallel high performance FFT algorithm based on a multi-dimensional formulation. We use this to solve a commonly encountered FFT based kernel on a distributed memory parallel machine, the IBM scalable parallel system, SP1. The kernel requires a forward FFT computation of an input sequence, multiplication of the transformed data by a coefficient array, and finally an inverse FFT computation of the resultant data. We show that the multidimensional formulation helps in reducing the communication costs and also improves the single node performance by effectively utilizing the memory system of the node. We implemented this kernel on the IBM SP1 and observed a performance of 1.25 GFLOPS on a 64-node machine."}
{"_id":"1f7594d3be7f5c32e117bc669ed898dd0af88aa3","title":"Dual-Band Textile MIMO Antenna Based on Substrate-Integrated Waveguide (SIW) Technology","text":"A dual-band textile antenna for multiple-input-multiple-output (MIMO) applications, based on substrate-integrated waveguide (SIW) technology, is designed. The fundamental SIW cavity mode is designed to resonate at 2.4 GHz. Meanwhile, the second and third modes are modified and combined by careful placement of a via within the cavity to enable wideband coverage in the 5-GHz WLAN band. The simple antenna topology can be fabricated fully using textiles in a planar form, ensuring reliability and comfort. Numerical and experimental results indicate satisfactory antenna performance when worn on body in terms of impedance bandwidth, radiation efficiency, and specific absorption ratio (SAR). In order to validate its potential for MIMO applications, two elements of the proposed SIW antenna are arranged in six configurations to study the performance in terms of mutual coupling and envelope correlation. It is observed that the placement of the shorted edges of the two elements adjacent to each other produces the lowest mutual coupling and consequently the best envelope correlation."}
{"_id":"a2204b1ae6109db076a2b3c8d0db8cf390008812","title":"Low self-esteem during adolescence predicts poor health, criminal behavior, and limited economic prospects during adulthood.","text":"Using prospective data from the Dunedin Multidisciplinary Health and Development Study birth cohort, the authors found that adolescents with low self-esteem had poorer mental and physical health, worse economic prospects, and higher levels of criminal behavior during adulthood, compared with adolescents with high self-esteem. The long-term consequences of self-esteem could not be explained by adolescent depression, gender, or socioeconomic status. Moreover, the findings held when the outcome variables were assessed using objective measures and informant reports; therefore, the findings cannot be explained by shared method variance in self-report data. The findings suggest that low self-esteem during adolescence predicts negative real-world consequences during adulthood."}
{"_id":"506277ae84149b82d215f76bc4f7135400f65b1d","title":"User-defined Interface Gestures: Dataset and Analysis","text":"We present a video-based gesture dataset and a methodology for annotating video-based gesture datasets. Our dataset consists of user-defined gestures generated by 18 participants from a previous investigation of gesture memorability. We design and use a crowd-sourced classification task to annotate the videos. The results are made available through a web-based visualization that allows researchers and designers to explore the dataset. Finally, we perform an additional descriptive analysis and quantitative modeling exercise that provide additional insights into the results of the original study. To facilitate the use of the presented methodology by other researchers we share the data, the source of the human intelligence tasks for crowdsourcing, a new taxonomy that integrates previous work, and the source code of the visualization tool."}
{"_id":"02227c94dd41fe0b439e050d377b0beb5d427cda","title":"Reading Digits in Natural Images with Unsupervised Feature Learning","text":"Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks."}
{"_id":"64887b38c382e331cd2b045f7a7edf05f17586a8","title":"Genetic and environmental influences on sexual orientation and its correlates in an Australian twin sample.","text":"We recruited twins systematically from the Australian Twin Registry and assessed their sexual orientation and 2 related traits: childhood gender nonconformity and continuous gender identity. Men and women differed in their distributions of sexual orientation, with women more likely to have slight-to-moderate degrees of homosexual attraction, and men more likely to have high degrees of homosexual attraction. Twin concordances for nonheterosexual orientation were lower than in prior studies. Univariate analyses showed that familial factors were important for all traits, but were less successful in distinguishing genetic from shared environmental influences. Only childhood gender nonconformity was significantly heritable for both men and women. Multivariate analyses suggested that the causal architecture differed between men and women, and, for women, provided significant evidence for the importance of genetic factors to the traits' covariation."}
{"_id":"7dd434b3799a6c8c346a1d7ee77d37980a4ef5b9","title":"Syntax-Directed Variational Autoencoder for Structured Data","text":"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, e.g., computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing stochastic lazy attributes. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only syntactically valid, but also semantically reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program\/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches."}
{"_id":"a8540ff90bc1cf9eb54a2ba1ad4125e726d1980d","title":"Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis","text":"Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is lightweight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our WarpingGAN that significantly outperforms all existing methods on two large datasets."}
{"_id":"265a32d3e5a55140389df0a0b666ac5c2dfaa0bd","title":"Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management","text":"Learning from sparse and delayed reward is a central issue in reinforcement learning. In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals. This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots. We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters."}
{"_id":"0ee47ca8e90f3dd2107b6791c0da42357c56f5bc","title":"Agile Software Development: The Business of Innovation","text":"T he rise and fall of the dot-com-driven Internet economy shouldn't distract us from seeing that the business environment continues to change at a dramatically increasing pace. To thrive in this turbulent environment, we must confront the business need for relentless innovation and forge the future workforce culture. Agile software development approaches such as Extreme Programming , Crystal methods, Lean Development, Scrum, Adaptive Software Development (ASD), and others view change from a perspective that mirrors today's turbulent business and technology environment. In a recent study of more than 200 software development projects, QSM Associates' Michael Mah reported that the researchers couldn't find nearly half of the projects' original plans to measure against. Why? Conforming to plan was no longer the primary goal; instead, satisfying customers\u2014at the time of delivery , not at project initiation\u2014took precedence. In many projects we review, major changes in the requirements, scope, and technology that are outside the development team's control often occur within a project's life span. Accepting that Barry Boehm's life cycle cost differentials theory\u2014the cost of change grows through the software's development life cycle\u2014remains valid, the question today is not how to stop change early in a project but how to better handle inevitable changes throughout its life cycle. Traditional approaches assumed that if we just tried hard enough, we could anticipate the complete set of requirements early and reduce cost by eliminating change. Today, eliminating change early means being unresponsive to business con-ditions\u2014in other words, business failure. Similarly, traditional process manage-ment\u2014by continuous measurement, error identification, and process refine-ments\u2014strove to drive variations out of processes. This approach assumes that variations are the result of errors. Today, while process problems certainly cause some errors, external environmental changes cause critical variations. Because we cannot eliminate these changes, driving down the cost of responding to them is the only viable strategy. Rather than eliminating rework, the new strategy is to reduce its cost. However, in not just accommodating change, but embracing it, we also must be careful to retain quality. Expectations have grown over the years. The market demands and expects innovative, high-quality software that meets its needs\u2014 and soon. Agile methods are a response to this expectation. Their strategy is to reduce the cost of change throughout a project. Extreme Programming (XP), for example, calls for the software development team to \u2022 produce the first delivery in weeks, to achieve an early win and rapid \u2026"}
{"_id":"12f3f9a5bbcaa3ab09eff325bd4554924ac1356d","title":"The Critical Success Factors for Effective ICT Governance in Malaysian Public Sector : A Delphi Study","text":"The fundamental issues in ICT Governance (ICTG) implementation for Malaysian Public Sector (MPS) is how ICT be applied to support improvements in productivity, management effectiveness and the quality of services offered to its citizens. Our main concern is to develop and adopt a common definition and framework to illustrate how ICTG can be used to better align ICT with government\u2019s operations and strategic focus. In particular, we want to identify and categorize factors that drive a successful ICTG process. This paper presents the results of an exploratory study to identify, validate and refine such Critical Success Factors (CSFs) and confirmed seven CSFs and nineteen sub-factors as influential factors that fit MPS after further validated and refined. The Delphi method applied in validation and refining process before being endorsed as appropriate for MPS. The identified CSFs reflect the focus areas that need to be considered strategically to strengthen ICT Governance implementation and ensure business success. Keywords\u2014IT Governance, Critical Success Factors."}
{"_id":"32cbd065ac9405530ce0b1832a9a58c7444ba305","title":"Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments","text":"We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets."}
{"_id":"2e7ebdd353c1de9e47fdd1cf0fce61bd33d87103","title":"Comparing Speech Recognition Systems (Microsoft API, Google API And CMU Sphinx)","text":"The idea of this paper is to design a tool that will be used to test and compare commercial speech recognition systems, such as Microsoft Speech API and Google Speech API, with open-source speech recognition systems such as Sphinx-4. The best way to compare automatic speech recognition systems in different environments is by using some audio recordings that were selected from different sources and calculating the word error rate (WER). Although the WER of the three aforementioned systems were acceptable, it was observed that the Google API is superior."}
{"_id":"76c44858b1a3f3add903a992f66b71f5cdcd18e3","title":"MDig : Multi-digit Recognition using Convolutional Nerual Network on Mobile","text":"Multi-character recognition in arbitrary photographs on mobile platform is difficult, in terms of both accuracy and real-time performance. In this paper, we focus on the domain of hand-written multi-digit recognition. Convolutional neural network (CNN) is the state-of-the-art solution for object recognition, and presents a workload that is both compute and data intensive. To reduce the workload, we train a shallow CNN offline, achieving 99.07% top-1 accuracy. And we utilize preprocessing and segmentation to reduce input image size fed into CNN. For CNN implementation on the mobile platform, we adopt and modify DeepBeliefSDK to support batching fully-connected layers. On NVIDIA SHIELD tablet, the application processes a frame and extracts 32 digits in approximately 60ms, and batching the fully-connected layers reduces the CNN runtime by another 12%."}
{"_id":"7c09d08cdeb688aece28d41feb406dbbcda9c5ac","title":"Nonlinear Model Predictive Control Based on a Self-Organizing Recurrent Neural Network","text":"A nonlinear model predictive control (NMPC) scheme is developed in this paper based on a self-organizing recurrent radial basis function (SR-RBF) neural network, whose structure and parameters are adjusted concurrently in the training process. The proposed SR-RBF neural network is represented in a general nonlinear form for predicting the future dynamic behaviors of nonlinear systems. To improve the modeling accuracy, a spiking-based growing and pruning algorithm and an adaptive learning algorithm are developed to tune the structure and parameters of the SR-RBF neural network, respectively. Meanwhile, for the control problem, an improved gradient method is utilized for the solution of the optimization problem in NMPC. The stability of the resulting control system is proved based on the Lyapunov stability theory. Finally, the proposed SR-RBF neural network-based NMPC (SR-RBF-NMPC) is used to control the dissolved oxygen (DO) concentration in a wastewater treatment process (WWTP). Comparisons with other existing methods demonstrate that the SR-RBF-NMPC can achieve a considerably better model fitting for WWTP and a better control performance for DO concentration."}
{"_id":"7e8a5e0a87fab337d71ce04ba02b7a5ded392421","title":"Detecting and Tracking Political Abuse in Social Media","text":"We study astroturf political campaigns on microblogging platforms: politically-motivated individuals and organizations that use multiple centrally-controlled accounts to create the appearance of widespread support for a candidate or opinion. We describe a machine learning framework that combines topological, content-based and crowdsourced features of information diffusion networks on Twitter to detect the early stages of viral spreading of political misinformation. We present promising preliminary results with better than 96% accuracy in the detection of astroturf content in the run-up to the 2010 U.S. midterm elections."}
{"_id":"2883a32fe32493c1519f404112cbdadd1fe90c7c","title":"Formal analysis of privacy requirements specifications for multi-tier applications","text":"Companies require data from multiple sources to develop new information systems, such as social networking, e-commerce and location-based services. Systems rely on complex, multi-stakeholder data supply-chains to deliver value. These data supply-chains have complex privacy requirements: privacy policies affecting multiple stakeholders (e.g. user, developer, company, government) regulate the collection, use and sharing of data over multiple jurisdictions (e.g. California, United States, Europe). Increasingly, regulators expect companies to ensure consistency between company privacy policies and company data practices. To address this problem, we propose a methodology to map policy requirements in natural language to a formal representation in Description Logic. Using the formal representation, we reason about conflicting requirements within a single policy and among multiple policies in a data supply chain. Further, we enable tracing data flows within the supply-chain. We derive our methodology from an exploratory case study of Facebook platform policy. We demonstrate the feasibility of our approach in an evaluation involving Facebook, Zynga and AOL-Advertising policies. Our results identify three conflicts that exist between Facebook and Zynga policies, and one conflict within the AOL Advertising policy."}
{"_id":"0ea94e9c83d2e138fbccc1116b57d4f2a7ba6868","title":"Measured Gene-Environment Interactions in Psychopathology: Concepts, Research Strategies, and Implications for Research, Intervention, and Public Understanding of Genetics.","text":"There is much curiosity about interactions between genes and environmental risk factors for psychopathology, but this interest is accompanied by uncertainty. This article aims to address this uncertainty. First, we explain what is and is not meant by gene-environment interaction. Second, we discuss reasons why such interactions were thought to be rare in psychopathology, and argue instead that they ought to be common. Third, we summarize emerging evidence about gene-environment interactions in mental disorders. Fourth, we argue that research on gene-environment interactions should be hypothesis driven, and we put forward strategies to guide future studies. Fifth, we describe potential benefits of studying measured gene-environment interactions for basic neuroscience, gene hunting, intervention, and public understanding of genetics. We suggest that information about nurture might be harnessed to make new discoveries about the nature of psychopathology."}
{"_id":"37c3303d173c055592ef923235837e1cbc6bd986","title":"Learning Fair Representations","text":"We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classification tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification."}
{"_id":"0f9065db0193be42173be5546a3dfb839f694521","title":"Distributed Representations for Compositional Semantics","text":"The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches\u2014meaning distributed representations that exploit co-occurrence statistics of large corpora\u2014have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP. This dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP. Part I focuses on distributed representations and their application. In particular, in Chapter 3 we explore the semantic usefulness of distributed representations by evaluating their use in the task of semantic frame identification. Part II describes the transition from semantic representations for words to compositional semantics. Chapter 4 covers the relevant literature in this field. Following this, Chapter 5 investigates the role of syntax in semantic composition. For this, we discuss a series of neural network-based models and learning mechanisms, and demonstrate how syntactic information can be incorporated into semantic composition. This study allows us to establish the effectiveness of syntactic information as a guiding parameter for semantic composition, and answer questions about the link between syntax and semantics. Following these discoveries regarding the role of syntax, Chapter 6 investigates whether it is possible to further reduce the impact of monolingual surface forms and syntax when attempting to capture semantics. Asking how machines can best approximate human signals of semantics, we propose multilingual information as one method for grounding semantics, and develop an extension to the distributional hypothesis for multilingual representations. Finally, Part III summarizes our findings and discusses future work."}
{"_id":"1319bf6218cbcd85ac7512991447ecd9d776577d","title":"Task constraints in visual working memory","text":"This paper examines the nature of visual representations that direct ongoing performance in sensorimotor tasks. Performance of such natural tasks requires relating visual information from different gaze positions. To explore this we used the technique of making task relevant display changes during saccadic eye movements. Subjects copied a pattern of colored blocks on a computer monitor, using the mouse to drag the blocks across the screen. Eye position was monitored using a dual-purkinje eye tracker, and the color of blocks in the pattern was changed at different points in task performance. When the target of the saccade changed color during the saccade, the duration of fixations on the model pattern increased, depending on the point in the task that the change was made. Thus different fixations on the same visual stimulus served a different purpose. The results also indicated that the visual information that is retained across successive fixations depends on moment by moment task demands. This is consistent with previous suggestions that visual representations are limited and task dependent. Changes in blocks in addition to the saccade target led to greater increases in fixation duration. This indicated that some global aspect of the pattern was retained across different fixations. Fixation durations revealed effects of the display changes that were not revealed in perceptual report. This can be understood by distinguishing between processes that operate at different levels of description and different time scales. Our conscious experience of the world may reflect events over a longer time scale than those underlying the substructure of the perceptuo-motor machinery."}
{"_id":"03406ec0118137ca1ab734a8b6b3678a35a43415","title":"A Morphable Model for the Synthesis of 3D Faces","text":"In this paper, a new technique for modeling textured 3D faces is introduced. 3D faces can either be generated automatically from one or more photographs, or modeled directly through an intuitive user interface. Users are assisted in two key problems of computer aided face modeling. First, new face images or new 3D face models can be registered automatically by computing dense one-to-one correspondence to an internal face model. Second, the approach regulates the naturalness of modeled faces avoiding faces with an \u201cunlikely\u201d appearance. Starting from an example set of 3D face models, we derive a morphable face model by transforming the shape and texture of the examples into a vector space representation. New faces and expressions can be modeled by forming linear combinations of the prototypes. Shape and texture constraints derived from the statistics of our example faces are used to guide manual modeling or automated matching algorithms. We show 3D face reconstructions from single images and their applications for photo-realistic image manipulations. We also demonstrate face manipulations according to complex parameters such as gender, fullness of a face or its distinctiveness."}
{"_id":"ae18fa7080e85922fa916591bc73cd100ff5e861","title":"Right nulled GLR parsers","text":"The right nulled generalized LR parsing algorithm is a new generalization of LR parsing which provides an elegant correction to, and extension of, Tomita's GLR methods whereby we extend the notion of a reduction in a shift-reduce parser to include right nulled items. The result is a parsing technique which runs in linear time on LR(1) grammars and whose performance degrades gracefully to a polynomial bound in the presence of nonLR(1) rules. Compared to other GLR-based techniques, our algorithm is simpler and faster."}
{"_id":"46b92b61c908eb8809af6d0a3b7a4a2728792161","title":"Commutation torque ripple reduction in brushless DC motor drives using a single DC current sensor","text":"This paper presents a comprehensive study on reducing commutation torque ripples generated in brushless DC motor drives with only a single DC-link current sensor provided. In such drives, commutation torque ripple suppression techniques that are practically effective in low speed as well as high speed regions are scarcely found. The commutation compensation technique proposed here is based on a strategy that the current slopes of the incoming and the outgoing phases during the commutation interval can be equalized by a proper duty-ratio control. Being directly linked with deadbeat current control scheme, the proposed control method accomplishes suppression of the spikes and dips superimposed on the current and torque responses during the commutation intervals of the inverter. Effectiveness of the proposed control method is verified through simulations and experiments."}
{"_id":"5425f7109dca2022ff9bde0ed3f113080d0d606b","title":"DFD: Efficient Functional Dependency Discovery","text":"The discovery of unknown functional dependencies in a dataset is of great importance for database redesign, anomaly detection and data cleansing applications. However, as the nature of the problem is exponential in the number of attributes none of the existing approaches can be applied on large datasets. We present a new algorithm DFD for discovering all functional dependencies in a dataset following a depth-first traversal strategy of the attribute lattice that combines aggressive pruning and efficient result verification. Our approach is able to scale far beyond existing algorithms for up to 7.5 million tuples, and is up to three orders of magnitude faster than existing approaches on smaller datasets."}
{"_id":"c2f4c6d7e06da14c4b3ce3a9b97394a64708dc52","title":"Database Dependency Discovery: A Machine Learning Approach","text":"Database dependencies, such as functional and multivalued dependencies, express the presence of structure in databas e relations, that can be utilised in the database design proce ss. The discovery of database dependencies can be viewed as an induction problem, in which general rules (dependencies) a re obtained from specific facts (the relation). This viewpoint has the advantage of abstracting away as much as possible from the particulars of the dependencies. The algorithms in this paper are designed such that they can easily be generalised to other kinds of dependencies. Like in current approaches to computational induction such as inductive logic programming, we distinguish between top down algorithms and bottom-up algorithms. In a top-down approach, hypotheses are generated in a systematic way and then tested against the given relation. In a bottom-up approach, the relation is inspected in order to see what dependencies it may satisfy or violate. We give algorithms for bot h approaches."}
{"_id":"28f840416cfe7aed3cda11e266119d247fcc3f9e","title":"GORDIAN: Efficient and Scalable Discovery of Composite Keys","text":"Identification of (composite) key attributes is of fundamental importance for many different data management tasks such as data modeling, data integration, anomaly detection, query formulation, query optimization, and indexing. However, information about keys is often missing or incomplete in many real-world database scenarios. Surprisingly, the fundamental problem of automatic key discovery has received little attention in the existing literature. Existing solutions ignore composite keys, due to the complexity associated with their discovery. Even for simple keys, current algorithms take a brute-force approach; the resulting exponential CPU and memory requirements limit the applicability of these methods to small datasets. In this paper, we describe GORDIAN, a scalable algorithm for automatic discovery of keys in large datasets, including composite keys. GORDIAN can provide exact results very efficiently for both real-world and synthetic datasets. GORDIAN can be used to find (composite) key attributes in any collection of entities, e.g., key column-groups in relational data, or key leaf-node sets in a collection of XML documents with a common schema. We show empirically that GORDIAN can be combined with sampling to efficiently obtain high quality sets of approximate keys even in very large datasets."}
{"_id":"5288d14f6a3937df5e10109d4e23d79b7ddf080f","title":"Fast Algorithms for Mining Association Rules in Large Databases","text":""}
{"_id":"57fb4b0c63400dc984893461b1f5a73244b3e3eb","title":"Logic and Databases: A Deductive Approach","text":"ion, Databases and Conceptual Modeling (Pingree Park, Colo., June), pp. 112-114; ACM SZGMOD Rec. 11, 2 (Feb.). CODD, E. F. 1982. Relational database: A practical foundation for productivity. Commun. ACM 25, 2 (Feb.), 109-117. COLMERAUER, A. 1973. Un systeme de communication homme-machine en francais. Rapport, Groupe Intelligence Artificielle, Universite d\u2019AixMarseille-Luminy, Marseilles, France. COLMERAUER, A., AND PIQUE, J. F. 1981. About natural logic. In Advances in Database Theory vol. 1, H. Gallaire, J. Minker, and J.-M. Nicolas, Eds. Plenum, New York, pp. 343-365. COOPER, E. C. 1980. On the expressive power of query languages for relational databases. Tech. Rep. 14-80, Computer Science Dept., Harvard Univ., Cambridge, Mass. DAHL, V. 1982. On database systems development through logic. ACM Trans. Database Syst. 7, 1 (Mar.), 102-123. DATE, C. J. 1977. An Introduction to Database Systems. Addison-Wesley, Reading, Mass. DATE, C. J. 1981. An Introduction to Database Systems, 3rd ed. Addison-Wesley, Reading, Mass. DELIYANNI, A., AND KOWALSKI, R. A. 1979. Logic and semantic networks. Commun. ACM 22, 3"}
{"_id":"7d81977db644f56b3291546598d2f53165f76117","title":"Regional cerebral blood flow correlates of visuospatial tasks in Alzheimer's disease.","text":"This study investigated the role of visuospatial tasks in identifying cognitive decline in patients with Alzheimer's disease (AD), by correlating neuropsychological performance with cerebral perfusion measures. There were 157 participants: 29 neurologically healthy controls (age: 70.3 +\/- 6.6, MMSE > or = 27), 86 patients with mild AD (age: 69.18 +\/- 8.28, MMSE > or = 21) and 42 patients moderate\/severe AD (age: 68.86 +\/- 10.69, MMSE 8-20). Single Photon Emission Computerized Tomography (SPECT) was used to derive regional perfusion ratios, and correlated using partial least squares (PLS) with neuropsychological test scores from the Benton Line Orientation (BLO) and the Rey-Osterrieth Complex Figure (RO). Cross-sectional analysis demonstrated that mean scores differed in accordance with disease status: control group (BLO 25.5, RO 33.3); mild AD (BLO 20.1, RO 25.5); moderate\/severe AD (BLO 10.7, RO 16). Correlations were observed between BLO\/RO and right parietal SPECT regions in the AD groups. Visuospatial performance, often undersampled in cognitive batteries for AD, is clearly impaired even in mild AD and correlates with functional deficits as indexed by cerebral perfusion ratios on SPECT implicating right hemisphere circuits. Furthermore, PLS reveals that usual spatial tasks probe a distributed brain network in both hemispheres including many areas targeted by early AD pathology."}
{"_id":"ce2eb2cda28e8883d9475acd6f034733de38ae91","title":"Articulatory , positional and coarticulatory characteristics for clear \/ l \/ and dark \/ l \/ : evidence from two Catalan dialects","text":"Electropalatographic and acoustic data reported in this study show differences in closure location and degree, dorsopalatal contact size, closure duration, relative timing of events and formant frequency between clear \/l\/ and dark \/l\/ in two dialects of Catalan (Valencian and Majorcan). The two Catalan dialects under investigation differ also regarding degree of darkness but essentially not regarding coarticulatory resistance at the word edges, i.e. the alveolar lateral is equally dark word-initially and word-finally in Majorcan, and clearer in the former position vs. than the latter in Valencian, and more resistant to vowel effects in the two positions than intervocalically in both dialects. With reference to data from the literature, it appears that languages and dialects may differ as to whether \/l\/ is dark or clear in all word positions or whether or not initial \/l\/ is clearer than final \/l\/, and that articulatory strengthening occurs not only wordand utterance-initially but wordand utterance-finally as well. These and other considerations confirm the hypothesis that degree of darkness in \/l\/ proceeds gradually rather than categorically from one language to another."}
{"_id":"13179f0c9959a2bd357838aac3d0a97aea96d7b5","title":"Risk factors for involvement in cyber bullying : Victims , bullies and bully \u2013 victims \u2606","text":"The use of online technology is exploding worldwide and is fast becoming a preferred method of interacting. While most online interactions are neutral or positive the Internet provides a new means through which children and youth are bullied. The aim of this grounded theory approach was to explore technology, virtual relationships and cyber bullying from the perspectives of students. Seven focus groups were held with 38 students between fifth and eighth grades. The participants considered cyber bullying to be a serious problem and some characterized online bullying as more serious than  \u0333traditional\u2018 bullying because of the associated anonymity. Although the students depicted anonymity as integral to cyber bullying, the findings suggest that much of the cyber bullying occurred within the context of their social groups and relationships. Findings revealed five major themes: technology embraced at younger ages and becoming the dominant medium for communication; definitions and views of cyber bullying; factors unique to cyber bullying; types of cyber bullying; and telling adults. The findings highlight the complexity of the perceived anonymity provided by the Internet and how AC C EP TE D M AN U SC R IP T ACCEPTED MANUSCRIPT 3 this may impact cyber bullying. The study offers greater awareness of the meanings of online relationships for children and youth."}
{"_id":"5cf701e4588067b52ead26188904005b59e71139","title":"The Gut Microbiota and Autism Spectrum Disorders","text":"Gastrointestinal (GI) symptoms are a common comorbidity in patients with autism spectrum disorder (ASD), but the underlying mechanisms are unknown. Many studies have shown alterations in the composition of the fecal flora and metabolic products of the gut microbiome in patients with ASD. The gut microbiota influences brain development and behaviors through the neuroendocrine, neuroimmune and autonomic nervous systems. In addition, an abnormal gut microbiota is associated with several diseases, such as inflammatory bowel disease (IBD), ASD and mood disorders. Here, we review the bidirectional interactions between the central nervous system and the gastrointestinal tract (brain-gut axis) and the role of the gut microbiota in the central nervous system (CNS) and ASD. Microbiome-mediated therapies might be a safe and effective treatment for ASD."}
{"_id":"28c41a507c374c15443fa5ccf1209e1eec34f317","title":"Compiling Knowledge into Decomposable Negation Normal Form","text":"We propose a method for compiling proposit ional theories into a new tractable form that we refer to as decomposable negation normal form (DNNF). We show a number of results about our compilation approach. First, we show that every propositional theory can be compiled into D N N F and present an algorithm to this effect. Second, we show that if a clausal form has a bounded treewidth, then its DNNF compilation has a linear size and can be computed in linear t ime \u2014 treewidth is a graphtheoretic parameter which measures the connectivity of the clausal form. Th i rd , we show that once a propositional theory is compiled into DNNF, a number of reasoning tasks, such as satisfiability and forgett ing, can be performed in linear t ime. Finally, we propose two techniques for approximating the DNNF compilat ion of a theory when the size of such compilation is too large to be practical. One of the techniques generates a sound but incomplete compilation, while the other generates a complete but unsound compilation. Together, these approximations bound the exact compilation from below and above in terms for their abil i ty to answer queries."}
{"_id":"d62f6185ed2c3878d788582fdeabbb5423c04c01","title":"Object volume estimation based on 3D point cloud","text":"An approach to estimating the volume of an object based on 3D point cloud is proposed in this paper. Firstly, 3D point cloud is cut into slices of equal thickness along the z-axis. Bisect each slice along the y-axis and cut each slice into sub-intervals along the x-axis. Using the minimum and maximum coordinates in each sub-interval, two surface curve functions can be fitted accordingly for the bisected slices. Then, the curves are integrated to estimate the area of each slice and further integrated along the z-axis for an estimated volume of the object."}
{"_id":"6b50b1fa78307a7505eb35ef58d5f51bf977f2a3","title":"Auto-Encoding User Ratings via Knowledge Graphs in Recommendation Scenarios","text":"In the last decade, driven also by the availability of an unprecedented computational power and storage capabilities in cloud environments, we assisted to the proliferation of new algorithms, methods, and approaches in two areas of artificial intelligence: knowledge representation and machine learning. On the one side, the generation of a high rate of structured data on the Web led to the creation and publication of the so-called knowledge graphs. On the other side, deep learning emerged as one of the most promising approaches in the generation and training of models that can be applied to a wide variety of application fields. More recently, autoencoders have proven their strength in various scenarios, playing a fundamental role in unsupervised learning. In this paper, we instigate how to exploit the semantic information encoded in a knowledge graph to build connections between units in a Neural Network, thus leading to a new method, SEM-AUTO, to extract and weight semantic features that can eventually be used to build a recommender system. As adding content-based side information may mitigate the cold user problems, we tested how our approach behaves in the presence of a few ratings from a user on the Movielens 1M dataset and compare results with BPRSLIM."}
{"_id":"6cd3ea4361e035969e6cf819422d0262f7c0a186","title":"3D Deep Learning for Efficient and Robust Landmark Detection in Volumetric Data","text":"Recently, deep learning has demonstrated great success in computer vision with the capability to learn powerful image features from a large training set. However, most of the published work has been confined to solving 2D problems, with a few limited exceptions that treated the 3D space as a composition of 2D orthogonal planes. The challenge of 3D deep learning is due to a much larger input vector, compared to 2D, which dramatically increases the computation time and the chance of over-fitting, especially when combined with limited training samples (hundreds to thousands), typical for medical imaging applications. To address this challenge, we propose an efficient and robust deep learning algorithm capable of full 3D detection in volumetric data. A two-step approach is exploited for efficient detection. A shallow network (with one hidden layer) is used for the initial testing of all voxels to obtain a small number of promising candidates, followed by more accurate classification with a deep network. In addition, we propose two approaches, i.e., separable filter decomposition and network sparsification, to speed up the evaluation of a network. To mitigate the over-fitting issue, thereby increasing detection robustness, we extract small 3D patches from a multi-resolution image pyramid. The deeply learned image features are further combined with Haar wavelet features to increase the detection accuracy. The proposed method has been quantitatively evaluated for carotid artery bifurcation detection on a head-neck CT dataset from 455 patients. Compared to the state-ofthe-art, the mean error is reduced by more than half, from 5.97 mm to 2.64 mm, with a detection speed of less than 1 s\/volume."}
{"_id":"2ee7ee38745e9fcf89860dfb3d41c2155521e3a3","title":"Residual Memory Networks in Language Modeling: Improving the Reputation of Feed-Forward Networks","text":"We introduce the Residual Memory Network (RMN) architecture to language modeling. RMN is an architecture of feedforward neural networks that incorporates residual connections and time-delay connections that allow us to naturally incorporate information from a substantial time context. As this is the first time RMNs are applied for language modeling, we thoroughly investigate their behaviour on the well studied Penn Treebank corpus. We change the model slightly for the needs of language modeling, reducing both its time and memory consumption. Our results show that RMN is a suitable choice for small-sized neural language models: With test perplexity 112.7 and as few as 2.3M parameters, they out-perform both a much larger vanilla RNN (PPL 124, 8M parameters) and a similarly sized LSTM (PPL 115, 2.08M parameters), while being only by less than 3 perplexity points worse than twice as big LSTM."}
{"_id":"772aa928a1bbc901795b78c1bee6d539aa2d1a36","title":"Author Disambiguation in PubMed: Evidence on the Precision and Recall of Author-ity among NIH-Funded Scientists","text":"We examined the usefulness (precision) and completeness (recall) of the Author-ity author disambiguation for PubMed articles by associating articles with scientists funded by the National Institutes of Health (NIH). In doing so, we exploited established unique identifiers-Principal Investigator (PI) IDs-that the NIH assigns to funded scientists. Analyzing a set of 36,987 NIH scientists who received their first R01 grant between 1985 and 2009, we identified 355,921 articles appearing in PubMed that would allow us to evaluate the precision and recall of the Author-ity disambiguation. We found that Author-ity identified the NIH scientists with 99.51% precision across the articles. It had a corresponding recall of 99.64%. Precision and recall, moreover, appeared stable across common and uncommon last names, across ethnic backgrounds, and across levels of scientist productivity."}
{"_id":"c9946fedf333df0c6404765ba6ccbf8006779753","title":"Motion planning based on learning models of pedestrian and driver behaviors","text":"Autonomous driving has shown the capability of providing driver convenience and enhancing safety. While introducing autonomous driving into our current traffic system, one significant issue is to make the autonomous vehicle be able to react in the same way as real human drivers. In order to ensure that an autonomous vehicle of the future will perform like human drivers, this paper proposes a vehicle motion planning model, which can represent how drivers control vehicles based on the assessment of traffic environments in the real signalized intersection. The proposed motion planning model comprises functions of pedestrian intention detection, gap detection and vehicle dynamic control. The three functions are constructed based on the analysis of actual data collected from real traffic environments. Finally, this paper demonstrates the performance of the proposed method by comparing the behaviors of our model with the behaviors of real pedestrians and human drivers. The experimental results show that our proposed model can achieve 85% recognition rate for the pedestrian crossing intention. Moreover, the vehicle controlled by the proposed motion planning model and the actual human-driven vehicle are highly similar with respect to the gap acceptance in intersections."}
{"_id":"ffe504583a03a224f4b14938aa9d7b625d326736","title":"Facing prejudice: implicit prejudice and the perception of facial threat.","text":"We propose that social attitudes, and in particular implicit prejudice, bias people's perceptions of the facial emotion displayed by others. To test this hypothesis, we employed a facial emotion change-detection task in which European American participants detected the offset (Study 1) or onset (Study 2) of facial anger in both Black and White targets. Higher implicit (but not explicit) prejudice was associated with a greater readiness to perceive anger in Black faces, but neither explicit nor implicit prejudice predicted anger perceptions regarding similar White faces. This pattern indicates that European Americans high in implicit racial prejudice are biased to perceive threatening affect in Black but not White faces, suggesting that the deleterious effects of stereotypes may take hold extremely early in social interaction."}
{"_id":"e8cd182c70220f7c381a26c80c0a82b5c8e4d5c1","title":"Recurrent networks with attention and convolutional networks for sentence representation and classification","text":"In this paper, we propose a bi-attention, a multi-layer attention and an attention mechanism and convolution neural network based text representation and classification model (ACNN). The bi-attention have two attention mechanism to learn two context vectors, forward RNN with attention to learn forward context vector c\u20d7 $\\overrightarrow {\\mathbf {c}}$ and backward RNN with attention to learn backward context vector c\u20d6 $\\overleftarrow {\\mathbf {c}}$, and then concatenation c\u20d7 $\\overrightarrow {\\mathbf {c}}$ and c\u20d6 $\\overleftarrow {\\mathbf {c}}$ to get context vector c. The multi-layer attention is the stack of the bi-attention. In the ACNN, the context vector c is obtained by the bi-attention, then the convolution operation is performed on the context vector c, and the max-pooling operation is used to reduce the dimension. After max-pooling operation the text is converted to low-dimensional sentence vector m. Finally, the Softmax classifier be used for text classification. We test our model on 8 benchmarks text classification datasets, and our model achieved a better or the same performance compare with the state-of-the-art methods."}
{"_id":"92fff676dc28d962e79c0450531ebba12a341896","title":"Modeling changing dependency structure in multivariate time series","text":"We show how to apply the efficient Bayesian changepoint detection techniques of Fearnhead in the multivariate setting. We model the joint density of vector-valued observations using undirected Gaussian graphical models, whose structure we estimate. We show how we can exactly compute the MAP segmentation, as well as how to draw perfect samples from the posterior over segmentations, simultaneously accounting for uncertainty about the number and location of changepoints, as well as uncertainty about the covariance structure. We illustrate the technique by applying it to financial data and to bee tracking data."}
{"_id":"c7473ab608ec26b799e10a387067d503cdcc4e7e","title":"Habits \u2014 A Repeat Performance","text":"Habits are response dispositions that are activated automatically by the context cues that co-occurred with responses during past performance. Experiencesampling diary studies indicate that much of everyday action is characterized by habitual repetition. We consider various mechanisms that could underlie the habitual control of action, and we conclude that direct cuing and motivated contexts best account for the characteristic features of habit responding\u2014in particular, for the rigid repetition of action that can be initiated without intention and that runs to completion with minimal conscious control. We explain the utility of contemporary habit research for issues central to psychology, especially for behavior prediction, behavior change, and self-regulation. KEYWORDS\u2014habit; automaticity; motivation; goals; behavior change; behavior prediction; self-regulation From self-help guru Anthony Robbins to the religion of Zen Buddhism, received wisdom exhorts people to be mindful, deliberative, and conscious in all they do. In contrast, contemporary research in psychology shows that it is actually people\u2019s unthinking routines\u2014or habits\u2014that form the bedrock of everyday life. Without habits, people would be doomed to plan, consciously guide, and monitor every action, from making that first cup of coffee in the morning to sequencing the finger movements in a Chopin piano concerto. But what is a habit? The cognitive revolution radically reshaped the behaviorist view that habits rely on simple stimulus\u2013 response associations devoid of mental representation. Emerging is a more nuanced construct that includes roles for consciousness, goals, and motivational states. Fundamental questions persist, however, especially when comparing evidence across neuropsychology, animal-learning, and social-cognition literatures. Data from these fields support three views of habit, which we term the direct-context-cuing, implicit-goal, and motivated-context models. In this article, we consider these models and explain the relevance for psychology of a reinvigorated habit construct. HABITS AFTER BEHAVIORISM Within current theorizing, habits are automated response dispositions that are cued by aspects of the performance context (i.e., environment, preceding actions). They are learned through a process in which repetition incrementally tunes cognitive processors in procedural memory (i.e., the memory system that supports the minimally conscious control of skilled action). The relatively primitive associative learning that promotes habits is shared in some form across mammalian species. Our own interest in habits has been fueled by the recognition that much of everyday action is characterized by repetition. In experience-sampling diary studies using both student and community samples, approximately 45% of everyday behaviors tended to be repeated in the same location almost every day (Quinn & Wood, 2005; Wood, Quinn, & Kashy, 2002). In these studies, people reported a heterogeneous set of actions that varied in habit strength, including reading the newspaper, exercising, and eating fast food. Although a consensual perspective on habit mechanisms has yet to develop, common to all views is the idea that many behavioral sequences (e.g., one\u2019s morning coffee-making routine) are performed repeatedly in similar contexts. When responses and features of context occur in contiguity, the potential exists for associations to form between them, such that contexts come to cue responses. In what follows, we outline three views of habitual control that build on this understanding. Direct Context Cuing According to the direct-context-cuing model, repeated coactivation forges direct links in memory between context and response representations. Once these links are formed via associative learning, merely perceiving a context triggers associated responses. Supporting evidence comes from research in which merely activating a construct, such as the elderly stereotype, influences the performance of relevant behaviors, such as a slow speed of walking (e.g., Bargh, Chen, & Burrows, 1996). Address correspondence to David Neal, Department of Psychology, Box 90085, Duke University, Durham, NC 27708; e-mail: dneal@ duke.edu. CURRENT DIRECTIONS IN PSYCHOLOGICAL SCIENCE 198 Volume 15\u2014Number 4 Copyright r 2006 Association for Psychological Science Readers might wonder if it is realistic that contexts cue responses through this simple mechanism in the absence of an implicit or explicit goal. The answer is not clear, given that social-cognition research has thus far demonstrated only a limited version of direct-cuing effects. For example, activating the elderly stereotype influences walking speed, but it remains to be demonstrated whether such activation can initiate walking itself. However, the direct cuing of repeated action by contexts is suggested by myriad findings in cognitive neuroscience that reveal reduced involvement of goal-related neural structures, such as the prefrontal cortex, when behaviors have come under habitual control (see Daw, Niv, & Dayan, 2005). Furthermore, animal-learning research using a clever paradigm in which reinforcers are devalued suggests direct control by context. When rats initially perform an instrumental behavior (e.g., pressing a bar for a food pellet), they appear to be guided by specific goal expectations; they cease the behavior if the reward is devalued (e.g., by pairing it with a toxin; Dickinson & Balleine, 1995). In contrast, when rats have extensively repeated a behavior, their responses appear to be cued directly by contextual stimuli (e.g., the bar); reward devaluation has little impact on continued performance. These data are commonly interpreted as indicating that habit formation involves a shift to direct context cuing. Implicit Goals Associative learning explains not only the direct binding of contexts and actions but also the binding of contexts and goals. In implicit-goal models, habits develop when people repeatedly pursue a goal via a specific behavior in a given context. An indirect association then forms between the context and behavior within the broader goal system. In support, Aarts and Dijksterhuis (2000) found in several experiments that the automatic activation of habitual responses (e.g., bicycle riding) only occurs when a relevant goal has first been made accessible (e.g., the thought of attending class). These studies did not measure people\u2019s real-world behavior, however, but focused instead on judgments about behavior. It remains to be seen whether such judgments tap the cognitive processes that actually drive behavior. In addition, there is good reason to think that habit performance itself does not depend on goal activation. Goaldriven responses tend to be dynamic and flexible, as evidenced by people sometimes substituting behaviors that serve a common goal. In contrast, habits emerge in a rigid pattern such that, for example, a habitual runner is unlikely to substitute a cycling class for running. Thus, although implicit goals provide potentially powerful guides to action, they do not plausibly explain the context cuing of habits. Motivated Contexts In another framework for understanding context-cued responses, contexts can acquire diffuse motivational value when they have preceded rewards in the past. When contexts predict rewards in this way, they energize associated responses without activating specific goals. Evidence of the motivating quality of contexts comes from animal studies of the neurotransmitters that mediate reward learning. For example, when monkeys first learn that a feature of the environment (e.g., a light) predicts a reward (e.g., a drop of juice) when a response is made (e.g., a lever press), neurotransmitter activity (i.e., dopamine release) occurs just after the reward (see Schultz, Dayan, & Montague, 1997). After repeated practice, the animal reaches for the lever when the light is illuminated. Furthermore, the neurotransmitter response is no longer elicited by the juice but instead by the light. In this way, environmental cues can acquire motivational value. Reward-predicting environments are thought to signal the cached (or long-run future) value of an action without signaling a specific outcome (e.g., juice; Daw et al., 2005). This diffuse motivation may explain the rigid nature of context cuing, given that cached values do not convey a specific desired outcome that could be met by substitutable means. Contributing further to the rigidity of habits, neural evidence indicates that, with repetition, whole sequences of responses become chunked or integrated in memory with the contexts that predict them (Barnes, Kubota, Hu, Jin, & Graybiel, 2005). Chunked responses are cued and implemented as a unit, consistent with the idea that habits require limited conscious control to proceed to completion. This quality of habitual responding is frustratingly evident when, for example, trying to fix a well-practiced but badly executed golf swing or dance-step sequence. As yet, the motivated-context idea has been tested primarily with animals. Its promise as a model of human habits comes from evidence that reward-related neurotransmitter systems are shared across species (e.g., in humans, dopamine is elicited by monetary reward). Multiple Habit Mechanisms The high degree of repetition in daily life observed in the diary research of Wood et al. (2002) is likely to be a product of multiple habit-control mechanisms that draw, in various cases, on direct context associations as well as on diffuse motivations. Although we consider implicit goals to be an implausible mediator of habitual behavior, they undoubtedly contribute to some types of repetition. Whether habits are cued directly or are diffusely motivated, they are triggered automatically by contexts and performed in a relatively rigid way. These features of responding have important implications for theories of behavior prediction, behavior cha"}
{"_id":"32578f50dfcd443505450c79b61d59cc71b8d685","title":"Digital Enterprise Architecture - Transformation for the Internet of Things","text":"Excellence in IT is both a driver and a key enabler of the digital transformation. The digital transformation changes the way we live, work, learn, communicate, and collaborate. The Internet of Things (IoT) fundamentally influences today's digital strategies with disruptive business operating models and fast changing markets. New business information systems are integrating emerging Internet of Things infrastructures and components. With the huge diversity of Internet of Things technologies and products organizations have to leverage and extend previous Enterprise Architecture efforts to enable business value by integrating Internet of Things architectures. Both architecture engineering and management of current information systems and business models are complex and currently integrating beside the Internet of Things synergistic subjects, like Enterprise Architecture in context with services & cloud computing, semantic-based decision support through ontologies and knowledge-based systems, big data management, as well as mobility and collaboration networks. To provide adequate decision support for complex business\/IT environments, we have to make transparent the impact of business and IT changes over the integral landscape of affected architectural capabilities, like directly and transitively impacted IoT-objects, business categories, processes, applications, services, platforms and infrastructures. The paper describes a new metamodel-based approach for integrating Internet of Things architectural objects, which are semi-automatically federated into a holistic Digital Enterprise Architecture environment."}
{"_id":"e8859b92af978e29cb3931b27bf781be6f98e3d0","title":"A novel bridgeless buck-boost PFC converter","text":"Conventional cascade buck-boost PFC (CBB-PFC) converter suffers from the high conduction loss in the input rectifier bridge. To resolve the above problem, a novel bridgeless buck-boost PFC topology is proposed in this paper. The proposed PFC converter which removes the input rectifier bridge has three conduction semiconductors at every moment. Comparing with CBB-PFC topology, the proposed topology reduces the conduction semiconductors, reduces conduction losses effectively, improves the efficiency of converter and is suitable for use in the wide input voltage range. In this paper, the average current mode control was implemented with UC3854, the theoretical analysis and design of detection circuits was presented. The experimental prototype with 400 V\/600 W output and line input voltage range from 220 VAC to 380 VAC was built. Experimental results show that the proposed converter can improve 0.8% efficiency comparing CBB-PFC converter."}
{"_id":"9a7ff2f35a9e874fcf7d3a6a3c8671406cd7829a","title":"Scene text recognition and tracking to identify athletes in sport videos","text":"We present an athlete identification module forming part of a system for the personalization of sport video broadcasts. The aim of this module is the localization of athletes in the scene, their identification through the reading of names or numbers printed on their uniforms, and the labelling of frames where athletes are visible. Building upon a previously published algorithm we extract text from individual frames and read these candidates by means of an optical character recognizer (OCR). The OCR-ed text is then compared to a known list of athletes\u2019 names (or numbers), to provide a presence score for each athlete. Text regions are tracked in subsequent frames using a template matching technique. In this way blurred or distorted text, normally unreadable by the OCR, is exploited to provide a denser labelling of the video sequences. Extensive experiments show that the method proposed is fast, robust and reliable, out-performing results of other systems in the literature."}
{"_id":"0214778162cb1bb1fb15b6b8e5c5d669a3985fb5","title":"A Knowledge Graph based Bidirectional Recurrent Neural Network Method for Literature-based Discovery","text":"In this paper, we present a model which incorporates biomedical knowledge graph, graph embedding and deep learning methods for literature-based discovery. Firstly, the relations between entities are extracted from biomedical abstracts and then a knowledge graph is constructed by using these obtained relations. Secondly, the graph embedding technologies are applied to convert the entities and relations in the knowledge graph into a low-dimensional vector space. Thirdly, a bidirectional Long Short-Term Memory network is trained based on the entity associations represented by the pre-trained graph embeddings. Finally, the learned model is used for open and closed literature-based discovery tasks. The experimental results show that our method could not only effectively discover hidden associations between entities, but also reveal the corresponding mechanism of interactions. It suggests that incorporating knowledge graph and deep learning methods is an effective way for capturing the underlying complex associations between entities hidden in the literature."}
{"_id":"e74949ae81efa58f562d44b86339571701674284","title":"A system for generating and injecting indistinguishable network decoys","text":"We propose a novel trap-based architecture for detecting passive, \u201csilent\u201d, attackers who are eavesdropping on enterprise networks. Motivated by the increasing number of incidents where attackers sniff the local network for interesting information, such as credit card numbers, account credentials, and passwords, we introduce a methodology for building a trap-based network that is designed to maximize the realism of bait-laced traffic. Our proposal relies on a \u201crecord, modify, replay\u201d paradigm that can be easily adapted to different networked environments. The primary contributions of our architecture are the ease of automatically injecting large amounts of believable bait, and the integration of different detection mechanisms in the back-end. We demonstrate our methodology in a prototype platform that uses our decoy injection API to dynamically create and dispense network traps on a subset of our campus wireless network. Our network traps consist of several types of monitored passwords, authentication cookies, credit cards, and documents containing beacons to alarm when opened. The efficacy of our decoys against a model attack program is also discussed, along with results obtained from experiments in the field. In addition, we present a user study that demonstrates the believability of our decoy traffic, and finally, we provide experimental results to show that our solution causes only negligible interference to ordinary users."}
{"_id":"b8febc6422057c0476db7e64ac88df8fb0a619eb","title":"Reflect , and React : A Culturally Responsive Model for Pre-service Secondary Social Studies Teachers","text":"The purpose of this qualitative study was to design and implement a model of cultural-responsiveness within a social studies teacher education program. Specifically, we sought to understand how pre-service grades 6-12 social studies practitioners construct culturally responsive teaching (CRT) in their lesson planning. In addition, we examined the professional barriers that prevented teacher-candidates from actualizing culturally responsive pedagogy. Incorporating a conceptual model of Review, Reflect, and React, 20 teacher candidates in a social studies methods course engaged CRT theory and practice. Thematic analysis of lesson plans and clinical reflections indicated successful proponents of CRT critically analyzed their curriculum, explored the diverse needs of their students, and engaged learners in culturally appropriate social studies pedagogy. Findings also showed that unsuccessful CRT was characterized by a lack of content knowledge, resistance from the cooperating teacher, and a reliance on the textbook materials."}
{"_id":"0ed4df86b942e8b4979bac640a720db0187a32ef","title":"Deep and Broad Learning on Content-Aware POI Recommendation","text":"POI recommendation has attracted lots of research attentions recently. There are several key factors that need to be modeled towards effective POI recommendation - POI properties, user preference and sequential momentum of check- ins. The challenge lies in how to synergistically learn multi-source heterogeneous data. Previous work tries to model multi-source information in a flat manner, using either embedding based methods or sequential prediction models in a cross-related space, which cannot generate mutually reinforce results. In this paper, a deep and broad learning approach based on a Deep Context- aware POI Recommendation (DCPR) model was proposed to structurally learn POI and user characteristics. The proposed DCPR model includes three collaborative layers, a CNN layer for POI feature mining, an RNN layer for sequential dependency and user preference modeling, and an interactive layer based on matrix factorization to jointly optimize the overall model. Experiments over three data sets demonstrate that DCPR model achieves significant improvement over state-of-the-art POI recommendation algorithms and other deep recommendation models."}
{"_id":"2d113bbeb6f2f393a09a82bc05fdff61b391d05d","title":"A Rule Based Approach to Discourse Parsing","text":"In this paper we present an overview of recent developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic theory of discourse structure. We give a novel approach to the problem of discourse segmentation based on discourse semantics and sketch a limited but robust approach to symbolic discourse parsing based on syntactic, semantic and lexical rules. To demonstrate the utility of the system in a real application, we briefly describe the architecture of the PALSUMM system, a symbolic summarization system being developed at FX Palo Alto Laboratory that uses discourse structures constructed using the theory outlined to summarize written English prose texts. 1"}
{"_id":"9778197c8b8a4a1c297edb180b63f4a29f612895","title":"Emerging Principles of Gene Expression Programs and Their Regulation.","text":"Many mechanisms contribute to regulation of gene expression to ensure coordinated cellular behaviors and fate decisions. Transcriptional responses to external signals can consist of many hundreds of genes that can be parsed into different categories based on kinetics of induction, cell-type and signal specificity, and duration of the response. Here we discuss the structure of transcription programs and suggest a basic framework to categorize gene expression programs based on characteristics related to their control mechanisms. We also discuss possible evolutionary implications of this framework."}
{"_id":"14318685b5959b51d0f1e3db34643eb2855dc6d9","title":"Going deeper with convolutions","text":"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."}
{"_id":"1827de6fa9c9c1b3d647a9d707042e89cf94abf0","title":"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","text":"Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}
{"_id":"5424a7cd6a6393ceb8759494ee6a7f7f16a0a179","title":"You Can Run but You Can't Read: Preventing Disclosure Exploits in Executable Code","text":"Code reuse attacks allow an adversary to impose malicious behavior on an otherwise benign program. To mitigate such attacks, a common approach is to disguise the address or content of code snippets by means of randomization or rewriting, leaving the adversary with no choice but guessing. However, disclosure attacks allow an adversary to scan a process - even remotely - and enable her to read executable memory on-the-fly, thereby allowing the just-in time assembly of exploits on the target site. In this paper, we propose an approach that fundamentally thwarts the root cause of memory disclosure exploits by preventing the inadvertent reading of code while the code itself can still be executed. We introduce a new primitive we call Execute-no-Read (XnR) which ensures that code can still be executed by the processor, but at the same time code cannot be read as data. This ultimately forfeits the self-disassembly which is necessary for just-in-time code reuse attacks (JIT-ROP) to work. To the best of our knowledge, XnR is the first approach to prevent memory disclosure attacks of executable code and JIT-ROP attacks in general. Despite the lack of hardware support for XnR in contemporary Intel x86 and ARM processors, our software emulations for Linux and Windows have a run-time overhead of only 2.2% and 3.4%, respectively."}
{"_id":"8415d972023133d0f3f731ec7d070663ba878624","title":"TangiPaint: A Tangible Digital Painting System","text":"TangiPaint is a digital painting application that provides the experience of working with real materials such as canvas and oil paint. Using fingers on the touchscreen of an iPad or iPhone, users can lay down strokes of thick, three-dimensional paint on a simulated canvas. Then using the Tangible Display technology introduced by Darling and Ferwerda [1], users can tilt the display screen to see the gloss and relief or \"impasto\" of the simulated surface, and modify it until they get the appearance they desire. Scene lighting can also be controlled through direct gesture-based interaction. A variety of \"paints\" with different color and gloss properties and substrates with different textures are available and new ones can be created or imported. The tangiPaint system represents a first step toward developing digital art media that look and behave like real materials. Introduction The development of computer-based digital art tools has had a huge impact on a wide range of creative fields. In commercial art, advertisements incorporating images, text, and graphic elements can be laid out and easily modified using digital illustration applications. In cinema, background matte elements can be digitally drawn, painted, and seamlessly integrated with live footage. In fine art, painters, printers, and engravers have also been embracing the new creative possibilities of computer-based art tools. The recent introduction of mobile, tablet-based computers with high-resolution displays, graphics processing units (GPUs) and multi-touch capabilities is also creating new possibilities for direct interaction in digital painting. However a significant limitation of most digital painting tools is that the final product is just a digital image (typically an array of RGB color values). All the colors, textures and lighting effects that we see when we look at the digital painting are \u201cbaked in\u201d to the image by the painter. In contrast, when a painter works with real tools and media, the color, gloss, and textural properties of the work are a natural byproduct of the creative process, and lighting effects such as highlights and shadows are produced directly through interactions of the surface with light in the environment. In this paper we introduce tangiPaint, a new tablet-based digital painting system that attempts to bridge the gap between the real and digital worlds. tangiPaint is a tangible painting application that allows artists to work with digital media that look and behave like real materials. Figure 1 shows screenshots from the tangiPaint application implemented on an Apple iPad2. In Figure 1a an artist has painted a number of brushstrokes on a blank canvas. Note that in addition to color, the strokes vary in gloss, thickness, and texture, and run out just as if they were real paint laid down with a real brush. The paints also layer and mix realistically as they would on a real canvas. Figure 1. Screenshots of paintings created using the tangiPaint system. Note the gloss and relief of the brushstrokes and the texture of the underlying canvas. The system allows direct interaction with the \u201cpainted\u201d surface both in terms of paint application and manipulation of surface orientation and"}
{"_id":"741fd80f0a31fe77f91b1cce3d91c544d6d5b1b2","title":"Affective outcomes of virtual reality exposure therapy for anxiety and specific phobias: a meta-analysis.","text":"Virtual reality exposure therapy (VRET) is an increasingly common treatment for anxiety and specific phobias. Lacking is a quantitative meta-analysis that enhances understanding of the variability and clinical significance of anxiety reduction outcomes after VRET. Searches of electronic databases yielded 52 studies, and of these, 21 studies (300 subjects) met inclusion criteria. Although meta-analysis revealed large declines in anxiety symptoms following VRET, moderator analyses were limited due to inconsistent reporting in the VRET literature. This highlights the need for future research studies that report uniform and detailed information regarding presence, immersion, anxiety and\/or phobia duration, and demographics."}
{"_id":"53b85e4066944b1753aae8e3418028a67d9372e1","title":"The Chemical Basis of Morphogenesis","text":"The paper discussed is by Alan Turing. It was published in 1952 and presents an idea of how periodic patterns could be formed in nature. Looking on periodic structures \u2013 like the stripes on tigers, the dots on leopards or the whirly leaves on woodruff \u2013 it is hard to imagine those patterns are formated by pure chance. On the other hand, thinking of the unbelievable multitude of possible realizations, the patterns can not all be exactly encoded in the genes. The paper \u201cThe Chemical Basis of Morphogenesis\u201d proposes a possible mechanism due to an interaction of two \u201cmorphogenes\u201d which react and diffuse through the tissue. Fulfilling some constrains regarding the diffusibilities and the behaviour of the reactions, this mechanism \u2013 called Turing mechanism \u2013 can lead to a pattern of concentrations defining the structure we see."}
{"_id":"cb9155bf684f9146da4605f07fed9224fd8b146b","title":"The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?","text":"A successful grasp requires careful balancing of the contact forces. Deducing whether a particular grasp will be successful from indirect measurements, such as vision, is therefore quite challenging, and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping. However, in order to fully evaluate the value of touch sensing for grasp outcome prediction, we must understand how touch sensing can influence outcome prediction accuracy when combined with other modalities. Doing so using conventional model-based techniques is exceptionally difficult. In this work, we investigate the question of whether touch sensing aids in predicting grasp outcomes within a multimodal sensing framework that combines vision and touch. To that end, we collected more than 9,000 grasping trials using a two-finger gripper equipped with GelSight high-resolution tactile sensors on each finger, and evaluated visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually, and from both modalities together. Our experimental results indicate that incorporating tactile readings substantially improve grasping performance."}
{"_id":"79780f551413667a6a69dc130dcf843516cda6aa","title":"Real-Time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor","text":"We present an approach for real-time, robust and accurate hand pose estimation from moving egocentric RGB-D cameras in cluttered real environments. Existing methods typically fail for hand-object interactions in cluttered scenes imaged from egocentric viewpoints\u2014common for virtual or augmented reality applications. Our approach uses two subsequently applied Convolutional Neural Networks (CNNs) to localize the hand and regress 3D joint locations. Hand localization is achieved by using a CNN to estimate the 2D position of the hand center in the input, even in the presence of clutter and occlusions. The localized hand position, together with the corresponding input depth value, is used to generate a normalized cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real time. For added accuracy, robustness and temporal stability, we refine the pose estimates using a kinematic pose tracking energy. To train the CNNs, we introduce a new photorealistic dataset that uses a merged reality approach to capture and synthesize large amounts of annotated data of natural hand interaction in cluttered scenes. Through quantitative and qualitative evaluation, we show that our method is robust to self-occlusion and occlusions by objects, particularly in moving egocentric perspectives."}
{"_id":"9141d85998eadb1bca5cca027ae07670cfafb015","title":"Determining the Sentiment of Opinions","text":"Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results."}
{"_id":"0cfe588996f1bc319f87c6f75160d1cf1542d9a9","title":"ON THE EMERGENCE OF GR 4 AMMAR FROM THE LEXICON","text":""}
{"_id":"162cd1259ad106990c6bfd36db19751f940274d3","title":"Rapid word learning under uncertainty via cross-situational statistics.","text":"There are an infinite number of possible word-to-word pairings in naturalistic learning environments. Previous proposals to solve this mapping problem have focused on linguistic, social, representational, and attentional constraints at a single moment. This article discusses a cross-situational learning strategy based on computing distributional statistics across words, across referents, and, most important, across the co-occurrences of words and referents at multiple moments. We briefly exposed adults to a set of trials that each contained multiple spoken words and multiple pictures of individual objects; no information about word-picture correspondences was given within a trial. Nonetheless, over trials, subjects learned the word-picture mappings through cross-trial statistical relations. Different learning conditions varied the degree of within-trial reference uncertainty, the number of trials, and the length of trials. Overall, the remarkable performance of learners in various learning conditions suggests that they calculate cross-trial statistics with sufficient fidelity and by doing so rapidly learn word-referent pairs even in highly ambiguous learning contexts."}
{"_id":"53dd71dc5598d41c06d3eef1315e098dc4cbca28","title":"Word Segmentation : The Role of Distributional Cues","text":"One of the infant\u2019s first tasks in language acquisition is to discover the words embedded in a mostly continuous speech stream. This learning problem might be solved by using distributional cues to word boundaries\u2014for example, by computing the transitional probabilities between sounds in the language input and using the relative strengths of these probabilities to hypothesize word boundaries. The learner might be further aided by language-specific prosodic cues correlated with word boundaries. As a first step in testing these hypotheses, we briefly exposed adults to an artificial language in which the only cues available for word segmentation were the transitional probabilities between syllables. Subjects were able to learn the words of this language. Furthermore, the addition of certain prosodic cues served to enhance performance. These results suggest that distributional cues may play an important role in the initial word segmentation of language learners. q 1996 Academic Press, Inc."}
{"_id":"7085126d3d21b559e38231f3fa283aae0ca50cd8","title":"Statistical learning by 8-month-old infants.","text":"Learners rely on a combination of experience-independent and experience-dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-month-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 minutes of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input."}
{"_id":"a6ad17f9df9346c56bab090b35ef73ff94f56c01","title":"A computational study of cross-situational techniques for learning word-to-meaning mappings","text":"This paper presents a computational study of part of the lexical-acquisition task faced by children, namely the acquisition of word-to-meaning mappings. It first approximates this task as a formal mathematical problem. It then presents an implemented algorithm for solving this problem, illustrating its operation on a small example. This algorithm offers one precise interpretation of the intuitive notions of cross-situational learning and the principle of contrast applied between words in an utterance. It robustly learns a homonymous lexicon despite noisy multi-word input, in the presence of referential uncertainty, with no prior knowledge that is specific to the language being learned. Computational simulations demonstrate the robustness of this algorithm and illustrate how algorithms based on cross-situational learning and the principle of contrast might be able to solve lexical-acquisition problems of the size faced by children, under weak, worst-case assumptions about the type and quantity of data available."}
{"_id":"d2880069513ee3cfbe1136cee6133a6eedb51c00","title":"Substrate Integrated Waveguide Cavity-Backed Self-Triplexing Slot Antenna","text":"In this letter, a novel substrate integrated waveguide (SIW) cavity-backed self-triplexing slot antenna is proposed for triple-band communication. It is realized on a single-layer printed circuit board. The proposed antenna consists of a pair of bow-tie slots etched on an SIW cavity, and three different resonant modes are excited by two microstrip lines and a coaxial probe to operate at three distinct frequencies (7.89, 9.44, and 9.87 GHz). The band around 9.48 GHz is obtained due to radiation from one bow-tie slot fed by adjacent microstrip feedline and band around 9.87 GHz is due to radiation from other bow-tie slot fed by its adjacent microstrip feedline. On the other hand, lower band around 7.89 GHz is achieved because of radiation from both bow-tie slots excited by coaxial probe feedline. The measured isolation between any two feedlines is better than 22.5 dB. The measured realized gain is more than 7.2 dB at all the bands. Cross polarization below 36.5, 29.3, and 24.45 dB in broad sight direction and high front-to-back ratio of more than 17.3 dB at each resonant frequency are obtained."}
{"_id":"9c703258eca64936838f700b6d2a0e2c33b12b72","title":"Structural Health Monitoring Framework Based on Internet of Things: A Survey","text":"Internet of Things (IoT) has recently received a great attention due to its potential and capacity to be integrated into any complex system. As a result of rapid development of sensing technologies such as radio-frequency identification, sensors and the convergence of information technologies such as wireless communication and Internet, IoT is emerging as an important technology for monitoring systems. This paper reviews and introduces a framework for structural health monitoring (SHM) using IoT technologies on intelligent and reliable monitoring. Specifically, technologies involved in IoT and SHM system implementation as well as data routing strategy in IoT environment are presented. As the amount of data generated by sensing devices are voluminous and faster than ever, big data solutions are introduced to deal with the complex and large amount of data collected from sensors installed on structures."}
{"_id":"e4284c6b3cab23dc6221a9b8383546810f5ecb6b","title":"Comparative Analysis of Intelligent Transportation Systems for Sustainable Environment in Smart Cities","text":"In recent works on the Internet of Vehicles (IoV), \u201cintelligent\u201d and \u201csustainable\u201d have been the buzzwords in the context of transportation. Maintaining sustainability in IoV is always a challenge. Sustainability in IoV can be achieved not only by the use of pollution-free vehicular systems, but also by maintenance of road traffic safety or prevention of accidents or collisions. With the aim of establishing an effective sustainable transportation planning system, this study performs a short analysis of existing sustainable transportation methods in the IoV. This study also analyzes various characteristics of sustainability and the advantages and disadvantages of existing transportation systems. Toward the end, this study provides a clear suggestion for effective sustainable transportation planning aimed at the maintenance of an eco-friendly environment and road traffic safety, which, in turn, would lead to a sustainable transportation system."}
{"_id":"6505481758758ad1b4d98e7321801723d9773af2","title":"An Improved DC-Link Voltage Fast Control Scheme for a PWM Rectifier-Inverter System","text":"This paper presents an improved dc-link voltage fast control strategy based on energy balance for a pulse width modulation (PWM) rectifier-inverter system to reduce the fluctuation of dc-link voltage. A conclusion is drawn that the energy in dc-link capacitor cannot be kept constant in dynamic process when the system operates in rectifier mode, even in ideal case. Meanwhile, the minimum dc-link voltage deviation is analyzed. Accordingly, a predictive dc-link voltage control scheme based on energy balance is proposed, while the grid current is regulated by deadbeat predictive control. A prediction method for output power is also introduced. Furthermore, a small-signal model with the control delay is adopted to analyze the stability and robustness of the control strategy. The simulation and experimental results demonstrate both good dynamic and steady-state performances of the rectifier-inverter system with the proposed control scheme."}
{"_id":"adf434fe0bf7ff55edee25d5e50d3de94ad2c325","title":"Comparative Study on Load Balancing Techniques in Cloud Computing","text":"The present era has witnessed tremendous growth of the internet and various applications that are running over it. Cloud computing is the internet based technology, emphasizing its utility and follows pay-as-you-go model, hence became so popular with high demanding features. Load balancing is one of the interesting and prominent research topics in cloud computing, which has gained a large attention recently. Users are demanding more services with better results. Many algorithms and approaches are proposed by various researchers throughout the world, with the aim of balancing the overall workload among given nodes, while attaining the maximum throughput and minimum time. In this paper, various proposed algorithms addressing the issue of load balancing in Cloud Computing are analyzed and compared to provide a gist of the latest approaches in this research area."}
{"_id":"3e38e521ec579a6aad4e7ebc5f125123018b1683","title":"Application of spiking neural networks and the bees algorithm to control chart pattern recognition","text":"Statistical process control (SPC) is a method for improving the quality o f products. Control charting plays a most important role in SPC. SPC control charts arc used for monitoring and detecting unnatural process behaviour. Unnatural patterns in control charts indicate unnatural causes for variations. Control chart pattern recognition is therefore important in SPC. Past research shows that although certain types o f charts, such as the CUSUM chart, might have powerful detection ability, they lack robustness and do not function automatically. In recent years, neural network techniques have been applied to automatic pattern recognition. Spiking Neural Networks (SNNs) belong to the third generation o f artificial neural networks, with spiking neurons as processing elements. In SNNs, time is an important feature for information representation and processing. This thesis proposes the application o f SNN techniques to control chart pattern recognition. It is designed to present an analysis o f the existing learning algorithms o f SNN for pattern recognition and to explain how and why spiking neurons have more computational power in comparison to the previous generation o f neural networks. This thesis focuses on the architecture and the learning procedure o f the network. Four new learning algorithms are presented with their specific architecture: Spiking Learning Vector Quantisation (S-LVQ), Enhanced-Spiking Learning Vector Quantisation (NS-LVQ), S-LVQ with Bees and NS-LVQ with Bees. The latter two algorithms employ a new intelligent swarm-based optimisation called the Bees Algorithm to optimise the LVQ pattern recognition networks. Overall, the aim o f the research is to develop a simple architecture for the proposed network as well as to develop a network that is efficient for application to control chart pattern recognition. Experiments show that the proposed architecture and the learning procedure give high pattern recognition accuracies."}
{"_id":"ab430d9e3e250ab5dc61e12f9e7e40c83227b8a0","title":"On Different Facets of Regularization Theory","text":"This review provides a comprehensive understanding of regularization theory from different perspectives, emphasizing smoothness and simplicity principles. Using the tools of operator theory and Fourier analysis, it is shown that the solution of the classical Tikhonov regularization problem can be derived from the regularized functional defined by a linear differential (integral) operator in the spatial (Fourier) domain. State-ofthe-art research relevant to the regularization theory is reviewed, covering Occam's razor, minimum length description, Bayesian theory, pruning algorithms, informational (entropy) theory, statistical learning theory, and equivalent regularization. The universal principle of regularization in terms of Kolmogorov complexity is discussed. Finally, some prospective studies on regularization theory and beyond are suggested."}
{"_id":"3e8d2a11a3ed6d9ebdd178a420b3c019c5356fae","title":"Never-Ending Multiword Expressions Learning","text":"This paper introduces NEMWEL, a system that performs Never-Ending MultiWord Expressions Learning. Instead of using a static corpus and classifier, NEMWEL applies supervised learning on automatically crawled news texts. Moreover, it uses its own results to periodically retrain the classifier, bootstrapping on its own results. In addition to a detailed description of the system\u2019s architecture and its modules, we report the results of a manual evaluation. It shows that NEMWEL is capable of learning new expressions over time with improved precision."}
{"_id":"9860487cd9e840d946b93457d11605be643e6d4c","title":"Text Clustering for Topic Detection","text":"The world wide web represents vast stores of information. However, the sheer amount of such information makes it practically impossible for any human user to be aware of much of it. Therefore, it would be very helpful to have a system that automatically discovers relevant, yet previously unknown information, and reports it to users in human-readable form. As the first attempt to accomplish such a goal, we proposed a new clustering algorithm and compared it with existing clustering algorithms. The proposed method is motivated by constructive and competitive learning from neural network research. In the construction phase, it tries to find the optimal number of clusters by adding a new cluster when the intrinsic difference between the instance presented and the existing clusters is detected. Each cluster then moves toward the optimal cluster center according to the learning rate by adjusting its weight vector. From the experimental results on the three different real world data sets, the proposed method shows an even trend of performance across the different domains, while the performance of our algorithm on text domains was better than that reported in previous research."}
{"_id":"c7df32e449f1ba9767763a122bbdc2fac310f958","title":"Direct Desktop Printed-Circuits-on-Paper Flexible Electronics","text":"There currently lacks of a way to directly write out electronics, just like printing pictures on paper by an office printer. Here we show a desktop printing of flexible circuits on paper via developing liquid metal ink and related working mechanisms. Through modifying adhesion of the ink, overcoming its high surface tension by dispensing machine and designing a brush like porous pinhead for printing alloy and identifying matched substrate materials among different papers, the slightly oxidized alloy ink was demonstrated to be flexibly printed on coated paper, which could compose various functional electronics and the concept of Printed-Circuits-on-Paper was thus presented. Further, RTV silicone rubber was adopted as isolating inks and packaging material to guarantee the functional stability of the circuit, which suggests an approach for printing 3D hybrid electro-mechanical device. The present work paved the way for a low cost and easygoing method in directly printing paper electronics."}
{"_id":"aea427bcea46c83021e62f4fb10178557510ca5d","title":"Latent Variable Dialogue Models and their Diversity","text":"We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the \u2018boring output\u2019 issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model."}
{"_id":"20efcba63a0d9f12251a5e5dda745ac75a6a84a9","title":"Bio-Inspired Robotics Based on Liquid Crystalline Elastomers ( LCEs ) and Flexible Stimulators","text":""}
{"_id":"2a6783ae51d7ee781d584ef9a3eb8ab1997d0489","title":"A study of large-scale ethnicity estimation with gender and age variations","text":"In this paper we study large-scale ethnicity estimation under variations of age and gender. The biologically-inspired features are applied to ethnicity classification for the first time. Through a large number of experiments on a large database with more than 21,000 face images, we systematically study the effect of gender and age variations on ethnicity estimation. Our finding is that ethnicity classification can have high accuracy in most cases, but an interesting phenomenon is observed that the ethnic classification accuracies could be reduced by 6\u223c8% in average when female faces are used for training while males for testing. The study results provide a guide for face processing on a multi-ethnic database, e.g., image collection from the Internet, and may inspire further psychological studies on ethnic grouping with gender and age variations. We also apply the methods to the whole MORPH-II database with more than 55,000 face images for ethnicity classification of five races. It is the first time that ethnicity estimation is performed on so large a database."}
{"_id":"37caf807a77df2bf907b1e7561454419a328e34d","title":"Engineering for Predictive Modeling using Reinforcement Learning","text":"Feature engineering is a crucial step in the process of predictive modeling. It involves the transformation of given feature space, typically using mathematical functions, with the objective of reducing the modeling error for a given target. However, there is no well-defined basis for performing effective feature engineering. It involves domain knowledge, intuition, and most of all, a lengthy process of trial and error. The human attention involved in overseeing this process significantly influences the cost of model generation. We present a new framework to automate feature engineering. It is based on performance driven exploration of a transformation graph, which systematically and compactly enumerates the space of given options. A highly efficient exploration strategy is derived through reinforcement learning on past examples."}
{"_id":"e7ab1a736bb105df8678af5191640fd534c8c430","title":"Power Transformer Economic Evaluation in Decentralized Electricity Markets","text":"Owing to deregulation, privatization, and competition, estimating financial benefits of electrical power system projects is becoming increasingly important. In other words, it is necessary to assess the project profitability under the light of new developments in the electricity market. In this paper, a detailed methodology for the least cost choice of a distribution transformer is proposed, showing how the higher price of a facility can be traded against its operational cost over its life span. The proposed method involves the incorporation of the discounted cost of transformer losses to their economic evaluation, providing the ability to take into account variable energy cost during the transformer operating lifetime. In addition, the influence of the variability in the energy loss cost is investigated, taking into account a potential policy intended to be adopted by distribution network operators. The method is combined with statistical and probabilistic assessment of electricity price volatility in order to derive its impact on the transformer purchasing policy."}
{"_id":"fe8e3898e203086496ae33e355f450bd32b5daff","title":"Use of opposition method in the test of high-power electronic converters","text":"The test and the characterization of medium or high-power electronic converters, under nominal operating conditions, are made difficult by the requirement of high-power electrical source and load. In addition, the energy lost during the test may be very significant. The opposition method, which consists of an association of two identical converters supplied by the same source, one operating as a generator, the other as a receptor, can be a better way to do these test. Another advantage is the possibility to realize accurate measurements of the different losses in the converters under test. In the first part of this paper, the characteristics of the method concerning loss measurements are compared to those of the electrical or calorimetric methods, then it is shown how it can be applied to different types of power electronic converters, choppers, switched mode power supplies, and pulsewidth modulation inverters. In the second part, different examples of studies conducted by the authors, and using this method, are presented. They have varying goals, from the test of soft-switching inverters to the characterization of integrated gate-commutated thyristor (IGCT) devices mounted into 2-MW choppers."}
{"_id":"b9728a8279c12f93fff089b6fac96afd2d3bab04","title":"Mollifying Networks","text":"The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed \u2013 or mollified \u2013 objective function which becomes more complex as the training proceeds. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship between recent works on continuation methods for neural networks and mollifiers."}
{"_id":"1a3fe9032cf48e877bf44e24f5d06d9d14e04c9e","title":"Robot-assisted movement training compared with conventional therapy techniques for the rehabilitation of upper-limb motor function after stroke.","text":"OBJECTIVE\nTo compare the effects of robot-assisted movement training with conventional techniques for the rehabilitation of upper-limb motor function after stroke.\n\n\nDESIGN\nRandomized controlled trial, 6-month follow-up.\n\n\nSETTING\nA Department of Veterans Affairs rehabilitation research and development center.\n\n\nPARTICIPANTS\nConsecutive sample of 27 subjects with chronic hemiparesis (>6mo after cerebrovascular accident) randomly allocated to group.\n\n\nINTERVENTIONS\nAll subjects received twenty-four 1-hour sessions over 2 months. Subjects in the robot group practiced shoulder and elbow movements while assisted by a robot manipulator. Subjects in the control group received neurodevelopmental therapy (targeting proximal upper limb function) and 5 minutes of exposure to the robot in each session.\n\n\nMAIN OUTCOME MEASURES\nFugl-Meyer assessment of motor impairment, FIMtrade mark instrument, and biomechanic measures of strength and reaching kinematics. Clinical evaluations were performed by a therapist blinded to group assignments.\n\n\nRESULTS\nCompared with the control group, the robot group had larger improvements in the proximal movement portion of the Fugl-Meyer test after 1 month of treatment (P<.05) and also after 2 months of treatment (P<.05). The robot group had larger gains in strength (P<.02) and larger increases in reach extent (P<.01) after 2 months of treatment. At the 6-month follow-up, the groups no longer differed in terms of the Fugl-Meyer test (P>.30); however, the robot group had larger improvements in the FIM (P<.04).\n\n\nCONCLUSIONS\nCompared with conventional treatment, robot-assisted movements had advantages in terms of clinical and biomechanical measures. Further research into the use of robotic manipulation for motor rehabilitation is justified."}
{"_id":"552aa062d2895901bca03b26fa25766b3f64bf6f","title":"A Brief survey of Data Mining Techniques Applied to Agricultural Data","text":"As with many other sectors the amount of agriculture data based are increasing on a daily basis. However, the application of data mining methods and techniques to discover new insights or knowledge is a relatively a novel research area. In this paper we provide a brief review of a variety of Data Mining techniques that have been applied to model data from or about the agricultural domain. The Data Mining techniques applied on Agricultural data include k-means, bi clustering, k nearest neighbor, Neural Networks (NN) Support Vector Machine (SVM), Naive Bayes Classifier and Fuzzy c-means. As can be seen the appropriateness of data mining techniques is to a certain extent determined by the different types of agricultural data or the problems being addressed. This survey summarize the application of data mining techniques and predictive modeling application in the agriculture field."}
{"_id":"6ee0d9a40c60e50fab636cca74c6301853d42367","title":"Stargazer: Automated regression-based GPU design space exploration","text":"Graphics processing units (GPUs) are of increasing interest because they offer massive parallelism for high-throughput computing. While GPUs promise high peak performance, their challenge is a less-familiar programming model with more complex and irregular performance trade-offs than traditional CPUs or CMPs. In particular, modest changes in software or hardware characteristics can lead to large or unpredictable changes in performance. In response to these challenges, our work proposes, evaluates, and offers usage examples of Stargazer1, an automated GPU performance exploration framework based on stepwise regression modeling. Stargazer sparsely and randomly samples parameter values from a full GPU design space and simulates these designs. Then, our automated stepwise algorithm uses these sampled simulations to build a performance estimator that identifies the most significant architectural parameters and their interactions. The result is an application-specific performance model which can accurately predict program runtime for any point in the design space. Because very few initial performance samples are required relative to the extremely large design space, our method can drastically reduce simulation time in GPU studies. For example, we used Stargazer to explore a design space of nearly 1 million possibilities by sampling only 300 designs. For 11 GPU applications, we were able to estimate their runtime with less than 1.1% average error. In addition, we demonstrate several usage scenarios of Stargazer."}
{"_id":"ccaab0cee02fe1e5ffde33b79274b66aedeccc65","title":"Ethical and Social Aspects of Self-Driving Cars","text":"As an envisaged future of transportation, self-driving cars are being discussed from various perspectives, including social, economical, engineering, computer science, design, and ethics. On the one hand, self-driving cars present new engineering problems that are being gradually successfully solved. On the other hand, social and ethical problems are typically being presented in the form of an idealized unsolvable decision-making problem, the so-called trolley problem, which is grossly misleading. We argue that an applied engineering ethical approach for the development of new technology is what is needed; the approach should be applied, meaning that it should focus on the analysis of complex real-world engineering problems. Software plays a crucial role for the control of self-driving cars; therefore, software engineering solutions should seriously handle ethical and social considerations. In this paper we take a closer look at the regulative instruments, standards, design, and implementations of components, systems, and services and we present practical social and ethical challenges that have to be met, as well as novel expectations for software engineering."}
{"_id":"bbd9b5e4d4761d923d21a060513e826bf5bfc620","title":"Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations","text":"Recent advances with Convolutional Networks (ConvNets) have shifted the bottleneck for many computer vision tasks to annotated data collection. In this paper, we present a geometry-driven approach to automatically collect annotations for human pose prediction tasks. Starting from a generic ConvNet for 2D human pose, and assuming a multi-view setup, we describe an automatic way to collect accurate 3D human pose annotations. We capitalize on constraints offered by the 3D geometry of the camera setup and the 3D structure of the human body to probabilistically combine per view 2D ConvNet predictions into a globally optimal 3D pose. This 3D pose is used as the basis for harvesting annotations. The benefit of the annotations produced automatically with our approach is demonstrated in two challenging settings: (i) fine-tuning a generic ConvNet-based 2D pose predictor to capture the discriminative aspects of a subjects appearance (i.e.,personalization), and (ii) training a ConvNet from scratch for single view 3D human pose prediction without leveraging 3D pose groundtruth. The proposed multi-view pose estimator achieves state-of-the-art results on standard benchmarks, demonstrating the effectiveness of our method in exploiting the available multi-view information."}
{"_id":"66e3aa516a7124befa7d2a8b0872e9619acf7f58","title":"JM: An R Package for the Joint Modelling of Longitudinal and Time-to-Event Data","text":"In longitudinal studies measurements are often collected on different types of outcomes for each subject. These may include several longitudinally measured responses (such as blood values relevant to the medical condition under study) and the time at which an event of particular interest occurs (e.g., death, development of a disease or dropout from the study). These outcomes are often separately analyzed; however, in many instances, a joint modeling approach is either required or may produce a better insight into the mechanisms that underlie the phenomenon under study. In this paper we present the R package JM that fits joint models for longitudinal and time-to-event data."}
{"_id":"4f57643b95e854bb05fa0c037cbf8898accdbdef","title":"Technical evaluation of the Carolo-Cup 2014 - A competition for self-driving miniature cars","text":"The Carolo-Cup competition conducted for the eighth time this year, is an international student competition focusing on autonomous driving scenarios implemented on 1:10 scale car models. Three practical sub-competitions have to be realized in this context and represent a complex, interdisciplinary challenge. Hence, students have to cope with all core topics like mechanical development, electronic design, and programming as addressed usually by robotic applications. In this paper we introduce the competition challenges in detail and evaluate the results of all 13 participating teams from the 2014 competition. For this purpose, we analyze technical as well as non-technical configurations of each student group and derive best practices, lessons learned, and criteria as a precondition for a successful participation. Due to the comprehensive orientation of the Carolo-Cup, this knowledge can be applied on comparable projects and related competitions as well."}
{"_id":"21fb86020f68bf2dd57cd1b8a0e8adead5d9a9ae","title":"Data Mining : Concepts and Techniques","text":"Association rule mining was first proposed by Agrawal, Imielinski, and Swami [AIS93]. The Apriori algorithm discussed in Section 5.2.1 for frequent itemset mining was presented in Agrawal and Srikant [AS94b]. A variation of the algorithm using a similar pruning heuristic was developed independently by Mannila, Tiovonen, and Verkamo [MTV94]. A joint publication combining these works later appeared in Agrawal, Mannila, Srikant, Toivonen, and Verkamo [AMS96]. A method for generating association rules from frequent itemsets is described in Agrawal and Srikant [AS94a]."}
{"_id":"4eae6ee36de5f9ae3c05c6ca385938de98cd5ef8","title":"Combining Text and Linguistic Document Representations for Authorship Attribution","text":"In this paper, we provide several alternatives to the classical Bag-Of-Words model for automatic authorship attribution. To this end, we consider linguistic and writing style information such as grammatical structures to construct different document representations. Furthermore we describe two techniques to combine the obtained representations: combination vectors and ensemble based meta classification. Our experiments show the viability of our approach."}
{"_id":"288c67457f09c0c30cadd7439040114e9c377bc3","title":"Finding Interesting Rules from Large Sets of Discovered Association Rules","text":"Association rules, introduced by Agrawal, Imielinski, and Swami, are rules of the form \u201cfor 90% of the rows of the relation, if the row has value 1 in the columns in set W, then it has 1 also in column B\u201d. Efficient methods exist for discovering association rules from large collections of data. The number of discovered rules can, however, be so large that browsing the rule set and finding interesting rules from it can be quite difficult for the user. We show how a simple formalism of rule templates makes it possible to easily describe the structure of interesting rules. We also give examples of visualization of rules, and show how a visualization tool interfaces with rule templates."}
{"_id":"384bb3944abe9441dcd2cede5e7cd7353e9ee5f7","title":"Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods","text":""}
{"_id":"49fa97db6b7f3ab2b3a623c3552aa680b80c8dd2","title":"Automatically Categorizing Written Texts by Author Gender","text":"The problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution. Nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80% accuracy. The same techniques can be used to determine if a document is fiction or non-fiction with approximately 98% accuracy."}
{"_id":"883224c3b28b0563a393746066738f52e6fcc70d","title":"To Create What You Tell: Generating Videos from Captions","text":"We are creating multimedia contents everyday and everywhere. While automatic content generation has played a fundamental challenge to multimedia community for decades, recent advances of deep learning have made this problem feasible. For example, the Generative Adversarial Networks (GANs) is a rewarding approach to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs to generate videos. The difficulty originates from the intrinsic structure where a video is a sequence of visually coherent and semantically dependent frames. This motivates us to explore semantic and temporal coherence in designing GANs to generate videos. In this paper, we present a novel Temporal GANs conditioning on Captions, namely TGANs-C, in which the input to the generator network is a concatenation of a latent noise vector and caption embedding, and then is transformed into a frame sequence with 3D spatio-temporal convolutions. Unlike the naive discriminator which only judges pairs as fake or real, our discriminator additionally notes whether the video matches the correct caption. In particular, the discriminator network consists of three discriminators: video discriminator classifying realistic videos from generated ones and optimizes video-caption matching, frame discriminator discriminating between real and fake frames and aligning frames with the conditioning caption, and motion discriminator emphasizing the philosophy that the adjacent frames in the generated videos should be smoothly connected as in real ones. We qualitatively demonstrate the capability of our TGANs-C to generate plausible videos conditioning on the given captions on two synthetic datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover, quantitative experiments on MSVD are performed to validate our proposal via Generative Adversarial Metric and human study."}
{"_id":"f4b3804f052f15f8a25af78db24e32dc25254722","title":"Number : AUTCON-D16-00058 R 1 Title : BIM for Infrastructure : An Overall Review and Constructor Perspective","text":"6 The subject of Building Information Modelling (BIM) has become a central topic to the improvement of the AECOO 7 (Architecture, Engineering, Construction, Owner and Operator) industry around the world, to the point where the 8 concept is being expanded into domains it was not originally conceived to address. Transitioning BIM into the 9 domain of infrastructure projects has provided challenges and emphasized the constructor perspective of BIM. 10 Therefore, this study aims to collect the relevant literature regarding BIM within the Infrastructure domain and its 11 use from the constructor perspective to review and analyse the current industry positioning and research state of 12 the art, with regards to the set criteria. The review highlighted a developing base of BIM for infrastructure. From 13 the analysis, the related research gaps were identified regarding information integration, alignment of BIM 14 processes to constructor business processes & the effective governance and value of information. From this a 15 unique research strategy utilising a framework for information governance coupled with a graph based distributed 16 data environment is outlined to further progress the integration and efficiency of AECOO Infrastructure projects. 17"}
{"_id":"36d167397872c36713e8a274113b30ea5cd3ad7d","title":"Enterprise Database Applications and the Cloud: A Difficult Road Ahead","text":"There is considerable interest in moving DBMS applications from inside enterprise data centers to the cloud, both to reduce cost and to increase flexibility and elasticity. Some of these applications are \"green field\" projects (i.e., new applications), others are existing legacy systems that must be migrated to the cloud. In another dimension, some are decision support applications while others are update-oriented. In this paper, we discuss the technical and political challenges that these various enterprise applications face when considering cloud deployment. In addition, a requirement for quality-of-service (QoS) guarantees will generate additional disruptive issues. In some circumstances, achieving good DBMS performance on current cloud architectures and future hardware technologies will be non-trivial. In summary, there is a difficult road ahead for enterprise database applications."}
{"_id":"f61ca00abf165ea5590f67942c9bd7538187752d","title":"Laughbot: Detecting Humor in Spoken Language with Language and Audio Cues","text":"We propose detecting and responding to humor in spoken dialogue by extracting language and audio cues and subsequently feeding these features into a combined recurrent neural network (RNN) and logistic regression model. In this paper, we parse Switchboard phone conversations to build a corpus of punchlines and unfunny lines where punchlines precede laughter tokens in Switchboard transcripts. We create a combined RNN and logistic regression model that uses both acoustic and language cues to predict whether a conversational agent should respond to an utterance with laughter. Our model achieves an F1-score of 63.2 and accuracy of 73.9. This model outperforms our logistic language model (F1-score 56.6) and RNN acoustic model (59.4) as well as the final RNN model of D. Bertero, 2016 (52.9). Using our final model, we create a \u201claughbot\u201d that audibly responds to a user with laughter when their utterance is classified as a punchline. A conversational agent outfitted with a humorrecognition system such as the one we present in this paper would be valuable as these agents gain utility in everyday life. Keywords\u2014Chatbots; spoken natural language processing; deep learning; machine learning"}
{"_id":"4c27eef7fa83900ef8f2e48a523750d035830342","title":"Reconstruction of dorsal and\/or caudal nasal septum deformities with septal battens or by septal replacement: an overview and comparison of techniques.","text":"OBJECTIVES\nThe objectives of this study were to describe and compare two techniques used to correct nasal septum deviations located in the dorsal and\/or caudal septum.\n\n\nSTUDY DESIGN\nThe authors conducted a retrospective clinical chart review.\n\n\nMETHODS\nThe authors conducted a comparison of functional and technical results between surgery in the L-strut of the septum in 114 patients with septal battens or by septal replacement by subjective self-evaluation and by examination of the position of the septum during follow up.\n\n\nRESULTS\nThere was subjective improvement in nasal breathing in 86% of the septal batten group and in 94% of the septal replacement group. This difference was not statistically significant. The technical result was judged by examining the position of the septum during follow up as midline, slightly deviated, or severely deviated. The septum was significantly more often located in the midline during follow up in the septal replacement group than in the septal batten group.\n\n\nCONCLUSION\nTreatment of deformities located in the structurally important L-strut of the septum may be technically challenging and many functional, structural, and esthetic considerations must be taken into account. On the basis of this series, both septal battens and septal replacement techniques may be considered for correction of deviations in this area. The functional improvement rates were not significantly different between the techniques, although during follow up, the septum appeared to be significantly more often located in the midline in the septal replacement group. The techniques are described and their respective advantages and potential drawbacks are discussed."}
{"_id":"1b5f18498b42e464b81e3d81b8d32237aea4a234","title":"DroidTrace: A ptrace based Android dynamic analysis system with forward execution capability","text":"Android, being an open source smartphone operating system, enjoys a large community of developers who create new mobile services and applications. However, it also attracts malware writers to exploit Android devices in order to distribute malicious apps in the wild. In fact, Android malware are becoming more sophisticated and they use advanced \u201cdynamic loading\u201d techniques like Java reflection or native code execution to bypass security detection. To detect dynamic loading, one has to use dynamic analysis. Currently, there are only a handful of Android dynamic analysis tools available, and they all have shortcomings in detecting dynamic loading. The aim of this paper is to design and implement a dynamic analysis system which allows analysts to perform systematic analysis of dynamic payloads with malicious behaviors. We propose \u201cDroidTrace\u201d, a ptrace based dynamic analysis system with forward execution capability. Our system uses ptrace to monitor selected system calls of the target process which is running the dynamic payloads, and classifies the payloads behaviors through the system call sequence, e.g., behaviors such as file access, network connection, inter-process communication and even privilege escalation. Also, DroidTrace performs \u201cphysical modification\u201d to trigger different dynamic loading behaviors within an app. Using DroidTrace, we carry out a large scale analysis on 36,170 dynamic payloads in 50,000 apps and 294 malware in 10 families (four of them are zero-day) with various dynamic loading behaviors."}
{"_id":"ef9473055dd96e5d146c88ae3cc88d06e7adfd07","title":"Understanding the Dynamic Interplay of Social Buzz and Contribution Behavior within and between Online Platforms - Evidence from Crowdfunding","text":"Motivated by the growing interconnection between online platforms, we examine the dynamic interplay between social buzz and contribution behavior in the crowdfunding context. Since the utility of crowdfunding projects is usually difficult to ascertain, prospective backers draw on quality signals, such as social buzz and prior-contribution behavior, to make their funding decisions. We employ the panel vector autoregression (PVAR) methodology to investigate both intraand cross-platform effects based on data collected from three platforms: Indiegogo, one of the largest crowdfunding platforms on the web, Twitter and Facebook. Our results show a positive influence of social buzz on project backing, but a negative relationship in the reverse direction. Furthermore, we observe strong positive feedback cycles within each platform. Our results are supplemented by split-sample analyses for project orientation (Social, Cause and Entrepreneurial) and project success (Winners vs. Losers), in which Facebook shares were identified as a critical success factor."}
{"_id":"6e5d8a30531680beb200cd6f0de91a7919381520","title":"Comparing exploration strategies for Q-learning in random stochastic mazes","text":"Balancing the ratio between exploration and exploitation is an important problem in reinforcement learning. This paper evaluates four different exploration strategies combined with Q-learning using random stochastic mazes to investigate their performances. We will compare: UCB-1, softmax, \u2208-greedy, and pursuit. For this purpose we adapted the UCB-1 and pursuit strategies to be used in the Q-learning algorithm. The mazes consist of a single optimal goal state and two suboptimal goal states that lie closer to the starting position of the agent, which makes efficient exploration an important part of the learning agent. Furthermore, we evaluate two different kinds of reward functions, a normalized one with rewards between 0 and 1, and an unnormalized reward function that penalizes the agent for each step with a negative reward. We have performed an extensive grid-search to find the best parameters for each method and used the best parameters on novel randomly generated maze problems of different sizes. The results show that softmax exploration outperforms the other strategies, although it is harder to tune its temperature parameter. The worst performing exploration strategy is \u2208-greedy."}
{"_id":"a22aa5a7e98fe4fad1ec776df8d423b1c8b373ef","title":"Character-based movie summarization","text":"A decent movie summary is helpful for movie producer to promote the movie as well as audience to capture the theme of the movie before watching the whole movie. Most exiting automatic movie summarization approaches heavily rely on video content only, which may not deliver ideal result due to the semantic gap between computer calculated low-level features and human used high-level understanding. In this paper, we incorporate script into movie analysis and propose a novel character-based movie summarization approach, which is validated by modern film theory that what actually catches audiences' attention is the character. We first segment scenes in the movie by analysis and alignment of script and movie. Then we conduct substory discovery and content attention analysis based on the scent analysis and character interaction features. Given obtained movie structure and content attention value, we calculate movie attraction scores at both shot and scene levels and adopt this as criterion to generate movie summary. The promising experimental results demonstrate that character analysis is effective for movie summarization and movie content understanding."}
{"_id":"7af4c0e2899042aea87d4a37aadd9f60b53cf272","title":"Distributed Denial-of-Service (DDoS) Threat in Collaborative Environment - A Survey on DDoS Attack Tools and Traceback Mechanisms","text":"Collaborative applications are feasible nowadays and are becoming more popular due to the advancement in internetworking technology. The typical collaborative applications, in India include the Space research, Military applications, Higher learning in Universities and Satellite campuses, State and Central government sponsored projects, e-governance, e-healthcare systems, etc. In such applications, computing resources for a particular institution\/organization spread across districts and states and communication is achieved through internetworking. Therefore the computing and communication resources must be protected against security attacks as any compromise on these resources would jeopardize the entire application\/mission. Collaborative environment is prone for various threats, of which Distributed Denial of Service (DDoS) attacks are of major concern. DDoS attack prevents legitimate access to critical resources. A survey by Arbor networks reveals that approximately 1,200 DDoS attacks occur per day. As the DDoS attack is coordinated, the defense for the same has to be a collaborative one. To counter DDoS attacks in a collaborative environment, all the routers need to work collaboratively by exchanging their caveat messages with their neighbors. This paper analyses the security measures in a collaborative environment, identifles the popular DDoS attack tools, and surveys the existing traceback mechanisms to trace the real attacker."}
{"_id":"34977babbdc735c56b04668c19da31d89161a2b9","title":"Geolocation of RF Emitters by Many UAVs","text":"This paper presents an approach to using a large team of UAVs to find radio frequency (RF) emitting targets in a large area. Small, inexpensive UAVs that can collectively and rapidly determine the approximate location of intermittently broadcasting and mobile RF emitters have a range of applications in both military, e.g., for finding SAM batteries, and civilian, e.g., for finding lost hikers, domains. Received Signal Strength Indicator (RSSI) sensors on board the UAVs measure the strength of RF signals across a range of frequencies. The signals, although noisy and ambiguous due to structural noise, e.g., multipath effects, overlapping signals and sensor noise, allow estimates to be made of emitter locations. Generating a probability distribution over emitter locations requires integrating multiple signals from different UAVs into a Bayesian filter, hence requiring cooperation between the UAVs. Once likely target locations are identified, EO-camera equipped UAVs must be tasked to provide a video stream of the area to allow a user to identify the emitter."}
{"_id":"cb32e2100a853e7ea491b1ac17b941f64f8720df","title":"75\u201385 GHz flip-chip phased array RFIC with simultaneous 8-transmit and 8-receive paths for automotive radar applications","text":"This paper presents the first simultaneous 8-transmit and 8-receive paths 75-85 GHz phased array RFIC for FMCW automotive radars. The receive path has two separate I\/Q mixers each connected to 4-element phased arrays for RF and digital beamforming. The chip also contains a build-in-self-test system (BIST) for the transmit and receive paths. Measurements on a flip-chip prototype show a gain >24 dB at 77 GHz, -25 dB coupling between adjacent channels in the transmit and receive paths (<;-45 dB between non-adjacent channels), and <;-50 dB coupling between the transmit and receive portions of the chip."}
{"_id":"d21ebaab3f715dc7178966ff146711882e6a6fee","title":"Globally and locally consistent image completion","text":"We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces."}
{"_id":"aae7bde6972328de6c23a510fe59254854163308","title":"DEFO-NET: Learning Body Deformation Using Generative Adversarial Networks","text":"Modelling the physical properties of everyday objects is a fundamental prerequisite for autonomous robots. We present a novel generative adversarial network (DEFO-NET), able to predict body deformations under external forces from a single RGB-D image. The network is based on an invertible conditional Generative Adversarial Network (IcGAN) and is trained on a collection of different objects of interest generated by a physical finite element model simulator. Defo-netinherits the generalisation properties of GANs. This means that the network is able to reconstruct the whole 3-D appearance of the object given a single depth view of the object and to generalise to unseen object configurations. Contrary to traditional finite element methods, our approach is fast enough to be used in real-time applications. We apply the network to the problem of safe and fast navigation of mobile robots carrying payloads over different obstacles and floor materials. Experimental results in real scenarios show how a robot equipped with an RGB-D camera can use the network to predict terrain deformations under different payload configurations and use this to avoid unsafe areas."}
{"_id":"651adaa058f821a890f2c5d1053d69eb481a8352","title":"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples","text":"We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers."}
{"_id":"33eb066178d7ec2307f1db171f90c8338108bcc6","title":"Graphical SLAM - a self-correcting map","text":"We describe an approach to simultaneous localization and mapping, SLAM. This approach has the highly desirable property of robustness to data association errors. Another important advantage of our algorithm is that non-linearities are computed exactly, so that global constraints can be imposed even if they result in large shifts to the map. We represent the map as a graph and use the graph to find an efficient map update algorithm. We also show how topological consistency can be imposed on the map, such as, closing a loop. The algorithm has been implemented on an outdoor robot and we have experimental validation of our ideas. We also explain how the graph can be simplified leading to linear approximations of sections of the map. This reduction gives us a natural way to connect local map patches into a much larger global map."}
{"_id":"711d59dba9bd4284170ccae24fdc2a14519cf941","title":"A novel approach to American sign language recognition using MAdaline neural network","text":"Sign language interpretation is gaining a lot of research attention because of its social contributions which is proved to be extremely beneficial for the people suffering from hearing or speaking disabilities. This paper proposes a novel image processing sign language detection framework that employs MAdaline network for classification purpose. This paper mainly highlights two novel aspects, firstly it introduces an advanced feature set comprising of seven distinct features that has not been used widely for sign language interpretation purpose, more over utilization of such features negates the cumbersome step of cropping of irrelevant background image, thus reducing system complexity. Secondly it suggests a possible solution of the concerned problem can be obtained using an extension of the traditional Adaline network, formally termed as MAdaline Network. Although the concept of MAdaline network has originated much earlier, the provision of application of this framework in this domain definitely help in designing an improved sign language interpreting interface. The newly formulated framework has been implemented to recognize standardized American sign language containing 26 English alphabets from \u2018A\u2019 to \u2018Z\u2019. The performance of the proposed algorithm has also been compared with the standardized algorithms, and in each case the former one outperformed its contender algorithms by a large margin establishing the efficiency of the same."}
{"_id":"8f9a6313e525e33d88bb6f756e22bfec5272aab3","title":"Design and Optimization a Circular Shape Network Antenna Micro Strip for Some Application","text":"To meet the demands of high speed required by mobile communication of past generations ,one solution is to increase the number of antennas to the show and the reception of the wireless link this is called MIMO (Multiple input ,Multiple output )technology .however ,the integration of multiple antennas on the same PCB is delicate because of the small volume that require some applications and electromagnetic antenna between the coupling ,phenomena that we cannot neglect them .indeed a strong isolation between them has been reached to reduce fading of the signal caused by the electromagnetic antenna reached to reduce fading of the signal caused by the electromagnetic coupling and maximize the overall gain .in this article we are interested then integration on the same printed circuit of eight antennas MIMO are not operation in the same frequency band .the first antenna of this last work at 2.4GHz .other antennas have resonance frequency folling each with 20MHz offset this device is characterized by its original form that keeps is highly isolated antennas from the point of view electromagnetic coupling . INDEX TERMS MIMO, Technology Micro-strip, Microwave, Network Antenna"}
{"_id":"2ad08da69a014691ae76cf7f53534b40b412c0e4","title":"Network Traffic Anomaly Detection","text":"This paper presents a tutorial for network anomaly detection, focusing on non-signature-based approaches. Network traffic anomalies are unusual and significant changes in the traffic of a network. Networks play an important role in today\u2019s social and economic infrastructures. The security of the network becomes crucial, and network traffic anomaly detection constitutes an important part of network security. In this paper, we present three major approaches to non-signature-based network detection: PCA-based, sketch-based, and signal-analysis-based. In addition, we introduce a framework that subsumes the three approaches and a scheme for network anomaly extraction. We believe network anomaly detection will become more important in the future because of the increasing importance of network security."}
{"_id":"3cbb64df30f2581016542d2c0441f35e8a8c2147","title":"Forward-Private Dynamic Searchable Symmetric Encryption with Efficient Search","text":"Dynamic Searchable Symmetric Encryption (DSSE) allows to delegate keyword search and file update over an encrypted database via encrypted indexes, and therefore provides opportunities to mitigate the data privacy and utilization dilemma in cloud storage platforms. Despite its merits, recent works have shown that efficient DSSE schemes are vulnerable to statistical attacks due to the lack of forward-privacy, whereas forward-private DSSE schemes suffers from practicality concerns as a result of their extreme computation overhead. Due to significant practical impacts of statistical attacks, there is a critical need for new DSSE schemes that can achieve the forward-privacy in a more practical and efficient manner. We propose a new DSSE scheme that we refer to as Forward-private Sublinear DSSE (FS-DSSE). FS-DSSE harnesses special secure update strategies and a novel caching strategy to reduce the computation cost of repeated queries. Therefore, it achieves forward-privacy, sublinear search complexity, low end-to-end delay, and parallelization capability simultaneously. We fully implemented our proposed method and evaluated its performance on a real cloud platform. Our experimental evaluation results showed that the proposed scheme is highly secure and highly efficient compared with state-of-the-art DSSE techniques. Specifically, FS-DSSE is up to three magnitude of times faster than forward-secure DSSE counterparts, depending on the frequency of the searched keyword in the database."}
{"_id":"18d5fc8a3f2c7e9bac55fff40e0ecf3112196813","title":"Performance Analysis of Classification Algorithms on Medical Diagnoses-a Survey","text":"Corresponding Author: Vanaja, S., Research Scholar and Research guide, Research and Development, Bharathiar University, Coimbatore, Tamil Nadu, India Email: vanajasha@yahoo.com Abstract: The aim of this research paper is to study and discuss the various classification algorithms applied on different kinds of medical datasets and compares its performance. The classification algorithms with maximum accuracies on various kinds of medical datasets are taken for performance analysis. The result of the performance analysis shows the most frequently used algorithms on particular medical dataset and best classification algorithm to analyse the specific disease. This study gives the details of different classification algorithms and feature selection methodologies. The study also discusses about the data constraints such as volume and dimensionality problems. This research paper also discusses the new features of C5.0 classification algorithm over C4.5 and performance of classification algorithm on high dimensional datasets. This research paper summarizes various reviews and technical articles which focus on the current research on Medical diagnosis."}
{"_id":"132b3bd259bf10a41c00330a49de701c4e59a7ca","title":"Semantic MEDLINE: An advanced information management application for biomedicine","text":"Semantic MEDLINE integrates information retrieval, advanced natural language processing, automatic summarization, and visualization into a single Web portal. The application is intended to help manage the results of PubMed searches by condensing core semantic content in the citations retrieved. Output is presented as a connected graph of semantic relations, with links to the original MEDLINE citations. The ability to connect salient information across documents helps users keep up with the research literature and discover connections which might otherwise go unnoticed. Semantic MEDLINE can make an impact on biomedicine by supporting scientific discovery and the timely translation of insights from basic research into advances in clinical practice and patient care. Marcelo Fiszman has an M.D. from the State University of Rio de Janeiro and a Ph.D. in biomedical informatics from the University of Utah. He was awarded a postdoctoral fellowship in biomedical informatics at the National Library of Medicine (NLM) and is currently a research scientist there. His work focuses on natural language processing algorithms that exploit symbolic, rule-based techniques for semantic interpretation of biomedical text. He is also interested in using extracted semantic information for automatic abstraction summarization and literaturebased discovery. These efforts underpin Semantic MEDLINE, which is currently under development at NLM. This innovative biomedical information management application combines document retrieval, semantic interpretation, automatic summarization, and knowledge visualization into a single application."}
{"_id":"49b6601bd93f4cfb606c6c9d6be2ae7d4da7e5ac","title":"Effects of Professional Development on Teachers ' Instruction : Results from a Three-Year Longitudinal Study","text":"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. American Educational Research Association is collaborating with JSTOR to digitize, preserve and extend access to Educational Evaluation and Policy Analysis. This article examines the effects of professional development on teachers' instruction. Using a purposefully selected sample of about 207 teachers in 30 schools, in 10 districts infive states, we examine features of teachers' professional development and its effects on changing teaching practice in mathematics and science from 1996-1999. We found that professional developmentfocused on specific instructional practices increases teachers' use of those practices in the classroom. Furthermore, we found that specificfeatures, such as active learning opportunities, increase the effect of the professional development on teacher's instruction. What are the characteristics of professional development that affect teaching practice? This study adds to the knowledge base on effective professional development. The success of standards-based reform depends on teachers' ability to foster both basic knowledge and advanced thinking and problem solving among their students (Loucks-Desimone et al. Education 1999a), and surveys of teachers about their preservice preparation and in-service professional development experiences (e.g., Carey & Frechtling, 1997). In addition, there is a considerable amount of literature describing \"best practices\" in professional development, drawing on expert experiences (e.g., Loucks-Horsley et al., 1998). A professional consensus is emerging about particular characteristics of \"high quality\" professional development. These characteristics include a focus on content and how students learn content; in-depth, active learning opportunities; links to high standards, opportunities for teachers to engage in leadership roles; extended duration ; and the collective participation of groups of teachers from the same school, grade, or department. Although lists of characteristics such as these commonly appear in the literature on effective professional development, there is little direct evidence on the extent to which these characteristics are related to better teaching and increased student achievement. Some studies conducted over the past decade suggest that professional development experiences that share all or most of these characteristics can have a substantial, positive influence on teachers' classroom practice and student achieve-A few recent studies have begun to examine the relative importance of specific characteristics of professional development. Several studies have found that the intensity and \u2026"}
{"_id":"853ba021b20e566a632a3c6e047b06c8914ec37d","title":"\"It's alive, it's magic, it's in love with you\": opportunities, challenges and open questions for actuated interfaces","text":"Actuated Interfaces are receiving a great deal of interest from the research community. The field can now present a range of point designs, illustrating the potential design space of Actuated Interfaces. However, despite the increasing interest in Actuated Interfaces, the research carried out is nevertheless primarily preoccupied with the technical challenges and potential application areas, rather than how users actually approach, experience, interpret and understand Actuated Interfaces. Based on three case studies, investigating how people experience Actuated Interfaces, we point to; magic, movement and ambiguity as fruitful perspectives for understanding users' experiences with Actuated Interfaces. The three perspectives are employed to reflect upon opportunities and challenges, as well as point to open questions and relevant areas for future research for Actuated Interfaces."}
{"_id":"8c5ca158d90b3b034db872e5a82986af0146abf3","title":"Automatic Airspace Sectorisation: A Survey","text":"Airspace sectorisation provides a partition of a given airspace into sectors, subject to geometric constraints and workload constraints, so that some cost metric is minimised. We survey the algorithmic aspects of methods for automatic airspace sectorisation, for an intended readership of experts on air traffic management."}
{"_id":"cc54251f84c8577ca862fec41a1766c9a0d4a7b8","title":"Updating P300: An integrative theory of P3a and P3b","text":"The empirical and theoretical development of the P300 event-related brain potential (ERP) is reviewed by considering factors that contribute to its amplitude, latency, and general characteristics. The neuropsychological origins of the P3a and P3b subcomponents are detailed, and how target\/standard discrimination difficulty modulates scalp topography is discussed. The neural loci of P3a and P3b generation are outlined, and a cognitive model is proffered: P3a originates from stimulus-driven frontal attention mechanisms during task processing, whereas P3b originates from temporal-parietal activity associated with attention and appears related to subsequent memory processing. Neurotransmitter actions associating P3a to frontal\/dopaminergic and P3b to parietal\/norepinephrine pathways are highlighted. Neuroinhibition is suggested as an overarching theoretical mechanism for P300, which is elicited when stimulus detection engages memory operations."}
{"_id":"160404fb0d05a1a2efa593c448fcb8796c24b873","title":"The emulation theory of representation: motor control, imagery, and perception.","text":"The emulation theory of representation is developed and explored as a framework that can revealingly synthesize a wide variety of representational functions of the brain. The framework is based on constructs from control theory (forward models) and signal processing (Kalman filters). The idea is that in addition to simply engaging with the body and environment, the brain constructs neural circuits that act as models of the body and environment. During overt sensorimotor engagement, these models are driven by efference copies in parallel with the body and environment, in order to provide expectations of the sensory feedback, and to enhance and process sensory information. These models can also be run off-line in order to produce imagery, estimate outcomes of different actions, and evaluate and develop motor plans. The framework is initially developed within the context of motor control, where it has been shown that inner models running in parallel with the body can reduce the effects of feedback delay problems. The same mechanisms can account for motor imagery as the off-line driving of the emulator via efference copies. The framework is extended to account for visual imagery as the off-line driving of an emulator of the motor-visual loop. I also show how such systems can provide for amodal spatial imagery. Perception, including visual perception, results from such models being used to form expectations of, and to interpret, sensory input. I close by briefly outlining other cognitive functions that might also be synthesized within this framework, including reasoning, theory of mind phenomena, and language."}
{"_id":"4ab868acf51fd6d78ba2d15357de673f8ec0bad1","title":"ICTs for Improving Patients Rehabilitation Research Techniques","text":"The world population is rapidly aging and becoming a burden to health systems around the world. In this work we present a conceptual framework to encourage the research community to develop more comprehensive and adaptive ICT solutions for prevention and rehabilitation of chronic conditions in the daily life of the aging population and beyond health facilities. We first present an overview of current international standards in human functioning and disability, and how chronic conditions are interconnected in older age. We then describe innovative mobile and sensor technologies, predictive data analysis in healthcare, and game-based prevention and rehabilitation techniques. We then set forth a multidisciplinary approach for the personalized prevention and rehabilitation of chronic conditions using unobtrusive and pervasive sensors, interactive activities, and predictive analytics, which also eases the tasks of health-related researchers, caregivers and providers. Our proposal represents a conceptual basis for future research, in which much remains to be done in terms of standardization of technologies and health terminology, as well as data protection and privacy legislation."}
{"_id":"56e362c661d575b908e8a9f9bbb48f535a9312a5","title":"On Managing Very Large Sensor-Network Data Using Bigtable","text":"Recent advances and innovations in smart sensor technologies, energy storage, data communications, and distributed computing paradigms are enabling technological breakthroughs in very large sensor networks. There is an emerging surge of next-generation sensor-rich computers in consumer mobile devices as well as tailor-made field platforms wirelessly connected to the Internet. Billions of such sensor computers are posing both challenges and opportunities in relation to scalable and reliable management of the peta- and exa-scale time series being generated over time. This paper presents a Cloud-computing approach to this issue based on the two well-known data storage and processing paradigms: Bigtable and MapReduce."}
{"_id":"70bb8edcac8802816fbfe90e7c1643d67419dd34","title":"Conspicuous Consumption and Household Indebtedness","text":"Using a novel, large dataset of consumer transactions in Singapore, we study how conspicuous consumption affects household indebtedness. The coexistence of private housing (condominiums) and subsidized public housing (HDB) allows us to identify conspicuous consumers. Conditional on the same income and other socioeconomic characteristics, those who choose to reside in condominiums\u2014considered a status good\u2014are likely to be more conspicuous than their counterparts living in HDB units. We consistently find that condominium residents spend considerably more (by up to 44%) on conspicuous goods but not differently on inconspicuous goods. Compared with their matched HDB counterparts, more conspicuous consumers have 13% more credit card debt and 151% more delinquent credit card debt. Furthermore, the association between conspicuous consumption and credit card debt is concentrated among younger, male, single individuals. These results suggest that status-seeking-induced conspicuous consumption is an important determinant of household indebtedness."}
{"_id":"1c6ee895c202a91a808de59445e3dbde2f4cda0e","title":"Any domain parsing: automatic domain adaptation for natural language parsing","text":"of \u201cAny Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing\u201d by David McClosky , Ph.D., Brown University, May, 2010. Current efforts in syntactic parsing are largely data-driv en. These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures. Labeled data typically requires expert annotators which makes it both time consuming and costly to p roduce. Furthermore, once training data has been created for one textual domain, portability to similar domains is limited. This domain-dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain. The simplest approach to this task is to assume that the targe t domain is essentially the same as the source domain. No additional knowledge about the target domain is i cluded. A more realistic approach assumes that only raw text from the target domain is available. This a s umption lends itself well to semi-supervised learning methods since these utilize both labeled and unlab eled examples. This dissertation focuses on a family of semi-supervised me thods called self-training. Self-training creates semi-supervised learners from existing supervised learne rs with minimal effort. We first show results on self-training for constituency parsing within a single dom ain. While self-training has failed here in the past, we present a simple modification which allows it to succeed, p roducing state-of-the-art results for English constituency parsing. Next, we show how self-training is be neficial when parsing across domains and helps further when raw text is available from the target domain. On e of the remaining issues is that one must choose a training corpus appropriate for the target domain or perfo rmance may be severely impaired. Humans can do this in some situations, but this strategy becomes less prac tical as we approach larger data sets. We present a technique, Any Domain Parsing, which automatically detect s useful source domains and mixes them together to produce a customized parsing model. The resulting models perform almost as well as the best seen parsing models (oracle) for each target domain. As a result, we have a fully automatic syntactic constituency parser which can produce high-quality parses for all types of text, regardless of domain. Abstract of \u201cAny Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing\u201d by David McClosky , Ph.D., Brown University, May, 2010.of \u201cAny Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing\u201d by David McClosky , Ph.D., Brown University, May, 2010. Current efforts in syntactic parsing are largely data-driv en. These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures. Labeled data typically requires expert annotators which makes it both time consuming and costly to p roduce. Furthermore, once training data has been created for one textual domain, portability to similar domains is limited. This domain-dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain. The simplest approach to this task is to assume that the targe t domain is essentially the same as the source domain. No additional knowledge about the target domain is i cluded. A more realistic approach assumes that only raw text from the target domain is available. This a s umption lends itself well to semi-supervised learning methods since these utilize both labeled and unlab eled examples. This dissertation focuses on a family of semi-supervised me thods called self-training. Self-training creates semi-supervised learners from existing supervised learne rs with minimal effort. We first show results on self-training for constituency parsing within a single dom ain. While self-training has failed here in the past, we present a simple modification which allows it to succeed, p roducing state-of-the-art results for English constituency parsing. Next, we show how self-training is be neficial when parsing across domains and helps further when raw text is available from the target domain. On e of the remaining issues is that one must choose a training corpus appropriate for the target domain or perfo rmance may be severely impaired. Humans can do this in some situations, but this strategy becomes less prac tical as we approach larger data sets. We present a technique, Any Domain Parsing, which automatically detect s useful source domains and mixes them together to produce a customized parsing model. The resulting models perform almost as well as the best seen parsing models (oracle) for each target domain. As a result, we have a fully automatic syntactic constituency parser which can produce high-quality parses for all types of text, regardless of domain. Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing by David McClosky B. S., University of Rochester, 2004 Sc. M., Brown University, 2006 A dissertation submitted in partial fulfillment of the requirements for the Degree of Doctor of Philosophy in the Department of Computer Science at Brown University. Providence, Rhode Island May, 2010 c \u00a9 Copyright 2010 by David McClosky This dissertation by David McClosky is accepted in its prese nt form by the Department of Computer Science as satisfying the disser tation requirement for the degree of Doctor of Philosophy. Date Eugene Charniak, Director Recommended to the Graduate Council Date Mark Johnson, Reader Cognitive and Linguistic Sciences Date Dan Klein, Reader University of California at Berkeley Approved by the Graduate Council Date Sheila Bond Dean of the Graduate School"}
{"_id":"36927265f588ed093c2cbdbf7bf95ddd72f000a9","title":"Performance Evaluation of Bridgeless PFC Boost Rectifiers","text":"In this paper, a systematic review of bridgeless power factor correction (PFC) boost rectifiers, also called dual boost PFC rectifiers, is presented. Performance comparison between the conventional PFC boost rectifier and a representative member of the bridgeless PFC boost rectifier family is performed. Loss analysis and experimental efficiency evaluation for both CCM and DCM\/CCM boundary operations are provided."}
{"_id":"77c87f82a73edab2c46d600fc3d7821cdb15359a","title":"State-of-the-art, single-phase, active power-factor-correction techniques for high-power applications - an overview","text":"A review of high-performance, state-of-the-art, active power-factor-correction (PFC) techniques for high-power, single-phase applications is presented. The merits and limitations of several PFC techniques that are used in today's network-server and telecom power supplies to maximize their conversion efficiencies are discussed. These techniques include various zero-voltage-switching and zero-current-switching, active-snubber approaches employed to reduce reverse-recovery-related switching losses, as well as techniques for the minimization of the conduction losses. Finally, the effect of recent advancements in semiconductor technology, primarily silicon-carbide technology, on the performance and design considerations of PFC converters is discussed."}
{"_id":"864e1700594dfdf46a4981b5bc07a54ebeab11ba","title":"Bridgeless PFC implementation using one cycle control technique","text":"Conventional boost PFC suffers from the high conduction loss in the input rectifier-bridge. Higher efficiency can be achieved by using the bridgeless boost topology. This new circuit has issues such as voltage sensing, current sensing and EMI noise. In this paper, one cycle control technique is used to solve the issues of the voltage sensing and current sensing. Experimental results show efficiency improvement and EMI performance"}
{"_id":"c0b7e09f212ec85da22974c481e7b93efeba1504","title":"Common mode noise modeling and analysis of dual boost PFC circuit","text":"To achieve high efficiency PFC front stage in switching mode power supply (SMPS), dual boost PFC (DBPFC) topology shows superior characteristics compared with traditional boost PFC, but it by nature brings higher EMI noise, especially common mode (CM) noise. This paper deals with the modeling and analysis of DBPFC CM noise based on and compared with boost PFC, noise propagation equivalent circuits of both topologies are deduced, and theoretical analysis illustrates the difference. Experiments are performed to validate the EMI model and analysis."}
{"_id":"60ba158cb1a619726db31b684a7bf818e2f8256b","title":"Common mode EMI noise suppression in bridgeless boost PFC converter","text":"Bridgeless boost PFC converter has high efficiency by eliminating the input diode bridge. However, Common Mode (CM) conducted EMI becomes a great issue. The goal of this paper is to study the possibility to minimize the CM noise in this converter. First the noise model is studied. Then a balance concept is introduced and applied to cancel the CM noise. Two approaches to minimize CM noise are introduced and compared. Experiments verify the effectiveness of both approaches."}
{"_id":"6e81f26db102e3e261f4fd35251e2f1315709258","title":"A design methodology of chip-to-chip wireless power transmission system","text":"A design methodology to transmit power using a chip-to-chip wireless interface is proposed. The proposed power transmission system is based on magnetic coupling, and the power transmission of 5mW\/mm2 was verified. The transmission efficiency trade-off with the transmitted power is also discussed."}
{"_id":"4155199a98a1c618d2c73fe850f582483addd7ff","title":"Options for Control of Reactive Power by Distributed Photovoltaic Generators","text":"High-penetration levels of distributed photovoltaic (PV) generation on an electrical distribution circuit present several challenges and opportunities for distribution utilities. Rapidly varying irradiance conditions may cause voltage sags and swells that cannot be compensated by slowly responding utility equipment resulting in a degradation of power quality. Although not permitted under current standards for interconnection of distributed generation, fast-reacting, VAR-capable PV inverters may provide the necessary reactive power injection or consumption to maintain voltage regulation under difficult transient conditions. As side benefit, the control of reactive power injection at each PV inverter provides an opportunity and a new tool for distribution utilities to optimize the performance of distribution circuits, e.g., by minimizing thermal losses. We discuss and compare via simulation various design options for control systems to manage the reactive power generated by these inverters. An important design decision that weighs on the speed and quality of communication required is whether the control should be centralized or distributed (i.e., local). In general, we find that local control schemes are able to maintain voltage within acceptable bounds. We consider the benefits of choosing different local variables on which to control and how the control system can be continuously tuned between robust voltage control, suitable for daytime operation when circuit conditions can change rapidly, and loss minimization better suited for nighttime operation."}
{"_id":"1e6123e778caa1e7d77292ffc40920b78d769ce7","title":"Deep Convolutional Neural Network Design Patterns","text":"Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https:\/\/github.com\/iPhysicist\/CNNDesignPatterns). We hope others are inspired to build on our preliminary work."}
{"_id":"88c3904153d831f6d854067f2ad69c5a330f4af3","title":"A linear regression analysis of the effects of age related pupil dilation change in iris biometrics","text":"Medical studies have shown that average pupil size decreases linearly throughout adult life. Therefore, on average, the longer the time between acquisition of two images of the same iris, the larger the difference in dilation between the two images. Several studies have shown that increased difference in dilation causes an increase in the false nonmatch rate for iris recognition. Thus, increased difference in pupil dilation is one natural mechanism contributing to an iris template aging effect. We present an experimental analysis of the change in genuine match scores in the context of dilation differences due to aging."}
{"_id":"bbaee955dd1280cb50ee26040f0e8c939369cf78","title":"Controlling Robot Morphology From Incomplete Measurements","text":"Mobile robots with complex morphology are essential for traversing rough terrains in Urban Search & Rescue missions. Since teleoperation of the complex morphology causes high cognitive load of the operator, the morphology is controlled autonomously. The autonomous control measures the robot state and surrounding terrain which is usually only partially observable, and thus the data are often incomplete. We marginalize the control over the missing measurements and evaluate an explicit safety condition. If the safety condition is violated, tactile terrain exploration by the body-mounted robotic arm gathers the missing data."}
{"_id":"40213ebcc1e03c25ba97f4110c0b2030fd2e79b6","title":"Computational imaging with multi-camera time-of-flight systems","text":"Depth cameras are a ubiquitous technology used in a wide range of applications, including robotic and machine vision, human-computer interaction, autonomous vehicles as well as augmented and virtual reality. In this paper, we explore the design and applications of phased multi-camera time-of-flight (ToF) systems. We develop a reproducible hardware system that allows for the exposure times and waveforms of up to three cameras to be synchronized. Using this system, we analyze waveform interference between multiple light sources in ToF applications and propose simple solutions to this problem. Building on the concept of orthogonal frequency design, we demonstrate state-of-the-art results for instantaneous radial velocity capture via Doppler time-of-flight imaging and we explore new directions for optically probing global illumination, for example by de-scattering dynamic scenes and by non-line-of-sight motion detection via frequency gating."}
{"_id":"627c05039285cee961f9802c41db1a5eaa550b13","title":"Merging What's Cracked, Cracking What's Merged: Adaptive Indexing in Main-Memory Column-Stores","text":"Adaptive indexing is characterized by the partial creation and refinement of the index as side effects of query execution. Dynamic or shifting workloads may benefit from preliminary index structures focused on the columns and specific key ranges actually queried \u2014 without incurring the cost of full index construction. The costs and benefits of adaptive indexing techniques should therefore be compared in terms of initialization costs, the overhead imposed upon queries, and the rate at which the index converges to a state that is fully-refined for a particular workload component. Based on an examination of database cracking and adaptive merging, which are two techniques for adaptive indexing, we seek a hybrid technique that has a low initialization cost and also converges rapidly. We find the strengths and weaknesses of database cracking and adaptive merging complementary. One has a relatively high initialization cost but converges rapidly. The other has a low initialization cost but converges relatively slowly. We analyze the sources of their respective strengths and explore the space of hybrid techniques. We have designed and implemented a family of hybrid algorithms in the context of a column-store database system. Our experiments compare their behavior against database cracking and adaptive merging, as well as against both traditional full index lookup and scan of unordered data. We show that the new hybrids significantly improve over past methods while at least two of the hybrids come very close to the \u201cideal performance\u201d in terms of both overhead per query and convergence to a final state."}
{"_id":"bcc18022ba80d2e2a6d41c64fb6c8a9e3a898140","title":"Positioning for the Internet of Things: A 3GPP Perspective","text":"Many use cases in the Internet of Things (IoT) will require or benefit from location information, making positioning a vital dimension of the IoT. The 3GPP has dedicated a significant effort during its Release 14 to enhance positioning support for its IoT technologies to further improve the 3GPPbased IoT eco-system. In this article, we identify the design challenges of positioning support in LTE-M and NB-IoT, and overview the 3GPP's work in enhancing the positioning support for LTE-M and NB-IoT. We focus on OTDOA, which is a downlink based positioning method. We provide an overview of the OTDOA architecture and protocols, summarize the designs of OTDOA positioning reference signals, and present simulation results to illustrate the positioning performance."}
{"_id":"24fe8b910028522424f2e8dd5ecb9dc2bd3e9153","title":"Autonomous Takeoff and Flight of a Tethered Aircraft for Airborne Wind Energy","text":"A control design approach to achieve fully autonomous takeoff and flight maneuvers with a tethered aircraft is presented and demonstrated in real-world flight tests with a small-scale prototype. A ground station equipped with a controlled winch and a linear motion system accelerates the aircraft to takeoff speed and controls the tether reeling in order to limit the pulling force. This setup corresponds to airborne wind energy (AWE) systems with ground-based energy generation and rigid aircrafts. A simple model of the aircraft\u2019s dynamics is introduced and its parameters are identified from experimental data. A model-based, hierarchical feedback controller is then designed, whose aim is to manipulate the elevator, aileron, and propeller inputs in order to stabilize the aircraft during the takeoff and to achieve figure-of-eight flight patterns parallel to the ground. The controller operates in a fully decoupled mode with respect to the ground station. Parameter tuning and stability\/robustness aspect are discussed, too. The experimental results indicate that the controller is able to achieve satisfactory performance and robustness, notwithstanding its simplicity, and confirm that the considered takeoff approach is technically viable and solves the issue of launching this kind of AWE systems in a compact space and at low additional cost."}
{"_id":"7d6968a25c81e4bc0fb958173a61ea82362cbd03","title":"STOCK MARKET PREDICTION USING BIO-INSPIRED COMPUTING : A SURVEY","text":"Bio-inspired evolutionary algorithms are probabilistic search methods that mimic natural biological evolution. They show the behavior of the biological entities interacting locally with one another or with their environment to solve complex problems. This paper aims to analyze the most predominantly used bio-inspired optimization techniques that have been used for stock market prediction and hence present a comparative study between them."}
{"_id":"717bba8eec2457f1af024a008929fbe4c7ad0612","title":"Security slicing for auditing XML, XPath, and SQL injection vulnerabilities","text":"XML, XPath, and SQL injection vulnerabilities are among the most common and serious security issues for Web applications and Web services. Thus, it is important for security auditors to ensure that the implemented code is, to the extent possible, free from these vulnerabilities before deployment. Although existing taint analysis approaches could automatically detect potential vulnerabilities in source code, they tend to generate many false warnings. Furthermore, the produced traces, i.e. dataflow paths from input sources to security-sensitive operations, tend to be incomplete or to contain a great deal of irrelevant information. Therefore, it is difficult to identify real vulnerabilities and determine their causes. One suitable approach to support security auditing is to compute a program slice for each security-sensitive operation, since it would contain all the information required for performing security audits (Soundness). A limitation, however, is that such slices may also contain information that is irrelevant to security (Precision), thus raising scalability issues for security audits. In this paper, we propose an approach to assist security auditors by defining and experimenting with pruning techniques to reduce original program slices to what we refer to as security slices, which contain sound and precise information. To evaluate the proposed pruning mechanism by using a number of open source benchmarks, we compared our security slices with the slices generated by a state-of-the-art program slicing tool. On average, our security slices are 80% smaller than the original slices, thus suggesting significant reduction in auditing costs."}
{"_id":"bf277908733127ada3acf7028947a5eb0e9be38b","title":"A fully integrated multi-CPU, GPU and memory controller 32nm processor","text":"This paper describes the 32nm Sandy Bridge processor that integrates up to 4 high performance Intel Architecture (IA) cores, a power\/performance optimized graphic processing unit (GPU) and memory and PCIe controllers in the same die. The Sandy Bridge architecture block diagram is shown in Fig. 15.1.1 and the floorplan of a four IA-core version is shown in Fig. 15.1.2. The Sandy Bridge IA core implements an improved branch prediction algorithm, a micro-operation (Uop) cache, a floating point Advanced Vector Extension (AVX), a second load port in the L1 cache and bigger register files in the out-of-order part of the machine; all these architecture improvements boost the IA core performance without increasing the thermal power dissipation envelope or the average power consumption (to preserve battery life in mobile systems). The CPUs and GPU share the same 8MB level-3 cache memory. The data flow is optimized by a high performance on die interconnect fabric (called \u201cring\u201d) that connects between the CPUs, the GPU, the L3 cache and the system agent (SA) unit that houses a 1600MT\/s, dual channel DDR3 memory controller, a 20-lane PCIe gen2 controller, a two parallel pipe display engine, the power management control unit and the testability logic. An on die EPROM is used for configurability and yield optimization."}
{"_id":"daac2e9298d513de49d9e6d01c6ec78daf8feb47","title":"Position-based impedance control of an industrial hydraulic manipulator","text":"This paper addresses the problem of impedance control in hydraulic robots. Whereas most impedance and hybrid force\/position control formulations have focussed on electrically-driven robots with controllable actuator torques, torque control of hydraulic actuators is a difficult task. Motivated by the previous works [2,9, lo], a position-based impedance controller is proposed and demonstrated on an existing industrial hydraulic robot (a Unimate MKII-2000). A nonlinear proportional-integral (NPI) controller is first developed to meet the accurate positioning requirements of this impedance control formulation. The NPI controller is shown to make the manipulator match a range of second-order target impedances. Various experiments in free space and in environmental contact, including a simple impedance modulation experiment, demonstrate the feasibility and the promise of the technique."}
{"_id":"b57a4f80f2b216105c6c283e18236c2474927590","title":"Clustered Collaborative Filtering Approach for Distributed Data Mining on Electronic Health Records","text":"Distributed Data Mining (DDM) has become one of the promising areas of Data Mining. DDM techniques include classifier approach and agent-approach. Classifier approach plays a vital role in mining distributed data, having homogeneous and heterogeneous approaches depend on data sites. Homogeneous classifier approach involves ensemble learning, distributed association rule mining, meta-learning and knowledge probing. Heterogeneous classifier approach involves collective principle component analysis, distributed clustering, collective decision tree and collective bayesian learning model. In this paper, classifier approach for DDM is summarized and an architectural model based on clustered-collaborative filtering for Electronic Health Records (EHR) is proposed."}
{"_id":"9f2923984ca3f0bbcca4415f47bee061d848120e","title":"Regulating the new information intermediaries as gatekeepers of information diversity","text":"Natali Helberger is Professor at the Institute for Information Law, University of Amsterdam, Amsterdam, The Netherlands. Katharina Kleinen-von K\u00f6nigsl\u00f6w is Assistant Professor for Political Communication at the University of Zurich, Zurich, Switzerland. Rob van der Noll is Senior Researcher at SEO Economic Research, Amsterdam, The Netherlands. Abstract Purpose \u2013 The purposes of this paper are to deal with the questions: because search engines, social networks and app-stores are often referred to as gatekeepers to diverse information access, what is the evidence to substantiate these gatekeeper concerns, and to what extent are existing regulatory solutions to control gatekeeper control suitable at all to address new diversity concerns? It will also map the different gatekeeper concerns about media diversity as evidenced in existing research before the background of network gatekeeping theory critically analyses some of the currently discussed regulatory approaches and develops the contours of a more user-centric approach towards approaching gatekeeper control and media diversity."}
{"_id":"8a3afde910fc3ebd95fdb51a157883b81bfc7e73","title":"A hierarchical lstm model with multiple features for sentiment analysis of sina weibo texts","text":"Sentiment analysis has long been a hot topic in natural language processing. With the development of social network, sentiment analysis on social media such as Facebook, Twitter and Weibo becomes a new trend in recent years. Many different methods have been proposed for sentiment analysis, including traditional methods (SVM and NB) and deep learning methods (RNN and CNN). In addition, the latter always outperform the former. However, most of existing methods only focus on local text information and ignore the user personality and content characteristics. In this paper, we propose an improved LSTM model with considering the user-based features and content-based features. We first analysis the training dataset to extract artificial features which consists of user-based and content-based. Then we construct a hierarchical LSTM model, named LSTM-MF (a hierarchical LSTM model with multiple features), and introduce the features into the model to generate sentence and document representations. The experimental results show that our model achieves significant and consistent improvements compared to all state-of-the-art methods."}
{"_id":"933cba585a12e56a8f60511ebeb74b8cb42634b1","title":"A Distribution-Based Clustering Algorithm for Mining in Large Spatial Databases","text":"The problem of detecting clusters of points belonging to a spatial point process arises in many applications. In this paper , we introduce the new clustering algorithm DBCLASD (Distribution Based Clustering of LArge Spatial Databases) to discover clusters of this type. The results of experiments demonstrate that DBCLASD, contrary to partitioning algorithms such as CLARANS, discovers clusters of arbitrary shape. Furthermore, DBCLASD does not require any input parameters, in contrast to the clustering algorithm DBSCAN requiring two input parameters which may be difficult to provide for large databases. In terms of efficiency, DBCLASD is between CLARANS and DBSCAN, close to DBSCAN. Thus, the efficiency of DBCLASD on large spatial databases is very attractive when considering its nonparametric nature and its good quality for clusters of arbitrary shape."}
{"_id":"c66dc716dc0ab674386255e64a743c291c3a1ab5","title":"Impact of Depression and Antidepressant Treatment on Heart Rate Variability: A Review and Meta-Analysis","text":"BACKGROUND\nDepression is associated with an increase in the likelihood of cardiac events; however, studies investigating the relationship between depression and heart rate variability (HRV) have generally focused on patients with cardiovascular disease (CVD). The objective of the current report is to examine with meta-analysis the impact of depression and antidepressant treatment on HRV in depressed patients without CVD.\n\n\nMETHODS\nStudies comparing 1) HRV in patients with major depressive disorder and healthy control subjects and 2) the HRV of patients with major depressive disorder before and after treatment were considered for meta-analysis.\n\n\nRESULTS\nMeta-analyses were based on 18 articles that met inclusion criteria, comprising a total of 673 depressed participants and 407 healthy comparison participants. Participants with depression had lower HRV (time frequency: Hedges' g = -.301, p < .001; high frequency: Hedges' g = -.293, p < .001; nonlinear: Hedges' g = -1.955, p = .05; Valsalva ratio: Hedges' g = -.712, p < .001) than healthy control subjects, and depression severity was negatively correlated with HRV (r = -.354, p < .001). Tricyclic medication decreased HRV, although serotonin reuptake inhibitors, mirtazapine, and nefazodone had no significant impact on HRV despite patient response to treatment.\n\n\nCONCLUSIONS\nDepression without CVD is associated with reduced HRV, which decreases with increasing depression severity, most apparent with nonlinear measures of HRV. Critically, a variety of antidepressant treatments do not resolve these decreases despite resolution of symptoms, highlighting that antidepressant medications might not have HRV-mediated cardioprotective effects and the need to identify individuals at risk among patients in remission."}
{"_id":"2cae7a02082722145a6977469f74a0eb5bd10585","title":"Temporal Sequence Learning, Prediction, and Control: A Review of Different Models and Their Relation to Biological Mechanisms","text":"In this review, we compare methods for temporal sequence learning (TSL) across the disciplines machine-control, classical conditioning, neuronal models for TSL as well as spike-timing-dependent plasticity (STDP). This review introduces the most influential models and focuses on two questions: To what degree are reward-based (e.g., TD learning) and correlation-based (Hebbian) learning related? and How do the different models correspond to possibly underlying biological mechanisms of synaptic plasticity? We first compare the different models in an open-loop condition, where behavioral feedback does not alter the learning. Here we observe that reward-based and correlation-based learning are indeed very similar. Machine control is then used to introduce the problem of closed-loop control (e.g., actor-critic architectures). Here the problem of evaluative (rewards) versus nonevaluative (correlations) feedback from the environment will be discussed, showing that both learning approaches are fundamentally different in the closed-loop condition. In trying to answer the second question, we compare neuronal versions of the different learning architectures to the anatomy of the involved brain structures (basal-ganglia, thalamus, and cortex) and the molecular biophysics of glutamatergic and dopaminergic synapses. Finally, we discuss the different algorithms used to model STDP and compare them to reward-based learning rules. Certain similarities are found in spite of the strongly different timescales. Here we focus on the biophysics of the different calcium-release mechanisms known to be involved in STDP."}
{"_id":"b83a6c77e61a38ada308992cc579c8cd49ee16f4","title":"A Survey of Outlier Detection Methods in Network Anomaly Identification","text":"The detection of outliers has gained considerable interest in data mining with the realization that outliers can be the key discovery to be made from very large databases. Outliers arise due to various reasons such as mechanical faults, changes in system behavior, fraudulent behavior, human error and instrument error. Indeed, for many applications the discovery of outliers leads to more interesting and useful results than the discovery of inliers. Detection of outliers can lead to identification of system faults so that administrators can take preventive measures before they escalate. It is possible that anomaly detection may enable detection of new attacks. Outlier detection is an important anomaly detection approach. In this paper, we present a comprehensive survey of well known distance-based, density-based and other techniques for outlier detection and compare them. We provide definitions of outliers and discuss their detection based on supervised and unsupervised learning in the context of network anomaly detection."}
{"_id":"f0bb2eac780d33a2acc129a17e502a3aca28d3a3","title":"Managing Risk in Software Process Improvement: An Action Research Approach","text":"Many software organizations engage in software process improvement (SPI) initiatives to increase their capability to develop quality solutions at a competitive level. Such efforts, however, are complex and very demanding. A variety of risks makes it difficult to develop and implement new processes. We studied SPI in its organizational context through collaborative practice research (CPR), a particular form of action research. The CPR program involved close collaboration between practitioners and researchers over a three-year period to understand and improve SPI initiatives in four Danish software organizations. The problem of understanding and managing risks in SPI teams emerged in one of the participating organizations and led to this research. We draw upon insights from the literature on SPI and software risk management as well as practical lessons learned from managing SPI risks in the participating software organizations. Our research offers two contributions. First, we contribute to knowledge on SPI by proposing an approach to understand and manage risks in SPI teams. This risk management approach consists of a framework for understanding risk areas and risk resolution strategies within SPI and a related Iversen et al.\/Managing Risk in SPI 396 MIS Quarterly Vol. 28 No. 3\/September 2004 process for managing SPI risks. Second, we contribute to knowledge on risk management within the information systems and software engineering disciplines. We propose an approach to tailor risk management to specific contexts. This approach consists of a framework for understanding and choosing between different forms of risk management and a process to tailor risk management to specific contexts."}
{"_id":"2b1d83d6d7348700896088b34154eb9e0b021962","title":"CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORKS","text":"We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods."}
{"_id":"4edae1c443cd9bede2af016c23e13d6e664bfe7e","title":"Ensemble methods for spoken emotion recognition in call-centres","text":"Machine-based emotional intelligence is a requirement for more natural interaction between humans and computer interfaces and a basic level of accurate emotion perception is needed for computer systems to respond adequately to human emotion. Humans convey emotional information both intentionally and unintentionally via speech patterns. These vocal patterns are perceived and understood by listeners during conversation. This research aims to improve the automatic perception of vocal emotion in two ways. First, we compare two emotional speech data sources: natural, spontaneous emotional speech and acted or portrayed emotional speech. This comparison demonstrates the advantages and disadvantages of both acquisition methods and how these methods affect the end application of vocal emotion recognition. Second, we look at two classification methods which have not been applied in this field: stacked generalisation and unweighted vote. We show how these techniques can yield an improvement over traditional classification methods. 2006 Elsevier B.V. All rights reserved."}
{"_id":"32c79c5a66e97469465875a31685864e0df8faee","title":"Laser ranging : a critical review of usual techniques for distance measurement","text":"Marc Rioux National Research Council, Canada Institute for Information Technology Visual Information Technology Ottawa, Canada, K1A 0R6 Abstract. We review some usual laser range finding techniques for industrial applications. After outlining the basic principles of triangulation and time of flight [pulsed, phase-shift and frequency modulated continuous wave (FMCW)], we discuss their respective fundamental limitations. Selected examples of traditional and new applications are also briefly presented. \u00a9 2001 Society of Photo-Optical Instrumentation Engineers. [DOI: 10.1117\/1.1330700]"}
{"_id":"2913553d5e1ff6447b555e458fe1e0de78c0d45a","title":"Liquid cooled permanent-magnet traction motor design considering temporary overloading","text":"This paper focuses on traction motor design in a variable speed drive of an Electric Mini Bus, aiming to deliver high mean mechanical power over the whole speed range, using single gear transmission. Specific design considerations aim to minimize oversizing, by utilizing temporary overloading capability using liquid cooled motor housing. Constant toque - constant power control strategy is adopted by appropriately programming the over-torque and field weakening operation areas of the motor. A good quantitative analysis between over-torque, rated and field weakening operation areas, is given, focusing on efficiency and paying particular attention to iron losses both on stator and rotor. The motor has been designed in order to meet traction application specifications and allow operation in higher specific electric loading to increase power density."}
{"_id":"ba459a7bd17d8af7642dd0c0ddf9e897dff3c4a8","title":"Do citations and readership identify seminal publications?","text":"This work presents a new approach for analysing the ability of existing research metrics to identify research which has strongly influenced future developments. More specifically, we focus on the ability of citation counts and Mendeley reader counts to distinguish between publications regarded as seminal and publications regarded as literature reviews by field experts. The main motivation behind our research is to gain a better understanding of whether and how well the existing research metrics relate to research quality. For this experiment we have created a new dataset which we call TrueImpactDataset and which contains two types of publications, seminal papers and literature reviews. Using the dataset, we conduct a set of experiments to study how citation and reader counts perform in distinguishing these publication types, following the intuition that causing a change in a field signifies research quality. Our research shows that citation counts work better than a random baseline (by a margin of 10%) in distinguishing important seminal research papers from literature reviews while Mendeley reader counts do not work better than the baseline."}
{"_id":"4954bb26107d69eb79bb32ffa247c8731cf20fcf","title":"Improving privacy and security in multi-authority attribute-based encryption","text":"Attribute based encryption (ABE) [13] determines decryption ability based on a user's attributes. In a multi-authority ABE scheme, multiple attribute-authorities monitor different sets of attributes and issue corresponding decryption keys to users, and encryptors can require that a user obtain keys for appropriate attributes from each authority before decrypting a message. Chase [5] gave a multi-authority ABE scheme using the concepts of a trusted central authority (CA) and global identifiers (GID). However, the CA in that construction has the power to decrypt every ciphertext, which seems somehow contradictory to the original goal of distributing control over many potentially untrusted authorities. Moreover, in that construction, the use of a consistent GID allowed the authorities to combine their information to build a full profile with all of a user's attributes, which unnecessarily compromises the privacy of the user. In this paper, we propose a solution which removes the trusted central authority, and protects the users' privacy by preventing the authorities from pooling their information on particular users, thus making ABE more usable in practice."}
{"_id":"0825788b9b5a18e3dfea5b0af123b5e939a4f564","title":"Glove: Global Vectors for Word Representation","text":"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."}
{"_id":"10cfa5bfab3da9c8026d3a358695ea2a5eba0f33","title":"Parallel Tracking and Mapping for Small AR Workspaces","text":"This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems."}
{"_id":"272216c1f097706721096669d85b2843c23fa77d","title":"Adam: A Method for Stochastic Optimization","text":"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and\/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and\/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}
{"_id":"5f8e64cc066886a99cc8e30e68d1b29d8bb1961d","title":"The Human Hippocampus and Spatial and Episodic Memory","text":"Finding one's way around an environment and remembering the events that occur within it are crucial cognitive abilities that have been linked to the hippocampus and medial temporal lobes. Our review of neuropsychological, behavioral, and neuroimaging studies of human hippocampal involvement in spatial memory concentrates on three important concepts in this field: spatial frameworks, dimensionality, and orientation and self-motion. We also compare variation in hippocampal structure and function across and within species. We discuss how its spatial role relates to its accepted role in episodic memory. Five related studies use virtual reality to examine these two types of memory in ecologically valid situations. While processing of spatial scenes involves the parahippocampus, the right hippocampus appears particularly involved in memory for locations within an environment, with the left hippocampus more involved in context-dependent episodic or autobiographical memory."}
{"_id":"46f91a6cf5047498c0bf4c75852ecdb1a72fadd7","title":"A-PNR: Automatic Plate Number Recognition","text":"Automatic Plate Number Recognition (APNR) has important applications in traffic surveillance, toll booth, protected parking lot, No parking zone, etc. It is a challenging problem, especially when the license plates have varying sizes, the number of lines, fonts, background diversity etc. This work aims to address APNR using deep learning method for real-time traffic images. We first extract license plate candidates from each frame using edge information and geometrical properties, ensuring high recall using one class SVM. The verified candidates are used for NP recognition purpose along with a spatial transformer network (STN) for character recognition. Our system is evaluated on several traffic images with vehicles having different license plate formats in terms of tilt, distances, colors, illumination, character size, thickness etc. Also, the background was very challenging. Results demonstrate robustness to such variations and impressive performance in both the localization and recognition."}
{"_id":"05357314fe2da7c2248b03d89b7ab9e358cbf01e","title":"Learning with kernels","text":"All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher."}
{"_id":"66078dcc6053a2b314942f738048ee0359726cb5","title":"COMPUTATION OF CONDITIONAL PROBABILITY STATISTICS BY 8-MONTH-OLD INFANTS","text":"321 Abstract\u2014 A recent report demonstrated that 8-month-olds can segment a continuous stream of speech syllables, containing no acoustic or prosodic cues to word boundaries, into wordlike units after only 2 min of listening experience (Saffran, Aslin, & Newport, 1996). Thus, a powerful learning mechanism capable of extracting statistical information from fluent speech is available early in development. The present study extends these results by documenting the particular type of statistical computation\u2014transitional (conditional) probability\u2014used by infants to solve this word-segmentation task. An artificial language corpus, consisting of a continuous stream of trisyllabic nonsense words, was presented to 8-month-olds for 3 min. A postfamiliarization test compared the infants' responses to words versus part-words (tri-syllabic sequences spanning word boundaries). The corpus was constructed so that test words and part-words were matched in frequency, but differed in their transitional probabilities. Infants showed reliable discrimination of words from part-words, thereby demonstrating rapid segmentation of continuous speech into words on the basis of transitional probabilities of syllable pairs. Many aspects of the patterns of human languages are signaled in the speech stream by what is called distributional evidence, that is, regularities in the relative positions and order of elements over a corpus of utterances (Bloomfield, 1933; Maratsos & Chalkley, 1980). This type of evidence, along with linguistic theories about the characteristics of human languages, is what comparative linguists use to discover the structure of exotic languages (Harris, 1951). Similarly, this type of evidence , along with tendencies to perform certain kinds of analyses on language input (Chomsky, 1957), could be used by human language learners to acquire their native languages. However, using such evidence would require rather complex distributional and statistical computations , and surprisingly little is known about the abilities of human infants and young children to perform these computations. By using the term computation, we do not mean, of course, that infants are consciously performing a mathematical calculation, but rather that they might be sensitive to and able to store quantitative aspects of distribu-tional information about a language corpus. Recently, we have begun studying this problem by investigating the abilities of human learners to use statistical information to dis-Words are known to vary dramatically from one language to another, so finding the words of a language is clearly a problem that must involve learning from the linguistic environment. Moreover, the beginnings and ends of the sequences of sounds that form words in a \u2026"}
{"_id":"db52d3520f9ac17d20bd6195e03f4a650c923fba","title":"A New Modular Bipolar High-Voltage Pulse Generator","text":"Adapting power-electronic converters that are used in pulsed-power application attracted considerable attention during recent years. In this paper, a modular bipolar high-voltage pulse generator is proposed based on the voltage multipliers and half-bridge converters concept by using power electronics switches. This circuit is capable to generate repetitive high voltage bipolar pulses with a flexible output pattern, by using low voltage power supply and elements. The proposed topology was simulated in MATLAB\/Simulink. To verify the circuit operation a four-stage laboratory prototype has been assembled and tested. The results confirm the theoretical analysis and show the validity of the converter scheme."}
{"_id":"b3870b319e32b0a2f687aa873d7935f0043f6aa5","title":"An analysis and critique of Research through Design: towards a formalization of a research approach","text":"The field of HCI is experiencing a growing interest in Research through Design (RtD), a research approach that employs methods and processes from design practice as a legitimate method of inquiry. We are interested in expanding and formalizing this research approach, and understanding how knowledge, or theory, is generated from this type of design research. We conducted interviews with 12 leading HCI design researchers, asking them about design research, design theory, and RtD specifically. They were easily able to identify different types of design research and design theory from contemporary and historical design research efforts, and believed that RtD might be one of the most important contributions of design researchers to the larger research community. We further examined three historical RtD projects that were repeatedly mentioned in the interviews, and performed a critique of current RtD practices within the HCI research and interaction design communities. While our critique summarizes the problems, it also shows possible directions for further developments and refinements of the approach."}
{"_id":"067c7857753e21e7317b556c86e30be60aa7cac0","title":"Xen and the art of virtualization","text":"Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests."}
{"_id":"0c668ee24d58ecca165f788d40765e79ed615471","title":"Classification and Regression Trees","text":""}
{"_id":"0b8f4edf1a7b4d19d47d419f41cde432b9708ab7","title":"Silicon-Filled Rectangular Waveguides and Frequency Scanning Antennas for mm-Wave Integrated Systems","text":"We present a technology for the manufacturing of silicon-filled integrated waveguides enabling the realization of low-loss high-performance millimeter-wave passive components and high gain array antennas, thus facilitating the realization of highly integrated millimeter-wave systems. The proposed technology employs deep reactive-ion-etching (DRIE) techniques with aluminum metallization steps to integrate rectangular waveguides with high geometrical accuracy and continuous metallic side walls. Measurement results of integrated rectangular waveguides are reported exhibiting losses of 0.15 dB\/ \u03bbg at 105 GHz. Moreover, ultra-wideband coplanar to waveguide transitions with 0.6 dB insertion loss at 105 GHz and return loss better than 15 dB from 80 to 110 GHz are described and characterized. The design, integration and measured performance of a frequency scanning slotted-waveguide array antenna is reported, achieving a measured beam steering capability of 82 \u00b0 within a band of 23 GHz and a half-power beam-width (HPBW) of 8.5 \u00b0 at 96 GHz. Finally, to showcase the capability of this technology to facilitate low-cost mm-wave system level integration, a frequency modulated continuous wave (FMCW) transmit-receive IC for imaging radar applications is flip-chip mounted directly on the integrated array and experimentally characterized."}
{"_id":"ec1df457a2be681227f79de3ce932fccb65ee2bb","title":"Opportunistic Computation Offloading in Mobile Edge Cloud Computing Environments","text":"The dynamic mobility and limitations in computational power, battery resources, and memory availability are main bottlenecks in fully harnessing mobile devices as data mining platforms. Therefore, the mobile devices are augmented with cloud resources in mobile edge cloud computing (MECC) environments to seamlessly execute data mining tasks. The MECC infrastructures provide compute, network, and storage services in one-hop wireless distance from mobile devices to minimize the latency in communication as well as provide localized computations to reduce the burden on federated cloud systems. However, when and how to offload the computation is a hard problem. In this paper, we present an opportunistic computation offloading scheme to efficiently execute data mining tasks in MECC environments. The scheme provides the suitable execution mode after analyzing the amount of unprocessed data, privacy configurations, contextual information, and available on-board local resources (memory, CPU, and battery power). We develop a mobile application for online activity recognition and evaluate the proposed scheme using the event data stream of 5 million activities collected from 12 users for 15 days. The experiments show significant improvement in execution time and battery power consumption resulting in 98% data reduction."}
{"_id":"31864e13a9b3473ebb07b4f991f0ae3363517244","title":"A Computational Approach to Edge Detection","text":"This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge."}
{"_id":"e526fd18329ad0da5dd989358fd20e4dd2a2a3e1","title":"A framework for designing cloud forensic-enabled services (CFeS)","text":"Cloud computing is used by consumers to access cloud services. Malicious actors exploit vulnerabilities of cloud services to attack consumers. The link between these two assumptions is the cloud service. Although cloud forensics assists in the direction of investigating and solving cloud-based cyber-crimes, in many cases the design and implementation of cloud services fall back. Software designers and engineers should focus their attention on the design and implementation of cloud services that can be investigated in a forensic sound manner. This paper presents a methodology that aims on assisting designers to design cloud forensic-enabled services. The methodology supports the design of cloud services by implementing a number of steps to make the services cloud forensic enabled. It consists of a set of cloud forensic constraints, a modeling language expressed through a conceptual model and a process based on the concepts identified and presented in the model. The main advantage of the proposed methodology is the correlation of cloud services\u2019 characteristics with the cloud investigation while providing software engineers the ability to design and implement cloud forensic-enabled services via the use of a set of predefined forensic-related tasks."}
{"_id":"9cbef9d9cdbe4007444bd3a6e83551ae0865b648","title":"Dynamic Conditional Random Fields for Joint Sentence Boundary and Punctuation Prediction","text":"The use of dynamic conditional random fields (DCRF) has been shown to outperform linear-chain conditional random fields (LCRF) for punctuation prediction on conversational speech texts [1]. In this paper, we combine lexical, prosodic, and modified n-gram score features into the DCRF framework for a joint sentence boundary and punctuation prediction task on TDT3 English broadcast news. We show that the joint prediction method outperforms the conventional two-stage method using LCRF or maximum entropy model (MaxEnt). We show the importance of various features using DCRF, LCRF, MaxEnt, and hidden-event n-gram model (HEN) respectively. In addition, we address the practical issue of feature explosion by introducing lexical pruning, which reduces model size and improves the F1-measure. We adopt incremental local training to overcome memory size limitation without incurring significant performance penalty. Our results show that adding prosodic and n-gram score features gives about 20% relative error reduction in all cases. Overall, DCRF gives the best accuracy, followed by LCRF, MaxEnt, and HEN."}
{"_id":"47310b4e14990becd5d473a07092ded4df2fbef1","title":"DeepCas: An End-to-end Predictor of Information Cascades","text":"Information cascades, effectively facilitated by most social network platforms, are recognized as a major factor in almost every social success and disaster in these networks. Can cascades be predicted? While many believe that they are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted by a machine learning algorithm that combines many features. These predictors all depend on a bag of hand-crafting features to represent the cascade network and the global network structures. Such features, always carefully and sometimes mysteriously designed, are not easy to extend or to generalize to a different platform or domain. Inspired by the recent successes of deep learning in multiple data mining tasks, we investigate whether an end-to-end deep learning approach could effectively predict the future size of cascades. Such a method automatically learns the representation of individual cascade graphs in the context of the global network structure, without hand-crafted features or heuristics. We find that node embeddings fall short of predictive power, and it is critical to learn the representation of a cascade graph as a whole. We present algorithms that learn the representation of cascade graphs in an end-to-end manner, which significantly improve the performance of cascade prediction over strong baselines including feature based methods, node embedding methods, and graph kernel methods. Our results also provide interesting implications for cascade prediction in general."}
{"_id":"72696bce8a55e6d4beb49dcc168be2b3c05ef243","title":"Convergence guarantees for RMSProp and ADAM in non-convex optimization and their comparison to Nesterov acceleration on autoencoders","text":"RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical foundations have remained unclear. In this work we make progress towards that by giving proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives and we give bounds on the running time. We then design experiments to compare the performances of RMSProp and ADAM against Nesterov Accelerated Gradient method on a variety of autoencoder setups. Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \u03b21. We show that in terms of getting lower training and test losses, at very high values of the momentum parameter (\u03b21 = 0.99) (and large enough nets if using mini-batches) ADAM outperforms NAG at any momentum value tried for the latter. On the other hand, NAG can sometimes do better when ADAM\u2019s \u03b21 is set to the most commonly used value: \u03b21 = 0.9. We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms and finding weights which increase the minimum eigenvalue of the Hessian of the loss function."}
{"_id":"4f3dbfec5c67f0fb0602d9c803a391bc2f6ee4c7","title":"A 20-GHz phase-locked loop for 40-gb\/s serializing transmitter in 0.13-\/spl mu\/m CMOS","text":"A 20-GHz phase-locked loop with 4.9 ps\/sub pp\/\/0.65 ps\/sub rms\/ jitter and -113.5 dBc\/Hz phase noise at 10-MHz offset is presented. A half-duty sampled-feedforward loop filter that simply replaces the resistor with a switch and an inverter suppresses the reference spur down to -44.0 dBc. A design iteration procedure is outlined that minimizes the phase noise of a negative-g\/sub m\/ oscillator with a coupled microstrip resonator. Static frequency dividers made of pulsed latches operate faster than those made of flip-flops and achieve near 2:1 frequency range. The phase-locked loop fabricated in a 0.13-\/spl mu\/m CMOS operates from 17.6 to 19.4GHz and dissipates 480mW."}
{"_id":"15cf63f8d44179423b4100531db4bb84245aa6f1","title":"Deep Learning","text":"Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress."}
{"_id":"8f67ff7d7a4fc72d87f82ae340dba4365b7ea664","title":"Learning Filter Banks Using Deep Learning For Acoustic Signals","text":"Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urbansound8K dataset [1], the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process."}
{"_id":"923d01e0ff983c0ecd3fde5e831e5faff8485bc5","title":"Improving Restaurants by Extracting Subtopics from Yelp Reviews","text":"In this paper, we describe latent subtopics discovered from Yelp restaurant reviews by running an online Latent Dirichlet Allocation (LDA) algorithm. The goal is to point out demand of customers from a large amount of reviews, with high dimensionality. These topics can provide meaningful insights to restaurants about what customers care about in order to increase their Yelp ratings, which directly affects their revenue. We used the open dataset from the Yelp Dataset Challenge with over 158,000 restaurant reviews. To find latent subtopics from reviews, we adopted Online LDA, a generative probabilistic model for collections of discrete data such as text corpora. We present the breakdown of hidden topics over all reviews, predict stars per hidden topics discovered, and extend our findings to that of temporal information regarding restaurants peak hours. Overall, we have found several interesting insights and a method which could definitely prove useful to restaurant owners."}
{"_id":"505c58c2c100e7512b7f7d906a9d4af72f6e8415","title":"Genetic programming - on the programming of computers by means of natural selection","text":"Page ii Complex Adaptive Systems John H. Holland, Christopher Langton, and Stewart W. Wilson, advisors Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence, MIT Press edition John H. Holland Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life edited by Francisco J. Varela and Paul Bourgine Genetic Programming: On the Programming of Computers by Means of Natural Selection John R. Koza"}
{"_id":"f4cdd1d15112a3458746b58a276d97e79d8f495d","title":"Gradient Regularization Improves Accuracy of Discriminative Models","text":"Regularizing the gradient norm of the output of a neural network with respect to its inputs is a powerful technique, rediscovered several times. This paper presents evidence that gradient regularization can consistently improve classification accuracy on vision tasks, using modern deep neural networks, especially when the amount of training data is small. We introduce our regularizers as members of a broader class of Jacobian-based regularizers. We demonstrate empirically on real and synthetic data that the learning process leads to gradients controlled beyond the training points, and results in solutions that generalize well."}
{"_id":"6fee63e0ae4bc1f3fd08044d7d694bb17b9c059c","title":"Cross-Cultural Software Production and Use: A Structurational Analysis","text":"This paper focuses on cross-cultural software production and use, which is increasingly common in today's more globalized world. A theoretical basis for analysis is developed, using concepts drawn from structuration theory. The theory is illustrated using two cross-cultural case studies. It is argued that structurational analysis provides a deeper examination of cross-cultural working and IS than is found in the current literature, which is dominated by Hofstede-type studies, in particular, the theoretical approach can be used to analyze cross-cultural conflict and contradiction, cultural heterogeneity, detailed work patterns, and the dynamic nature of culture. The paper contributes to the growing body of literature that emphasizes the essential role of cross-cultural understanding in contemporary society. 'Michael D. Myers was the accepting senior editor for this paper. introduction There has been much debate over the last decade about the major sociat transformations taking place in the world such as the increasing interconnectedness of different societies, the compression of time and space, and an intensification of consciousness of the world as a whole (Robertson 1992), Such changes are often tabeted with the term globatization, atthough the precise nature of this phenomenon is highly complex on closer examination. For example. Beck (2000) distinguishes between globality, the change in consciousness of the world as a single entity, and globaiism, the ideology of neoliberatism which argues that the world market eliminates or supplants the importance of locat potiticat action. Despite the complexity of the globalization phenomena, all commentators would agree that information and communication technologies (ICTs) are deeply implicated in the changes that are taking ptace through their abitity to enabte new modes of work, communication, and organization MiS Quarterly Vol. 26 No. 4. pp. 359-380\/December 2002 359 Walsham\/Cross-Cultural Software Production & Use across time and space. For example, the influential work of Castells (1996, 1997, 1998) argues that we are in the \"information age\" where information generation, processing, and transformation are fundamental to societal functioning and societal change, and where ICTs enable the pervasive expansion of networking throughout the social structure. However, does globalization, and the related spread of ICTs, imply that the world is becoming a homogeneous arena for gtobat business and gtobal attitudes, with differences between organizations and societies disappearing? There are many authors who take exception to this conclusion. For exampie, Robertson (1992) discussed the way in which imported themes are indigenized in particular societies with tocat culture constraining receptivity to some ideas rather than others, and adapting them in specific ways. He cited Japan as a good example of these glocalization processes. White accepting the idea of time-space compression facilitated by ICTs, Robertson argued that one of its main consequences is an exacerbation of collisions between gtobat, societat, and communat attitudes. Simitarty, Appadural {1997), coming from a nonWestern background, argued against the gtobat homogenization thesis on the grounds that difl'erent societies witt appropriate the \"materials of modernity\" differently depending on their specific geographies, histories, and languages. Watsham {2001) devetoped a related argument, with a specific focus on the rote of ICTs, concluding that globat diversity needs to be a key focus when devetoping and using such technotogies. If these latter arguments are broadty correct, then working with tCTs in and across different cuttures should prove to be problematic, in that there wilt be different views ofthe relevance, appticabitity, and vatue of particular modes of working and use of ICTs which may produce conflict. For exampte, technotogy transfer from one society to another involves the importing of that technology into an \"atien\" cutturat context where its value may not be similarly perceived to that in its original host cutture. Simitarty, cross-cuttural communication through tCTs, or cross-cultural information systems (IS) devetopment teams, are likely to confront issues of incongruence of values and attitudes. The purpose of this paper is to examine a particular topic within the area of cross-cutturat working and tCTs, namety that of software production and use; in particutar, where the software is not devetoped in and for a specific cutturat group. A primary goat is to devetop a theoreticat basis for anatysis of this area. Key eiements of this basis, which draws on structuration theory, are described in the next section of the paper, tn order to iltustrate the theoreticat basis and its vatue in analyzing real situations, the subsequent sections draw on the field data from two published case studies of cross-cultural software development and application. There is an extensive titerature on cross-cutturat working and IS, and the penultimate section ofthe paper reviews key etements of this titerature, and shows how the anatysis of this paper makes a new contribution. In particular, it witt be argued that the structurationat analysis enabtes a more sophisticated and detailed consideration of issues in cross-culturat software production under four specific headings: cross-cultural contradiction and conflict; cultural heterogeneity; detailed work patterns in different cuttures; and the dynamic, emergent nature of cutture. The final section of the paper wilt summarize some theoretical and practical implications. Structuration Theory, Cuiture and iS The theoretical basis for this paper draws on structuration theory {Giddens 1979, 1984). This theory has been highty inftuentiat in sociology and the social sciences generalty since Giddens first developed the ideas some 20 years ago. In addition, the theory has received considerable attention in the IS field {for a good review, see Jones 1998). The focus here, however, wilt be on how structuration theory can offer a new way of looking 360 MIS Quarterly Vol. 26 No. 4\/December 2002 Walsham\/Cross-Cuitural Software Production & Use Table 1. Structuration Theory, Culture, and ICTs: Some Key Concepts"}
{"_id":"1a090df137014acab572aa5dc23449b270db64b4","title":"LIBSVM: a library for support vector machines","text":""}
{"_id":"1e56ed3d2c855f848ffd91baa90f661772a279e1","title":"Latent Dirichlet Allocation","text":"We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes\/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification."}
{"_id":"00844516c86828a4cc81471b573cb1a1696fcde9","title":"Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion","text":"Here, we demonstrate that subject motion produces substantial changes in the timecourses of resting state functional connectivity MRI (rs-fcMRI) data despite compensatory spatial registration and regression of motion estimates from the data. These changes cause systematic but spurious correlation structures throughout the brain. Specifically, many long-distance correlations are decreased by subject motion, whereas many short-distance correlations are increased. These changes in rs-fcMRI correlations do not arise from, nor are they adequately countered by, some common functional connectivity processing steps. Two indices of data quality are proposed, and a simple method to reduce motion-related effects in rs-fcMRI analyses is demonstrated that should be flexibly implementable across a variety of software platforms. We demonstrate how application of this technique impacts our own data, modifying previous conclusions about brain development. These results suggest the need for greater care in dealing with subject motion, and the need to critically revisit previous rs-fcMRI work that may not have adequately controlled for effects of transient subject movements."}
{"_id":"21bfc289cf7e2309e70f390ae14d89df7c911a67","title":"Modeling regional and psychophysiologic interactions in fMRI: the importance of hemodynamic deconvolution","text":"The analysis of functional magnetic resonance imaging (fMRI) time-series data can provide information not only about task-related activity, but also about the connectivity (functional or effective) among regions and the influences of behavioral or physiologic states on that connectivity. Similar analyses have been performed in other imaging modalities, such as positron emission tomography. However, fMRI is unique because the information about the underlying neuronal activity is filtered or convolved with a hemodynamic response function. Previous studies of regional connectivity in fMRI have overlooked this convolution and have assumed that the observed hemodynamic response approximates the neuronal response. In this article, this assumption is revisited using estimates of underlying neuronal activity. These estimates use a parametric empirical Bayes formulation for hemodynamic deconvolution."}
{"_id":"261208c69aeca0243e43511845a0d8023d31acbe","title":"Common regions of the human frontal lobe recruited by diverse cognitive demands","text":"Though many neuroscientific methods have been brought to bear in the search for functional specializations within prefrontal cortex, little consensus has emerged. To assess the contribution of functional neuroimaging, this article reviews patterns of frontal-lobe activation associated with a broad range of different cognitive demands, including aspects of perception, response selection, executive control, working memory, episodic memory and problem solving. The results show a striking regularity: for many demands, there is a similar recruitment of mid-dorsolateral, mid-ventrolateral and dorsal anterior cingulate cortex. Much of the remainder of frontal cortex, including most of the medial and orbital surfaces, is largely insensitive to these demands. Undoubtedly, these results provide strong evidence for regional specialization of function within prefrontal cortex. This specialization, however, takes an unexpected form: a specific frontal-lobe network that is consistently recruited for solution of diverse cognitive problems."}
{"_id":"2ce0d2f6efe74b9df4c0eccb434322d931c5dd47","title":"Prefrontal cortical function and anxiety: controlling attention to threat-related stimuli","text":"Threat-related stimuli are strong competitors for attention, particularly in anxious individuals. We used functional magnetic resonance imaging (fMRI) with healthy human volunteers to study how the processing of threat-related distractors is controlled and whether this alters as anxiety levels increase. Our work builds upon prior analyses of the cognitive control functions of lateral prefrontal cortex (lateral PFC) and anterior cingulate cortex (ACC). We found that rostral ACC was strongly activated by infrequent threat-related distractors, consistent with a role for this area in responding to unexpected processing conflict caused by salient emotional stimuli. Participants with higher anxiety levels showed both less rostral ACC activity overall and reduced recruitment of lateral PFC as expectancy of threat-related distractors was established. This supports the proposal that anxiety is associated with reduced top-down control over threat-related distractors. Our results suggest distinct roles for rostral ACC and lateral PFC in governing the processing of task-irrelevant, threat-related stimuli, and indicate reduced recruitment of this circuitry in anxiety."}
{"_id":"0ca9e60d077c97f8f9f9e43110e899ed45284ecd","title":"Other minds in the brain: a functional imaging study of \u201ctheory of mind\u201d in story comprehension","text":"The ability of normal children and adults to attribute independent mental states to self and others in order to explain and predict behaviour (\"theory of mind\") has been a focus of much recent research. Autism is a biologically based disorder which appears to be characterised by a specific impairment in this \"mentalising\" process. The present paper reports a functional neuroimaging study with positron emission tomography in which we studied brain activity in normal volunteers while they performed story comprehension tasks necessitating the attribution of mental states. The resultant brain activity was compared with that measured in two control tasks: \"physical\" stories which did not require this mental attribution, and passages of unlinked sentences. Both story conditions, when compared to the unlinked sentences, showed significantly increased regional cerebral blood flow in the following regions: the temporal poles bilaterally, the left superior temporal gyrus and the posterior cingulate cortex. Comparison of the \"theory of mind\" stories with \"physical\" stores revealed a specific pattern of activation associated with mental state attribution: it was only this task which produced activation in the medial frontal gyrus on the left (Brodmann's area 8). This comparison also showed significant activation in the posterior cingulate cortex. These surprisingly clear-cut findings are discussed in relation to previous studies of brain activation during story comprehension. The localisation of brain regions involved in normal attribution of mental states and contextual problem solving is feasible and may have implication for the neural basis of autism."}
{"_id":"723b30edce2a7a46626a38c8f8cac929131b9ed4","title":"Daemo: A Self-Governed Crowdsourcing Marketplace","text":"Crowdsourcing marketplaces provide opportunities for autonomous and collaborative professional work as well as social engagement. However, in these marketplaces, workers feel disrespected due to unreasonable rejections and low payments, whereas requesters do not trust the results they receive. The lack of trust and uneven distribution of power among workers and requesters have raised serious concerns about sustainability of these marketplaces. To address the challenges of trust and power, this paper introduces Daemo, a self-governed crowdsourcing marketplace. We propose a prototype task to improve the work quality and open-governance model to achieve equitable representation. We envisage Daemo will enable workers to build sustainable careers and provide requesters with timely, quality labor for their businesses."}
{"_id":"1eda2af8492a67d66afdb26b70d15e07d9bd11fe","title":"Discriminative shape from shading in uncalibrated illumination","text":"Estimating surface normals from just a single image is challenging. To simplify the problem, previous work focused on special cases, including directional lighting, known reflectance maps, etc., making shape from shading impractical outside the lab. To cope with more realistic settings, shading cues need to be combined and generalized to natural illumination. This significantly increases the complexity of the approach, as well as the number of parameters that require tuning. Enabled by a new large-scale dataset for training and analysis, we address this with a discriminative learning approach to shape from shading, which uses regression forests for efficient pixel-independent prediction and fast learning. Von Mises-Fisher distributions in the leaves of each tree enable the estimation of surface normals. To account for their expected spatial regularity, we introduce spatial features, including texton and silhouette features. The proposed silhouette features are computed from the occluding contours of the surface and provide scale-invariant context. Aside from computational efficiency, they enable good generalization to unseen data and importantly allow for a robust estimation of the reflectance map, extending our approach to the uncalibrated setting. Experiments show that our discriminative approach outperforms state-of-the-art methods on synthetic and real-world datasets."}
{"_id":"84a505024b58c0c3bb5b1bf12ee76f162ebf52d0","title":"System Integration and Power-Flow Management for a Series Hybrid Electric Vehicle Using Supercapacitors and Batteries","text":"In this paper, system integration and power-flow management algorithms for a four-wheel-driven series hybrid electric vehicle (HEV) having multiple power sources composed of a diesel-engine-based generator, lead acid battery bank, and supercapacitor bank are presented. The super-capacitor is utilized as a short-term energy storage device to meet the dynamic performance of the vehicle, while the battery is utilized as a mid-term energy storage for the electric vehicle (EV) mode operation due to its higher energy density. The generator based on an interior permanent magnet machine (IPMM), run by a diesel engine, provides the average power for the normal operation of the vehicle. Thanks to the proposed power-flow management algorithm, each of the energy sources is controlled appropriately and also the dynamic performance of the vehicle has been improved. The proposed power-flow management algorithm has been experimentally verified with a full-scale prototype vehicle."}
{"_id":"dfd653ad1409ecbcd8970f2b505dea0807e316ca","title":"Linear AC LED driver with the multi-level structure and variable current regulator","text":"This paper proposes a linear AC LED driver for LED lighting applications. The proposed circuit is small in size because the circuit structure consists of only semiconductors and resistors without any reactors and electrolytic capacitors. The current bypass circuit which is connected in parallel to the LED string consists of single MOSFET, single zener diode and two resistors. The MOSFET is operated in an active state by a self-bias circuit. Thus, an external controller and high voltage gate drivers are not required. The proposed circuit is experimentally validated by using a 7 W prototype. From the experimental results, the THD of input current is 2.1% and the power factor is 0.999. In addition, the simulation loss analysis demonstrates an efficiency of 87% for a 7 W prototype."}
{"_id":"166cee31d458a41872a50e81532b787845d92e70","title":"Space-Vector PWM Control Synthesis for an H-Bridge Drive in Electric Vehicles","text":"This paper deals with a synthesis of space-vector pulsewidth modulation (SVPWM) control methods applied for an H-bridge inverter feeding a three-phase permanent-magnet synchronous machine (PMSM) in electric-vehicle (EV) applications. First, a short survey of existing architectures of power converters, particularly those adapted to degraded operating modes, is presented. Standard SVPWM control methods are compared with three innovative methods using EV drive specifications in the normal operating mode. Then, a rigorous analysis of the margins left in the control strategy is presented for a semiconductor switch failure to fulfill degraded operating modes. Finally, both classic and innovative strategies are implemented in numerical simulation; their results are analyzed and discussed."}
{"_id":"a5a147be45a38cacff1b21a9c05fc8c408df237b","title":"WALNUT: Waging Doubt on the Integrity of MEMS Accelerometers with Acoustic Injection Attacks","text":"Cyber-physical systems depend on sensors to make automated decisions. Resonant acoustic injection attacks are already known to cause malfunctions by disabling MEMS-based gyroscopes. However, an open question remains on how to move beyond denial of service attacks to achieve full adversarial control of sensor outputs. Our work investigates how analog acoustic injection attacks can damage the digital integrity of a popular type of sensor: the capacitive MEMS accelerometer. Spoofing such sensors with intentional acoustic interference enables an out-of-spec pathway for attackers to deliver chosen digital values to microprocessors and embedded systems that blindly trust the unvalidated integrity of sensor outputs. Our contributions include (1) modeling the physics of malicious acoustic interference on MEMS accelerometers, (2) discovering the circuit-level security flaws that cause the vulnerabilities by measuring acoustic injection attacks on MEMS accelerometers as well as systems that employ on these sensors, and (3) two software-only defenses that mitigate many of the risks to the integrity of MEMS accelerometer outputs. We characterize two classes of acoustic injection attacks with increasing levels of adversarial control: output biasing and output control. We test these attacks against 20 models of capacitive MEMS accelerometers from 5 different manufacturers. Our experiments find that 75% are vulnerable to output biasing, and 65% are vulnerable to output control. To illustrate end-to-end implications, we show how to inject fake steps into a Fitbit with a $5 speaker. In our self-stimulating attack, we play a malicious music file from a smartphone's speaker to control the on-board MEMS accelerometer trusted by a local app to pilot a toy RC car. In addition to offering hardware design suggestions to eliminate the root causes of insecure amplification and filtering, we introduce two low-cost software defenses that mitigate output biasing attacks: randomized sampling and 180 degree out-of-phase sampling. These software-only approaches mitigate attacks by exploiting the periodic and predictable nature of the malicious acoustic interference signal. Our results call into question the wisdom of allowing microprocessors and embedded systems to blindly trust that hardware abstractions alone will ensure the integrity of sensor outputs."}
{"_id":"0d4d20c9f025a54b4c55f8d674f475306ebc88a6","title":"Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem","text":"In this paper we introduce Ant-Q, a family of algorithms which present many similarities with Q-learning (Watkins, 1989), and which we apply to the solution of symmetric and asymmetric instances of the traveling salesman problem (TSP). Ant-Q algorithms were inspired by work on the ant system (AS), a distributed algorithm for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in (Dorigo, 1992; Dorigo, Maniezzo and Colorni, 1996). We show that AS is a particular instance of the Ant-Q family, and that there are instances of this family which perform better than AS. We experimentally investigate the functioning of Ant-Q and we show that the results obtained by Ant-Q on symmetric TSP's are competitive with those obtained by other heuristic approaches based on neural networks or local search. Finally, we apply Ant-Q to some difficult asymmetric TSP's obtaining very good results: Ant-Q was able to find solutions of a quality which usually can be found only by very specialized algorithms."}
{"_id":"18977c6f7abb245691f4268ccd116036bd2391f0","title":"All-at-once Optimization for Coupled Matrix and Tensor Factorizations","text":"Joint analysis of data from multiple sources has the potential to improve our understanding of the underlying structures in complex data sets. For instance, in restaurant recommendation systems, recommendations can be based on rating histories of customers. In addition to rating histories, customers\u2019 social networks (e.g., Facebook friendships) and restaurant categories information (e.g., Thai or Italian) can also be used to make better recommendations. The task of fusing data, however, is challenging since data sets can be incomplete and heterogeneous, i.e., data consist of both matrices, e.g., the person by person social network matrix or the restaurant by category matrix, and higher-order tensors, e.g., the \u201cratings\u201d tensor of the form restaurant by meal by person. In this paper, we are particularly interested in fusing data sets with the goal of capturing their underlying latent structures. We formulate this problem as a coupled matrix and tensor factorization (CMTF) problem where heterogeneous data sets are modeled by fitting outer-product models to higher-order tensors and matrices in a coupled manner. Unlike traditional approaches solving this problem using alternating algorithms, we propose an all-at-once optimization approach called CMTF-OPT (CMTF-OPTimization), which is a gradient-based optimization approach for joint analysis of matrices and higher-order tensors. We also extend the algorithm to handle coupled incomplete data sets. Using numerical experiments, we demonstrate that the proposed all-at-once approach is more accurate than the alternating least squares approach."}
{"_id":"7d6acd022c000e57f46795cc54506215b9b9ec33","title":"A Tagged Corpus and a Tagger for Urdu","text":"In this paper, we describe a release of a sizeable monolingual Urdu corpus automatically tagged with part-of-speech tags. We extend the work of Jawaid and Bojar (2012) who use three different taggers and then apply a voting scheme to disambiguate among the different choices suggested by each tagger. We run this complex ensemble on a large monolingual corpus and release the tagged corpus. Additionally, we use this data to train a single standalone tagger which will hopefully significantly simplify Urdu processing. The standalone tagger obtains the accuracy of 88.74% on test data."}
{"_id":"539ea86fa738afd939fb18566107c971461f8548","title":"Learning as search optimization: approximate large margin methods for structured prediction","text":"Mappings to structured output spaces (strings, trees, partitions, etc.) are typically learned using extensions of classification algorithms to simple graphical structures (eg., linear chains) in which search and parameter estimation can be performed exactly. Unfortunately, in many complex problems, it is rare that exact search or parameter estimation is tractable. Instead of learning exact models and searching via heuristic means, we embrace this difficulty and treat the structured output problem in terms of approximate search. We present a framework for learning as search optimization, and two parameter updates with convergence the-orems and bounds. Empirical evidence shows that our integrated approach to learning and decoding can outperform exact models at smaller computational cost."}
{"_id":"1219fb39b46aabd74879a7d6d3c724fb4e55aeae","title":"Bricolage versus breakthrough : distributed and embedded agency in technology entrepreneurship","text":"We develop a perspective on technology entrepreneurship as involving agency that is distributed across different kinds of actors. Each actor becomes involved with a technology, and, in the process, generates inputs that result in the transformation of an emerging technological path. The steady accumulation of inputs to a technological path generates a momentum that enables and constrains the activities of distributed actors. In other words, agency is not only distributed, but it is embedded as well. We explicate this perspective through a comparative study of processes underlying the emergence of wind turbines in Denmark and in United States. Through our comparative study, we flesh out \u201cbricolage\u201d and \u201cbreakthrough\u201d as contrasting approaches to the engagement of actors in shaping technological paths. \u00a9 2002 Elsevier Science B.V. All rights reserved."}
{"_id":"2266636d87e44590ade738b92377d1fe1bc5c970","title":"Threshold selection using Renyi's entropy","text":""}
{"_id":"1672a134e0cebfef817c0c832eea1e54ffb094b0","title":"UTHealth at SemEval-2016 Task 12: an End-to-End System for Temporal Information Extraction from Clinical Notes","text":"The 2016 Clinical TempEval challenge addresses temporal information extraction from clinical notes. The challenge is composed of six sub-tasks, each of which is to identify: (1) event mention spans, (2) time expression spans, (3) event attributes, (4) time attributes, (5) events\u2019 temporal relations to the document creation times (DocTimeRel), and (6) narrative container relations among events and times. In this article, we present an end-to-end system that addresses all six sub-tasks. Our system achieved the best performance for all six sub-tasks when plain texts were given as input. It also performed best for narrative container relation identification when gold standard event\/time annotations were given."}
{"_id":"beaaba420f5cef9b4564bc4e1ff88094a5fa2054","title":"Discovering Molecular Functional Groups Using Graph Convolutional Neural Networks","text":"Functional groups (FGs) serve as a foundation for analyzing chemical properties of organic molecules. Automatic discovery of FGs will impact various fields of research, including medicinal chemistry, by reducing the amount of lab experiments required for discovery or synthesis of new molecules. Here, we investigate methods based on graph convolutional neural networks (GCNNs) for localizing FGs that contribute to specific chemical properties. Molecules are modeled as undirected graphs with atoms as nodes and bonds as edges. Using this graph structure, we trained GCNNs in a supervised way on experimentally-validated molecular training sets to predict specific chemical properties, e.g., toxicity. Upon learning a GCNN, we analyzed its activation patterns to automatically identify FGs using four different methods: gradient-based saliency maps, Class Activation Mapping (CAM), gradient-weighted CAM (Grad-CAM), and Excitation Back-Propagation. We evaluated the contrastive power of these methods with respect to the specificity of the identified molecular substructures and their relevance for chemical functions. GradCAM had the highest contrastive power and generated qualitatively the best FGs. This work paves the way for automatic analysis and design of new molecules."}
{"_id":"a8f87a5ab16764e61aef3cbadcc52ca927bb392d","title":"How to make large self-organizing maps for nonvectorial data","text":"The self-organizing map (SOM) represents an open set of input samples by a topologically organized, finite set of models. In this paper, a new version of the SOM is used for the clustering, organization, and visualization of a large database of symbol sequences (viz. protein sequences). This method combines two principles: the batch computing version of the SOM, and computation of the generalized median of symbol strings."}
{"_id":"83c2183c5fd530bd1ff00ba51939680b4419840b","title":"Structurally-Sensitive Multi-Scale Deep Neural Network for Low-Dose CT Denoising","text":"Computed tomography (CT) is a popular medical imaging modality and enjoys wide clinical applications. At the same time, the X-ray radiation dose associated with CT scannings raises a public concern due to its potential risks to the patients. Over the past years, major efforts have been dedicated to the development of low-dose CT (LDCT) methods. However, the radiation dose reduction compromises the signal-to-noise ratio, leading to strong noise and artifacts that down-grade the CT image quality. In this paper, we propose a novel 3-D noise reduction method, called structurally sensitive multi-scale generative adversarial net, to improve the LDCT image quality. Specifically, we incorporate 3-D volumetric information to improve the image quality. Also, different loss functions for training denoising models are investigated. Experiments show that the proposed method can effectively preserve the structural and textural information in reference to the normal-dose CT images and significantly suppress noise and artifacts. Qualitative visual assessments by three experienced radiologists demonstrate that the proposed method retrieves more information and outperforms competing methods."}
{"_id":"2af586c64c32baeb445992e0ea6b76bbbbc30c7f","title":"Massive parallelization of approximate nearest neighbor search on KD-tree for high-dimensional image descriptor matching","text":""}
{"_id":"15a4ef82d92b08c5c1332324d0820ec3d082bf3e","title":"REGULARIZATION TOOLS: A Matlab package for analysis and solution of discrete ill-posed problems","text":"The package REGULARIZATION TOOLS consists of 54 Matlab routines for analysis and solution of discrete ill-posed problems, i.e., systems of linear equations whose coefficient matrix has the properties that its condition number is very large, and its singular values decay gradually to zero. Such problems typically arise in connection with discretization of Fredholm integral equations of the first kind, and similar ill-posed problems. Some form of regularization is always required in order to compute a stabilized solution to discrete ill-posed problems. The purpose of REGULARIZATION TOOLS is to provide the user with easy-to-use routines, based on numerical robust and efficient algorithms, for doing experiments with regularization of discrete ill-posed problems. By means of this package, the user can experiment with different regularization strategies, compare them, and draw conclusions from these experiments that would otherwise require a major programming effert. For discrete ill-posed problems, which are indeed difficult to treat numerically, such an approach is certainly superior to a single black-box routine. This paper describes the underlying theory gives an overview of the package; a complete manual is also available."}
{"_id":"cb0da1ed189087c9ba716cc5c99c75b52430ec06","title":"Transparent and Efficient CFI Enforcement with Intel Processor Trace","text":"Current control flow integrity (CFI) enforcement approaches either require instrumenting application executables and even shared libraries, or are unable to defend against sophisticated attacks due to relaxed security policies, or both, many of them also incur high runtime overhead. This paper observes that the main obstacle of providing transparent and strong defense against sophisticated adversaries is the lack of sufficient runtime control flow information. To this end, this paper describes FlowGuard, a lightweight, transparent CFI enforcement approach by a novel reuse of Intel Processor Trace (IPT), a recent hardware feature that efficiently captures the entire runtime control flow. The main challenge is that IPT is designed for offline performance analysis and software debugging such that decoding collected control flow traces is prohibitively slow on the fly. FlowGuard addresses this challenge by reconstructing applications' conservative control flow graphs (CFG) to be compatible with the compressed encoding format of IPT, and labeling the CFG edges with credits in the help of fuzzing-like dynamic training. At runtime, FlowGuard separates fast and slow paths such that the fast path compares the labeled CFGs with the IPT traces for fast filtering, while the slow path decodes necessary IPT traces for strong security. We have implemented and evaluated FlowGuard on a commodity Intel Skylake machine with IPT support. Evaluation results show that FlowGuard is effective in enforcing CFI for several applications, while introducing only small performance overhead. We also show that, with minor hardware extensions, the performance overhead can be further reduced."}
{"_id":"f812347d46035d786de40c165a158160bb2988f0","title":"Predictive coding as a model of cognition","text":"Previous work has shown that predictive coding can provide a detailed explanation of a very wide range of low-level perceptual processes. It is also widely believed that predictive coding can account for high-level, cognitive, abilities. This article provides support for this view by showing that predictive coding can simulate phenomena such as categorisation, the influence of abstract knowledge on perception, recall and reasoning about conceptual knowledge, context-dependent behavioural control, and naive physics. The particular implementation of predictive coding used here (PC\/BC-DIM) has previously been used to simulate low-level perceptual behaviour and the neural mechanisms that underlie them. This algorithm thus provides a single framework for modelling both perceptual and cognitive brain function."}
{"_id":"f59d8504d7c6e209e6f8bcb62346140214b244b7","title":"Fine-tuning Deep Convolutional Networks for Plant Recognition","text":"This paper describes the participation of the ECOUAN team in the LifeCLEF 2015 challenge. We used a deep learning approach in which the complete system was learned without hand-engineered components. We pre-trained a convolutional neural network using 1.8 million images and used a fine-tuning strategy to transfer learned recognition capabilities from general domains to the specific challenge of Plant Identification task. The classification accuracy obtained by our method outperformed the best result obtained in 2014. Our group obtained the 4th position among all teams and the 10th position among 18 runs."}
{"_id":"681faa552d147d606815bbd008bc1de0005f63ba","title":"A Hybrid PCA-CART-MARS-Based Prognostic Approach of the Remaining Useful Life for Aircraft Engines","text":"Prognostics is an engineering discipline that predicts the future health of a system. In this research work, a data-driven approach for prognostics is proposed. Indeed, the present paper describes a data-driven hybrid model for the successful prediction of the remaining useful life of aircraft engines. The approach combines the multivariate adaptive regression splines (MARS) technique with the principal component analysis (PCA), dendrograms and classification and regression trees (CARTs). Elements extracted from sensor signals are used to train this hybrid model, representing different levels of health for aircraft engines. In this way, this hybrid algorithm is used to predict the trends of these elements. Based on this fitting, one can determine the future health state of a system and estimate its remaining useful life (RUL) with accuracy. To evaluate the proposed approach, a test was carried out using aircraft engine signals collected from physical sensors (temperature, pressure, speed, fuel flow, etc.). Simulation results show that the PCA-CART-MARS-based approach can forecast faults long before they occur and can predict the RUL. The proposed hybrid model presents as its main advantage the fact that it does not require information about the previous operation states of the input variables of the engine. The performance of this model was compared with those obtained by other benchmark models (multivariate linear regression and artificial neural networks) also applied in recent years for the modeling of remaining useful life. Therefore, the PCA-CART-MARS-based approach is very promising in the field of prognostics of the RUL for aircraft engines."}
{"_id":"28ff2f3bf5403d7adc58f6aac542379806fa3233","title":"Interpreting random forest models using a feature contribution method","text":"Model interpretation is one of the key aspects of the model evaluation process. The explanation of the relationship between model variables and outputs is easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance. For \u201cblack box\u201d models, such as random forest, this information is hidden inside the model structure. This work presents an approach for computing feature contributions for random forest classification models. It allows for the determination of the influence of each variable on the model prediction for an individual instance. Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology. The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models."}
{"_id":"c8bbf44d7454f37e2e12713f48ca99b3f19ef915","title":"Methodology of band rejection \/ addition for microstrip antennas design using slot line theory and current distribution analysis","text":"Radio or wireless communication means to transfer information over long distance without using any wires. Millions of people exchange information every day using pager, cellular, telephones, laptops, various types of personal digital assistance (PDAs) and other wireless communication products. The worldwide interoperability for microwave access (Wi-Max) aims to provide wireless data over a long distance in variety of ways. It was based on IEEE 802.16 standard. It is an effective metropolitan area access technique with many favorable features like flexibility, cost, efficiency and fast networking, which not only provides wireless access, but also serves to expand the access to wired network. The coverage area of Wi-Max is around 30-50 km. it can provide high 100 Mbps data rates in 20 MHz bandwidth on fixed and nomadic applications in the 2-11 GHz frequencies. In this paper, a methodology of band rejection\/addition for microstrip antennas design using slot line theory and current distribution analysis has been introduced and analyzed. The analysis and design are done by a commercial software. The radiation characteristics, such as; return loss, VSWR, input impedance, and the surface current densities have been introduced and discussed. Finally, the proposed optimum antenna design structure has been fabricated and the measured S-parameters of the proposed structure can be analyzed with network analyzer and compared with simulation results to demonstrate the excellent performance and meet the requirements for wireless communication applications."}
{"_id":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition","text":"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."}
{"_id":"4e26e488c02b3647e0f1566760555ebe5d002558","title":"A Quadratic-Complexity Observability-Constrained Unscented Kalman Filter for SLAM","text":"This paper addresses two key limitations of the unscented Kalman filter (UKF) when applied to the simultaneous localization and mapping (SLAM) problem: the cubic computational complexity in the number of states and the inconsistency of the state estimates. To address the first issue, we introduce a new sampling strategy for the UKF, which has constant computational complexity. As a result, the overall computational complexity of UKF-based SLAM becomes of the same order as that of the extended Kalman filter (EKF)-based SLAM, i.e., quadratic in the size of the state vector. Furthermore, we investigate the inconsistency issue by analyzing the observability properties of the linear-regression-based model employed by the UKF. Based on this analysis, we propose a new algorithm, termed observability-constrained (OC)-UKF, which ensures the unobservable subspace of the UKF's linear-regression-based system model is of the same dimension as that of the nonlinear SLAM system. This results in substantial improvement in the accuracy and consistency of the state estimates. The superior performance of the OC-UKF over other state-of-the-art SLAM algorithms is validated by both Monte-Carlo simulations and real-world experiments."}
{"_id":"9746994d09b5bf6c40bee3693ee8678e191f84b8","title":"The Sixth PASCAL Recognizing Textual Entailment Challenge","text":"This paper presents the Fifth Recognizing Textual Entailment Challenge (RTE5). Following the positive experience of the last campaign, RTE-5 has been proposed for the second time as a track at the Text Analysis Conference (TAC). The structure of the RTE-5 Main Task remained unchanged, offering both the traditional two-way task and the threeway task introduced in the previous campaign. Moreover, a pilot Search Task was set up, consisting of finding all the sentences in a set of documents that entail a given hypothesis. 21 teams participated in the campaign, among which 20 in the Main Task (for a total of 54 runs) and 8 in the Pilot Task (for a total of 20 runs). Another important innovation introduced in this campaign was mandatory ablation tests that participants had to perform for all major knowledge resources employed by their systems."}
{"_id":"9d9fc59f80f41915793e7f59895c02dfa6c1e5a9","title":"Trajectory Tracking Control of a Four-Wheel Differentially Driven Mobile Robot","text":"We consider the trajecto ry tracking control problem for a 4-wheel differentially driven mobile robot moving on an outdoor terrain. A dynamic model is presented accounting for the effects of wheel skidding. A model-based nonlinear controller is designed, following the dynamic feedback linearization paradigm. A n operational nonholonomic constraint is added at this stage, so as to obtain a predictable behavior for the instantaneous center of rotation thus preventing excessive skidding. The controller is then robustijied, using conventional linear techniques, against uncertainty in the soil parameters at the ground-wheel contact. Simulation results show the good performance in tracking spline-type trajectories on a virtual terrain with varying characteristics."}
{"_id":"096cc955e19446c3445e331c62d62897833d3e46","title":"On partial least squares in head pose estimation: How to simultaneously deal with misalignment","text":"Head pose estimation is a critical problem in many computer vision applications. These include human computer interaction, video surveillance, face and expression recognition. In most prior work on heads pose estimation, the positions of the faces on which the pose is to be estimated are specified manually. Therefore, the results are reported without studying the effect of misalignment. We propose a method based on partial least squares (PLS) regression to estimate pose and solve the alignment problem simultaneously. The contributions of this paper are two-fold: 1) we show that the kernel version of PLS (kPLS) achieves better than state-of-the-art results on the estimation problem and 2) we develop a technique to reduce misalignment based on the learned PLS factors."}
{"_id":"7c1ac5a078b2d45740ea18caa12ceddef5b4d122","title":"An approach for selecting seed URLs of focused crawler based on user-interest ontology","text":"Seed URLs selection for focused Web crawler intends to guide related and valuable information that meets a user\u2019s personal information requirement and provide more effective information retrieval. In this paper, we propose a seed URLs selection approach based on user-interest ontology. In order to enrich semantic query, we first intend to apply Formal Concept Analysis to construct user-interest concept lattice with user log profile. By using concept lattice merger, we construct the user-interest ontology which can describe the implicit concepts and relationships between them more appropriately for semantic representation eed URLs ormal concept analysis ser-interest ontology ipartite graph eb crawler and query match. On the other hand, we make full use of the user-interest ontology for extracting the user interest topic area and expanding user queries to receive the most related pages as seed URLs, which is an entrance of the focused crawler. In particular, we focus on how to refine the user topic area using the bipartite directed graph. The experiment proves that the user-interest ontology can be achieved effectively by merging concept lattices and that our proposed approach can select high quality seed URLs collection and improve the average precision of focused Web crawler."}
{"_id":"7c3a4b84214561d8a6e4963bbb85a17a5b1e003a","title":"Programs for Machine Learning. Part I","text":""}
{"_id":"167e1359943b96b9e92ee73db1df69a1f65d731d","title":"A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts","text":"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \u201cthumbs up\u201d or \u201cthumbs down\u201d. To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints. Publication info: Proceedings of the ACL, 2004."}
{"_id":"02733207813127f30d89ddb7a59d0374509c789c","title":"Cognitive control signals for neural prosthetics.","text":"Recent development of neural prosthetics for assisting paralyzed patients has focused on decoding intended hand trajectories from motor cortical neurons and using this signal to control external devices. In this study, higher level signals related to the goals of movements were decoded from three monkeys and used to position cursors on a computer screen without the animals emitting any behavior. Their performance in this task improved over a period of weeks. Expected value signals related to fluid preference, the expected magnitude, or probability of reward were decoded simultaneously with the intended goal. For neural prosthetic applications, the goal signals can be used to operate computers, robots, and vehicles, whereas the expected value signals can be used to continuously monitor a paralyzed patient's preferences and motivation."}
{"_id":"cc415579249532aa33651c8eca1aebf5ce26af1d","title":"Today \u2019 s State-Owned Enterprises of China : Are They Dying Dinosaurs or Dynamic Dynamos ?","text":"NOTE: The authors thank Bianca Bain for her assistance on an earlier draft of the paper and the University of Macao for providing financial support. Summary This paper raises the question and provides empirical evidence regarding the status of the evolution of the state-owned enterprises (SOEs) in China today. In this study, we compare the SOEs to domestic private-owned enterprises (POEs) and foreign-controlled businesses (FCBs) in the context of their organizational cultures. While a new ownership form, many of the POEs, evolved from former collectives that reflect the traditional values of Chinese business. Conversely, the FCBs are much more indicative of the large global MNCs. Therefore, we look at the SOEs in the context of these two reference points. We conclude that the SOEs of today have substantially transformed to approximate a configuration desired by the Chinese government when it began the SOE transformation a couple of decades ago to make them globally competitive. The SOEs of today appear to be appropriately described as China's economic dynamic dynamo for the future."}
{"_id":"045a975c1753724b3a0780673ee92b37b9827be6","title":"Wait-Free Synchronization","text":"A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, \u201cthere is no wait-free implementation of X by Y.\u201d We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that atomic read\/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest&set and fetch&add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object."}
{"_id":"0541d5338adc48276b3b8cd3a141d799e2d40150","title":"MapReduce: Simplified Data Processing on Large Clusters","text":"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day."}
{"_id":"2032be0818be583f159cc75f2022ed78222fb772","title":"Salt-and-pepper noise removal by median-type noise detectors and detail-preserving regularization","text":"This paper proposes a two-phase scheme for removing salt-and-pepper impulse noise. In the first phase, an adaptive median filter is used to identify pixels which are likely to be contaminated by noise (noise candidates). In the second phase, the image is restored using a specialized regularization method that applies only to those selected noise candidates. In terms of edge preservation and noise suppression, our restored images show a significant improvement compared to those restored by using just nonlinear filters or regularization methods only. Our scheme can remove salt-and-pepper-noise with a noise level as high as 90%."}
{"_id":"43089ffed8c6c653f6994fb96f7f48bbcff2a598","title":"Adaptive median filters: new algorithms and results","text":"Based on two types of image models corrupted by impulse noise, we propose two new algorithms for adaptive median filters. They have variable window size for removal of impulses while preserving sharpness. The first one, called the ranked-order based adaptive median filter (RAMF), is based on a test for the presence of impulses in the center pixel itself followed by a test for the presence of residual impulses in the median filter output. The second one, called the impulse size based adaptive median filter (SAMF), is based on the detection of the size of the impulse noise. It is shown that the RAMF is superior to the nonlinear mean L(p) filter in removing positive and negative impulses while simultaneously preserving sharpness; the SAMF is superior to Lin's (1988) adaptive scheme because it is simpler with better performance in removing the high density impulsive noise as well as nonimpulsive noise and in preserving the fine details. Simulations on standard images confirm that these algorithms are superior to standard median filters."}
{"_id":"1ef2b855e8a447b17ca7470ae5f3fff667d2fc28","title":"AP3: cooperative, decentralized anonymous communication","text":"This paper describes a cooperative overlay network that provides anonymous communication services for participating users. The Anonymizing Peer-to-Peer Proxy (AP3) system provides clients with three primitives: (i) anonymous message delivery, (ii) anonymous channels, and (iii) secure pseudonyms. AP3 is designed to be lightweight, low-cost and provides \"probable innocence\" anonymity to participating users, even under a large-scale coordinated attack by a limited fraction of malicious overlay nodes. Additionally, we use AP3's primitives to build novel anonymous group communication facilities (multicast and anycast), which shield the identity of both publishers and subscribers."}
{"_id":"683c8f5c60916751bb23f159c86c1f2d4170e43f","title":"Probabilistic Encryption","text":""}
{"_id":"7bbf34f4766a424d8fa934f5d1bda580e9ae814c","title":"THE ROLE OF MUSIC COMMUNICATION IN CINEMA","text":"[Authors\u2019 note: This paper is an abbreviated version of a chapter included in a forthcoming book entitled Music Communication (D. Miell, R. MacDonald, & D. Hargreaves, Eds.), to be published by Oxford University Press.] Past research leaves no doubt about the efficacy of music as a means of communication. In the following pages, after presenting a general model of music communication, the authors will introduce models \u2013 both empirical and theoretical \u2013 of film music perception and the role of music in film, referencing some of the most significant research investigating the relationship between sound and image in the cinematic context. We shall then enumerate the many ways in which the motion picture soundtrack can supplement, enhance, and expand upon the meaning of a film\u2019s narrative. The relationship between the auditory and visual components in cinema is both active and dynamic, affording a multiplicity of possible relations than can evolve \u2013 sometimes dramatically \u2013 as the narrative unfolds. This paper will take a cognitive approach to the study of musical communication in cinema. As a result, much credence will be given to the results of empirical research investigating human cognitive processing in response to the motion picture experience. In conclusion, the present authors will argue for a more inclusive definition of the term \u201cfilm music\u201d than that utilized or implied in previous publications. In our view, film music is one component of a sonic fabric that includes the musical score, ambient sound, dialogue, sound effects, and silence. The functions of these constituent elements often overlap or interact with one another, creating a harmonious counterpoint to the visual image. 1. A MODEL OF MUSIC COMMUNICATION Many studies have investigated various aspects of musical communication as a form of expression (Bengtsson & Gabrielsson, 1983; Clarke, 1988; Clynes, 1983, Gabrielsson, 1988, Seashore, 1967\/1938; Senju & Ohgushi, 1987; Sundberg, 1988; Sundberg et al., 1983). A tripartite communication model was proposed by Campbell and Heller (1980), consisting simply of a composer, performer, and listener. Using this previous model as a foundation, Kendall and Carterette (1990) elegantly expanded upon this model of music communication, detailing a process involving multiple states of coding, decoding, and recoding. Kendall and Carterette suggest that this process involves the \u201cgrouping and parsing of elementary thought units\u201d (p. 132), these \u201cthought units\u201d (metasymbols) are mental representations involved in the process of creating, performing, and listening to musical sound. 2. MODELS OF FILM MUSIC COMMUNICATION 2.1 Empirical Evidence Several researchers have proposed models specific to the perception and cognition of music within the cinematic context. Initiating this systematic effort, Marshall and Cohen\u2019s (1988) bipartite \u201ccongruence-associationist\u201d model suggests that the meaning of a film is altered by the music as the result of two complex cognitive processes. Based upon subject responses, the researchers determined that musical sound directly effects subject ratings on the Potency (strong-weak) and Activity (active-passive) dimensions, while the Evaluative dimension (good-bad) relies on the degree of congruence between the audio and visual components on all three dimensions, as determined by a \u201ccomparator\u201d component. The second part of the model describes how musical meaning is ascribed to the film. Marshall and Cohen claim that attention is directed to the overlapping congruent meaning of the music and the film. Referential meanings associated with the music are ascribed to the overlapped (congruent) audio-visual components upon which attention is focused. As a result, \u201cthe music alters meaning of a particular aspect of the film\u201d (1988, p. 109). Marshall and Cohen also acknowledge the important role played by temporal characteristics of the sound and image, stating that \u201cthe assignment of accent to events will affect retention, processing, and interpretation\u201d (1988, p. 108). Incorporation of this important component of the developing model was provided by Lipscomb and Kendall\u2019s (1994) Film Music Paradigm, in which two implicit processes are considered as the basis for whether attentional focus is shifted to the musical component or whether it is likely to remain at the subconscious \u2013 cognitively \u201cinaudible\u201d \u2013 level. The authors suggested that these two implicit processes include an association judgment (similar to Marshall and Cohen\u2019s assessment of \u201ccongruence\u201d) and an evaluation of the accent structure relationship between the auditory and visual components. Based on the results of a series of three experiments utilizing stimuli ranging from extremely simple, single-object animations to actual movie excerpts, Lipscomb (1995) determined that the role of the two implicit judgments appears to be dynamic such that, with simple stimuli (such as that used in Lipscomb, 1995, Experiment 1 and Marshall & Cohen, 1988), accent structure alignment plays a dominant role. As the stimuli become more complex (e.g., multi-object animations and actual movie excerpts) the primary determinant of meaning in the auditory domain appears to shift to the associational judgment, with the accent structure alignment aspect receding to a supporting role, i.e., focusing audience attention on certain aspects of the visual image (Boltz, 2001). The most complex and fully developed model of film music perception proposed to date is Cohen\u2019s (2001) \u201ccongruence-associationist framework for understanding film-music communication\u201d (p. 259; see Figure 1). This multi-stage model attempts to account for meaning derived from the spoken narrative, visual images, and musical sound. Level A represents bottom-up processing based on physical features derived from input to each perceptual modality. Level B represents the determination of cross-modal congruence, based on both semantic (associational) and syntactic (temporal) grouping features. Level D represents top-down processing, determined by an individual\u2019s past experience and the retention of that experience in long term memory. According to this model, the input from levels B (bottom-up) and D (top-down) meet in the observer\u2019s conscious mind (level C), where information is prepared for transfer to short term memory. In its details, clearly documented in Cohen (2001), this model is based on an assumption of visual primacy, citing several studies that have suggested a subservient role for the auditory component (Bolivar et al., 1994; Driver, 1997; Thompson et al., 1994). Though a common assumption throughout the literature, the present authors would like to express reservation about this assumption and suggest that additional research is required before such a claim can be supported definitively. Figure 1. Cohen\u2019s \u201ccongruence-associationist framework. 2.2 Theoretical Evidence Richard Wagner, creator of the idealized Gesamtkunstwerk in the form of the 19 century music drama, claimed that \u201cas pure organ of the feeling, [music] speaks out the very thing which word speech in itself can not speak out ... that which, looked at from the standpoint of our human intellect, is the unspeakable\u201d (Wagner 1849\/1964, p. 217). According to Suzanne K. Langer, \u201cmusic has all the earmarks of a true symbolism, except one: the existence of an assigned connotation\u201d and, though music is clearly a symbolic form, it remains an \u201cunconsummated symbol\u201d (1942, p. 240). In order for a film to make the greatest possible impact, there must be an interaction between the verbal dialogue (consummated symbol), the cinematic images (also, typically, a consummated symbol), and the musical score (unconsummated symbol). To answer the question \u201cHow does music in film narration create a point of experience for the spectator?,\u201d Gorbman (1987) suggests three methods by which music can \u201csignify\u201d in the context of a narrative film. Purely musical signification results from the highly coded syntactical relationships inherent in the association of one musical tone with another. Patterns of tension and release provide a sense of organization and meaning to the musical sound, apart from any extramusical association that might exist. Cultural musical codes are exemplified by music that has come to be associated with a certain mood or state of mind. These associations have been further canonized by the Hollywood film industry into certain conventional expectations \u2013 implicitly anticipated by enculturated audience members \u2013 determined by the narrative content of a given scene. Finally, cinematic codes influence musical meaning merely due to the placement of musical sound within the filmic context. Opening credit and end title music illustrate this type of signification, as well as recurring musical themes that come to represent characters or situations within the film. There is a commonly held belief that film music is not to be heard (Burt, 1994; Gorbman, 1987). Instead, it is believed to fulfill its role in communicating the underlying psychological drama of the narrative at a subconscious level (Lipscomb, 1989). There is, however, certain music that is intended to be heard by the audience as part of the cinematic diegesis, i.e., \u201cthe narratively implied spatiotemporal world of the actions and characters\u201d (Gorbman 1987, p. 21). This \u201cworld\u201d includes, naturally, a sonic component. Therefore, all sounds that are understood to be heard by characters in the narrative \u2013 including music \u2013 are referred to as diegetic, while those that are not part of the diegesis (e.g., the orchestral score) are referred to as nondiegetic. This would suggest that diegetic music is more likely to be processed at the conscious level while nondiegetic music might remain at the subconscious level, though research is needed to determine whether this is true, in fact. It is worth noting also, that the source of diegetic sound can be either seen or unseen. Michel Chion (199"}
{"_id":"3a116f2ae10a979c18787245933cb9f984569599","title":"Data Collection in Wireless Sensor Networks with Mobile Elements: A Survey","text":"Wireless sensor networks (WSNs) have emerged as an effective solution for a wide range of applications. Most of the traditional WSN architectures consist of static nodes which are densely deployed over a sensing area. Recently, several WSN architectures based on mobile elements (MEs) have been proposed. Most of them exploit mobility to address the problem of data collection in WSNs. In this article we first define WSNs with MEs and provide a comprehensive taxonomy of their architectures, based on the role of the MEs. Then we present an overview of the data collection process in such a scenario, and identify the corresponding issues and challenges. On the basis of these issues, we provide an extensive survey of the related literature. Finally, we compare the underlying approaches and solutions, with hints to open problems and future research directions."}
{"_id":"3b290393afef51b374f9daf9856ec3c1a5fa2968","title":"A Successive Approximation Recursive Digital Low-Dropout Voltage Regulator With PD Compensation and Sub-LSB Duty Control","text":"This paper presents a recursive digital low-dropout (RLDO) regulator that improves response time, quiescent power, and load regulation dynamic range over prior digital LDO designs by 1\u20132 orders of magnitude. The proposed RLDO enables a practical digital replacement to analog LDOs by using an SAR-like binary search algorithm in a coarse loop and a sub-LSB pulse width modulation duty control scheme in a fine loop. A proportional-derivative compensation scheme is employed to ensure stable operation independent of load current, the size of the output decoupling capacitor, and clock frequency. Implemented in 0.0023 mm2 in 65 nm CMOS, the 7-bit RLDO achieves, at a 0.5-V input, a response time of 15.1 ns with a figure of merit of 199.4 ps, along with stable operation across a 20 000 $\\times $  dynamic load range."}
{"_id":"d429ddfb32f921e630ded47a8fd1bc424f7283d9","title":"Imaging Cognition II: An Empirical Review of 275 PET and fMRI Studies","text":"Positron emission tomography (PET) and functional magnetic resonance imaging (fMRI) have been extensively used to explore the functional neuroanatomy of cognitive functions. Here we review 275 PET and fMRI studies of attention (sustained, selective, Stroop, orientation, divided), perception (object, face, space\/motion, smell), imagery (object, space\/ motion), language (written\/spoken word recognition, spoken\/ no spoken response), working memory (verbal\/numeric, object, spatial, problem solving), semantic memory retrieval (categorization, generation), episodic memory encoding (verbal, object, spatial), episodic memory retrieval (verbal, nonverbal, success, effort, mode, context), priming (perceptual, conceptual), and procedural memory (conditioning, motor, and nonmotor skill learning). To identify consistent activation patterns associated with these cognitive operations, data from 412 contrasts were summarized at the level of cortical Brodmann's areas, insula, thalamus, medial-temporal lobe (including hippocampus), basal ganglia, and cerebellum. For perception and imagery, activation patterns included primary and secondary regions in the dorsal and ventral pathways. For attention and working memory, activations were usually found in prefrontal and parietal regions. For language and semantic memory retrieval, typical regions included left prefrontal and temporal regions. For episodic memory encoding, consistently activated regions included left prefrontal and medial-temporal regions. For episodic memory retrieval, activation patterns included prefrontal, medial-temporal, and posterior midline regions. For priming, deactivations in prefrontal (conceptual) or extrastriate (perceptual) regions were consistently seen. For procedural memory, activations were found in motor as well as in non-motor brain areas. Analysis of regional activations across cognitive domains suggested that several brain regions, including the cerebellum, are engaged by a variety of cognitive challenges. These observations are discussed in relation to functional specialization as well as functional integration."}
{"_id":"ac6fdfa9d2ca8ec78ea1b2c8807ab9147b8a526d","title":"Exploring Synergies between Machine Learning and Knowledge Representation to Capture Scientific Knowledge","text":"In this paper we explore synergies between the machine learning and knowledge representation fields by considering how scientific knowledge is represented in these areas. We illustrate some of the knowledge obtained through machine learning methods, providing two contrasting examples of such models: probabilistic graphical models (aka Bayesian networks) and artificial neural networks (including deep learning networks). From knowledge representation, we give an overview of ontological representations, qualitative reasoning, and planning. Then we discuss potential synergies that would benefit both areas."}
{"_id":"e7b50e3f56e21fd2a5eb34923d427a0bc6dd8905","title":"Coupling Matrix Synthesis for a New Class of Microwave Filter Configuration","text":"In this paper a new approach to the synthesis of coupling matrices for microwave filters is presente d. The new approach represents an advance on existing direct a nd optimization methods for coupling matrix synthesis in that it will exhaustively discover all possible coupling matrix solutions for a network if more than one exists. This enables a se lection to be made of the set of coupling values, resonator frequ ency offsets, parasitic coupling tolerance etc that will be best suited to the technology it is intended to realize the microwave filter with. To demonstrate the use of the method, the case of the r cently \u2013 introduced \u2018extended box\u2019 (EB) coupling matrix configuration is taken. The EB represents a new class of filter con figuration featuring a number of important advantages, one of which is the existence of multiple coupling matrix solutions for each prototype filtering function, eg 16 for 8 degree cases. This case is taken as an example to demonstrate the use of the synthesis method \u2013 yielding one solution suitable for dual-mode realiz ation and one where some couplings are small enough to neglect. Index Terms \u2014 Coupling matrix, filter synthesis, Groebner basis, inverted characteristic, multiple solutions."}
{"_id":"5c876c7c26fec05ba1e3876f49f44de219838629","title":"An Artificial Bee Colony-Based COPE Framework for Wireless Sensor Network","text":"In wireless communication, network coding is one of the intelligent approaches to process the packets before transmitting for efficient information exchange. The goal of this work is to enhance throughput by using the intelligent technique, which may give comparatively better optimization. This paper introduces a biologically-inspired coding approach called Artificial Bee Colony Network Coding (ABC-NC), a modification in the COPE framework. The existing COPE and its variant are probabilistic approaches, which may not give good results in all of the real-time scenarios. Therefore, it needs some intelligent technique to find better packet combinations at intermediate nodes before forwarding to optimize the energy and maximize the throughput in wireless networks. This paper proposes ABC-NC over the existing COPE framework for the wireless environment."}
{"_id":"73f38ffa54ca4dff09d42cb18461187b9315a735","title":"Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition","text":"Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks."}
{"_id":"6c9b39fe6b5615a012a99eac0aaaf527343feefb","title":"What are mobile developers asking about? A large scale study using stack overflow","text":"The popularity of mobile devices has been steadily growing in recent years. These devices heavily depend on software from the underlying operating systems to the applications they run. Prior research showed that mobile software is different than traditional, large software systems. However, to date most of our research has been conducted on traditional software systems. Very little work has focused on the issues that mobile developers face. Therefore, in this paper, we use data from the popular online Q&A site, Stack Overflow, and analyze 13,232,821 posts to examine what mobile developers ask about. We employ Latent Dirichlet allocation-based topic models to help us summarize the mobile-related questions. Our findings show that developers are asking about app distribution, mobile APIs, data management, sensors and context, mobile tools, and user interface development. We also determine what popular mobile-related issues are the most difficult, explore platform specific issues, and investigate the types (e.g., what, how, or why) of questions mobile developers ask. Our findings help highlight the challenges facing mobile developers that require more attention from the software engineering research and development communities in the future and establish a novel approach for analyzing questions asked on Q&A forums."}
{"_id":"f3a1246d3a0c7de004db9ef9f312bcedb5e22532","title":"Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval","text":"Thanks to the success of deep learning, cross-modal retrieval has made significant progress recently. However, there still remains a crucial bottleneck: how to bridge the modality gap to further enhance the retrieval accuracy. In this paper, we propose a self-supervised adversarial hashing (SSAH) approach, which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition, we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods."}
{"_id":"58fcd1e5ca46415ff5b06a84fd08160e43b13205","title":"Arthrodesis with Intramedular Fixation in Posttraumatic Arthrosis of the Midfoot: A Case Report","text":"We present two middle-aged men with posttraumatic arthrosis of the midfoot. Both patients suffered from severe pain and their foot was unable to bear weight. Both were operated using new fusion bolt 6.5 mm and additional screws. In their case, arthrodesis was mandatory and effective intervention. After surgical treatment both patients were pain free and able to walk without crutches and return to daily work."}
{"_id":"4c0074521b708af5009526a8bacfab9fcdf48f96","title":"Wavelet analysis of EEG for seizure detection: Coherence and phasesynchrony estimation.","text":"This paper deals with the wavelet analysis method f or seizure detection in EEG time series and coherence estimation. The main part of the paper pr esents the basic principles of signal decomposition in connection with the EEG frequency ba nds. Wavelet analysis method has been used for detection of seizure onset. The wavelet fi ltered signal is used for the computation of spectral power ratio. The results show that our met hod can identify pre seizure, seizure, post seizure and non seizure phases. When dealing with s eizure detection and prediction problems it is important to identify seizure precursor dynamics and necessary to identify information about onset and spread of seizures. Therefore in th e second part the coherence and phase synchrony during pre seizure, seizure, post seizure an d non seizure are computed. We expect this method to provide more insight into dynamic aspects of the seizure generating process."}
{"_id":"3b1b13271544fb55c227c980f6452bb945ae58d0","title":"Evaluating Digital Forensic Options for the Apple iPad","text":"The iPod Touch, iPhone and iPad from Apple are among the most popular mobile computing platforms in use today. These devices are of forensic interest because of their high adoption rate and potential for containing digital evidence. The uniformity in their design and underlying operating system (iOS) also allows forensic tools and methods to be shared across product types. This paper analyzes the tools and methods available for conducting forensic examinations of the Apple iPad. These include commercial software products, updated methodologies based on existing jailbreaking processes and the analysis of the device backup contents provided by iTunes. While many of the available commercial tools offer promise, the results of our analysis indicate that most comprehensive examination of the iPad requires jailbreaking to perform forensic duplication and manual analysis of its media content."}
{"_id":"9c3cfc2c07a1a7e3b456db463f527340221e9f73","title":"Enhancing scholarly use of digital libraries: A comparative survey and review of bibliographic metadata ontologies","text":"The HathiTrust Research Center (HTRC) is engaged in the development of tools that will give scholars the ability to analyze the HathiTrust digital library's 14 million volume corpus. A cornerstone of the HTRC's digital infrastructure is the workset -- a kind of scholar-built research collection intended for use with the HTRC's analytics platform. Because more than 66% of the digital corpus is subject to copyright restrictions, scholarly users remain dependent upon the descriptive accounts provided by traditional metadata records in order to identify and gather together bibliographic resources for analysis. This paper compares the MADSRDF\/MODSRDF, Bibframe, schema.org, BIBO, and FaBiO ontologies by assessing their suitability for employment by the HTRC to meet scholars' needs. These include distinguishing among multiple versions of the same work; representing the complex historical and physical relationships among those versions; and identifying and providing access to finer grained bibliographic entities, e.g., poems, chapters, sections, and even smaller segments of content."}
{"_id":"ae081edc60a62b1b1d542167dbe716ce7c5ec9ff","title":"Decreased gut microbiota diversity, delayed Bacteroidetes colonisation and reduced Th1 responses in infants delivered by caesarean section.","text":"OBJECTIVE\nThe early intestinal microbiota exerts important stimuli for immune development, and a reduced microbial exposure as well as caesarean section (CS) has been associated with the development of allergic disease. Here we address how microbiota development in infants is affected by mode of delivery, and relate differences in colonisation patterns to the maturation of a balanced Th1\/Th2 immune response.\n\n\nDESIGN\nThe postnatal intestinal colonisation pattern was investigated in 24 infants, born vaginally (15) or by CS (nine). The intestinal microbiota were characterised using pyrosequencing of 16S rRNA genes at 1 week and 1, 3, 6, 12 and 24 months after birth. Venous blood levels of Th1- and Th2-associated chemokines were measured at 6, 12 and 24 months.\n\n\nRESULTS\nInfants born through CS had lower total microbiota diversity during the first 2 years of life. CS delivered infants also had a lower abundance and diversity of the Bacteroidetes phylum and were less often colonised with the Bacteroidetes phylum. Infants born through CS had significantly lower levels of the Th1-associated chemokines CXCL10 and CXCL11 in blood.\n\n\nCONCLUSIONS\nCS was associated with a lower total microbial diversity, delayed colonisation of the Bacteroidetes phylum and reduced Th1 responses during the first 2 years of life."}
{"_id":"986b967c4bb2a7c4ef753a41fc625530828be503","title":"Whole-function vectorization","text":"Data-parallel programming languages are an important component in today's parallel computing landscape. Among those are domain-specific languages like shading languages in graphics (HLSL, GLSL, RenderMan, etc.) and \"general-purpose\" languages like CUDA or OpenCL. Current implementations of those languages on CPUs solely rely on multi-threading to implement parallelism and ignore the additional intra-core parallelism provided by the SIMD instruction set of those processors (like Intel's SSE and the upcoming AVX or Larrabee instruction sets). In this paper, we discuss several aspects of implementing dataparallel languages on machines with SIMD instruction sets. Our main contribution is a language- and platform-independent code transformation that performs whole-function vectorization on low-level intermediate code given by a control flow graph in SSA form. We evaluate our technique in two scenarios: First, incorporated in a compiler for a domain-specific language used in realtime ray tracing. Second, in a stand-alone OpenCL driver. We observe average speedup factors of 3.9 for the ray tracer and factors between 0.6 and 5.2 for different OpenCL kernels."}
{"_id":"62659da8c3d0a450e6a528ad13f94f56d2621759","title":"Argumentation Theory: A Very Short Introduction","text":"Since the time of the ancient Greek philosophers and rhetoricians, argumentation theorists have searched for the requirements that make an argument correct, by some appropriate standard of proof, by examining the errors of reasoning we make when we try to use arguments. These errors have long been called fallacies, and the logic textbooks have for over 2000 years tried to help students to identify these fallacies, and to deal with them when they are encountered. The problem was that deductive logic did not seem to be much use for this purpose, and there seemed to be no other obvious formal structure that could usefully be applied to them. The radical approach taken by Hamblin (1970) was to refashion the concept of an argument to think of it not just as an arbitrarily designated set of propositions, but as a move one party makes in a dialog to offer premises that may be acceptable to another party who doubts the conclusion of the argument. Just after Hamblin's time a school of thought called informal logic grew up that wanted to take a new practical approach to teaching students skills of critical thinking by going beyond deductive logic to seek other methods for analyzing and evaluating arguments. Around the same time, an interdisciplinary group of scholars associated with the term 'argumentation', coming from fields like speech communication, joined with the informal logic group to help build up such practical methods and apply them to real examples of argumentation (Johnson and Blair, 1987). The methods that have been developed so far are still in a process of rapid evolution. More recently, improvements in them have been due to some computer scientists joining the group, and to collaborative research efforts between argumentation theorists and computer scientists. Another recent development has been the adaption of argumentation models and techniques to fields in artificial intelligence, like multi-agent systems and artificial intelligence for legal reasoning. In a short paper, it is not possible to survey all these developments. The best that can be done is to offer an introduction to some of the basic concepts and methods of argumentation theory as they have evolved to the present point, and to briefly indicate some problems and limitations in them. 1. Arguments and Argumentation There are four tasks undertaken by argumentation, or informal logic, as it is also often called: identification, analysis, evaluation and invention. The task of identification \u2026"}
{"_id":"62feb51dbb8c3a94cbfc91c950f39dc2c7506e1a","title":"Super Normal Vector for Activity Recognition Using Depth Sequences","text":"This paper presents a new framework for human activity recognition from video sequences captured by a depth camera. We cluster hypersurface normals in a depth sequence to form the polynormal which is used to jointly characterize the local motion and shape information. In order to globally capture the spatial and temporal orders, an adaptive spatio-temporal pyramid is introduced to subdivide a depth video into a set of space-time grids. We then propose a novel scheme of aggregating the low-level polynormals into the super normal vector (SNV) which can be seen as a simplified version of the Fisher kernel representation. In the extensive experiments, we achieve classification results superior to all previous published results on the four public benchmark datasets, i.e., MSRAction3D, MSRDailyActivity3D, MSRGesture3D, and MSRActionPairs3D."}
{"_id":"a90b9b5edac31a4320f2a003fef519a399b67f6b","title":"A Seed-Based Method for Generating Chinese Confusion Sets","text":"In natural language, people often misuse a word (called a \u201cconfused word\u201d) in place of other words (called \u201cconfusing words\u201d). In misspelling corrections, many approaches to finding and correcting misspelling errors are based on a simple notion called a \u201cconfusion set.\u201d The confusion set of a confused word consists of confusing words. In this article, we propose a new method of building Chinese character confusion sets.\n Our method is composed of two major phases. In the first phase, we build a list of seed confusion sets for each Chinese character, which is based on measuring similarity in character pinyin or similarity in character shape. In this phase, all confusion sets are constructed manually, and the confusion sets are organized into a graph, called a \u201cseed confusion graph\u201d (SCG), in which vertices denote characters and edges are pairs of characters in the form (confused character, confusing character).\n In the second phase, we extend the SCG by acquiring more pairs of (confused character, confusing character) from a large Chinese corpus. For this, we use several word patterns (or patterns) to generate new confusion pairs and then verify the pairs before adding them into a SCG. Comprehensive experiments show that our method of extending confusion sets is effective. Also, we shall use the confusion sets in Chinese misspelling corrections to show the utility of our method."}
{"_id":"393ddf850d806c4eeaec52a1e2ea4c4dcc5c76ee","title":"Learning Over Long Time Lags","text":"The advantage of recurrent neural networks (RNNs) in learni ng dependencies between time-series data has distinguished RNNs from other deep learning models. Recent ly, many advances are proposed in this emerging field. However, there is a lack of comprehensive review on mem ory models in RNNs in the literature. This paper provides a fundamental review on RNNs and long short te rm memory (LSTM) model. Then, provides a surveys of recent advances in different memory enhancement s and learning techniques for capturing long term dependencies in RNNs."}
{"_id":"488d861cd5122ae7e4ac89ff082b159c4889870c","title":"Ethical Artificial Intelligence","text":"First Edition Please send typo and error reports, and any other comments, to hibbard@wisc.edu."}
{"_id":"649922386f1222a2e64c1c80bcc171431c070e92","title":"Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data","text":"Part-of-speech information is a pre-requisite in many NLP algorithms. However, Twitter text is difficult to part-of-speech tag: it is noisy, with linguistic errors and idiosyncratic style. We present a detailed error analysis of existing taggers, motivating a series of tagger augmentations which are demonstrated to improve performance. We identify and evaluate techniques for improving English part-of-speech tagging performance in this genre. Further, we present a novel approach to system combination for the case where available taggers use different tagsets, based on voteconstrained bootstrapping with unlabeled data. Coupled with assigning prior probabilities to some tokens and handling of unknown words and slang, we reach 88.7% tagging accuracy (90.5% on development data). This is a new high in PTB-compatible tweet part-of-speech tagging, reducing token error by 26.8% and sentence error by 12.2%. The model, training data and tools are made available."}
{"_id":"6d23073dbb68d353f30bb97f4803cfbd66546444","title":"Ensemble Algorithms in Reinforcement Learning","text":"This paper describes several ensemble methods that combine multiple different reinforcement learning (RL) algorithms in a single agent. The aim is to enhance learning speed and final performance by combining the chosen actions or action probabilities of different RL algorithms. We designed and implemented four different ensemble methods combining the following five different RL algorithms: Q-learning, Sarsa, actor-critic (AC), QV-learning, and AC learning automaton. The intuitively designed ensemble methods, namely, majority voting (MV), rank voting, Boltzmann multiplication (BM), and Boltzmann addition, combine the policies derived from the value functions of the different RL algorithms, in contrast to previous work where ensemble methods have been used in RL for representing and learning a single value function. We show experiments on five maze problems of varying complexity; the first problem is simple, but the other four maze tasks are of a dynamic or partially observable nature. The results indicate that the BM and MV ensembles significantly outperform the single RL algorithms."}
{"_id":"2bdcc4dbf14e13d33740531ea8954463ca7e68a2","title":"Social coding in GitHub: transparency and collaboration in an open software repository","text":"Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation."}
{"_id":"b224196347525fee20677711436b0e77bc51abc2","title":"Individual Tree Segmentation from LiDAR Point Clouds for Urban Forest Inventory","text":"The objective of this study is to develop new algorithms for automated urban forest inventory at the individual tree level using LiDAR point cloud data. LiDAR data contain three-dimensional structure information that can be used to estimate tree height, base height, crown depth, and crown diameter. This allows precision urban forest inventory down to individual trees. Unlike most of the published algorithms that detect individual trees from a LiDAR-derived raster surface, we worked directly with the LiDAR point cloud data to separate individual trees and estimate tree metrics. Testing results in typical urban forests are encouraging. Future works will be oriented to synergize LiDAR data and optical imagery for urban tree characterization through data fusion techniques."}
{"_id":"46e78e418c76db11fff5563ec1905e8b616252d3","title":"Blockchained Post-Quantum Signatures","text":"Inspired by the blockchain architecture and existing Merkle tree based signature schemes, we propose BPQS, an extensible post-quantum (PQ) resistant digital signature scheme best suited to blockchain and distributed ledger technologies (DLTs). One of the unique characteristics of the protocol is that it can take advantage of application-specific chain\/graph structures in order to decrease key generation, signing and verification costs as well as signature size. Compared to recent improvements in the field, BPQS outperforms existing hash-based algorithms when a key is reused for reasonable numbers of signatures, while it supports a fallback mechanism to allow for a practically unlimited number of signatures if required. We provide an open source implementation of the scheme and benchmark it."}
{"_id":"3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","title":"Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors","text":"Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts."}
{"_id":"87c13f4ec110495837056295909ceb503158c821","title":"Peppermint oil for treatment of irritable bowel syndrome.","text":"22 AM J HEALTH-SYST PHARM | VOLUME 73 | NUMBER 2 | JANUARY 15, 2016 Peppermint oil for treatment of irritable bowel syndrome Irritable bowel syndrome (IBS) is a chronic gastrointestinal disorder that affects 5\u201315% of people worldwide. Typically, patients with IBS experience abdominal pain or discomfort and constipation or diarrhea with other gastrointestinal symptoms, such as abdominal distention and bloating; these symptoms reduce quality of life and can be difficult to control with currently available prescription agents. For these reasons, patients are often compelled to try nonprescription therapies, including natural products. In studies conducted in Australia and the United Kingdom, about 20\u201350% of patients with IBS reported using complementary and alternative medicines. Thus, it is important that healthcare professionals are knowledgeable of these therapies, including dietary supplements such as peppermint oil. Historically, the ingestion of peppermint oil has been associated with effects on the gastrointestinal tract. It is a classic essential oil known to have carminative properties (i.e., it is a naturally occurring remedy thought to help decrease bloating and gas by allowing passage of flatus) and may even be beneficial as an antiemetic. Peppermint oil comes from a perennial herb (Mentha \u00d7 piperita), a plant found across North America and Europe. Mentha \u00d7 piperita is a sterile hybrid of two herbs, spearmint (Menthaspicata) and water mint (Menthaaquatica). The main constituents of peppermint oil include menthol (35\u201355%), menthone (20\u201331%), menthyl acetate (3\u201310%), isomenthone, 1,8-cineole, limonene, b-myrcene, and carvone. Peppermint oil may be obtained through steam distillation of flowering parts that grow aboveground. It is a volatile oil, with menthol accounting for a majority of its potency. Due to the various constituents of peppermint oil, it has a variety of uses, including topical application as an antiseptic and for aches and pains, inhalation as aromatherapy, and oral formulations for flavoring or for use as digestive aids. Relaxation of intestinal muscle, both in vivo and in vitro, and relaxation of the lower esophageal sphincter have been reported with the use of peppermint oil, which is thought of as an antispasmodic that may confer benefits in conditions such as IBS. Several clinical studies of the use of peppermint oil for the treatment of"}
{"_id":"1b90ee5c846aafe7feb38b439a3e8fa212757899","title":"Detection and analysis of drive-by-download attacks and malicious JavaScript code","text":"JavaScript is a browser scripting language that allows developers to create sophisticated client-side interfaces for web applications. However, JavaScript code is also used to carry out attacks against the user's browser and its extensions. These attacks usually result in the download of additional malware that takes complete control of the victim's platform, and are, therefore, called \"drive-by downloads.\" Unfortunately, the dynamic nature of the JavaScript language and its tight integration with the browser make it difficult to detect and block malicious JavaScript code.\n This paper presents a novel approach to the detection and analysis of malicious JavaScript code. Our approach combines anomaly detection with emulation to automatically identify malicious JavaScript code and to support its analysis. We developed a system that uses a number of features and machine-learning techniques to establish the characteristics of normal JavaScript code. Then, during detection, the system is able to identify anomalous JavaScript code by emulating its behavior and comparing it to the established profiles. In addition to identifying malicious code, the system is able to support the analysis of obfuscated code and to generate detection signatures for signature-based systems. The system has been made publicly available and has been used by thousands of analysts."}
{"_id":"3032182c47b75d9c1d16877815dab8f8637631a2","title":"Beyond blacklists: learning to detect malicious web sites from suspicious URLs","text":"Malicious Web sites are a cornerstone of Internet criminal activities. As a result, there has been broad interest in developing systems to prevent the end user from visiting such sites. In this paper, we describe an approach to this problem based on automated URL classification, using statistical methods to discover the tell-tale lexical and host-based properties of malicious Web site URLs. These methods are able to learn highly predictive models by extracting and automatically analyzing tens of thousands of features potentially indicative of suspicious URLs. The resulting classifiers obtain 95-99% accuracy, detecting large numbers of malicious Web sites from their URLs, with only modest false positives."}
{"_id":"6ba2b0a92408789eec23c008a9beb1b574b42470","title":"Anomaly Based Web Phishing Page Detection","text":"Many anti-phishing schemes have recently been proposed in literature. Despite all those efforts, the threat of phishing attacks is not mitigated. One of the main reasons is that phishing attackers have the adaptability to change their tactics with little cost. In this paper, we propose a novel approach, which is independent of any specific phishing implementation. Our idea is to examine the anomalies in Web pages, in particular, the discrepancy between a Web site's identity and its structural features and HTTP transactions. It demands neither user expertise nor prior knowledge of the Web site. The evasion of our phishing detection entails high cost to the adversary. As shown by the experiments, our phishing detector functions with low miss rate and low false-positive rate"}
{"_id":"9cbe8c8ba680a4e55517a8cf322603334ac68be1","title":"Effective analysis, characterization, and detection of malicious web pages","text":"The steady evolution of the Web has paved the way for miscreants to take advantage of vulnerabilities to embed malicious content into web pages. Up on a visit, malicious web pages steal sensitive data, redirect victims to other malicious targets, or cease control of victim's system to mount future attacks. Approaches to detect malicious web pages have been reactively effective at special classes of attacks like drive-by-downloads. However, the prevalence and complexity of attacks by malicious web pages is still worrisome. The main challenges in this problem domain are (1) fine-grained capturing and characterization of attack payloads (2) evolution of web page artifacts and (3) exibility and scalability of detection techniques with a fast-changing threat landscape. To this end, we proposed a holistic approach that leverages static analysis, dynamic analysis, machine learning, and evolutionary searching and optimization to effectively analyze and detect malicious web pages. We do so by: introducing novel features to capture fine-grained snapshot of malicious web pages, holistic characterization of malicious web pages, and application of evolutionary techniques to fine-tune learning-based detection models pertinent to evolution of attack payloads. In this paper, we present key intuition and details of our approach, results obtained so far, and future work."}
{"_id":"d69ae114a54a0295fe0a882d205611a121f981e1","title":"ADAM: Detecting Intrusions by Data Mining","text":"Intrusion detection systems have traditionally been based on the characterization of an attack and the tracking of the activity on the system to see if it matches that characterization. Recently, new intrusion detection systems based on data mining are making their appearance in the field. This paper describes the design and experiences with the ADAM ( Audit Data Analysis and Mining) system, which we use as a testbed to study how useful data mining techniques can be in intrusion detection. Keywords\u2014Intrusion Detection, Data Mining, Association Rules, Classifiers."}
{"_id":"a3fe9f3b248417db3cdcf07ab6f9a63c03a6345f","title":"Robust Cell Detection and Segmentation in Histopathological Images Using Sparse Reconstruction and Stacked Denoising Autoencoders","text":"Computer-aided diagnosis (CAD) is a promising tool for accurate and consistent diagnosis and prognosis. Cell detection and segmentation are essential steps for CAD. These tasks are challenging due to variations in cell shapes, touching cells, and cluttered background. In this paper, we present a cell detection and segmentation algorithm using the sparse reconstruction with trivial templates and a stacked denoising autoencoder (sDAE). The sparse reconstruction handles the shape variations by representing a testing patch as a linear combination of shapes in the learned dictionary. Trivial templates are used to model the touching parts. The sDAE, trained with the original data and their structured labels, is used for cell segmentation. To the best of our knowledge, this is the first study to apply sparse reconstruction and sDAE with structured labels for cell detection and segmentation. The proposed method is extensively tested on two data sets containing more than 3000 cells obtained from brain tumor and lung cancer images. Our algorithm achieves the best performance compared with other state of the arts."}
{"_id":"636bb6a9fd77f6811fc0339c44542ab25ba552cc","title":"A shared \" passengers & goods \" city logistics system","text":"Many strategic planning models have been developed to help decision making in city logistics. Such models do not take into account, or very few, the flow of passengers because the considered unit does not have the same nature (a person is active and a good is passive). However, it seems fundamental to gather the goods and the passengers in one model when their respective transports interact with each other. In this context, we suggest assessing a shared passengers & goods city logistics system where the spare capacity of public transport is used to distribute goods toward the city core. We model the problem as a vehicle routing problem with transfers and give a mathematical formulation. Then we propose an Adaptive Large Neighborhood Search (ALNS) to solve it. This approach is evaluated on data sets generated following a field study in the city of La Rochelle in France."}
{"_id":"86f7488e6ad64ad3a7aab65f936c9686aee91a1a","title":"Speaker Identification using Mel Frequency Cepstral Coefficient and BPNN","text":"Speech processing is emerged as one of the important application area of digital signal processing. Various fields for research in speech processing are speech recognition, speaker recognition, speech synthesis, speech coding etc. The objective of automatic speaker recognition is to extract, characterize and recognize the information about speaker identity. Feature extraction is the first step for speaker recognition. Many algorithms are suggested\/developed by the researchers for feature extraction. In this work, the Mel Frequency Cepstrum Coefficient (MFCC) feature has been used for designing a text dependent speaker identification system. BPNN is used for identification of speaker after training the feature set from MFCC. Some modifications to the existing technique of MFCC for feature extraction are also suggested to improve the speaker recognition efficiency. Information from speech recognition can be used in various ways in state-of-the-art speaker recognition systems. This includes the obvious use of recognized words to enable the use of text-dependent speaker modeling techniques when the words spoken are not given. Furthermore, it has been shown that the choice of words and phones itself can be a useful indicator of speaker identity. Also, recognizer output enables higher-level features, in particular those related to prosodic properties of speech. Keywords\u2014 Speaker identification, BPNN, MFCC, speech processing, feature extraction, speech signal"}
{"_id":"8188d1381f8c77f7df0117fd0dab1919693c1295","title":"Language support for fast and reliable message-based communication in singularity OS","text":"Message-based communication offers the potential benefits of providing stronger specification and cleaner separation between components. Compared with shared-memory interactions, message passing has the potential disadvantages of more expensive data exchange (no direct sharing) and more complicated programming.In this paper we report on the language, verification, and run-time system features that make messages practical as the sole means of communication between processes in the Singularity operating system. We show that using advanced programming language and verification techniques, it is possible to provide and enforce strong system-wide invariants that enable efficient communication and low-overhead software-based process isolation. Furthermore, specifications on communication channels help in detecting programmer mistakes early---namely at compile-time---thereby reducing the difficulty of the message-based programming model.The paper describes our communication invariants, the language and verification features that support them, as well as implementation details of the infrastructure. A number of benchmarks show the competitiveness of this approach."}
{"_id":"6c0cfbb0e02b8d5ea3f0d6f94eb25c7b93ff3e85","title":"Timeline generation with social attention","text":"Timeline generation is an important research task which can help users to have a quick understanding of the overall evolution of any given topic. It thus attracts much attention from research communities in recent years. Nevertheless, existing work on timeline generation often ignores an important factor, the attention attracted to topics of interest (hereafter termed \"social attention\"). Without taking into consideration social attention, the generated timelines may not reflect users' collective interests. In this paper, we study how to incorporate social attention in the generation of timeline summaries. In particular, for a given topic, we capture social attention by learning users' collective interests in the form of word distributions from Twitter, which are subsequently incorporated into a unified framework for timeline summary generation. We construct four evaluation sets over six diverse topics. We demonstrate that our proposed approach is able to generate both informative and interesting timelines. Our work sheds light on the feasibility of incorporating social attention into traditional text mining tasks."}
{"_id":"998065c6747d8fb05dca5977415179e20371c3d4","title":"Analyzing the State of Static Analysis: A Large-Scale Evaluation in Open Source Software","text":"The use of automatic static analysis has been a software engineering best practice for decades. However, we still do not know a lot about its use in real-world software projects: How prevalent is the use of Automated Static Analysis Tools (ASATs) such as FindBugs and JSHint? How do developers use these tools, and how does their use evolve over time? We research these questions in two studies on nine different ASATs for Java, JavaScript, Ruby, and Python with a population of 122 and 168,214 open-source projects. To compare warnings across the ASATs, we introduce the General Defect Classification (GDC) and provide a grounded-theory-derived mapping of 1,825 ASAT-specific warnings to 16 top-level GDC classes. Our results show that ASAT use is widespread, but not ubiquitous, and that projects typically do not enforce a strict policy on ASAT use. Most ASAT configurations deviate slightly from the default, but hardly any introduce new custom analyses. Only a very small set of default ASAT analyses is widely changed. Finally, most ASAT configurations, once introduced, never change. If they do, the changes are small and have a tendency to occur within one day of the configuration's initial introduction."}
{"_id":"587f8411391bf2f9d7586eed05416977c6024dd0","title":"Microrobot Design Using Fiber Reinforced Composites","text":"Mobile microrobots with characteristic dimensions on the order of 1cm are difficult to design using either MEMS (microelectromechanical systems) technology or precision machining. This is due to the challenges associated with constructing the high strength links and high-speed, low-loss joints with micron scale features required for such systems. Here we present an entirely new framework for creating microrobots which makes novel use of composite materials. This framework includes a new fabrication process termed Smart Composite Microstructures (SCM) for integrating rigid links and large angle flexure joints through a laser micromachining and lamination process. We also present solutions to actuation and integrated wiring issues at this scale using SCM. Along with simple design rules that are customized for this process, our new complete microrobotic framework is a cheaper, quicker, and altogether superior method for creating microrobots that we hope will become the paradigm for robots at this scale."}
{"_id":"e5e989029e7e87fe8faddd233a97705547d06dda","title":"Mr. DLib's Living Lab for Scholarly Recommendations","text":"We introduce the first living lab for scholarly recommender systems. This lab allows recommender-system researchers to conduct online evaluations of their novel algorithms for scholarly recommendations, i.e., research papers, citations, conferences, research grants etc. Recommendations are delivered through the living lab \u0301s API in platforms such as reference management software and digital libraries. The living lab is built on top of the recommender system as-a-service Mr. DLib. Current partners are the reference management software JabRef and the CORE research team. We present the architecture of Mr. DLib\u2019s living lab as well as usage statistics on the first ten months of operating it. During this time, 970,517 recommendations were delivered with a mean click-through rate of 0.22%."}
{"_id":"a75a6ed085fb57762fa148b82aa47607c0d9d92c","title":"A Genetic Algorithm Approach to Dynamic Job Shop Scheduling Problems","text":"This paper describes a genetic algorithm approach to the dynamic job shop scheduling problem with jobs arriving continually. Both deterministic and stochastic models of the dynamic problem were investigated. The objective functions examined were weighted flow time, maximum tardiness, weighted tardiness, weighted lateness, weighted number of tardy jobs, and weighted earliness plus weighted tardi-ness. In the stochastic model, we further tested the approach under various manufacturing environments with respect to the machine workload, imbalance of machine workload, and due date tightness. The results indicate that the approach performs well and is robust with regard to the objective function and the manufacturing environment in comparison with priority rule approaches."}
{"_id":"7703a2c5468ecbee5b62c048339a03358ed5fe19","title":"Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping","text":"We introduce an encoder-decoder recurrent neural network model called Recurrent Neural Aligner (RNA) that can be used for sequence to sequence mapping tasks. Like connectionist temporal classification (CTC) models, RNA defines a probability distribution over target label sequences including blank labels corresponding to each time step in input. The probability of a label sequence is calculated by marginalizing over all possible blank label positions. Unlike CTC, RNA does not make a conditional independence assumption for label predictions; it uses the predicted label at time t\u22121 as an additional input to the recurrent model when predicting the label at time t. We apply this model to end-to-end speech recognition. RNA is capable of streaming recognition since the decoder does not employ attention mechanism. The model is trained on transcribed acoustic data to predict graphemes and no external language and pronunciation models are used for decoding. We employ an approximate dynamic programming method to optimize negative log likelihood, and a sampling-based sequence discriminative training technique to fine-tune the model to minimize expected word error rate. We show that the model achieves competitive accuracy without using an external language model nor doing beam search decoding."}
{"_id":"a23179010e83ebdc528b4318bcea8edace96cbe5","title":"Effective Bug Triage Based on Historical Bug-Fix Information","text":"For complex and popular software, project teams could receive a large number of bug reports. It is often tedious and costly to manually assign these bug reports to developers who have the expertise to fix the bugs. Many bug triage techniques have been proposed to automate this process. In this paper, we describe our study on applying conventional bug triage techniques to projects of different sizes. We find that the effectiveness of a bug triage technique largely depends on the size of a project team (measured in terms of the number of developers). The conventional bug triage methods become less effective when the number of developers increases. To further improve the effectiveness of bug triage for large projects, we propose a novel recommendation method called Bug Fixer, which recommends developers for a new bug report based on historical bug-fix information. Bug Fixer constructs a Developer-Component-Bug (DCB) network, which models the relationship between developers and source code components, as well as the relationship between the components and their associated bugs. A DCB network captures the knowledge of \"who fixed what, where\". For a new bug report, Bug Fixer uses a DCB network to recommend to triager a list of suitable developers who could fix this bug. We evaluate Bug Fixer on three large-scale open source projects and two smaller industrial projects. The experimental results show that the proposed method outperforms the existing methods for large projects and achieves comparable performance for small projects."}
{"_id":"0b242d5123f79defd5f775d49d8a7047ad3153bc","title":"How Important is Weight Symmetry in Backpropagation?","text":"Gradient backpropagation (BP) requires symmetric feedforward and feedback connections\u2014the same weights must be used for forward and backward passes. This \u201cweight transport problem\u201d [1] is thought to be one of the main reasons of BP\u2019s biological implausibility. Using 15 different classification datasets, we systematically study to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.\u2019s demonstration [2] but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter\u2014the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations\/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) [3] and\/or a \u201cBatch Manhattan\u201d (BM) update rule. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. ar X iv :1 51 0. 05 06 7v 3 [ cs .L G ] 2 D ec 2 01 5"}
{"_id":"cfdfb7fecab1c795a52d0d201064fe876e0aae2f","title":"Smarter universities: A vision for the fast changing digital era","text":"In this paper we analyze the current situation of education in universities, with particular reference to the European scenario. Specifically, we observe that recent evolutions, such as pervasive networking and other enabling technologies, have been dramatically changing human life, knowledge acquisition, and the way works are performed and people learn. In this societal change, universities must maintain their leading role. Historically, they set trends primarily in education but now they are called to drive the change in other aspects too, such as management, safety, and environment protection. The availability of newer and newer technology reflects on how the relevant processes should be performed in the current fast changing digital era. This leads to the adoption of a variety of smart solutions in university environments to enhance the quality of life and to improve the performances of both teachers and students. Nevertheless, we argue that being smart is not enough for a modern university. In fact, universities should better become smarter. By \u201csmarter university\u201d we mean a place where knowledge is shared between employees, teachers, students, and all stakeholders in a seamless way. In this paper we propose, and discuss a smarter university model, derived from the one designed for the development of"}
{"_id":"baca9a36855ee7e5d80a860072be24a865ec8bf1","title":"Impact of dietary fiber intake on glycemic control, cardiovascular risk factors and chronic kidney disease in Japanese patients with type 2 diabetes mellitus: the Fukuoka Diabetes Registry","text":"BACKGROUND\nDietary fiber is beneficial for the treatment of type 2 diabetes mellitus, although it is consumed differently in ethnic foods around the world. We investigated the association between dietary fiber intake and obesity, glycemic control, cardiovascular risk factors and chronic kidney disease in Japanese type 2 diabetic patients.\n\n\nMETHODS\nA total of 4,399 patients were assessed for dietary fiber intake using a brief self-administered diet history questionnaire. The associations between dietary fiber intake and various cardiovascular risk factors were investigated cross-sectionally.\n\n\nRESULTS\nBody mass index, fasting plasma glucose, HbA1c, triglyceride and high-sensitivity C-reactive protein negatively associated with dietary fiber intake after adjusting for age, sex, duration of diabetes, current smoking, current drinking, total energy intake, fat intake, saturated fatty acid intake, leisure-time physical activity and use of oral hypoglycemic agents or insulin. The homeostasis model assessment insulin sensitivity and HDL cholesterol positively associated with dietary fiber intake. Dietary fiber intake was associated with reduced prevalence of abdominal obesity, hypertension and metabolic syndrome after multivariate adjustments including obesity. Furthermore, dietary fiber intake was associated with lower prevalence of albuminuria, low estimated glomerular filtration rate and chronic kidney disease after multivariate adjustments including protein intake. Additional adjustments for obesity, hypertension or metabolic syndrome did not change these associations.\n\n\nCONCLUSION\nWe demonstrated that increased dietary fiber intake was associated with better glycemic control and more favorable cardiovascular disease risk factors including chronic kidney disease in Japanese type 2 diabetic patients. Diabetic patients should be encouraged to consume more dietary fiber in daily life."}
{"_id":"241e2b442812d843dbd30e924c2f2f6ad8e12179","title":"Concept Decompositions for Large Sparse Text Data Using Clustering","text":"Unlabeled document collections are becoming increasingly common and available; mining such data sets represents a major contemporary challenge. Using words as features, text documents are often represented as high-dimensional and sparse vectors\u2013a few thousand dimensions and a sparsity of 95 to 99% is typical. In this paper, we study a certain spherical k-means algorithm for clustering such document vectors. The algorithm outputs k disjoint clusters each with a concept vector that is the centroid of the cluster normalized to have unit Euclidean norm. As our first contribution, we empirically demonstrate that, owing to the high-dimensionality and sparsity of the text data, the clusters produced by the algorithm have a certain \u201cfractal-like\u201d and \u201cself-similar\u201d behavior. As our second contribution, we introduce concept decompositions to approximate the matrix of document vectors; these decompositions are obtained by taking the least-squares approximation onto the linear subspace spanned by all the concept vectors. We empirically establish that the approximation errors of the concept decompositions are close to the best possible, namely, to truncated singular value decompositions. As our third contribution, we show that the concept vectors are localized in the word space, are sparse, and tend towards orthonormality. In contrast, the singular vectors are global in the word space and are dense. Nonetheless, we observe the surprising fact that the linear subspaces spanned by the concept vectors and the leading singular vectors are quite close in the sense of small principal angles between them. In conclusion, the concept vectors produced by the spherical k-means algorithm constitute a powerful sparse and localized \u201cbasis\u201d for text data sets."}
{"_id":"46ba80738127b19a892cfe687ece171251abb806","title":"Attribution versus persuasion as a means for modifying behavior.","text":"The present research compared the relative effectiveness of an attribution strategy with a persuasion strategy in changing behavior. Study 1 attempted to teach fifth graders not to litter and to clean up after others. An attribution group was repeatedly told that they were neat and tidy people, a persuasion group was repeatedly told that they should be neat and tidy, and a control group received no treatment. Attribution proved considerably more effective in modifying behavior. Study 2 tried to discover whether similar effects would hold for a more central aspect of school performance, math achievement and self-esteem, and whether an attribution of ability would be as effective as an attribution of motivation. Repeatedly attributing to second graders either the ability or the motivation to do well in math proved more effective than comparable persuasion or no-treatment control groups, although a group receiving straight reinforcement for math problem-solving behavior also did well. It is suggested that persuasion often suffers because it involves a negative attribution (a person should be what he is not), while attribution generally gains because it disguises persuasive intent."}
{"_id":"ea76ec431a7dd1c9a5b7ccf9fb0a4cb13b9b5037","title":"Taslihan Virtual Reconstruction - Interactive Digital Story or a Serious Game","text":"During the Ottoman period, Taslihan was the largest accommodation complex in Sarajevo, Bosnia and Herzegovina. Today, only one wall remains as a memento of its existence. In this paper, we compare user appreciation of an interactive digital story about this building and of a serious game about Taslihan to see which application offers more knowledge and immersion while bringing this monument to life in the collective memory of the people."}
{"_id":"b7fa7e6f1ebd1653ad0431a85dc46221b8e2a367","title":"Mobile apps for science learning: Review of research","text":"This review examined articles on mobile apps for science learning published from 2007 to 2014. A qualitative content analysis was used to investigate the science mobile app research for its mobile app design, underlying theoretical foundations, and students' measured outcomes. This review found that mobile apps for science learning offered a number of similar design features, including technology-based scaffolding, location-aware functionality, visual\/audio representations, digital knowledge-construction tools, digital knowledge-sharing mechanisms, and differentiated roles. Many of the studies cited a specific theoretical foundation, predominantly situated learning theory, and applied this to the design of the mobile learning environment. The most common measured outcome was students' basic scientific knowledge or conceptual understanding. A number of recommendations came out of this review. Future studies need to make use of newer, available technologies; isolate the testing of specific app features; and develop additional strategies around using mobile apps for collaboration. Researchers need to make more explicit connections between the instructional principles and the design features of their mobile learning environment in order to better integrate theory with practice. In addition, this review noted that stronger alignment is needed between the underlying theories and measured outcomes, and more studies are needed to assess students' higher-level cognitive outcomes, cognitive load, and skill-based outcomes such as problem solving. Finally, more research is needed on how science mobile apps can be used with more varied science topics and diverse audiences. \u00a9 2015 Elsevier Ltd. All rights reserved."}
{"_id":"c4dece35bb107170c9f76fcb254a191dc15cce27","title":"Characteristics and Expected Returns in Individual Equity Options","text":"I study excess returns from selling individual equity option portfolios that are leverageadjusted monthly and delta-hedged daily. Strikingly, I find that several measures of risk rise by maturity, although expected returns decrease. Based on my analysis, I identify three new factors \u2013level, slope, and value\u2013 in option returns, which together explain the cross-sectional variation in expected returns on option portfolios formed on moneyness, maturity and option value (the spread between historical volatility and the Black-Scholes implied volatility). This three-factor model helps explain expected returns on option portfolios formed on a variety of different characteristics that include carry, VRP, volatility momentum, idiosyncratic volatility, illiquidity, etc. While the level premium appears to be a compensation for marketwide volatility and jump shocks, theories of risk-averse financial intermediaries help us to understand the slope and the value premiums."}
{"_id":"0cdf9697538c46db78a948ede0f9b0c605b71d26","title":"Survey of fraud detection techniques","text":"Due to the dramatic increase of fraud which results in loss of billions of dollars worldwide each year, several modern techniques in detecting fraud are continually developed and applied to many business fields. Fraud detection involves monitoring the behavior of populations of users in order to estimate, detect, or avoid undesirable behavior. Undesirable behavior is a broad term including delinquency, fraud, intrusion, and account defaulting. This paper presents a survey of current techniques used in credit card fraud detection, telecommunication fraud detection, and computer intrusion detection. The goal of this paper is to provide a comprehensive review of different techniques to detect frauds."}
{"_id":"271c8b6d98ec65db2e5b6b28757c66fea2a5a463","title":"Measuring emotional intelligence with the MSCEIT V2.0.","text":"Does a recently introduced ability scale adequately measure emotional intelligence (EI) skills? Using the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT; J. D. Mayer, P. Salovey, & D. R. Caruso, 2002b), the authors examined (a) whether members of a general standardization sample and emotions experts identified the same test answers as correct, (b) the test's reliability, and (c) the possible factor structures of EI. Twenty-one emotions experts endorsed many of the same answers, as did 2,112 members of the standardization sample, and exhibited superior agreement, particularly when research provides clearer answers to test questions (e.g., emotional perception in faces). The MSCEIT achieved reasonable reliability, and confirmatory factor analysis supported theoretical models of EI. These findings help clarify issues raised in earlier articles published in Emotion."}
{"_id":"29452dfebc17d6b1fa57b68971ecae85f73fd1d7","title":"A complete formalized knowledge representation model for advanced digital forensics timeline analysis","text":"Having a clear view of events that occurred over time is a difficult objective to achieve in digital investigations (DI). Event reconstruction, which allows investigators to understand the timeline of a crime, is one of the most important step of a DI process. This complex task requires exploration of a large amount of events due to the pervasiveness of new technologies nowadays. Any evidence produced at the end of the investigative process must also meet the requirements of the courts, such as reproducibility, verifiability, validation, etc. For this purpose, we propose a new methodology, supported by theoretical concepts, that can assist investigators through the whole process including the construction and the interpretation of the events describing the case. The proposed approach is based on a model which integrates knowledge of experts from the fields of digital forensics and software development to allow a semantically rich representation of events related to the incident. The main purpose of this model is to allow the analysis of these events in an automatic and efficient way. This paper describes the approach and then focuses on the main conceptual and formal aspects: a formal incident modelization and operators for timeline reconstruction and analysis. \u00a9 2014 Digital Forensics Research Workshop. Published by Elsevier Limited. All rights"}
{"_id":"ec8630ea4cc06b9a51a7aa4ba50b91ccf112437d","title":"Inverse Reinforcement Learning Based Human Behavior Modeling for Goal Recognition in Dynamic Local Network Interdiction","text":"Goal recognition is the task of inferring an agent\u2019s goals given some or all of the agent\u2019s observed actions. Among different ways of problem formulation, goal recognition can be solved as a model-based planning problem using off-theshell planners. However, obtaining accurate cost or reward models of an agent and incorporating them into the planning model becomes an issue in real applications. Towards this end, we propose an Inverse Reinforcement Learning (IRL)based opponent behavior modeling method, and apply it in the goal recognition assisted Dynamic Local Network Interdiction (DLNI) problem. We first introduce the overall framework and the DLNI problem domain of our work. After that, an IRL-based human behavior modeling method and Markov Decision Process-based goal recognition are introduced. Experimental results indicate that our learned behavior model has a higher tracking accuracy and yields better interdiction outcomes than other models."}
{"_id":"20432d7fec7b15f414f51a1e4fe1983f353eff9d","title":"Author Disambiguation using Error-driven Machine Learning with a Ranking Loss Function","text":"Author disambiguation is the problem of determining whether records in a publications database refer to the same person. A common supervised machine learning approach is to build a classifier to predict whether a pair of records is coreferent, followed by a clustering step to enforce transitivity. However, this approach ignores powerful evidence obtainable by examining sets (rather than pairs) of records, such as the number of publications or co-authors an author has. In this paper we propose a representation that enables these first-order features over sets of records. We then propose a training algorithm well-suited to this representation that is (1) error-driven in that training examples are generated from incorrect predictions on the training data, and (2) rank-based in that the classifier induces a ranking over candidate predictions. We evaluate our algorithms on three author disambiguation datasets and demonstrate error reductions of up to 60% over the standard binary classification approach."}
{"_id":"5243700bf7f0863fc9d350921515767c69f754cd","title":"Closed-Loop Deep Brain Stimulation Is Superior in Ameliorating Parkinsonism","text":"Continuous high-frequency deep brain stimulation (DBS) is a widely used therapy for advanced Parkinson's disease (PD) management. However, the mechanisms underlying DBS effects remain enigmatic and are the subject of an ongoing debate. Here, we present and test a closed-loop stimulation strategy for PD in the 1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine (MPTP) primate model of PD. Application of pallidal closed-loop stimulation leads to dissociation between changes in basal ganglia (BG) discharge rates and patterns, providing insights into PD pathophysiology. Furthermore, cortico-pallidal closed-loop stimulation has a significantly greater effect on akinesia and on cortical and pallidal discharge patterns than standard open-loop DBS and matched control stimulation paradigms. Thus, closed-loop DBS paradigms, by modulating pathological oscillatory activity rather than the discharge rate of the BG-cortical networks, may afford more effective management of advanced PD. Such strategies have the potential to be effective in additional brain disorders in which a pathological neuronal discharge pattern can be recognized."}
{"_id":"5360ea82abdc3223b0ede0179fe5842c180b70ed","title":"Scientific Table Search Using Keyword Queries","text":"Tables are common and important in scientific documents, yet most text-based document search systems do not capture structures and semantics specific to tables. How to bridge different types of mismatch between keywords queries and scientific tables and what influences ranking quality needs to be carefully investigated. This paper considers the structure of tables and gives different emphasis to table components. On the query side, thanks to external knowledge such as knowledge bases and ontologies, key concepts are extracted and used to build structured queries, and target quantity types are identified and used to expand original queries. A probabilistic framework is proposed to incorporate structural and semantic information from both query and table sides. We also construct and release TableArXiv, a high quality dataset with 105 queries and corresponding relevance judgements for scientific table search. Experiments demonstrate significantly higher accuracy overall and at the top of the rankings than several baseline methods."}
{"_id":"929a376c6fea1376baf40fc2979cfbdd867f03ab","title":"Soft decoding of JPEG 2000 compressed images using bit-rate-driven deep convolutional neural networks","text":"Lossy image compression methods always introduce various unpleasant artifacts into the compressed results, especially at low bit-rates. In recent years, many effective soft decoding methods for JPEG compressed images have been proposed. However, to the best of our knowledge, very few works have been done on soft decoding of JPEG 2000 compressed images. Inspired by the outstanding performance of Convolution Neural Network (CNN) in various computer vision tasks, we presents a soft decoding method for JPEG 2000 by using multiple bit-rate-driven deep CNNs. More specifically, in training stage, we train a series of deep CNNs using lots of high quality training images and the corresponding JPEG 2000 compressed images at different coding bit-rates. In testing stage, for an input compressed image, the CNN trained with the nearest coding bit-rate is selected to perform soft decoding. Extensive experiments demonstrate the effectiveness of the presented soft decoding framework, which greatly improves the visual quality and objective scores of JPEG 2000 compressed images."}
{"_id":"4f803c5d435bef1985477366231503f10739fe11","title":"A CORDIC processor for FFT computation and its implementation using gallium arsenide technology","text":"In this paper, the architecture and the implementation of a complex fast Fourier transform (CFFT) processor using 0.6 m gallium arsenide (GaAs) technology are presented. This processor computes a 1024-point FFT of 16 bit complex data in less than 8 s, working at a frequency beyond 700 MHz, with a power consumption of 12.5 W. The architecture of the processor is based on the COordinate Rotation DIgital Computer (CORDIC) algorithm, which avoids the use of conventional multiplicationand-accumulation (MAC) units, but evaluates the trigonometric functions using only add and shift operations. Improvements to the basic CORDIC architecture are introduced in order to reduce the area and power of the processor. This together with the use of pipelining and carry save adders produces a very regular and fast processor. The CORDIC units were fabricated and tested in order to anticipate the final performance of the processor. This work also demonstrates the maturity of GaAs technology for implementing ultrahigh-performance signal processors."}
{"_id":"03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","title":"A Convolutional Neural Network for Modelling Sentences","text":"The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline."}
{"_id":"7f7d7e7d53febd451e263784b59c1c9038474499","title":"A systematic literature review of green software metrics","text":"Green IT is getting increasing attention in software engineering research. Nevertheless energy efficiency studies have mostly focused on the hardware side of IT, the software role still requires deepening in terms of methods and techniques. Furthermore, it is necessary to understand how to assess the software\u201cgreenness\u201d for stimulating the energy efficiency awareness, since early phases of the software lifecycle. The main goal of this study is to describe and to classify metrics related to software \u201cgreenness\u201d present in the software engineering literature. Furthermore, this study analyzes the evolution of those metrics, in terms of type, context, and evaluation methods. To achieve this goal, a systematic literature review has been performed surveying the metrics claimed in the last decade. After examined 960 publications, we selected 23 of them as primary studies, from which we isolated extracting 96 different green metrics. Therefore, we analyzed search results in order to show what is the trend of research about green software metrics, how metrics perform measurement on resources, and what type of metrics are more appealing for defined contexts."}
{"_id":"417997271d0c310e73c6454784244445253a15a0","title":"Enabling network programmability in LTE\/EPC architecture using OpenFlow","text":"Nowadays, mobile operators face the challenge to sustain the future data tsunami. In fact, today's increasing data and control traffic generated by new kinds of network usage puts strain on mobile operators', without creating any corresponding equivalent revenue. In our previous work, we analyzed the 3GPP LTE\/EPC architecture and showed that a redesign of this architecture is needed to suit future network usages and to provide new revenue generating services. Moreover, we proposed a new control plane based on the OpenFlow (OF) protocol for the LTE\/EPC architecture that enables flexibility and programmability aspects. In this paper, we are interested in the programmability aspect. We show how the data plane can be easily configured thanks to OF. In addition, we evaluate the signaling load of our proposed architecture and compare it to that of 3GPP LTE\/EPC architecture. The preliminary findings suggest that managing the data plane with OF has little impact on the signaling load while the network programmability is improved."}
{"_id":"74ec3d4cbb22453ce1d128c42ea66d2bdced64d6","title":"Novel Multilevel Inverter Carrier-Based PWM Method","text":"The advent of the transformerless multilevel inverter topology has brought forth various pulsewidth modulation (PWM) schemes as a means to control the switching of the active devices in each of the multiple voltage levels in the inverter. An analysis of how existing multilevel carrier-based PWM affects switch utilization for the different levels of a diode-clamped inverter is conducted. Two novel carrier-based multilevel PWM schemes are presented which help to optimize or balance the switch utilization in multilevel inverters. A 10-kW prototype sixlevel diode-clamped inverter has been built and controlled with the novel PWM strategies proposed in this paper to act as a voltage-source inverter for a motor drive."}
{"_id":"e68a6132f5536aad264ba62052005d0eca3356d5","title":"A New Neutral-Point-Clamped PWM Inverter","text":"A new neutral-point-clamped pulsewidth modulation (PWM) inverter composed of main switching devices which operate as switches for PWM and auxiliary switching devices to clamp the output terminal potential to the neutral point potential has been developed. This inverter output contains less harmonic content as compared with that of a conventional type. Two inverters are compared analytically and experimentally. In addition, a new PWM technique suitable for an ac drive system is applied to this inverter. The neutral-point-clamped PWM inverter adopting the new PWM technique shows an excellent drive system efficiency, including motor efficiency, and is appropriate for a wide-range variable-speed drive system."}
{"_id":"ff5c193fd7142b3f426baf997b43937eca1bbbad","title":"Multilevel inverters: a survey of topologies, controls, and applications","text":"Multilevel inverter technology has emerged recently as a very important alternative in the area of high-power medium-voltage energy control. This paper presents the most important topologies like diode-clamped inverter (neutral-point clamped), capacitor-clamped (flying capacitor), and cascaded multicell with separate dc sources. Emerging topologies like asymmetric hybrid cells and soft-switched multilevel inverters are also discussed. This paper also presents the most relevant control and modulation methods developed for this family of converters: multilevel sinusoidal pulsewidth modulation, multilevel selective harmonic elimination, and space-vector modulation. Special attention is dedicated to the latest and more relevant applications of these converters such as laminators, conveyor belts, and unified power-flow controllers. The need of an active front end at the input side for those inverters supplying regenerative loads is also discussed, and the circuit topology options are also presented. Finally, the peripherally developing areas such as high-voltage high-power devices and optical sensors and other opportunities for future development are addressed."}
{"_id":"3d8a29cf3843f92bf9897c4f2d3c02d96d59540a","title":"Multilevel PWM Methods at Low Modulation Indices","text":"When utilized at low amplitude modulation indices, existing multilevel carrier-based PWM strategies have no special provisions for this operating region, and several levels of the inverter go unused. This paper proposes some novel multilevel PWM strategies to take advantage of the multiple levels in both a diodeclamped inverter and a cascaded H-bridges inverter by utilizing all of the levels in the inverter even at low modulation indices. Simulation results show what effects the different strategies have on the active device utilization. A prototype 6-level diode-clamped inverter and an 11-level cascaded H-bridges inverter have been built and controlled with the novel PWM strategies proposed in this paper."}
{"_id":"40baa5d4632d807cc5841874be73415775b500fd","title":"Multilevel Converters for Large Electric Drives","text":"Traditional two-level high-frequency pulse width modulation (PWM) inverters for motor drives have several problems associated with their high frequency switching which produces common-mode voltage and high voltage change (dV\/dt) rates to the motor windings. Multilevel inverters solve these problems because their devices can switch at a much lower frequency. Two different multilevel topologies are identified for use as a converter for electric drives, a cascade inverter with separate dc sources and a back-to-back diode clamped converter. The cascade inverter is a natural fit for large automotive allelectric drives because of the high VA ratings possible and because it uses several levels of dc voltage sources which would be available from batteries or fuel cells. The back-to-back diode clamped converter is ideal where a source of ac voltage is available such as a hybrid electric vehicle. Simulation and experimental results show the superiority of these two converters over PWM based drives."}
{"_id":"0c53ef79bb8e5ba4e6a8ebad6d453ecf3672926d","title":"Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition","text":"Traditional feature encoding scheme (e.g., Fisher vector) with local descriptors (e.g., SIFT) and recent convolutional neural networks (CNNs) are two classes of successful methods for image recognition. In this paper, we propose a hybrid representation, which leverages the discriminative capacity of CNNs and the simplicity of descriptor encoding schema for image recognition, with a focus on scene recognition. To this end, we make three main contributions from the following aspects. First, we propose a patch-level and end-to-end architecture to model the appearance of local patches, called PatchNet. PatchNet is essentially a customized network trained in a weakly supervised manner, which uses the image-level supervision to guide the patch-level feature extraction. Second, we present a hybrid visual representation, called VSAD, by utilizing the robust feature representations of PatchNet to describe local patches and exploiting the semantic probabilities of PatchNet to aggregate these local patches into a global representation. Third, based on the proposed VSAD representation, we propose a new state-of-the-art scene recognition approach, which achieves an excellent performance on two standard benchmarks: MIT Indoor67 (86.2%) and SUN397 (73.0%)."}
{"_id":"a3345798b1faf238e8d805bbe9124b0b8e0c869f","title":"Autophagy as a regulated pathway of cellular degradation.","text":"Macroautophagy is a dynamic process involving the rearrangement of subcellular membranes to sequester cytoplasm and organelles for delivery to the lysosome or vacuole where the sequestered cargo is degraded and recycled. This process takes place in all eukaryotic cells. It is highly regulated through the action of various kinases, phosphatases, and guanosine triphosphatases (GTPases). The core protein machinery that is necessary to drive formation and consumption of intermediates in the macroautophagy pathway includes a ubiquitin-like protein conjugation system and a protein complex that directs membrane docking and fusion at the lysosome or vacuole. Macroautophagy plays an important role in developmental processes, human disease, and cellular response to nutrient deprivation."}
{"_id":"d35abb53c3c64717126deff65c26d6563276df45","title":"Machine learning aided cognitive RAT selection for 5G heterogeneous networks","text":"The starring role of the Heterogeneous Networks (HetNet) strategy as the key Radio Access Network (RAN) architecture for future 5G networks poses serious challenges to the current user association (cell selection) mechanisms used in cellular networks. The max-SINR algorithm, although historically effective for performing this function, is inefficient at best and obsolete at worst in 5G HetNets. The foreseen embarrassment of riches and diversified propagation characteristics of network attachment points spanning multiple Radio Access Technologies (RAT) requires novel and creative context-aware system designs that optimize the association and routing decisions in the context of single-RAT and multi-RAT connections, respectively. This paper proposes a framework under these guidelines that relies on Machine Learning techniques at the terminal device level for Cognitive RAT Selection and presents simulation results to suppport it."}
{"_id":"895fa1357bcfa9b845945c6505a6e48070fd5d89","title":"An Anonymous Electronic Voting Protocol for Voting Over The Internet","text":"In this work we propose a secure electronic voting protocol that is suitable for large scale voting over the Internet. The protocol allows a voter to cast his or her ballot anonymously, by exchanging untraceable yet authentic messages. The protocol ensures that (i) only eligible voters are able to cast votes, (ii) a voter is able to cast only one vote, (iii) a voter is able to verify that his or her vote is counted in the final tally, (iv) nobody, other than the voter, is able to link a cast vote with a voter, and (v) if a voter decides not to cast a vote, nobody is able to cast a fraudulent vote in place of the voter. The protocol does not require the cooperation of all registered voters. Neither does it require the use of complex cryptographic techniques like threshold cryptosystems or anonymous channels for casting votes. This is in contrast to other voting protocols that have been proposed in the literature. The protocol uses three agents, other than the voters, for successful operation. However, we do not require any of these agents to be trusted. That is, the agents may be physically co-located or may collude with one another to try to commit a fraud. If a fraud is committed, it can be easily detected and proven, so that the vote can be declared null and void. Although we propose the protocol with electronic voting in mind, the protocol can be used in other applications that involve exchanging an untraceable yet authentic message. Examples of such applications are answering confidential questionnaire anonymously or anonymous financial transactions."}
{"_id":"88ccb5b72cf96c9e34940c15e070c7d69a77a98c","title":"The Love\/Hate Relationship with the C Preprocessor: An Interview Study (Artifact)","text":"The C preprocessor has received strong criticism in academia, among others regarding separation of concerns, error proneness, and code obfuscation, but is widely used in practice. Many (mostly academic) alternatives to the preprocessor exist, but have not been adopted in practice. Since developers continue to use the preprocessor despite all criticism and research, we ask how practitioners perceive the C preprocessor. We performed interviews with 40 developers, used grounded theory to analyze the data, and cross-validated the results with data from a survey among 202 developers, repository mining, and results from previous studies. In particular, we investigated four research questions related to why the preprocessor is still widely used in practice, common problems, alternatives, and the impact of undisciplined annotations. Our study shows that developers are aware of the criticism the C preprocessor receives, but use it nonetheless, mainly for portability and variability. Many developers indicate that they regularly face preprocessorrelated problems and preprocessor-related bugs. The majority of our interviewees do not see any current C-native technologies that can entirely replace the C preprocessor. However, developers tend to mitigate problems with guidelines, even though those guidelines are not enforced consistently. We report the key insights gained from our study and discuss implications for practitioners and researchers on how to better use the C preprocessor to minimize its negative impact. 1998 ACM Subject Classification D.3.4 Processors"}
{"_id":"2201c7ebc6d0365d2ec0bdd94c344f5dd269aa04","title":"Inferring Mood Instability on Social Media by Leveraging Ecological Momentary Assessments","text":"Active and passive sensing technologies are providing powerful mechanisms to track, model, and understand a range of health behaviors and well-being states. Despite yielding rich, dense and high fidelity data, current sensing technologies often require highly engineered study designs and persistent participant compliance, making them difficult to scale to large populations and to data acquisition tasks spanning extended time periods. This paper situates social media as a new passive, unobtrusive sensing technology. We propose a semi-supervised machine learning framework to combine small samples of data gathered through active sensing, with large-scale social media data to infer mood instability (MI) in individuals. Starting from a theoretically-grounded measure of MI obtained from mobile ecological momentary assessments (EMAs), we show that our model is able to infer MI in a large population of Twitter users with 96% accuracy and F-1 score. Additionally, we show that, our model predicts self-identifying Twitter users with bipolar and borderline personality disorder to exhibit twice the likelihood of high MI, compared to that in a suitable control. We discuss the implications and the potential for integrating complementary sensing capabilities to address complex research challenges in precision medicine."}
{"_id":"abd81ffe23b23bf5cfdb2f1a02b66c8e14f11581","title":"The Therapeutic Potentials of Ayahuasca: Possible Effects against Various Diseases of Civilization","text":"Ayahuasca is an Amazonian psychoactive brew of two main components. Its active agents are \u03b2-carboline and tryptamine derivatives. As a sacrament, ayahuasca is still a central element of many healing ceremonies in the Amazon Basin and its ritual consumption has become common among the mestizo populations of South America. Ayahuasca use amongst the indigenous people of the Amazon is a form of traditional medicine and cultural psychiatry. During the last two decades, the substance has become increasingly known among both scientists and laymen, and currently its use is spreading all over in the Western world. In the present paper we describe the chief characteristics of ayahuasca, discuss important questions raised about its use, and provide an overview of the scientific research supporting its potential therapeutic benefits. A growing number of studies indicate that the psychotherapeutic potential of ayahuasca is based mostly on the strong serotonergic effects, whereas the sigma-1 receptor (Sig-1R) agonist effect of its active ingredient dimethyltryptamine raises the possibility that the ethnomedical observations on the diversity of treated conditions can be scientifically verified. Moreover, in the right therapeutic or ritual setting with proper preparation and mindset of the user, followed by subsequent integration of the experience, ayahuasca has proven effective in the treatment of substance dependence. This article has two important take-home messages: (1) the therapeutic effects of ayahuasca are best understood from a bio-psycho-socio-spiritual model, and (2) on the biological level ayahuasca may act against chronic low grade inflammation and oxidative stress via the Sig-1R which can explain its widespread therapeutic indications."}
{"_id":"117601fe80cc4b7d69a18da06949279395c62292","title":"Epigenetics and the embodiment of race: developmental origins of US racial disparities in cardiovascular health.","text":"The relative contribution of genetic and environmental influences to the US black-white disparity in cardiovascular disease (CVD) is hotly debated within the public health, anthropology, and medical communities. In this article, we review evidence for developmental and epigenetic pathways linking early life environments with CVD, and critically evaluate their possible role in the origins of these racial health disparities. African Americans not only suffer from a disproportionate burden of CVD relative to whites, but also have higher rates of the perinatal health disparities now known to be the antecedents of these conditions. There is extensive evidence for a social origin to prematurity and low birth weight in African Americans, reflecting pathways such as the effects of discrimination on maternal stress physiology. In light of the inverse relationship between birth weight and adult CVD, there is now a strong rationale to consider developmental and epigenetic mechanisms as links between early life environmental factors like maternal stress during pregnancy and adult race-based health disparities in diseases like hypertension, diabetes, stroke, and coronary heart disease. The model outlined here builds upon social constructivist perspectives to highlight an important set of mechanisms by which social influences can become embodied, having durable and even transgenerational influences on the most pressing US health disparities. We conclude that environmentally responsive phenotypic plasticity, in combination with the better-studied acute and chronic effects of social-environmental exposures, provides a more parsimonious explanation than genetics for the persistence of CVD disparities between members of socially imposed racial categories."}
{"_id":"87f452d4e9baabda4093007a9c6bbba30c35f3e4","title":"Face spoofing detection from single images using texture and local shape analysis","text":"Current face biometric systems are vulnerable to spoofing attacks. A spoofing attack occurs when a person tries to masquerade as someone else by falsifying data and thereby gaining illegitimate access. Inspired by image quality assessment, characterisation of printing artefacts and differences in light reflection, the authors propose to approach the problem of spoofing detection from texture analysis point of view. Indeed, face prints usually contain printing quality defects that can be well detected using texture and local shape features. Hence, the authors present a novel approach based on analysing facial image for detecting whether there is a live person in front of the camera or a face print. The proposed approach analyses the texture and gradient structures of the facial images using a set of low-level feature descriptors, fast linear classification scheme and score level fusion. Compared to many previous works, the authors proposed approach is robust and does not require user-cooperation. In addition, the texture features that are used for spoofing detection can also be used for face recognition. This provides a unique feature space for coupling spoofing detection and face recognition. Extensive experimental analysis on three publicly available databases showed excellent results compared to existing works."}
{"_id":"1f6f571f29d930bc2371c9eb044e03bb8ebd86ae","title":"Subjective Well \u2010 Being and Income : Is There Any Evidence of Satiation ? *","text":"Subjective Well\u2010Being and Income: Is There Any Evidence of Satiation? Many scholars have argued that once \u201cbasic needs\u201d have been met, higher income is no longer associated with higher in subjective well-being. We assess the validity of this claim in comparisons of both rich and poor countries, and also of rich and poor people within a country. Analyzing multiple datasets, multiple definitions of \u201cbasic needs\u201d and multiple questions about well-being, we find no support for this claim. The relationship between wellbeing and income is roughly linear-log and does not diminish as incomes rise. If there is a satiation point, we are yet to reach it. JEL Classification: D6, I3, N3, O1, O4"}
{"_id":"816cce72ad9fbeb854e3ca723ded5a51dfcb9311","title":"Dynamic mixed membership blockmodel for evolving networks","text":"In a dynamic social or biological environment, interactions between the underlying actors can undergo large and systematic changes. Each actor can assume multiple roles and their degrees of affiliation to these roles can also exhibit rich temporal phenomena. We propose a state space mixed membership stochastic blockmodel which can track across time the evolving roles of the actors. We also derive an efficient variational inference procedure for our model, and apply it to the Enron email networks, and rewiring gene regulatory networks of yeast. In both cases, our model reveals interesting dynamical roles of the actors."}
{"_id":"54f573383d23275731487f2a6b45845db29dbdf8","title":"Regression approaches to voice quality controll based on one-to-many eigenvoice conversion","text":"This paper proposes techniques for flexibly controlling voice quality of converted speech from a particular source speaker based on one-to-many eigenvoice conversion (EVC). EVC realizes a voice quality control based on the manipulation of a small number of parameters, i.e., weights for eigenvectors, of an eigenvoice Gaussian mixture model (EV-GMM), which is trained with multiple parallel data sets consisting of a single source speaker and many pre-stored target speakers. However, it is difficult to control intuitively the desired voice quality with those parametersbecause each eigenvector doesn\u2019t usually represent a specific physical meaning. In order to cope with this problem, we propose regression approaches to the EVC-based voice quality controller. The tractable voice quality control of the converted speech is achieved with a low-dimensionalvoice quality control vector capturing specific voice characteristics. We conducted experimental verifications of each of the proposed approaches."}
{"_id":"edc22d9a3aba2c9d457ef16acd7e6de7a17daed4","title":"A brief survey of blackhole detection and avoidance for ZRP protocol in MANETs","text":"Within Mobile Ad-Hoc Network(MANETs) unusual categories of routing protocols have been determined. It is a group of wireless system. MANET is other susceptible to a variety of attack than wired system. Black hole attack is further meticulous intimidation to MANETs. The routing protocols of MANET is less protected and therefore consequenced the system with malicious node. There are various routing protocols being used for MANETs. All routing protocols have been briefly discussed, however, Zone Routing Protocol (ZRP) is discussed in detail. Black hole is a major security threat for MANETs. Hence, in this paper, the various techniques used for detection and avoidance of Black hole attack in MANETs using ZRP routing protocol have been discussed."}
{"_id":"08c2ba4d7183d671b9a7652256de17110b81c723","title":"A Practical Attack to De-anonymize Social Network Users","text":"Social networking sites such as Facebook, LinkedIn, and Xing have been reporting exponential growth rates and have millions of registered users. In this paper, we introduce a novel de-anonymization attack that exploits group membership information that is available on social networking sites. More precisely, we show that information about the group memberships of a user (i.e., the groups of a social network to which a user belongs) is sufficient to uniquely identify this person, or, at least, to significantly reduce the set of possible candidates. That is, rather than tracking a user's browser as with cookies, it is possible to track a person. To determine the group membership of a user, we leverage well-known web browser history stealing attacks. Thus, whenever a social network user visits a malicious website, this website can launch our de-anonymization attack and learn the identity of its visitors. The implications of our attack are manifold, since it requires a low effort and has the potential to affect millions of social networking users. We perform both a theoretical analysis and empirical measurements to demonstrate the feasibility of our attack against Xing, a medium-sized social network with more than eight million members that is mainly used for business relationships. Furthermore, we explored other, larger social networks and performed experiments that suggest that users of Facebook and LinkedIn are equally vulnerable."}
{"_id":"cf9145aa55da660a8d32bf628235c615318463bf","title":"Cryptography on FPGAs: State of the Art Implementations and Attacks","text":"In the last decade, it has become aparent that embedded systems are integral parts of our every day lives. The wireless nature of many embedded applications as well as their omnipresence has made the need for security and privacy preserving mechanisms particularly important. Thus, as FPGAs become integral parts of embedded systems, it is imperative to consider their security as a whole. This contribution provides a state-of-the-art description of security issues on FPGAs, both from the system and implementation perspectives. We discuss the advantages of reconfigurable hardware for cryptographic applications, show potential security problems of FPGAs, and provide a list of open research problems. Moreover, we summarize both public and symmetric-key algorithm implementations on FPGAs."}
{"_id":"aee27f46bca631cc9c45b3ad5032ab9b771dfefe","title":"Compliance with a structured bedside handover protocol : An observational , multicentred study \u2606","text":"Background: Bedside handover is the delivery of the nurse-to-nurse shift handover at the patient\u2019s bedside. The method is increasingly used in nursing, but the evidence concerning the implementation process and compliance to the method is limited. Objectives: To determine the compliance with a structured bedside handover protocol following ISBARR and if there were differences in compliance between wards. Design: A multicentred observational study with unannounced and non-participatory observations (n=638) one month after the implementation of a structured bedside handover protocol. Settings and participants: Observations of individual patient handovers between nurses from the morning shift and the afternoon shift in 12 nursing wards in seven hospitals in Flanders, Belgium. Methods: A tailored and structured bedside handover protocol following ISBARR was developed, and nurses were trained accordingly. One month after implementation, a minimum of 50 observations were performed with a checklist, in each participating ward. To enhance reliability, 20% of the observations were conducted by two researchers, and inter-rater agreement was calculated. Data were analysed using descriptive statistics, one-way ANOVAs and multilevel analysis. Results: Average compliance rates to the structured content protocol during bedside handovers were high (83.63%; SD 11.44%), and length of stay, the type of ward and the nursing care model were influencing contextual factors. Items that were most often omitted included identification of the patient (46.27%), the introduction of nurses (36.51%), hand hygiene (35.89%), actively involving the patient (34.44%), and using the call light (21.37%). Items concerning the exchange of clinical information (e.g., test results, reason for admittance, diagnoses) were omitted less (8.09%\u20131.45%). Absence of the patients (27.29%) and staffing issues (26.70%) accounted for more than half of the non-executed bedside handovers. On average, a bedside handover took 146 s per patient. Conclusions: When the bedside handover was delivered, compliance to the structured content was high, indicating that the execution of a bedside handover is a feasible step for nurses. The compliance rate was influenced by the patient\u2019s length of stay, the nursing care model and the type of ward, but their influence was limited. Future implementation projects on bedside handover should focus sufficiently on standard hospital procedures and patient involvement. According to the nurses, there was however a high number of situations where bedside handovers could not be delivered, perhaps indicating a reluctance in practice to use bedside"}
{"_id":"00aa499569decb4e9abe40ebedca1b318b7664a8","title":"A Novel Feature Selection Approach Based on FODPSO and SVM","text":"A novel feature selection approach is proposed to address the curse of dimensionality and reduce the redundancy of hyperspectral data. The proposed approach is based on a new binary optimization method inspired by fractional-order Darwinian particle swarm optimization (FODPSO). The overall accuracy (OA) of a support vector machine (SVM) classifier on validation samples is used as fitness values in order to evaluate the informativity of different groups of bands. In order to show the capability of the proposed method, two different applications are considered. In the first application, the proposed feature selection approach is directly carried out on the input hyperspectral data. The most informative bands selected from this step are classified by the SVM. In the second application, the main shortcoming of using attribute profiles (APs) for spectral-spatial classification is addressed. In this case, a stacked vector of the input data and an AP with all widely used attributes are created. Then, the proposed feature selection approach automatically chooses the most informative features from the stacked vector. Experimental results successfully confirm that the proposed feature selection technique works better in terms of classification accuracies and CPU processing time than other studied methods without requiring the number of desired features to be set a priori by users."}
{"_id":"ef2237aea4815107db8ed1cb62476ef30c4dfa47","title":"CMiner: Opinion Extraction and Summarization for Chinese Microblogs","text":"Sentiment analysis of microblog texts has drawn lots of attention in both the academic and industrial fields. However, most of the current work only focuses on polarity classification. In this paper, we present an opinion mining system for Chinese microblogs called CMiner. Instead of polarity classification, CMiner focuses on more complicated opinion mining tasks - opinion target extraction and opinion summarization. Novel algorithms are developed for the two tasks and integrated into the end-to-end system. CMiner can help to effectively understand the users' opinion towards different opinion targets in a microblog topic. Specially, we develop an unsupervised label propagation algorithm for opinion target extraction. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. In addition, we build an aspect-based opinion summarization framework for microblog topics. After getting the opinion targets of all the microblog messages in a topic, we cluster the opinion targets into several groups and extract representative targets and summaries for each group. A co-ranking algorithm is proposed to rank both the opinion targets and microblog sentences simultaneously. Experimental results on a benchmark dataset show the effectiveness of our system and the algorithms."}
{"_id":"91e04ecd6ddd52642fde1cd2cce7d09c7c20d695","title":"Applying an optimized switching strategy to a high gain boost converter for input current ripple cancellation","text":"This paper discusses applying various switching strategies such as conventional complementary and proportional methods on a specific converter. This converter was recently presented and it can provide high voltage gain and has current ripple cancelation ability at a preselected duty-cycle. The input current ripple is zero when the converter works at a special duty-cycle. But load disturbance leads to output voltage changes that maybe cause the duty-cycle to deviate from its preselected value. In this situation, the input current ripple cannot be completely canceled. The proposed proportional strategy is an optimized method for minimizing input current ripple at other operating duty-cycles and also it provides a voltage gain lower than conventional complementary strategy. Here, the converter's performance is analyzed under two switching strategies and the effect of various strategies has been investigated on converter parameters including output voltage and input current ripple. These considerations are verified by implementing a 100-W prototype of the proposed converter in laboratory."}
{"_id":"748eb923d2c384d2b3af82af58d2e6692ef57aa1","title":"The Text Mining Handbook: Advanced Approaches to Analyzing Unstructured Data Ronen Feldman and James Sanger (Bar-Ilan University and ABS Ventures) Cambridge, England: Cambridge University Press, 2007, xii+410 pp; hardbound, ISBN 0-521-83657-3, $70.00","text":"Text mining is a new and exciting area of computer science that tries to solve the crisis of information overload by combining techniques from data mining, machine learning, natural language processing, information retrieval, and knowledge management. The Text Mining Handbook presents a comprehensive discussion of the latest techniques in text mining and link detection. In addition to providing an in-depth examination of core text mining and link detection algorithms and operations, the book examines advanced pre-processing techniques, knowledge representation considerations, and visualization approaches, ending with real-world applications."}
{"_id":"8d9ae24c9f1e59ee8cfc4c6317671b4947c2a153","title":"A fractional open circuit voltage based maximum power point tracker for photovoltaic arrays","text":"In this paper a fractional open circuit voltage based maximum power point tracker (MPPT) for photovoltaic (PV) arrays is proposed. The fractional open circuit voltage based MPPT utilizes the fact that the PV array voltage corresponding to the maximum power exhibits a linear dependence with respect to array open circuit voltage for different irradiation and temperature levels. This method is the simplest of all the MPPT methods described in the literature. The main disadvantage of this method is that the PV array is disconnected from the load after regular intervals for the sampling of the array voltage. This results in power loss. Another disadvantage is that if the duration between two successive samplings of the array voltage, called the sampling period, is too long, there is a considerable loss. This is because the output voltage of the PV array follows the unchanged reference during one sampling period. Once a maximum power point (MPP) is tracked and a change in irradiation occurs between two successive samplings, then the new MPP is not tracked until the next sampling of the PV array voltage. This paper proposes an MPPT circuit in which the sampling interval of the PV array voltage, and the sampling period have been shortened. The sample and hold circuit, which samples and holds the MPP voltage, has also been simplified. The proposed circuit does not utilize expensive microcontroller or a digital signal processor and is thus suitable for low cost photovoltaic applications."}
{"_id":"7774b582b9b3fa50775c7fddcfac712cdeef7c97","title":"HCI Education: Innovation, Creativity and Design Thinking","text":"Human-Computer Interaction (HCI) education needs re-thinking. In this paper, we explore how and what creativity and design thinking could contribute with, if included as a part of the HCI curriculum. The findings from courses where design thinking was included, indicate that design thinking contributed to increased focus on innovation and creativity, as well as prevented too early fixation on a single solution in the initial phases of HCI design processes, fostering increased flexibility and adaptability in learning processes. The creativity and adaptability may be the best long-term foci that HCI education can add to its curriculums and offer to students when preparing them for future work"}
{"_id":"902d5bc1b1b6e35aabe3494f0165e42a918e82ed","title":"Compressed matching for feature vectors","text":"The problem of compressing a large collection of feature vectors is investigated, so that object identification can be processed on the compressed form of the features. The idea is to perform matching of a query image against an image database, using directly the compressed form of the descriptor vectors, without decompression. Specifically, we concentrate on the Scale Invariant Feature Transform (SIFT), a known object detection method, as well as on Dense SIFT and PHOW features, that contain, for each image, about 300 times as many vectors as the original SIFT. Given two feature vectors, we suggest achieving our goal by compressing them using a lossless encoding by means of a Fibonacci code, for which the pairwise matching can be done directly on the compressed files. In our experiments, this approach improves the processing time and incurs only a small loss in compression efficiency relative to standard compressors requiring a decoding phase."}
{"_id":"2fef3ba4f888855e1d087572003553f485414ef1","title":"Revisit Behavior in Social Media: The Phoenix-R Model and Discoveries","text":"How many listens will an artist receive on a online radio? How about plays on a YouTube video? How many of these visits are new or returning users? Modeling and mining popularity dynamics of social activity has important implications for researchers, content creators and providers. We here investigate the effect of revisits (successive visits from a single user) on content popularity. Using four datasets of social activity, with up to tens of millions media objects (e.g., YouTube videos, Twitter hashtags or LastFM artists), we show the effect of revisits in the popularity evolution of such objects. Secondly, we propose the PHOENIX-R model which captures the popularity dynamics of individual objects. PHOENIX-R has the desired properties of being: (1) parsimonious, being based on the minimum description length principle, and achieving lower root mean squared error than state-of-the-art baselines; (2) applicable, the model is effective for predicting future popularity values of objects."}
{"_id":"0757817bf5714bb91c3d4f30cf3144e0837e57e5","title":"Tangible Bits: Towards Seamless Interfaces between People, Bits and Atoms","text":"This paper presents our vision of Human Computer Interaction (HCI): \"Tangible Bits.\" Tangible Bits allows users to \"grasp & manipulate\" bits in the center of users\u2019 attention by coupling the bits with everyday physical objects and architectural surfaces. Tangible Bits also enables users to be aware of background bits at the periphery of human perception using ambient display media such as light, sound, airflow, and water movement in an augmented space. The goal of Tangible Bits is to bridge the gaps between both cyberspace and the physical environment, as well as the foreground and background of human activities. This paper describes three key concepts of Tangible Bits: interactive surfaces; the coupling of bits with graspable physical objects; and ambient media for background awareness. We illustrate these concepts with three prototype systems \u2013 the metaDESK, transBOARD and ambientROOM \u2013 to identify underlying research issues."}
{"_id":"defbb82519f3500ec6488dfd4991c68868475afa","title":"A new weight initialization method for sigmoidal feedforward artificial neural networks","text":"Initial weight choice has been recognized to be an important aspect of the training methodology for sigmoidal feedforward neural networks. In this paper, a new mechanism for weight initialization is proposed. The mechanism distributes the initial input to output weights in a manner that all weights (including thresholds) leading into a hidden layer are uniformly distributed in a region and the center of the region from which the weights are sampled are such that no region overlaps for two distinct hidden nodes. The proposed method is compared against random weight initialization routines on five function approximation tasks using the Resilient Backpropagation (RPROP) algorithm for training. The proposed method is shown to lead to about twice as fast convergence to a pre-specifled goal for training as compared to any of the random weight initialization methods. Moreover, it is shown that at least for these problems the networks reach a deeper minima of the error functional during training and generalizes better than the networks trained whose weights were initialized by random weight initialization methods."}
{"_id":"5eb1d160d2be5467aaf08277a02d3159f5fd7a75","title":"Anisotropic diffusion map based spectral embedding for 3D CAD model retrieval","text":"In the product life cycle, design reuse can save cost and improve existing products conveniently in most new product development. To retrieve similar models from big database, most search algorithms convert CAD model into a shape descriptor and compute the similarity two models according to a descriptor metric. This paper proposes a new 3D shape matching approach by matching the coordinates directly. It is based on diffusion maps which integrate the rand walk and graph spectral analysis to extract shape features embedded in low dimensional spaces and then they are used to form coordinations for non-linear alignment of different models. These coordinates could capture multi-scale properties of the 3D geometric features and has shown good robustness to noise. The results also have shown better performance compared to the celebrated Eigenmap approach in the 3D model retrieval."}
{"_id":"f0acbfdde58c297ff1705be9e9f8e119f4ba38cc","title":"Asymmetrical duty cycle control with phase limit of LLC resonant inverter for an induction furnace","text":"This paper proposes a power control of LLC resonant inverter for an induction furnace using asymmetrical duty cycle control (ADC) with phase limit to guarantee zero voltage switching (ZVS) operation and protect switching devices from spike current during the operation. The output power can be simply controlled through duty cycle of gate signals of a full-bridge inverter. With the phase limit control, non-ZVS operation and spike current caused by a change of duty cycle with fixed frequency and load Curie's temperature can be eliminated. Theoretical and simulation analyses of the proposed method have been investigated. The experimental results of heating a 300 g of Tin from room temperature until it is melted at 232 \u00b0C provided."}
{"_id":"c8859b7ac5f466675c41561a6a299f7078a90df0","title":"Survey on Hadoop and Introduction to YARN Amogh","text":"Big Data, the analysis of large quantities of data to gain new insight has become a ubiquitous phrase in recent years. Day by day the data is growing at a staggering rate. One of the efficient technologies that deal with the Big Data is Hadoop, which will be discussed in this paper. Hadoop, for processing large data volume jobs uses MapReduce programming model. Hadoop makes use of different schedulers for executing the jobs in parallel. The default scheduler is FIFO (First In First Out) Scheduler. Other schedulers with priority, pre-emption and non-pre-emption options have also been developed. As the time has passed the MapReduce has reached few of its limitations. So in order to overcome the limitations of MapReduce, the next generation of MapReduce has been developed called as YARN (Yet Another Resource Negotiator). So, this paper provides a survey on Hadoop, few scheduling methods it uses and a brief introduction to YARN. Keywords\u2014Hadoop, HDFS, MapReduce, Schedulers, YARN."}
{"_id":"695d53820f45a0f174e51eed537c6dd4068e13ae","title":"The DEXMART hand: Mechatronic design and experimental evaluation of synergy-based control for human-like grasping","text":"This paper summarizes recent activities carried out for the development of an innovative anthropomorphic robotic hand called the DEXMART Hand. The main goal of this research is to face the problems that affect current robotic hands by introducing suitable design solutions aimed at achieving simplification and cost reduction while possibly enhancing robustness and performance. While certain aspects of the DEXMART Hand development have been presented in previous papers, this paper is the first to give a comprehensive description of the final hand version and its use to replicate humanlike grasping. In this paper, particular emphasis is placed on the kinematics of the fingers and of the thumb, the wrist architecture, the dimensioning of the actuation system, and the final implementation of the position, force and tactile sensors. The paper focuses also on how these solutions have been integrated into the mechanical structure of this innovative robotic hand to enable precise force and displacement control of the whole system. Another important aspect is the lack of suitable control tools that severely limits the development of robotic hand applications. To address this issue, a new method for the observation of human hand behavior during interaction with common day-to-day objects by means of a 3D computer vision system is presented in this work together with a strategy for mapping human hand postures to the robotic hand. A simple control strategy based on postural synergies has been used to reduce the complexity of the grasp planning problem. As a preliminary evaluation of the DEXMART Hand\u2019s capabilities, this approach has been adopted in this paper to simplify and speed up the transfer of human actions to the robotic hand, showing its effectiveness in reproducing human-like grasping."}
{"_id":"18d7a36d953480adba60c21e4b2a3f3208fedc77","title":"HERB: a home exploring robotic butler","text":"We describe the architecture, algorithms, and experiments with HERB, an autonomous mobile manipulator that performs useful manipulation tasks in the home. We present new algorithms for searching for objects, learning to navigate in cluttered dynamic indoor scenes, recognizing and registering objects accurately in high clutter using vision, manipulating doors and other constrained objects using caging grasps, grasp planning and execution in clutter, and manipulation on pose and torque constraint manifolds. S.S. Srinivasa ( ) \u00b7 D. Ferguson \u00b7 C.J. Helfrich Intel Research Pittsburgh, 4720 Forbes Avenue, Suite 410, Pittsburgh, PA 15213, USA e-mail: siddhartha.srinivasa@intel.com C.J. Helfrich e-mail: casey.j.helfrich@intel.com D. Berenson \u00b7 A. Collet \u00b7 R. Diankov \u00b7 G. Gallagher \u00b7 G. Hollinger \u00b7 J. Kuffner \u00b7 M.V. Weghe The Robotics Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA D. Berenson e-mail: dberenso@ri.cmu.edu A. Collet e-mail: acollet@ri.cmu.edu R. Diankov e-mail: rdiankov@ri.cmu.edu G. Gallagher e-mail: ggallagh@ri.cmu.edu G. Hollinger e-mail: gholling@ri.cmu.edu J. Kuffner e-mail: kuffner@ri.cmu.edu M.V. Weghe e-mail: vandeweg@ri.cmu.edu We also present numerous severe real-world test results from the integration of these algorithms into a single mobile manipulator."}
{"_id":"be9336fd5642e57b6c147c6eb97612b052fd43d4","title":"Projected texture stereo","text":"Passive stereo vision is widely used as a range sensing technology in robots, but suffers from dropouts: areas of low texture where stereo matching fails. By supplementing a stereo system with a strong texture projector, dropouts can be eliminated or reduced. This paper develops a practical stereo projector system, first by finding good patterns to project in the ideal case, then by analyzing the effects of system blur and phase noise on these patterns, and finally by designing a compact projector that is capable of good performance out to 3m in indoor scenes. The system has been implemented and has excellent depth precision and resolution, especially in the range out to 1.5m."}
{"_id":"cf2a86994505a96c19c73dbbaa4a39801bdee088","title":"Real-time 3D object pose estimation and tracking for natural landmark based visual servo","text":"A real-time solution for estimating and tracking the 3D pose of a rigid object is presented for image-based visual servo with natural landmarks. The many state-of-the-art technologies that are available for recognizing the 3D pose of an object in a natural setting are not suitable for real-time servo due to their time lags. This paper demonstrates that a real-time solution of 3D pose estimation become feasible by combining a fast tracker such as KLT [7] [8] with a method of determining the 3D coordinates of tracking points on an object at the time of SIFT based tracking point initiation, assuming that a 3D geometric model with SIFT description of an object is known a-priori. Keeping track of tracking points with KLT, removing the tracking point outliers automatically, and reinitiating the tracking points using SIFT once deteriorated, the 3D pose of an object can be estimated and tracked in real-time. This method can be applied to both mono and stereo camera based 3D pose estimation and tracking. The former guarantees higher frame rates with about 1 ms of local pose estimation, while the latter assures of more precise pose results but with about 16 ms of local pose estimation. The experimental investigations have shown the effectiveness of the proposed approach with real-time performance."}
{"_id":"0674c1e2fd78925a1baa6a28216ee05ed7b48ba0","title":"Object Recognition from Local Scale-Invariant Features","text":"Proc. of the International Conference on Computer Vision, Corfu (Sept. 1999) An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds."}
{"_id":"12cedcc79bec6403ffab5d4c85a1bf7500683eca","title":"Algorithmic Complexity in Coding Theory and the Minimum Distance Problem","text":"We startwithan overviewof algorithmiccomplexity problemsin coding theory We then show that the problemof computing the minimumdiktanceof a binaryIinwr code is NP-hard,and the correspondingdeci~\u201donproblemis W-complete. Thisconstitutes a proof of the conjecture Bedekamp, McEliece,vanTilborg, dating back to 1978. Extensionsand applicationsof this result to other problemsin codingtheqv are discussed."}
{"_id":"10ff61e6c2a99d8aafcf1706f3e88c7e2dfec188","title":"Nonparametric belief propagation","text":"Continuous quantities are ubiquitous in models of real-world phenomena, but are surprisingly difficult to reason about automatically. Probabilistic graphical models such as Bayesian networks and Markov random fields, and algorithms for approximate inference such as belief propagation (BP), have proven to be powerful tools in a wide range of applications in statistics and artificial intelligence. However, applying these methods to models with continuous variables remains a challenging task. In this work we describe an extension of BP to continuous variable models, generalizing particle filtering, and Gaussian mixture filtering techniques for time series to more complex models. We illustrate the power of the resulting nonparametric BP algorithm via two applications: kinematic tracking of visual motion and distributed localization in sensor networks."}
{"_id":"4873e56ce8bfa3d8edaa8cdc28ea3aff54b3e87c","title":"Feature-Enhanced Probabilistic Models for Diffusion Network Inference","text":"Cascading processes, such as disease contagion, viral marketing, and information diffusion, are a pervasive phenomenon in many types of networks. The problem of devising intervention strategies to facilitate or inhibit such processes has recently received considerable attention. However, a major challenge is that the underlying network is often unknown. In this paper, we revisit the problem of inferring latent network structure given observations from a diffusion process, such as the spread of trending topics in social media. We define a family of novel probabilistic models that can explain recurrent cascading behavior, and take into account not only the time differences between events but also a richer set of additional features. We show that MAP inference is tractable and can therefore scale to very large real-world networks. Further, we demonstrate the effectiveness of our approach by inferring the underlying network structure of a subset of the popular Twitter following network by analyzing the topics of a large number of messages posted by users over a 10-month period. Experimental results show that our models accurately recover the links of the Twitter network, and significantly improve the performance over previous models based entirely on time."}
{"_id":"e1bf2a95eb36afe7a6d9ae01db26ade2988226a0","title":"Map-reduce based parallel support vector machine for risk analysis","text":"Now a days people are enjoying the world of data because size and amount of the data has tremendously increased which acts like an invitation to Big data. But some of the classifier techniques like Support Vector Machine (SVM) is not able to handle the huge amount of data due to it's excessive memory requirement and unreasonable complexity in algorithm tough it is one of the most popularly used classifier in machine learning field. Hence a new technique comes into picture which performs parallel algorithm in a efficient way to work data having large scale called as PSVM. In this paper we are going to discuss a PSVM model for risk analysis which is based on map-reduce, and can easily handle a huge amount of data in a distributed manner."}
{"_id":"a47b4939945e9139ccbf37e2f232d5c57583b385","title":"Classificationof Mammographic BreastDensityUsinga CombinedClassifierParadigm Keir Bovis andSameerSingh","text":"In thispaperweinvestigateanew approach to theclassificationof mammographic imagesaccording to breasttype. The classificationof breastdensityin this studyis motivatedby its useasprior knowledge in theimageprocessingpipeline.By utilising thisknowledgeat differentstagesincludingenhanceme nt, segmentation and featureextraction, its applicationaims to increasethe sensiti vity of detectingbreastcancer . Our implementeddiscriminationof breastdensityis basedon the underlyingtexture containedwithin the breast tissueapparent on a digital mammogramandrealisedby utilising four approachesto quantifyingthe texture. Following featureextraction,we adopta variationon bootstrapaggregation(\u2019bagging\u2019) to meetthe assumptionsof independencein datarepresentationof theinputdataset,necessaryfor classifiercombination. Multiple classifierscomprisingfeed-forward Artifi cial NeuralNetwork (ANN) aresubseque ntly trainedwith the differentperturbedinput dataspacesusing10-fold cross-validation. Thesetof classifieroutputs,expressedin a probabilistic framework, aresubsequently combinedusingsix differentclassifiercombinationrulesandtheresultscompared.In this studywe examinetwo differentclassificationtasks;a four-classclassificationproblem differentiatingbetweenfatty, partly fatty, denseandextremelydensebreasttypesanda two-classproblems, differentiatingbetweendenseandfatty breasttypes. The datasetusedin this study is the Digital Database of ScreeningMammograms(DDSM) containingMedio-LateralOblique(MLO) views for eachbreastfor 377 patients. For both tasksthe bestcombinationstrategy was found using the productrule giving an average recognition rateon testof 71.4%for thefour-classproblemand96.7%for thetwo-classproblem."}
{"_id":"11bf72f89874b3bf3e950952543c96bf533d3399","title":"DTC-SVM Scheme for Induction Motors Fedwith a Three-level Inverter","text":"Direct Torque Control is a control technique in AC drive systems to obtain high performance torque control. The conventional DTC drive contains a pair of hysteresis comparators. DTC drives utilizing hysteresis comparators suffer from high torque ripple and variable switching frequency. The most common solution to those problems is to use the space vector depends on the reference torque and flux. In this Paper The space vector modulation technique (SVPWM) is applied to 2 level inverter control in the proposed DTC-based induction motor drive system, thereby dramatically reducing the torque ripple. Then the controller based on space vector modulation is designed to be applied in the control of Induction Motor (IM) with a three-level Inverter. This type of Inverter has several advantages over the standard two-level VSI, such as a greater number of levels in the output voltage waveforms, Lower dV\/dt, less harmonic distortion in voltage and current waveforms and lower switching frequencies. This paper proposes a general SVPWM algorithm for three-level based on standard two-level SVPWM. The proposed scheme is described clearly and simulation results are reported to demonstrate its effectiveness. The entire control scheme is implemented with Matlab\/Simulink. Keywords\u2014Direct torque control, space vector Pulsewidth modulation(SVPWM), neutral point clamped(NPC), two-level inverter."}
{"_id":"60a9035c45fe30e4e88dad530c4f5e476cc61b78","title":"Data Mining and Knowledge Discovery : Applications , Techniques , Challenges and Process Models in Healthcare","text":"Many healthcare leaders find themselves overwhelmed with data, but lack the information they need to make right decisions. Knowledge Discovery in Databases (KDD) can help organizations turn their data into information. Organizations that take advantage of KDD techniques will find that they can lower the healthcare costs while improving healthcare quality by using fast and better clinical decision making. In this paper, a review study is done on existing data mining and knowledge discovery techniques, applications and process models that are applicable to healthcare environments. The challenges for applying data mining techniques in healthcare environment will also be discussed."}
{"_id":"528db43eb99e4d3c6b0c7ed63d17332796b4270f","title":"MPTLsim: a cycle-accurate, full-system simulator for x86-64 multicore architectures with coherent caches","text":"The introduction of multicore microprocessors in the recent years has made it imperative to use cycleaccurate and full-system simulators in the architecture research community. We introduce MPTLsim - a multicore simulator for the X86 ISA that meets this need. MPTLsim is a uop-accurate, cycle-accurate, full-system simulator for multicore designs based on the X86-64 ISA. MPTLsim extends PTLsim, a publicly available single core simulator, with a host of additional features to support hyperthreading within a core and multiple cores, with detailed models for caches, on-chip interconnections and the memory data flow. MPTLsim incorporates detailed simulation models for cache controllers, interconnections and has built-in implementations of a number of cache coherency protocols."}
{"_id":"bbb9c3119edd9daa414fd8f2df5072587bfa3462","title":"Apache Spark: a unified engine for big data processing","text":"This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications."}
{"_id":"4b41aa7f0b0eae6beff4c6d98dd3631863ec51c2","title":"Bayesian Non-Exhaustive Classification A Case Study: Online Name Disambiguation using Temporal Record Streams","text":"The name entity disambiguation task aims to partition the records of multiple real-life persons so that each partition contains records pertaining to a unique person. Most of the existing solutions for this task operate in a batch mode, where all records to be disambiguated are initially available to the algorithm. However, more realistic settings require that the name disambiguation task be performed in an online fashion, in addition to, being able to identify records of new ambiguous entities having no preexisting records. In this work, we propose a Bayesian non-exhaustive classification framework for solving online name disambiguation task. Our proposed method uses a Dirichlet process prior with a Normal x Normal x Inverse Wishart data model which enables identification of new ambiguous entities who have no records in the training data. For online classification, we use one sweep Gibbs sampler which is very efficient and effective. As a case study we consider bibliographic data in a temporal stream format and disambiguate authors by partitioning their papers into homogeneous groups. Our experimental results demonstrate that the proposed method is better than existing methods for performing online name disambiguation task."}
{"_id":"3199197df603025a328e2c0837235e590acc10b1","title":"Further improvement in reducing superficial contamination in NIRS using double short separation measurements","text":"Near-Infrared Spectroscopy (NIRS) allows the recovery of the evoked hemodynamic response to brain activation. In adult human populations, the NIRS signal is strongly contaminated by systemic interference occurring in the superficial layers of the head. An approach to overcome this difficulty is to use additional NIRS measurements with short optode separations to measure the systemic hemodynamic fluctuations occurring in the superficial layers. These measurements can then be used as regressors in the post-experiment analysis to remove the systemic contamination and isolate the brain signal. In our previous work, we showed that the systemic interference measured in NIRS is heterogeneous across the surface of the scalp. As a consequence, the short separation measurement used in the regression procedure must be located close to the standard NIRS channel from which the evoked hemodynamic response of the brain is to be recovered. Here, we demonstrate that using two short separation measurements, one at the source optode and one at the detector optode, further increases the performance of the short separation regression method compared to using a single short separation measurement. While a single short separation channel produces an average reduction in noise of 33% for HbO, using a short separation channel at both source and detector reduces noise by 59% compared to the standard method using a general linear model (GLM) without short separation. For HbR, noise reduction of 3% is achieved using a single short separation and this number goes to 47% when two short separations are used. Our work emphasizes the importance of integrating short separation measurements both at the source and at the detector optode of the standard channels from which the hemodynamic response is to be recovered. While the implementation of short separation sources presents some difficulties experimentally, the improvement in noise reduction is significant enough to justify the practical challenges."}
{"_id":"ab8799dce29812a8e04cfa01eea095515c24b963","title":"Magnetic integration of LCC compensated resonant converter for inductive power transfer applications","text":"The aim of this paper is to present a novel magnetic integrated LCC series-parallel compensation topology for the design of both the primary and pickup pads in inductive power transfer (IPT) applications. A more compact structure can be realized by integrating the inductors of the compensation circuit into the coupled power-transmitting coils. The impact of the extra coupling between the compensated coils (inductors) and the power-transferring coils is modeled and analyzed. The basic characteristics of the proposed topology are studied based on the first harmonic approximation (FHA). High-order harmonics are taken into account to derive an analytical solution for the current at the switching instant, which is helpful for the design of soft-switching operation. An IPT system with up to 5.6kW output power for electric vehicles (EV) charger has been built to verify the validity of the proposed magnetic integrated compensation topology. A peak efficiency of 95.36% from DC power source to the battery load is achieved at rated operation condition."}
{"_id":"1b1337a166cdcf6ee51a70cb23f291c36e9eee34","title":"Fine-to-Coarse Global Registration of RGB-D Scans","text":"RGB-D scanning of indoor environments is important for many applications, including real estate, interior design, and virtual reality. However, it is still challenging to register RGB-D images from a hand-held camera over a long video sequence into a globally consistent 3D model. Current methods often can lose tracking or drift and thus fail to reconstruct salient structures in large environments (e.g., parallel walls in different rooms). To address this problem, we propose a fine-to-coarse global registration algorithm that leverages robust registrations at finer scales to seed detection and enforcement of new correspondence and structural constraints at coarser scales. To test global registration algorithms, we provide a benchmark with 10,401 manually-clicked point correspondences in 25 scenes from the SUN3D dataset. During experiments with this benchmark, we find that our fine-to-coarse algorithm registers long RGB-D sequences better than previous methods."}
{"_id":"89324b37187a8a4115e7619056bca5fcf78e8928","title":"Automatically Quantifying Radiographic Knee Osteoarthritis Severity Final Report-CS 229-Machine Learning","text":"In this paper, we implement machine learning algorithms to automatically quantify knee osteoarthritis severity from X-ray images according to the Kellgren & Lawrence (KL) grades. We implement and evaluate the performance of various machine learning models like transfer learning, support vector machines and fully connected neural networks based on their classification accuracy. We also implement the task of automatically extracting the knee-joint region from the X-ray images and quantifying their severity by training a faster region convolutional neural network (R-CNN)."}
{"_id":"010d1631433bb22a9261fba477b6e6f5a0d722b8","title":"Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory","text":"Perception and expression of emotion are key factors to the success of dialogue systems or conversational agents. However, this problem has not been studied in large-scale conversation generation so far. In this paper, we propose Emotional Chatting Machine (ECM) that can generate appropriate responses not only in content (relevant and grammatical) but also in emotion (emotionally consistent). To the best of our knowledge, this is the first work that addresses the emotion factor in large-scale conversation generation. ECM addresses the factor using three new mechanisms that respectively (1) models the high-level abstraction of emotion expressions by embedding emotion categories, (2) captures the change of implicit internal emotion states, and (3) uses explicit emotion expressions with an external emotion vocabulary. Experiments show that the proposed model can generate responses appropriate not only in content but also in emotion."}
{"_id":"a907e5609bb9efd1bafb11cd424faab14fd42e4a","title":"Information Retrieval with Verbose Queries","text":"Recently, the focus of many novel search applications shifted from short keyword queries to verbose natural language queries. Examples include question answering systems and dialogue systems, voice search on mobile devices and entity search engines like Facebook's Graph Search or Google's Knowledge Graph. However the performance of textbook information retrieval techniques for such verbose queries is not as good as that for their shorter counterparts. Thus, effective handling of verbose queries has become a critical factor for adoption of information retrieval techniques in this new breed of search applications. Over the past decade, the information retrieval community has deeply explored the problem of transforming natural language verbose queries using operations like reduction, weighting, expansion, reformulation and segmentation into more effective structural representations. However, thus far, there was not a coherent and organized tutorial on this topic. In this tutorial, we aim to put together various research pieces of the puzzle, provide a comprehensive and structured overview of various proposed methods, and also list various application scenarios where effective verbose query processing can make a significant difference."}
{"_id":"600a5d60cb96eda2a9849413e747547d70dfb00a","title":"Biologically inspired protection of deep networks from adversarial attacks","text":"Inspired by biophysical principles underlying nonlinear dendritic computation in neural circuits, we develop a scheme to train deep neural networks to make them robust to adversarial attacks. Our scheme generates highly nonlinear, saturated neural networks that achieve state of the art performance on gradient based adversarial examples on MNIST, despite never being exposed to adversarially chosen examples during training. Moreover, these networks exhibit unprecedented robustness to targeted, iterative schemes for generating adversarial examples, including second-order methods. We further identify principles governing how these networks achieve their robustness, drawing on methods from information geometry. We find these networks progressively create highly flat and compressed internal representations that are sensitive to very few input dimensions, while still solving the task. Moreover, they employ highly kurtotic weight distributions, also found in the brain, and we demonstrate how such kurtosis can protect even linear classifiers from adversarial attack."}
{"_id":"87b1b46129899f328017798e315cae82e0ba70d8","title":"A fully balanced pseudo-differential OTA with common-mode feedforward and inherent common-mode feedback detector","text":"A pseudo-differential fully balanced fully symmetric CMOS operational transconductance amplifier (OTA) architecture with inherent common-mode detection is proposed. Through judicious arrangement, the common-mode feedback circuit can be economically implemented. The OTA achieves a third harmonic distortion of 43 dB for 900 mVpp at 30 MHz. The OTA, fabricated in 0.5m CMOS process, is used to design a 100-MHz fourth-order linear phase filter. The measured filter\u2019s group delay ripple is 3% for frequencies up to 100 MHz, and the measured dynamic range is 45 dB for a total harmonic distortion of 46 dB. The filter consumes 42.9 mW per complex pole pair while operating from a 1.65-V power supply."}
{"_id":"11b8093f4a8c421a8638c1be0937151d968d95f9","title":"Emergence of Fatal PRRSV Variants: Unparalleled Outbreaks of Atypical PRRS in China and Molecular Dissection of the Unique Hallmark","text":"Porcine reproductive and respiratory syndrome (PRRS) is a severe viral disease in pigs, causing great economic losses worldwide each year. The causative agent of the disease, PRRS virus (PRRSV), is a member of the family Arteriviridae. Here we report our investigation of the unparalleled large-scale outbreaks of an originally unknown, but so-called \"high fever\" disease in China in 2006 with the essence of PRRS, which spread to more than 10 provinces (autonomous cities or regions) and affected over 2,000,000 pigs with about 400,000 fatal cases. Different from the typical PRRS, numerous adult sows were also infected by the \"high fever\" disease. This atypical PRRS pandemic was initially identified as a hog cholera-like disease manifesting neurological symptoms (e.g., shivering), high fever (40-42 degrees C), erythematous blanching rash, etc. Autopsies combined with immunological analyses clearly showed that multiple organs were infected by highly pathogenic PRRSVs with severe pathological changes observed. Whole-genome analysis of the isolated viruses revealed that these PRRSV isolates are grouped into Type II and are highly homologous to HB-1, a Chinese strain of PRRSV (96.5% nucleotide identity). More importantly, we observed a unique molecular hallmark in these viral isolates, namely a discontinuous deletion of 30 amino acids in nonstructural protein 2 (NSP2). Taken together, this is the first comprehensive report documenting the 2006 epidemic of atypical PRRS outbreak in China and identifying the 30 amino-acid deletion in NSP2, a novel determining factor for virulence which may be implicated in the high pathogenicity of PRRSV, and will stimulate further study by using the infectious cDNA clone technique."}
{"_id":"c257c6948fe63f7ee3df0a2d18916d2e3fdc85e5","title":"Destination bonding : Hybrid cognition using Instagram","text":"Article history: Received August 28, 2015 Received in revised format November 28, 2015 Accepted December 1, 2015 Available online December 1, 2015 Empirical research has identified the phenomenon of destination bonding as a result of summated physical and emotional values associated with the destination. Physical values, namely natural landscape & other physical settings and emotional values, namely the enculturation processes, have a significant role to play in portraying visitors\u2019 cognitive framework for destination preference. The physical values seemed to be the stimulator for bonding that embodies action or behavior tendencies in imagery. The emotional values were the conditions that lead to affective bonding and are reflected in attitudes for a place which were evident in text narratives. Social networking on virtual platforms offers the scope for hybrid cognitive expression using imagery and text to the visitors. Instagram has emerged as an application-window to capture these hybrid cognitions of visitors. This study focuses on assessing the relationship between hybrid cognition of visitors expressed via Instagram and their bond with the destination. Further to this, the study attempts to examine the impact of hybrid cognition of visitors on the behavioral pattern of prospective visitors to the destination. The study revealed that sharing of visual imageries and related text by the visitors is an expression of the physico-emotional bonding with the destination. It was further established that hybrid cognition strongly asserts destination bonding and has been also found to have moderating impact on the link between destination bonding and electronic-word-of-mouth. \u00a9 2016 Growing Science Ltd. All rights reserved."}
{"_id":"203af6916b501ee53d9c8c7164324ef4f019ca2d","title":"Hand grasp and motion for intent expression in mid-air virtual pottery","text":"We describe the design and evaluation of a geometric interaction technique for bare-hand mid-air virtual pottery. We model the shaping of a pot as a gradual and progressive convergence of the potprofile to the shape of the user\u2019s hand represented as a point-cloud (PCL). Our pottery-inspired application served as a platform for systematically revealing how users use their hands to express the intent of deformation during a pot shaping process. Through our approach, we address two specific problems: (a) determining start and end of deformation without explicit clutching and declutching, and (b) identifying user\u2019s intent by characterizing grasp and motion of the hand on the pot. We evaluated our approach\u2019s performance in terms of intent classification, users\u2019 behavior, and users\u2019 perception of controllability. We found that the expressive capability of hand articulation can be effectively harnessed for controllable shaping by organizing the deformation process in broad classes of intended operations such as pulling, pushing and fairing. After minimal practice with the pottery application, users could figure out their own strategy for reaching, grasping and deforming the pot. Further, the use of PCL as mid-air input allows for using common physical objects as tools for pot deformation. Users particularly enjoyed this aspect of our method for shaping pots."}
{"_id":"0cc24d8308665874bddf5cb874c7fb122c249666","title":"SoftGUESS: Visualization and Exploration of Code Clones in Context","text":"We introduce SoftGUESS, a code clone exploration system. SoftGUESS is built on the more general GUESS system which provides users with a mechanism to interactively explore graph structures both through direct manipulation as well as a domain-specific language. We demonstrate SoftGUESS through a number of mini-applications to analyze evolutionary code-clone behavior in software systems. The mini-applications of SoftGUESS represent a novel way of looking at code-clones in the context of many system features. It is our hope that SoftGUESS will form the basis for other analysis tools in the software-engineering domain."}
{"_id":"2ccbb28d9f3c0f4867826f24567b4183993037b3","title":"The Diffusion of Innovations in Social Networks \u2217","text":"This paper determines how different network structures influence the diffusion of innovations. We develop a model of diffusion where: 1. an individual\u2019s decision to adopt a new technology is influenced by his contacts; and 2. contacts can discuss, coordinate, and make adoption decisions together. A measure of connectedness, \u2018cohesion\u2019, determines diffusion. A cohesive community is defined as a group in which all members have a high proportion of their contacts within the group. We show a key trade-off: on one hand, a cohesive community can hinder diffusion by blocking the spread of a technology into the group; on the other hand, cohesive communities can be particularly effective at acting collectively to adopt an innovation. We find that for technologies with low externalities (that require few people to adopt before others are willing to adopt), social structures with loose ties, where people are not part of cohesive groups, enable greater diffusion. However, as externalities increase (technologies require more people to adopt before others are willing to adopt), social structures with increasingly cohesive groups enable greater diffusion. Given that societal structure is known to differ systematically along this dimension, our findings point to specialization in technological progress exhibiting these patterns. \u2217Bryony Reich, Faculty of Economics, University College London. Email: b.reich@ucl.ac.uk. I would like to thank Alberto Alesina, Antonio Cabrales, Sanjeev Goyal, and Jorgen Weibull for their invaluable guidance and support. I benefited greatly from conversations with and comments of Marco Bassetto, Lars Nesheim and Imran Rasul. I am grateful to Jonathan Newton for numerous interactions at all stages of this project. For helpful comments I would also like to thank Lucie Gadenne, Terri Kneeland, and Sueyhun Kwon, as well as seminar participants at Alicante, Cambridge, INET Contagion Conference, Oxford, PET Luxembourg, and UCL. I gratefully acknowledge financial support from the UK Economic and Social Research Council (grant number ES\/K001396\/1)."}
{"_id":"2e3fc086ff84d6589dc91200fbfa86903a2d3b76","title":"SLANGZY: a fuzzy logic-based algorithm for English slang meaning selection","text":"The text present on online forums and social media platforms conventionally does not follow a standard sentence structure and uses words that are commonly termed as slang or Internet language. Online text mining involves a surfeit of slang words; however, there is a distinct lack of reliable resources available to find accurate meanings of these words. We aim to bridge this gap by introducing SLANGZY, a fuzzy logic-based algorithm for English slang meaning selection which uses a mathematical factor termed as \u201cslang factor\u201d to judge the accuracy of slang word definitions found in Urban Dictionary, the largest Slang Dictionary on the Internet. This slang factor is used to rank definitions of English slang words retrieved from over 4 million unique words on popular social media platforms such as Twitter, YouTube and Reddit. We investigate the usefulness of SLANGZY over Urban Dictionary to find meanings of slang words in social media text and achieve encouraging results due to recognizing the importance of multiple criteria in the calculation of slang factor in the algorithm over successive experiments. The performance of SLANGZY with optimum weights for each criterion is further assessed using the accuracy, error rate, F-Score as well as a difference factor for English slang word definitions. To further illustrate the results, a web portal is created to display the contents of the Slang Dictionary consisting of definitions ranked according to the calculated slang factors."}
{"_id":"18ca2837d280a6b2250024b6b0e59345601064a7","title":"Nonlinear dimensionality reduction by locally linear embedding.","text":"Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."}
{"_id":"949b3eb7d26afeb1585729b8a78575f2dbc925b1","title":"Feature Selection and Kernel Learning for Local Learning-Based Clustering","text":"The performance of the most clustering algorithms highly relies on the representation of data in the input space or the Hilbert space of kernel methods. This paper is to obtain an appropriate data representation through feature selection or kernel learning within the framework of the Local Learning-Based Clustering (LLC) (Wu and Scho\u0308lkopf 2006) method, which can outperform the global learning-based ones when dealing with the high-dimensional data lying on manifold. Specifically, we associate a weight to each feature or kernel and incorporate it into the built-in regularization of the LLC algorithm to take into account the relevance of each feature or kernel for the clustering. Accordingly, the weights are estimated iteratively in the clustering process. We show that the resulting weighted regularization with an additional constraint on the weights is equivalent to a known sparse-promoting penalty. Hence, the weights of those irrelevant features or kernels can be shrunk toward zero. Extensive experiments show the efficacy of the proposed methods on the benchmark data sets."}
{"_id":"a11762d3a16d3d7951f2312cb89bbedff6cfbf21","title":"An Intra-Panel Interface With Clock-Embedded Differential Signaling for TFT-LCD Systems","text":"In this paper, an intra-panel interface with a clock embedded differential signaling for TFT-LCD systems is proposed. The proposed interface reduces the number of signal lines between the timing controller and the column drivers in a TFT-LCD panel by adopting the embedded clock scheme. The protocol of the proposed interface provides a delay-locked loop (DLL)-based clock recovery scheme for the receiver. The timing controller and the column driver integrated with the proposed interface are fabricated in 0.13- \u03bcm CMOS process technology and 0.18-\u03bcm high voltage CMOS process technology, respectively. The proposed interface is verified on a 47-inch Full High-Definition (FHD) (1920RGB&times;1080) TFT-LCD panel with 8-bit RGB and 120-Hz driving technology. The maximum data rate per differential pair was measured to be as high as 2.0 Gb\/s in a wafer test."}
{"_id":"566deec44c9788ec88a8f559bab6d42a8f69c10a","title":"Low Power Magnetic Full-Adder Based on Spin Transfer Torque MRAM","text":"Power issues have become a major problem of CMOS logic circuits as technology node shrinks below 90 nm. In order to overcome this limitation, emerging logic-in-memory architecture based on nonvolatile memories (NVMs) are being investigated. Spin transfer torque (STT) magnetic random access memory (MRAM) is considered one of the most promising NVMs thanks to its high speed, low power, good endurance, and 3-D back-end integration. This paper presents a novel magnetic full-adder (MFA) design based on perpendicular magnetic anisotropy (PMA) STT-MRAM. It provides advantageous power efficiency and die area compared with conventional CMOS-only full adder (FA). Transient simulations have been performed to validate this design by using an industrial CMOS 40 nm design kit and an accurate STT-MRAM compact model including physical models and experimental measurements."}
{"_id":"d4b68acdbe65c2520fddf1b3c92268c2f7a68159","title":"Improved Shortest Path Maps with GPU Shaders","text":"We present in this paper several improvements for computing shortest path maps using OpenGL shaders [1]. The approach explores GPU rasterization as a way to propagate optimal costs on a polygonal 2D environment, producing shortest path maps which can efficiently be queried at run-time. Our improved method relies on Compute Shaders for improved performance, does not require any CPU pre-computation, and handles shortest path maps both with source points and with line segment sources. The produced path maps partition the input environment into regions sharing a same parent point along the shortest path to the closest source point or segment source. Our method produces paths with global optimality, a characteristic which has been mostly neglected in animated virtual environments. The proposed approach is particularly suitable for the animation of multiple agents moving toward the entrances or exits of a virtual environment, a situation which is efficiently represented with the proposed path maps."}
{"_id":"15bdf3f1412cd762c40ad41dee5485de38ab0120","title":"On the Robust Control of Buck-Converter DC-Motor Combinations","text":"The concepts of active disturbance rejection control and flatness-based control are used in this paper to regulate the response of a dc-to-dc buck power converter affected by unknown, exogenous, time-varying load current demands. The generalized proportional integral observer is used to estimate and cancel the time-varying disturbance signals. A key element in the proposed control for the buck converter-dc motor combination is that even if the control input gain is imprecisely known, the control strategy still provides proper regulation and tracking. The robustness of this method is further extended to the case of a double buck topology driving two different dc motors affected by different load torque disturbances. Simulation results are provided."}
{"_id":"2d78fbe680b4501b0c21fbd49eb7652592cf077d","title":"Comparative study of Proportional Integral and Backstepping controller for Buck converter","text":"This paper describes the comparative study of Proportional Integral (PI) and Backstepping controller for Buck converter with R-load and DC motor. Backstepping approach is an efficient control design procedure for both regulation and tracking problems. This approach is based upon a systematic procedure which guarantees global regulation and tracking. The proposed control scheme is to stabilize the output (voltage or speed) and tracking error to converge zero asymptotically. Buck converter system is simulated in MATLAB, using state reconstruction techniques. Simulation results of buck converter with R-load and PMDC motor reveals that, settling time of Backstepping controller is less than PI controller"}
{"_id":"bba5386f9210f2996d403f09224926d860c763d7","title":"Robust Passivity-Based Control of a Buck\u2013Boost-Converter\/DC-Motor System: An Active Disturbance Rejection Approach","text":"This paper presents an active disturbance rejection (ADR) approach for the control of a buck-boost-converter feeding a dc motor. The presence of arbitrary, time-varying, load torque inputs on the dc motor and the lack of direct measurability of the motor's angular velocity variable prompts a generalized proportional integral (GPI) observer-based ADR controller which is synthesized on the basis of passivity considerations. The GPI observer simultaneously estimates the angular velocity and the exogenous disturbance torque input in an on-line cancellation scheme, known as the ADR control. The proposed control scheme is thus a sensorless one with robustness features added to the traditional energy shaping plus damping injection methodology. The discrete switching control realization of the designed continuous feedback control law is accomplished by means of a traditional PWM-modulation scheme. Additionally, an input to state stability property of the closed-loop system is established. Experimental and simulation results are provided."}
{"_id":"d8ca9a094f56e3fe542269ea272b46f5e46bdd99","title":"Closed-Loop Analysis and Cascade Control of a Nonminimum Phase Boost Converter","text":"In this paper, a cascade controller is designed and analyzed for a boost converter. The fast inner current loop uses sliding-mode control. The slow outer voltage loop uses the proportional-integral (PI) control. Stability analysis and selection of PI gains are based on the nonlinear closed-loop error dynamics. It is proven that the closed-loop system has a nonminimum phase behavior. The voltage transients and reference voltage are predictable. The current ripple and system sensitivity are studied. The controller is validated by a simulation circuit with nonideal circuit parameters, different circuit parameters, and various maximum switching frequencies. The simulation results show that the reference output voltage is well tracked under parametric changes, system uncertainties, or external disturbances with fast dynamic transients, confirming the validity of the proposed controller."}
{"_id":"06d22950a79a839d864b575569a0de91ded33135","title":"A general approach to control a Positive Buck-Boost converter to achieve robustness against input voltage fluctuations and load changes","text":"A positive buck-boost converter is a known DC- DC converter which may be controlled to act as buck or boost converter with same polarity of the input voltage. This converter has four switching states which include all the switching states of the above mentioned DC-DC converters. In addition there is one switching state which provides a degree of freedom for the positive buck-boost converter in comparison to the buck, boost, and inverting buck-boost converters. In other words the positive buck- boost converter shows a higher level of flexibility for its inductor current control compared to the other DC-DC converters. In this paper this extra degree of freedom is utilised to increase the robustness against input voltage fluctuations and load changes. To address this capacity of the positive buck-boost converter, two different control strategies are proposed which control the inductor current and output voltage against any fluctuations in input voltage and load changes. Mathematical analysis for dynamic and steady state conditions are presented in this paper and simulation results verify the proposed method."}
{"_id":"8b0e6c02a49dcd9ef946c2af4f2f9290b8e65b2f","title":"Wideband Millimeter-Wave Surface Micromachined Tapered Slot Antenna","text":"A millimeter-wave surface micromachined tapered slot antenna (TSA) fed by an air-filled rectangular coaxial line is proposed. Detailed parametric study is conducted to determine the effects of the TSA's key structural features on its VSWR and gain. Selected exponential taper with determined growth rate and corrugation length resulted in an antenna with VSWR <; 2.5, gain > 2.75 dBi, and stable patterns over a 43-140-GHz range. The TSA is fabricated, and good correlation between modeling and W-band measurements confirms its wideband performance."}
{"_id":"60bbeedf201a2fcdf9efac19ff32aafe2e33b606","title":"The genomic landscapes of human breast and colorectal cancers.","text":"Human cancer is caused by the accumulation of mutations in oncogenes and tumor suppressor genes. To catalog the genetic changes that occur during tumorigenesis, we isolated DNA from 11 breast and 11 colorectal tumors and determined the sequences of the genes in the Reference Sequence database in these samples. Based on analysis of exons representing 20,857 transcripts from 18,191 genes, we conclude that the genomic landscapes of breast and colorectal cancers are composed of a handful of commonly mutated gene \"mountains\" and a much larger number of gene \"hills\" that are mutated at low frequency. We describe statistical and bioinformatic tools that may help identify mutations with a role in tumorigenesis. These results have implications for understanding the nature and heterogeneity of human cancers and for using personal genomics for tumor diagnosis and therapy."}
{"_id":"818c13721db30a435044b37014fe7077e5a8a587","title":"Incorporating partitioning and parallel plans into the SCOPE optimizer","text":"Massive data analysis on large clusters presents new opportunities and challenges for query optimization. Data partitioning is crucial to performance in this environment. However, data repartitioning is a very expensive operation so minimizing the number of such operations can yield very significant performance improvements. A query optimizer for this environment must therefore be able to reason about data partitioning including its interaction with sorting and grouping. SCOPE is a SQL-like scripting language used at Microsoft for massive data analysis. A transformation-based optimizer is responsible for converting scripts into efficient execution plans for the Cosmos distributed computing platform. In this paper, we describe how reasoning about data partitioning is incorporated into the SCOPE optimizer. We show how relational operators affect partitioning, sorting and grouping properties and describe how the optimizer reasons about and exploits such properties to avoid unnecessary operations. In most optimizers, consideration of parallel plans is an afterthought done in a postprocessing step. Reasoning about partitioning enables the SCOPE optimizer to fully integrate consideration of parallel, serial and mixed plans into the cost-based optimization. The benefits are illustrated by showing the variety of plans enabled by our approach."}
{"_id":"8420f2f686890d9675538ec831dbb43568af1cb3","title":"Sentiment classification of Hinglish text","text":"In order to determine the sentiment polarity of Hinglish text written in Roman script, we experimented with different combinations of feature selection methods and a host of classifiers using term frequency-inverse document frequency feature representation. We carried out in total 840 experiments in order to determine the best classifiers for sentiment expressed in the news and Facebook comments written in Hinglish. We concluded that a triumvirate of term frequency-inverse document frequency-based feature representation, gain ratio based feature selection, and Radial Basis Function Neural Network as the best combination to classify sentiment expressed in the Hinglish text."}
{"_id":"6ffe544f38ddfdba86a83805ce807f3c8e775fd6","title":"Multi words quran and hadith searching based on news using TF-IDF","text":"Each week religious leaders need to give advice to their community. Religious advice matters ideally contain discussion and solution of the problem that arising in society. But the lot of religious resources that must be considered agains many arising problems make this religious task is not easy. Especially in moslem community, the religious resources are Quran and Kutubus Sitah, the six most referenced collection of Muhammad (pbuh) news (hadith). The problem that arising in society can be read from various online mass media. Doing manually, they must know the Arabic word of the problem, and make searching manually from Mu'jam, Quran and Hadith index, then write out the found verses or hadith. TF-IDF method is often used in the weighting informational retrieval and text mining. This research want to make tools that get input from mass media news, make multi words searching from database using TF-IDF (Term Frequency - Inverse Document Frequency), and give relevan verse of Quran and hadith. Top five the most relevan verse of Quran and hadith will be displayed. Justified by religious leader, application give 60% precision for Quranic verses, and 53% for hadith, with the average query time 2.706 seconds."}
{"_id":"422a675b71f8655b266524a552e0246cb29e9bd5","title":"GALE: Geometric Active Learning for Search-Based Software Engineering","text":"Multi-objective evolutionary algorithms (MOEAs) help software engineers find novel solutions to complex problems. When automatic tools explore too many options, they are slow to use and hard to comprehend. GALE is a near-linear time MOEA that builds a piecewise approximation to the surface of best solutions along the Pareto frontier. For each piece, GALE mutates solutions towards the better end. In numerous case studies, GALE finds comparable solutions to standard methods (NSGA-II, SPEA2) using far fewer evaluations (e.g. 20 evaluations, not 1,000). GALE is recommended when a model is expensive to evaluate, or when some audience needs to browse and understand how an MOEA has made its conclusions."}
{"_id":"a390073adc9c9d23d31404a9a8eb6dac7e684857","title":"Local force cues for strength and stability in a distributed robotic construction system","text":"Construction of spatially extended, self-supporting structures requires a consideration of structural stability throughout the building sequence. For collective construction systems, where independent agents act with variable order and timing under decentralized control, ensuring stability is a particularly pronounced challenge. Previous research in this area has largely neglected considering stability during the building process. Physical forces present throughout a structure may be usable as a cue to inform agent actions as well as an indirect communication mechanism (stigmergy) to coordinate their behavior, as adding material leads to redistribution of forces which then informs the addition of further material. Here we consider in simulation a system of decentralized climbing robots capable of traversing and extending a two-dimensional truss structure, and explore the use of feedback based on force sensing as a way for the swarm to anticipate and prevent structural failures. We consider a scenario in which robots are tasked with building an unsupported cantilever across a gap, as for a bridge, where the goal is for the swarm to build any stable spanning structure rather than to construct a specific predetermined blueprint. We show that access to local force measurements enables robots to build cantilevers that span significantly farther than those built by robots without access to such information. This improvement is achieved by taking measures to maintain both strength and stability, where strength is ensured by paying attention to forces during locomotion to prevent joints from breaking, and stability is maintained by looking at how loads transfer to the ground to ensure against toppling. We show that swarms that take both kinds of forces into account have improved building performance, in both structured settings with flat ground and unpredictable environments with rough terrain."}
{"_id":"511921e775ab05a1ab0770a63e57c93da51c8526","title":"Use of AI Techniques for Residential Fire Detection in Wireless Sensor Networks","text":"Early residential fire detection is important for prompt extinguishing and reducing damages and life losses. To detect fire, one or a combination of sensors and a detection algorithm are needed. The sensors might be part of a wireless sensor network (WSN) or work independently. The previous research in the area of fire detection using WSN has paid little or no attention to investigate the optimal set of sensors as well as use of learning mechanisms and Artificial Intelligence (AI) techniques. They have only made some assumptions on what might be considered as appropriate sensor or an arbitrary AI technique has been used. By closing the gap between traditional fire detection techniques and modern wireless sensor network capabilities, in this paper we present a guideline on choosing the most optimal sensor combinations for accurate residential fire detection. Additionally, applicability of a feed forward neural network (FFNN) and Na\u00efve Bayes Classifier is investigated and results in terms of detection rate and computational complexity are analyzed."}
{"_id":"c97ebb60531a86bea516d3582758a45ba494de10","title":"Smart Cars on Smart Roads: An IEEE Intelligent Transportation Systems Society Update","text":"To promote tighter collaboration between the IEEE Intelligent Transportation Systems Society and the pervasive computing research community, the authors introduce the ITS Society and present several pervasive computing-related research topics that ITS Society researchers are working on. This department is part of a special issue on Intelligent Transportation."}
{"_id":"e91196c1d0234da60314945c4812eda631004d8f","title":"Towards Multi-Agent Communication-Based Language Learning","text":"We propose an interactive multimodal framework for language learning. Instead of being passively exposed to large amounts of natural text, our learners (implemented as feed-forward neural networks) engage in cooperative referential games starting from a tabula rasa setup, and thus develop their own language from the need to communicate in order to succeed at the game. Preliminary experiments provide promising results, but also suggest that it is important to ensure that agents trained in this way do not develop an adhoc communication code only effective for the game they are playing."}
{"_id":"6acb911d57720367d1ae7b9bce8ab9f9dcd9aadb","title":"A region-based image caption generator with refined descriptions","text":"Describing the content of an image is a challenging task. To enable detailed description, it requires the detection and recognition of objects, people, relationships and associated attributes. Currently, the majority of the existing research relies on holistic techniques, which may lose details relating to important aspects in a scene. In order to deal with such a challenge, we propose a novel region-based deep learning architecture for image description generation. It employs a regional object detector, recurrent neural network (RNN)-based attribute prediction, and an encoderdecoder language generator embedded with two RNNs to produce refined and detailed descriptions of a given image. Most importantly, the proposed system focuses on a local based approach to further improve upon existing holistic methods, which relates specifically to image regions of people and objects in an image. Evaluated with the IAPR TC-12 dataset, the proposed system shows impressive performance, and outperforms state-of-the-art methods using various evaluation metrics. In particular, the proposed system shows superiority over existing methods when dealing with cross-domain indoor scene images."}
{"_id":"385467e7747904397134c3a16d8f3a417e6b16fc","title":"3D printing: Basic concepts mathematics and technologies","text":"3D printing is the process of being able to print any object layer by layer. But if we question this proposition, can we find any three dimensional objects that can't be printed layer by layer? To banish any disbeliefs we walked together through the mathematics that prove 3d printing is feasible for any real life object. 3d printers create three dimensional objects by building them up layer by layer. The current generation of 3d printers typically requires input from a CAD program in the form of an STL file, which defines a shape by a list of triangle vertices. The vast majority of 3d printers use two techniques, FDM (Fused Deposition Modelling) and PBP (Powder Binder Printing). One advanced form of 3d printing that has been an area of increasing scientific interest the recent years is bioprinting. Cell printers utilizing techniques similar to FDM were developed for bioprinting. These printers give us the ability to place cells in positions that mimic their respective positions in organs. Finally through series of case studies we show that 3d printers in medicine have made a massive breakthrough lately."}
{"_id":"500b7d63e64e13fa47934ec9ad20fcfe0d4c17a7","title":"3D strip meander delay line structure for multilayer LTCC-based SiP applications","text":"Recently, the timing control of high-frequency signals is strongly demanded due to the high integration density in three-dimensional (3D) LTCC-based SiP applications. Therefore, to control the skew or timing delay, new 3D delay lines will be proposed. For frailty of the signal via, we adopt the concept of coaxial line and proposed an advanced signal via structure with quasi coaxial ground (QCOX-GND) vias. We will show the simulated results using EM and circuit simulator."}
{"_id":"a5eeee49f3da9bb3ce75de4c28823dddb7ed23db","title":"Visual secret sharing scheme for (k,\u00a0n) threshold based on QR code with multiple decryptions","text":"In this paper, a novel visual secret sharing (VSS) scheme based on QR code (VSSQR) with (k,\u00a0n) threshold is investigated. Our VSSQR exploits the error correction mechanism in the QR code structure, to generate the bits corresponding to shares (shadow images) by VSS from a secret bit in the processing of encoding QR. Each output share is a valid QR code that can be scanned and decoded utilizing a QR code reader, which may reduce the likelihood of attracting the attention of potential attackers. Due to different application scenarios, two different recovered ways of the secret image are given. The proposed VSS scheme based on QR code can visually reveal secret image with the abilities of stacking and XOR decryptions as well as scan every shadow image, i.e., a QR code, by a QR code reader. The secret image could be revealed by human visual system without any computation based on stacking when no lightweight computation device. On the other hand, if the lightweight computation device is available, the secret image can be revealed with better visual quality based on XOR operation and could be lossless revealed when sufficient shares are collected. In addition, it can assist alignment for VSS recovery. The experiment results show the effectiveness of our scheme."}
{"_id":"87374ee9a49ab4b51176b4155eaa6285c02463a1","title":"Use of Web 2 . 0 technologies in K-12 and higher education : The search for evidence-based practice","text":"Evidence-based practice in education entails making pedagogical decisions that are informed by relevant empirical research evidence. The main purpose of this paper is to discuss evidence-based pedagogical approaches related to the use of Web 2.0 technologies in both K-12 and higher education settings. The use of such evidence-based practice would be useful to educators interested in fostering student learning through Web 2.0 tools. A comprehensive literature search across the Academic Search Premier, Education Research Complete, ERIC, and PsycINFO databases was conducted. Empirical studies were included for review if they specifically examined the impact of Web 2.0 technologies on student learning. Articles that merely described anecdotal studies such as student perception or feeling toward learning using Web 2.0, or studies that relied on student self-report data such as student questionnaire survey and interview were excluded. Overall, the results of our review suggested that actual evidence regarding the impact of Web 2.0 technologies on student learning is as yet fairly weak. Nevertheless, the use of Web 2.0 technologies appears to have a general positive impact on student learning. None of the studies reported a detrimental or inferior effect on learning. The positive effects are not necessarily attributed to the technologies per se but to how the technologies are used, and how one conceptualizes learning. It may be tentatively concluded that a dialogic, constructionist, or coconstructive pedagogy supported by activities such as Socratic questioning, peer review and self-reflection appeared to increase student achievement in blog-, wiki-, and 3-D immersive virtual world environments, while a transmissive pedagogy supported by review activities appeared to enhance student learning using podcast. 2012 Elsevier Ltd. All rights reserved."}
{"_id":"2e0305d97f2936ee2a87b87ea901d500a8fbcd16","title":"Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals","text":"Abstract Gaussian Process (GP) regression models typically assume that residuals are Gaussian and have the same variance for all observations. However, applications with input-dependent noise (heteroscedastic residuals) frequently arise in practice, as do applications in which the residuals do not have a Gaussian distribution. In this paper, we propose a GP Regression model with a latent variable that serves as an additional unobserved covariate for the regression. This model (which we call GPLC) allows for heteroscedasticity since it allows the function to have a changing partial derivative with respect to this unobserved covariate. With a suitable covariance function, our GPLC model can handle (a) Gaussian residuals with input-dependent variance, or (b) nonGaussian residuals with input-dependent variance, or (c) Gaussian residuals with constant variance. We compare our model, using synthetic datasets, with a model proposed by Goldberg, Williams and Bishop (1998), which we refer to as GPLV, which only deals with case (a), as well as a standard GP model which can handle only case (c). Markov Chain Monte Carlo methods are developed for both modelsl. Experiments show that when the data is heteroscedastic, both GPLC and GPLV give better results (smaller mean squared error and negative log-probability density) than standard GP regression. In addition, when the residual are Gaussian, our GPLC model is generally nearly as good as GPLV, while when the residuals are non-Gaussian, our GPLC model is better than GPLV."}
{"_id":"fb7920c6b16ead15a3c0f62cf54c2af9ff9c550f","title":"Atrous Convolutional Neural Network (ACNN) for Semantic Image Segmentation with full-scale Feature Maps","text":"Deep Convolutional Neural Networks (DCNNs) are used extensively in biomedical image segmentation. However, current DCNNs usually use down sampling layers for increasing the receptive field and gaining abstract semantic information. These down sampling layers decrease the spatial dimension of feature maps, which can be detrimental to semantic image segmentation. Atrous convolution is an alternative for the down sampling layer. It increases the receptive field whilst maintains the spatial dimension of feature maps. In this paper, a method for effective atrous rate setting is proposed to achieve the largest and fully-covered receptive field with a minimum number of atrous convolutional layers. Furthermore, different atrous blocks, shortcut connections and normalization methods are explored to select the optimal network structure setting. These lead to a new and full-scale DCNN Atrous Convolutional Neural Network (ACNN), which incorporates cascaded atrous II-blocks, residual learning and Fine Group Normalization (FGN). Application results of the proposed ACNN to Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) image segmentation demonstrate that the proposed ACNN can achieve comparable segmentation Dice Similarity Coefficients (DSCs) to U-Net, optimized U-Net and hybrid network, but with significantly reduced trainable parameters due to the use of fullscale feature maps and therefore computationally is much more efficient for both the training and inference."}
{"_id":"3ad1787e6690c80f8c934c150d31b7dd6d410903","title":"Orthogonal AMP","text":"Approximate message passing (AMP) is a low-cost iterative signal recovery algorithm for linear system models. When the system transform matrix has independent identically distributed (IID) Gaussian entries, the performance of AMP can be asymptotically characterized by a simple scalar recursion called state evolution (SE). However, SE may become unreliable for other matrix ensembles, especially for ill-conditioned ones. This imposes limits on the applications of AMP. In this paper, we propose an orthogonal AMP (OAMP) algorithm based on de-correlated linear estimation (LE) and divergence-free non-linear estimation (NLE). The Onsager term in standard AMP vanishes as a result of the divergence-free constraint on NLE. We develop an SE procedure for OAMP and show numerically that the SE for OAMP is accurate for general unitarily-invariant matrices, including IID Gaussian matrices and partial orthogonal matrices. We further derive optimized options for OAMP and show that the corresponding SE fixed point coincides with the optimal performance obtained via the replica method. Our numerical results demonstrate that OAMP can be advantageous over AMP, especially for ill-conditioned matrices."}
{"_id":"cfec3fb4352ebb004b0aaf8b0a3b9869f23e7765","title":"Learning Discrete Hashing Towards Efficient Fashion Recommendation","text":"In our daily life, how to match clothing well is always a troublesome problem especially when we are shopping online to select a pair of matched pieces of clothing from tens of thousands available selections. To help common customers overcome selection issues, recent studies in the recommender system area have started to infer the fashion matching results automatically. The traditional fashion recommendation is normally achieved by considering visual similarity of clothing items or\/and item co-purchase history from existing shopping transactions. Due to the high complexity of visual features and the lack of historical item purchase records, most of the existing work is unlikely to make an efficient and accurate recommendation. To address the problem, in this paper, we propose a new model called Discrete Supervised Fashion Coordinates Hashing. Its main objective is to learn meaningful yet compact high-level features of clothing items, which are represented as binary hash codes. In detail, this learning process is supervised by a clothing matching matrix, which is initially constructed based on limited known matching pairs and subsequently on the self-augmented ones. The proposed model jointly learns the intrinsic matching patterns from the matching matrix and the binary representations from the clothing items\u2019 images, where the visual feature of each clothing item is discretized into a fixed-length binary vector. The binary representation learning significantly reduces the memory cost and accelerates the recommendation speed. The experiments compared with several state-of-the-art approaches have evidenced the superior performance of the proposed approach on efficient fashion recommendation."}
{"_id":"1df0b93bd54a104c862002210cbb2051ab3901b4","title":"Improving malware detection by applying multi-inducer ensemble","text":"Detection of malicious software (malware) using ma chine learning methods has been explored extensively to enable fas t detection of new released malware. The performance of these classifiers depen ds on the induction algorithms being used. In order to benefit from mul tiple different classifiers, and exploit their strengths we suggest using an ens emble method that will combine the results of the individual classifiers i nto one final result to achieve overall higher detection accuracy. In this paper we evaluate several combining methods using five different base inducers (C4.5 Dec ision Tree, Na\u00efve Bayes, KNN, VFI and OneR) on five malware datasets. The mai n goal is to find the best combining method for the task of detecting mal icious files in terms of accuracy, AUC and execution time."}
{"_id":"9d852855ba9b805f092b271f940848c3009a6a90","title":"Unikernel-based approach for software-defined security in cloud infrastructures","text":"The heterogeneity of cloud resources implies sub-stantial overhead to deploy and configure adequate security mechanisms. In that context, we propose a software-defined security strategy based on unikernels to support the protection of cloud infrastructures. This approach permits to address management issues by uncoupling security policy from their enforcement through programmable security interfaces. It also takes benefits from unikernel virtualization properties to support this enforcement and provide resources with low attack surface. These resources correspond to highly constrained configurations with the strict minimum for a given period. We describe the management framework supporting this software-defined security strategy, formalizing the generation of unikernel images that are dynamically built to comply with security requirements over time. Through an implementation based on MirageOS, and extensive experiments, we show that the cost induced by our security integration mechanisms is small while the gains in limiting the security exposure are high."}
{"_id":"4d767a7a672536922a6f393b4b70db8776e4821d","title":"Sentiment Classification based on Latent Dirichlet Allocation","text":"Opinion miningrefers to the use of natural language processing, text analysis and computational linguistics to identify and extract the subjective information. Opinion Mining has become an indispensible part of online reviews which is in the present scenario. In the field of information retrieval, a various kinds of probabilistic topic modeling techniques have been used to analyze contents present in a document. A topic model is a generative technique for document. All"}
{"_id":"1a07186bc10592f0330655519ad91652125cd907","title":"A unified architecture for natural language processing: deep neural networks with multitask learning","text":"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."}
{"_id":"27e38351e48fe4b7da2775bf94341738bc4da07e","title":"Semantic Compositionality through Recursive Matrix-Vector Spaces","text":"Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them."}
{"_id":"303b0b6e6812c60944a4ac9914222ac28b0813a2","title":"Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis","text":"This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify thecontextual polarityfor a large subset of sentiment expressions, achieving results that are significantly better than baseline."}
{"_id":"dcfae19ad20ee57b3f68891d8b21570ab2601613","title":"An Empirical Study on Modeling and Prediction of Bitcoin Prices With Bayesian Neural Networks Based on Blockchain Information","text":"Bitcoin has recently attracted considerable attention in the fields of economics, cryptography, and computer science due to its inherent nature of combining encryption technology and monetary units. This paper reveals the effect of Bayesian neural networks (BNNs) by analyzing the time series of Bitcoin process. We also select the most relevant features from Blockchain information that is deeply involved in Bitcoin\u2019s supply and demand and use them to train models to improve the predictive performance of the latest Bitcoin pricing process. We conduct the empirical study that compares the Bayesian neural network with other linear and non-linear benchmark models on modeling and predicting the Bitcoin process. Our empirical studies show that BNN performs well in predicting Bitcoin price time series and explaining the high volatility of the recent Bitcoin price."}
{"_id":"3bf22713709f58c8a64dd56a69257ceae8532013","title":"Robust real-time lane and road detection in critical shadow conditions","text":"This paper presents the vision-based road detection system currently installed onto the MOB-LAB land vehicle. Based on a geometrical transform and on a fast morphological processing, the system is capable to detect road markings even in extremely severe shadow conditions on at and structured roads. The use of a special-purpose massively architecture (PAPRICA) allows to achieve a processing rate of about 17 Hz."}
{"_id":"34760b63a2ae964a0b04d1850dc57002f561ddcb","title":"Decoding by linear programming","text":"This paper considers a natural error correcting problem with real valued input\/output. We wish to recover an input vector f\/spl isin\/R\/sup n\/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the \/spl lscr\/\/sub 1\/-minimization problem (\/spl par\/x\/spl par\/\/sub \/spl lscr\/1\/:=\/spl Sigma\/\/sub i\/|x\/sub i\/|) min(g\/spl isin\/R\/sup n\/) \/spl par\/y - Ag\/spl par\/\/sub \/spl lscr\/1\/ provided that the support of the vector of errors is not too large, \/spl par\/e\/spl par\/\/sub \/spl lscr\/0\/:=|{i:e\/sub i\/ \/spl ne\/ 0}|\/spl les\/\/spl rho\/\/spl middot\/m for some \/spl rho\/>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of \/spl lscr\/\/sub 1\/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail."}
{"_id":"aeb7a82c61a1733fafa2a36b9bb664ac555e5d86","title":"Chatbot for IT Security Training: Using Motivational Interviewing to Improve Security Behaviour","text":"We conduct a pre-study with 25 participants on Mechanical Turk to find out which security behavioural problems are most important for online users. These questions are based on motivational interviewing (MI), an evidence-based treatment methodology that enables to train people about different kinds of behavioural changes. Based on that the chatbot is developed using Artificial Intelligence Markup Language (AIML). The chatbot is trained to speak about three topics: passwords, privacy and secure browsing. These three topics were \u2019most-wanted\u2019 by the users of the pre-study. With the chatbot three training sessions with people are conducted."}
{"_id":"21968ae000669eb4cf03718a0d97e23a6bf75926","title":"Learning influence probabilities in social networks","text":"Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance."}
{"_id":"24986435c73066aaea0b21066db4539270106bee","title":"Novelty and redundancy detection in adaptive filtering","text":"This paper addresses the problem of extending an adaptive information filtering system to make decisions about the novelty and redundancy of relevant documents. It argues that relevance and redundance should each be modelled explicitly and separately. A set of five redundancy measures are proposed and evaluated in experiments with and without redundancy thresholds. The experimental results demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language models are both effective for identifying redundant documents."}
{"_id":"39282ff070f62ceeaa6495815098cbac8411101f","title":"Collaborative location and activity recommendations with GPS history data","text":"With the increasing popularity of location-based services, such as tour guide and location-based social network, we now have accumulated many location data on the Web. In this paper, we show that, by using the location data based on GPS and users' comments at various locations, we can discover interesting locations and possible activities that can be performed there for recommendations. Our research is highlighted in the following location-related queries in our daily life: 1) if we want to do something such as sightseeing or food-hunting in a large city such as Beijing, where should we go? 2) If we have already visited some places such as the Bird's Nest building in Beijing's Olympic park, what else can we do there? By using our system, for the first question, we can recommend her to visit a list of interesting locations such as Tiananmen Square, Bird's Nest, etc. For the second question, if the user visits Bird's Nest, we can recommend her to not only do sightseeing but also to experience its outdoor exercise facilities or try some nice food nearby. To achieve this goal, we first model the users' location and activity histories that we take as input. We then mine knowledge, such as the location features and activity-activity correlations from the geographical databases and the Web, to gather additional inputs. Finally, we apply a collective matrix factorization method to mine interesting locations and activities, and use them to recommend to the users where they can visit if they want to perform some specific activities and what they can do if they visit some specific places. We empirically evaluated our system using a large GPS dataset collected by 162 users over a period of 2.5 years in the real-world. We extensively evaluated our system and showed that our system can outperform several state-of-the-art baselines."}
{"_id":"03f9b5389df52f42cabcf0c4a9ac6e10ff6d4395","title":"A mobile application framework for the geospatial web","text":"In this paper we present an application framework that leverages geospatial content on the World Wide Web by enabling innovative modes of interaction and novel types of user interfaces on advanced mobile phones and PDAs. We discuss the current development steps involved in building mobile geospatial Web applications and derive three technological pre-requisites for our framework: spatial query operations based on visibility and field of view, a 2.5D environment model, and a presentationindependent data exchange format for geospatial query results. We propose the Local Visibility Model as a suitable XML-based candidate and present a prototype implementation."}
{"_id":"08a8c653b4f20f2b63ac6734f24fa5f5f819782a","title":"Mining interesting locations and travel sequences from GPS trajectories","text":"The increasing availability of GPS-enabled devices is changing the way people interact with the Web, and brings us a large amount of GPS trajectories representing people's location histories. In this paper, based on multiple users' GPS trajectories, we aim to mine interesting locations and classical travel sequences in a given geospatial region. Here, interesting locations mean the culturally important places, such as Tiananmen Square in Beijing, and frequented public areas, like shopping malls and restaurants, etc. Such information can help users understand surrounding locations, and would enable travel recommendation. In this work, we first model multiple individuals' location histories with a tree-based hierarchical graph (TBHG). Second, based on the TBHG, we propose a HITS (Hypertext Induced Topic Search)-based inference model, which regards an individual's access on a location as a directed link from the user to that location. This model infers the interest of a location by taking into account the following three factors. 1) The interest of a location depends on not only the number of users visiting this location but also these users' travel experiences. 2) Users' travel experiences and location interests have a mutual reinforcement relationship. 3) The interest of a location and the travel experience of a user are relative values and are region-related. Third, we mine the classical travel sequences among locations considering the interests of these locations and users' travel experiences. We evaluated our system using a large GPS dataset collected by 107 users over a period of one year in the real world. As a result, our HITS-based inference model outperformed baseline approaches like rank-by-count and rank-by-frequency. Meanwhile, when considering the users' travel experiences and location interests, we achieved a better performance beyond baselines, such as rank-by-count and rank-by-interest, etc."}
{"_id":"274ed239919074cbe96b1d9d357bb930788e6668","title":"A Framework for Evaluating Intrusion Detection Architectures in Advanced Metering Infrastructures","text":"The scale and complexity of Advanced Metering Infrastructure (AMI) networks requires careful planning for the deployment of security solutions. In particular, the large number of AMI devices and the volume and diversity of communication expected to take place on the various AMI networks make the role of intrusion detection systems (IDSes) critical. Understanding the trade-offs for a scalable and comprehensive IDS is key to investing in the right technology and deploying sensors at optimal locations. This paper reviews the benefits and costs associated with different IDS deployment options, including either centralized or distributed solution. A general cost-model framework is proposed to help utilities (AMI asset owners) make more informed decisions when selecting IDS deployment architectures and managing their security investments. We illustrate how the framework can be applied through case studies, and highlight the interesting cost\/benefit trade-offs that emerge."}
{"_id":"5a26c0ad196cd5e0a3fcd3d3a306c3ce545977ae","title":"Interactive volumetric fog display","text":"Traditional fog screens are 2D. We propose a new design of fog screen that can generate fog at different depth positions and at only where necessary. Our approach applies projection mapping onto a non-planar and reconfigurable fog screen, thus enabling interactive visual contents to be displayed at multiple depth levels. Viewers can perceive the three-dimensionality naturally, and interact with the unencumbered images by touching them directly in mid-air. The display can be used in mixed reality settings where physical objects co-exist and interact with the 3D imagery in physical space."}
{"_id":"86c00866dc78aaadf255e7a94cc4dd7219245167","title":"Adversarial Reinforcement Learning in a Cyber Security Simulation","text":"This paper focuses on cyber-security simulations in networks modeled as a Markov game with incomplete information and stochastic elements. The resulting game is an adversarial sequential decision making problem played with two agents, the attacker and defender. The two agents pit one reinforcement learning technique, like neural networks, Monte Carlo learning and Q-learning, against each other and examine their effectiveness against learning opponents. The results showed that Monte Carlo learning with the Softmax exploration strategy is most effective in performing the defender role and also for learning attacking strategies."}
{"_id":"d33d0140d8e36e23dfa88dfc40b837af44709533","title":"SALMA: Standard Arabic Language Morphological Analysis","text":"Morphological analyzers are preprocessors for text analysis. Many Text Analytics applications need them to perform their tasks. This paper reviews the SALMA-Tools (Standard Arabic Language Morphological Analysis) [1]. The SALMA-Tools is a collection of open-source standards, tools and resources that widen the scope of Arabic word structure analysis - particularly morphological analysis, to process Arabic text corpora of different domains, formats and genres, of both vowelized and non-vowelized text. Tag-assignment is significantly more complex for Arabic than for many languages. The morphological analyzer should add the appropriate linguistic information to each part or morpheme of the word (proclitic, prefix, stem, suffix and enclitic); in effect, instead of a tag for a word, we need a subtag for each part. Very fine-grained distinctions may cause problems for automatic morphosyntactic analysis - particularly probabilistic taggers which require training data, if some words can change grammatical tag depending on function and context; on the other hand, fine-grained distinctions may actually help to disambiguate other words in the local context. The SALMA - Tagger is a fine grained morphological analyzer which is mainly depends on linguistic information extracted from traditional Arabic grammar books and prior-knowledge broad-coverage lexical resources; the SALMA - ABCLexicon. More fine-grained tag sets may be more appropriate for some tasks. The SALMA - Tag Set is a standard tag set for encoding, which captures long-established traditional fine-grained morphological features of Arabic, in a notation format intended to be compact yet transparent."}
{"_id":"3caedff0a82950046730bce6f8d85aec46cf2e8c","title":"Empirical evidence in global software engineering: a systematic review","text":"Recognized as one of the trends of the 21st century, globalization of the world economies brought significant changes to nearly all industries, and in particular it includes software development. Many companies started global software engineering (GSE) to benefit from cheaper, faster and better development of software systems, products and services. However, empirical studies indicate that achieving these benefits is not an easy task. Here, we report our findings from investigating empirical evidence in GSE-related research literature. By conducting a systematic review we observe that the GSE field is still immature. The amount of empirical studies is relatively small. The majority of the studies represent problem-oriented reports focusing on different aspects of GSE management rather than in-depth analysis of solutions for example in terms of useful practices or techniques. Companies are still driven by cost reduction strategies, and at the same time, the most frequently discussed recommendations indicate a necessity of investments in travelling and socialization. Thus, at the same time as development goes global there is an ambition to minimize geographical, temporal and cultural separation. These are normally integral parts of cross-border collaboration. In summary, the systematic review results in several descriptive classifications of the papers on empirical studies in GSE and also reports on some best practices identified from literature."}
{"_id":"4f2eb8902bbea3111b1ec2a974eab31e92bb1435","title":"Fundamental limits of RSS fingerprinting based indoor localization","text":"Indoor localization has been an active research field for decades, where the received signal strength (RSS) fingerprinting based methodology is widely adopted and induces many important localization techniques such as the recently proposed one building the fingerprint database with crowd-sourcing. While efforts have been dedicated to improve the accuracy and efficiency of localization, the fundamental limits of RSS fingerprinting based methodology itself is still unknown in a theoretical perspective. In this paper, we present a general probabilistic model to shed light on a fundamental question: how good the RSS fingerprinting based indoor localization can achieve? Concretely, we present the probability that a user can be localized in a region with certain size, given the RSS fingerprints submitted to the system. We reveal the interaction among the localization accuracy, the reliability of location estimation and the number of measurements in the RSS fingerprinting based location determination. Moreover, we present the optimal fingerprints reporting strategy that can achieve the best accuracy for given reliability and the number of measurements, which provides a design guideline for the RSS fingerprinting based indoor localization facilitated by crowdsourcing paradigm."}
{"_id":"3d88c7e728e94278a2f3ef654818a5e93d937743","title":"Human ES cell-derived neural rosettes reveal a functionally distinct early neural stem cell stage.","text":"Neural stem cells (NSCs) yield both neuronal and glial progeny, but their differentiation potential toward multiple region-specific neuron types remains remarkably poor. In contrast, embryonic stem cell (ESC) progeny readily yield region-specific neuronal fates in response to appropriate developmental signals. Here we demonstrate prospective and clonal isolation of neural rosette cells (termed R-NSCs), a novel NSC type with broad differentiation potential toward CNS and PNS fates and capable of in vivo engraftment. R-NSCs can be derived from human and mouse ESCs or from neural plate stage embryos. While R-NSCs express markers classically associated with NSC fate, we identified a set of genes that specifically mark the R-NSC state. Maintenance of R-NSCs is promoted by activation of SHH and Notch pathways. In the absence of these signals, R-NSCs rapidly lose rosette organization and progress to a more restricted NSC stage. We propose that R-NSCs represent the first characterized NSC stage capable of responding to patterning cues that direct differentiation toward region-specific neuronal fates. In addition, the R-NSC-specific genetic markers presented here offer new tools for harnessing the differentiation potential of human ESCs."}
{"_id":"01ef8a09ffa6ab5756a74b5aee1f8563d95d6e86","title":"Material classification and semantic segmentation of railway track images with deep convolutional neural networks","text":"The condition of railway tracks needs to be periodically monitored to ensure passenger safety. Cameras mounted on a moving vehicle such as a hi-rail vehicle or a geometry inspection car can generate large volumes of high resolution images. Extracting accurate information from those images has been challenging due to background clutter in railroad environments. In this paper, we describe a novel approach to visual track inspection using material classification and semantic segmentation with Deep Convolutional Neural Networks (DCNN). We show that DCNNs trained end-to-end for material classification are more accurate than shallow learning machines with hand-engineered features and are more robust to noise. Our approach results in a material classification accuracy of 93.35% using 10 classes of materials. This allows for the detection of crumbling and chipped tie conditions at detection rates of 86.06% and 92.11%, respectively, at a false positive rate of 10 FP\/mile on the 85-mile Northeast Corridor (NEC) 2012-2013 concrete tie dataset."}
{"_id":"b1e57ea60b291ddfe6b97f28f5b73a76f3e65bc3","title":"A conversational intelligent tutoring system to automatically predict learning styles","text":"This paper proposes a generic methodology and architecture for developing a novel conversational intelligent tutoring system (CITS) called Oscar that leads a tutoring conversation and dynamically predicts and adapts to a student\u2019s learning style. Oscar aims to mimic a human tutor by implicitly modelling the learning style during tutoring, and personalising the tutorial to boost confidence and improve the effectiveness of the learning experience. Learners can intuitively explore and discuss topics in natural language, helping to establish a deeper understanding of the topic. The Oscar CITS methodology and architecture are independent of the learning styles model and tutoring subject domain. Oscar CITS was implemented using the Index of Learning Styles (ILS) model (Felder & Silverman 1988) to deliver an SQL tutorial. Empirical studies involving real students have validated the prediction of learning styles in a real-world teaching\/learning environment. The results showed that all learning styles in the ILS model were successfully predicted from a natural language tutoring conversation, with an accuracy of 61-100%. Participants also found Oscar\u2019s tutoring helpful and achieved an average learning gain of 13%."}
{"_id":"96de937b4bb3dfacdf5f6b5ed653994fafbc1aed","title":"Magic Quadrant for Data Quality Tools","text":"Extensive data on functional capabilities, customer base demographics, financial status, pricing and other quantitative attributes gained via a requestfor-information process engaging vendors in this market. Interactive briefings in which vendors provided Gartner with updates on their strategy, market positioning, recent key developments and product road map. A Web-based survey of reference customers provided by each vendor, which captured data on usage patterns, levels of satisfaction with major product functionality categories, various nontechnology-related vendor attributes (such as pricing, product support and overall service delivery), and more. In total, 333 organizations across all major world regions provided input on their experiences with vendors and tools in this manner. Feedback about tools and vendors captured during conversations with users of Gartner's client inquiry service. Market share and revenue growth estimates developed by Gartner's Technology and Service Provider research unit."}
{"_id":"366d2c9a55c13654bcb235c66cf79163999c60b9","title":"A study of near-field direct antenna modulation systems using convex optimization","text":"This paper studies the constellation diagram design for a class of communication systems known as near-field direct antenna modulation (NFDAM) systems. The modulation is carried out in a NFDAM system by means of a control unit that switches among a number of pre-designed passive controllers such that each controller generates a desired voltage signal at the far field. To find an optimal number of signals that can be transmitted and demodulated reliably in a NFDAM system, the coverage area of the signal at the far field should be identified. It is shown that this coverage area is a planar convex region in general and simply a circle in the case when no constraints are imposed on the input impedance of the antenna and the voltage received at the far field. A convex optimization method is then proposed to find a polygon that is able to approximate the coverage area of the signal constellation diagram satisfactorily. A similar analysis is provided for the identification of the coverage area of the antenna input impedance, which is beneficial for designing an energy-efficient NFDAM system."}
{"_id":"0eff3eb68ae892012f0d478444f8bb6f50361be5","title":"BFS and Coloring-Based Parallel Algorithms for Strongly Connected Components and Related Problems","text":"Finding the strongly connected components (SCCs) of a directed graph is a fundamental graph-theoretic problem. Tarjan's algorithm is an efficient serial algorithm to find SCCs, but relies on the hard-to-parallelize depth-first search (DFS). We observe that implementations of several parallel SCC detection algorithms show poor parallel performance on modern multicore platforms and large-scale networks. This paper introduces the Multistep method, a new approach that avoids work inefficiencies seen in prior SCC approaches. It does not rely on DFS, but instead uses a combination of breadth-first search (BFS) and a parallel graph coloring routine. We show that the Multistep method scales well on several real-world graphs, with performance fairly independent of topological properties such as the size of the largest SCC and the total number of SCCs. On a 16-core Intel Xeon platform, our algorithm achieves a 20X speedup over the serial approach on a 2 billion edge graph, fully decomposing it in under two seconds. For our collection of test networks, we observe that the Multistep method is 1.92X faster (mean speedup) than the state-of-the-art Hong et al. SCC method. In addition, we modify the Multistep method to find connected and weakly connected components, as well as introduce a novel algorithm for determining articulation vertices of biconnected components. These approaches all utilize the same underlying BFS and coloring routines."}
{"_id":"90e3ec000125d579ec1724781410d4201be6d2a8","title":"Evaluation of communication technologies for IEC 61850 based distribution automation system with distributed energy resources","text":"This paper presents the study of different communication systems between IEC 61850 based distribution substation and distributed energy resources (DERs). Communication networks have been simulated for a typical distribution automation system (DAS) with DERs using OPNET software. The simulation study shows the performance of wired and wireless communication systems for different messages, such as GOOSE and measured (metered) values between DAS and DERs. A laboratory set-up has been implemented using commercial relay and communication devices for evaluating the performance of GOOSE messages, using wired and wireless physical medium. Finally, simulation and laboratory results are discussed in detail."}
{"_id":"3c8ffc499c5748f28203b40e44da2d9142d8d396","title":"A clinical study of Noonan syndrome.","text":"Clinical details are presented on 151 individuals with Noonan syndrome (83 males and 68 females, mean age 12.6 years). Polyhydramnios complicated 33% of affected pregnancies. The commonest cardiac lesions were pulmonary stenosis (62%), and hypertrophic cardiomyopathy (20%), with a normal echocardiogram present in only 12.5% of all cases. Significant feeding difficulties during infancy were present in 76% of the group. Although the children were short (50% with a height less than 3rd centile), and underweight (43% with a weight less than 3rd centile), the mean head circumference of the group was on the 50th centile. Motor milestone delay was usual, the cohort having a mean age of sitting unsupported of 10 months and walking of 21 months. Abnormal vision (94%) and hearing (40%) were frequent findings, but 89% of the group were attending normal primary or secondary schools. Other associations included undescended testicles (77%), hepatosplenomegaly (50%), and evidence of abnormal bleeding (56%). The mean age at diagnosis of Noonan syndrome in this group was 9.0 years. Earlier diagnosis of this common condition would aid both clinical management and genetic counselling."}
{"_id":"ca45e17cf41cf1fd0aa7c9536f0a27bc0f4d3b33","title":"Superneurons: dynamic GPU memory management for training deep neural networks","text":"Going deeper and wider in neural architectures improves their accuracy, while the limited GPU DRAM places an undesired restriction on the network design domain. Deep Learning (DL) practitioners either need to change to less desired network architectures, or nontrivially dissect a network across multiGPUs. These distract DL practitioners from concentrating on their original machine learning tasks. We present SuperNeurons: a dynamic GPU memory scheduling runtime to enable the network training far beyond the GPU DRAM capacity. SuperNeurons features 3 memory optimizations, Liveness Analysis, Unified Tensor Pool, and Cost-Aware Recomputation; together they effectively reduce the network-wide peak memory usage down to the maximal memory usage among layers. We also address the performance issues in these memory-saving techniques. Given the limited GPU DRAM, SuperNeurons not only provisions the necessary memory for the training, but also dynamically allocates the memory for convolution workspaces to achieve the high performance. Evaluations against Caffe, Torch, MXNet and TensorFlow have demonstrated that SuperNeurons trains at least 3.2432 deeper network than current ones with the leading performance. Particularly, SuperNeurons can train ResNet2500 that has 104 basic network layers on a 12GB K40c."}
{"_id":"c7baff1a1c70e01d6bc71a29c633fb1fd326f5a7","title":"BRAVO: Balanced Reliability-Aware Voltage Optimization","text":"Defining a processor micro-architecture for a targeted productspace involves multi-dimensional optimization across performance, power and reliability axes. A key decision in sucha definition process is the circuit-and technology-driven parameterof the nominal (voltage, frequency) operating point. This is a challenging task, since optimizing individually orpair-wise amongst these metrics usually results in a designthat falls short of the specification in at least one of the threedimensions. Aided by academic research, industry has nowadopted early-stage definition methodologies that considerboth energy-and performance-related metrics. Reliabilityrelatedenhancements, on the other hand, tend to get factoredin via a separate thread of activity. This task is typically pursuedwithout thorough pre-silicon quantifications of the energyor even the performance cost. In the late-CMOS designera, reliability needs to move from a post-silicon afterthoughtor validation-only effort to a pre-silicon definitionprocess. In this paper, we present BRAVO, a methodologyfor such reliability-aware design space exploration. BRAVOis supported by a multi-core simulation framework that integratesperformance, power and reliability modeling capability. Errors induced by both soft and hard fault incidence arecaptured within the reliability models. We introduce the notionof the Balanced Reliability Metric (BRM), that we useto evaluate overall reliability of the processor across soft andhard error incidences. We demonstrate up to 79% improvementin reliability in terms of this metric, for only a 6% dropin overall energy efficiency over design points that maximizeenergy efficiency. We also demonstrate several real-life usecaseapplications of BRAVO in an industrial setting."}
{"_id":"65b0ffbab1ae29deff6dee7d401bf14b4edf8477","title":"Ensemble Methods for Multi-label Classification","text":"Ensemble methods have been shown to be an effectiv e tool for solving multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm, each member of the en semble is associated with a small randomly-selected subset of k labels. Then, a single label classifier is trained according to each combination of elements in the subset. In this paper we adopt a similar approach, however, instead of rando mly choosing subsets, we select the minimum required su bsets of k labels that cover all labels and meet additional constraints such as coverage of inter-label correla tions. Construction of the cover is achieved by form ulating the subset selection as a minimum set covering prob lem (SCP) and solving it by using approximation algo rithms. Every cover needs only to be prepared once by offline algorithms. Once prepared, a cover may b e applied to the classification of any given multi-la bel dataset whose properties conform with those of the cover. The contribution of this paper is two-fold. Fir st, we introduce SCP as a general framework for con structing label covers while allowing the user to incorpo rate cover construction constraints. We demonstrate he effectiveness of this framework by proposing two co nstruction constraints whose enforcement produces c overs that improve the prediction performance of ran dom selection. Second, we provide theoretical bound s that quantify the probabilities of random selection t produce covers that meet the proposed construct ion riteria. The experimental results indicate that the p roposed methods improve multi-label classification accuracy and stability compared with the RAKEL algorithm a nd to other state-of-the-art algorithms."}
{"_id":"6f33c49e983acf93e98bfa085de18ca489a27659","title":"Sensor networks for medical care","text":"Sensor networks have the potential to greatly impact many aspects of medical care. By outfitting patients with wireless, wearable vital sign sensors, collecting detailed real-time data on physiological status can be greatly simplified. However, there is a significant gap between existing sensor network systems and the needs of medical care. In particular, medical sensor networks must support multicast routing topologies, node mobility, a wide range of data rates and high degrees of reliability, and security. This paper describes our experiences with developing a combined hardware and software platform for medical sensor networks, called CodeBlue. CodeBlue provides protocols for device discovery and publish\/subscribe multihop routing, as well as a simple query interface that is tailored for medical monitoring. We have developed several medical sensors based on the popular MicaZ and Telos mote designs, including a pulse oximeter, EKG and motion-activity sensor. We also describe a new, miniaturized sensor mote designed for medical use. We present initial results for the CodeBlue prototype demonstrating the integration of our medical sensors with the publish\/subscribe routing substrate. We have experimentally validated the prototype on our 30-node sensor network testbed, demonstrating its scalability and robustness as the number of simultaneous queries, data rates, and transmitting sensors are varied. We also study the effect of node mobility, fairness across multiple simultaneous paths, and patterns of packet loss, confirming the system\u2019s ability to maintain stable routes despite variations in node location and"}
{"_id":"4df65842a527e752bd487c180c368eec85f8b61b","title":"Digital Forensic Analysis of SIM Cards","text":"Smart cards are fundamental technology in modern life. It is embedded in numerous devices such as GPS devices, ATM cards, Mobile SIM cards and many others. Mobile devices became the evolution of technology. It becomes smaller, faster and supports large storage capabilities. Digital forensics of mobile devices that maybe found in crime scene is becoming inevitable. The purpose of this research is to address the SIM cards digital forensics analysis. It presents sound forensic methodology and process of SIM cards forensic examination. In particular, the main aim of the research is to answer the following research questions: (1) what forensic evidence could be extracted from a SIM card and (2) what are limitations that may hinder a forensic"}
{"_id":"167895bdf0f1ef88acc962e7a6f255ab92769485","title":"Object Detection and Classification by Decision-Level Fusion for Intelligent Vehicle Systems","text":"To understand driving environments effectively, it is important to achieve accurate detection and classification of objects detected by sensor-based intelligent vehicle systems, which are significantly important tasks. Object detection is performed for the localization of objects, whereas object classification recognizes object classes from detected object regions. For accurate object detection and classification, fusing multiple sensor information into a key component of the representation and perception processes is necessary. In this paper, we propose a new object-detection and classification method using decision-level fusion. We fuse the classification outputs from independent unary classifiers, such as 3D point clouds and image data using a convolutional neural network (CNN). The unary classifiers for the two sensors are the CNN with five layers, which use more than two pre-trained convolutional layers to consider local to global features as data representation. To represent data using convolutional layers, we apply region of interest (ROI) pooling to the outputs of each layer on the object candidate regions generated using object proposal generation to realize color flattening and semantic grouping for charge-coupled device and Light Detection And Ranging (LiDAR) sensors. We evaluate our proposed method on a KITTI benchmark dataset to detect and classify three object classes: cars, pedestrians and cyclists. The evaluation results show that the proposed method achieves better performance than the previous methods. Our proposed method extracted approximately 500 proposals on a 1226 \u00d7 370 image, whereas the original selective search method extracted approximately 10 6 \u00d7 n proposals. We obtained classification performance with 77.72% mean average precision over the entirety of the classes in the moderate detection level of the KITTI benchmark dataset."}
{"_id":"c669d5efe471abcb3a28223845a318a562070cc8","title":"A switching ringing suppression scheme of SiC MOSFET by Active Gate Drive","text":"This paper proposes an Active Gate Drive (AGD) to reduce unwanted switching ringing of Silicon Carbide (SiC) MOSFET module with a rating of 120 A and 1200 V. While SiC MOSFET can be operated under high switching frequency and high temperature with very low power losses, one of the key challenges for SiC MOSFET is the electromagnetic interference (EMI) caused by steep switching transients and continuous switching ringing. Compared to Si MOSFET, the higher rate of SiC MOSFET drain current variation introduces worse EMI problems. To reduce EMI generated from the switching ringing, this paper investigates the causes of switching ringing by considering the combined impact of parasitic inductances, capacitances, and low circuit loop resistance. In addition, accurate mathematical expressions are established to explain the ringing behavior and quantitative analysis is carried out to investigate the relationship between the switching transient and gate drive voltage. Thereafter, an AGD method for mitigating SiC MOSFET switching ringing is presented. Substantially reduced switching ringing can be observed from circuit simulations. As a result, the EMI generation is mitigated."}
{"_id":"9b618fa0cd834f7c4122c8e53539085e06922f8c","title":"Adversarial Perturbations Against Deep Neural Networks for Malware Classification","text":"Deep neural networks, like many other machine learning models, have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive machine learning models into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since the highentropy of images can be conveniently manipulated without changing the images\u2019 overall visual appearance. Yet, it remains unclear how such attacks translate to more securitysensitive applications such as malware detection\u2013which may pose significant challenges in sample generation and arguably grave consequences for failure. In this paper, we show how to construct highly-effective adversarial sample crafting attacks for neural networks used as malware classifiers. The application domain of malware classification introduces additional constraints in the adversarial sample crafting problem when compared to the computer vision domain: (i) continuous, differentiable input domains are replaced by discrete, often binary inputs; and (ii) the loose condition of leaving visual appearance unchanged is replaced by requiring equivalent functional behavior. We demonstrate the feasibility of these attacks on many different instances of malware classifiers that we trained using the DREBIN Android malware data set. We furthermore evaluate to which extent potential defensive mechanisms against adversarial crafting can be leveraged to the setting of malware classification. While feature reduction did not prove to have a positive impact, distillation and re-training on adversarially crafted samples show promising results."}
{"_id":"5e4deed61eaf561f2ef2a26f11ce32345ce64981","title":"Lung nodules diagnosis based on evolutionary convolutional neural network","text":"Lung cancer presents the highest cause of death among patients around the world, in addition of being one of the smallest survival rates after diagnosis. In this paper, we exploit a deep learning technique jointly with the genetic algorithm to classify lung nodules in whether malignant or benign, without computing the shape and texture features. The methodology was tested on computed tomography (CT) images from the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI), with the best sensitivity of 94.66%, specificity of 95.14%, accuracy of 94.78% and area under the ROC curve of 0.949."}
{"_id":"4cad1f0023e1bc904c3c615d791c1518dac8243f","title":"Crop disease classification using texture analysis","text":"With the Agriculture Sector being the backbone of not only a large number of industries but also society as a whole, there is a rising need to grow good quality crops which in turn will give a high yield. For this to happen, it is crucial to monitor the crops throughout their growth period. In this paper, Image processing is used to detect and classify sunflower crop diseases based on the image of their leaf. The images are taken through a high resolution digital camera and after preprocessing, are subjected to k-means clustering to get the diseased part of the leaf. These are then run through the various machine learning algorithms and classified based on their color and texture features. A comparison based on accuracy between various machine learning algorithms is done namely K-Nearest Neighbors, Multi-Class Support Vector Machine, Naive Bayes and Multinomial Logistic Regression to achieve maximum accuracy. The implementation has been done using MATLAB."}
{"_id":"d30bf3722157c71938dc94419802239ef4e4e0db","title":"Practical Private Set Intersection Protocols with Linear Complexity","text":"The constantly increasing dependence on anytime-anywhere availability of data and the commensurately increasing fear of losing privacy motivate the need for privacy-preserving techniques. One interesting and common problem occurs when two parties need to privately compute an intersection of their respective sets of data. In doing so, one or both parties must obtain the intersection (if one exists), while neither should learn anything about other set elements. Although prior work has yielded a number of effective and elegant Private Set Intersection (PSI) techniques, the quest for efficiency is still underway. This paper explores some PSI variations and constructs several secure protocols that are appreciably more efficient than the state-of-the-art."}
{"_id":"c62e6bebdef5d71961384db654e9b99d68a42d39","title":"Investigating Social Entrepreneurship in Developing Countries","text":"Social entrepreneurship has drawn interest from global policy makers and social entrepreneurs to target developing countries. Generally, not-forprofit organizations, funded by government and donor grants have played a significant role in poverty alleviation. We argue that, by applying entrepreneurial concepts, organizations can create social value, hence mitigate poverty. This is a theoretical paper that builds upon a multidimensional model in analysing how three social enterprises from India and Kenya create social value to address social problems. The findings suggest that whilst the social mission is central to all these organizations, they also create social value through innovation and pro-activeness. Additionally, the cultural and political environmental contexts hinder their attempt to create social value. Building networks and partnerships to achieve social value creation is vital for these organizations. Policy makers should devise policies that would assist social enterprises to achieve development goals."}
{"_id":"11651db02c4a243b5177516e62a45f952dc54430","title":"Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding","text":"The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques."}
{"_id":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging","text":"Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce stateof-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result."}
{"_id":"ff1577528a34a11c2a81d2451d346c412c674c02","title":"Character-based Neural Machine Translation","text":"We introduce a neural machine translation model that views the input and output sentences as sequences of characters rather than words. Since word-level information provides a crucial source of bias, our input model composes representations of character sequences into representations of words (as determined by whitespace boundaries), and then these are translated using a joint attention\/translation model. In the target language, the translation is modeled as a sequence of word vectors, but each word is generated one character at a time, conditional on the previous character generations in each word. As the representation and generation of words is performed at the character level, our model is capable of interpreting and generating unseen word forms. A secondary benefit of this approach is that it alleviates much of the challenges associated with preprocessing\/tokenization of the source and target languages. We show that our model can achieve translation results that are on par with conventional word-based models."}
{"_id":"00a28138c74869cfb8236a18a4dbe3a896f7a812","title":"Better Word Representations with Recursive Neural Networks for Morphology","text":"Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way."}
{"_id":"6ce9d1c1e9a8b889a04faa98f96b0df1b93fcc84","title":"Adoption and Impact of IT Governance and Management Practices: A COBIT 5 Perspective","text":"This paper empirically investigates how adoption of IT governance and management processes, as identified in the IT governance framework COBIT 5, relates to the level of IT-related goals achievement, which in turn associates to the level of enterprise goals achievement. Simultaneously, this research project provides an international benchmark on how organizations are currently adopting the governance and management processes as identified in COBIT 5. The findings suggest that organizations are best in adopting the \u201cIT factory\u201d related management processes and that implementation scores drop in management and governance processes when more business and board involvement is required. Additionally, there are significant differences in perceived implementation maturity of COBIT 5 processes between SMEs and larger organizations. Also, the data offers empirical evidence that the COBIT 5 processes have a positive association with enterprise value creation. KeywORdS COBIT 5, Enterprise Goals, Enterprise Governance and Management of IT, IT Governance, SMEs"}
{"_id":"a8fd9be2f7775b123f62094eadd59d18bbbef027","title":"Peephole: Predicting Network Performance Before Training","text":"The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predicting the performance of a network before training, based on its architecture. Specifically, we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via LSTM. Taking advantage of the recurrent network\u2019s strong expressive power, this method can reliably predict the performances of various network architectures. Our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets \u2013 a key desideratum in performance prediction."}
{"_id":"7313097125b82b876e89b47b078fe85a56655e95","title":"Bangladeshi banknote recognition by neural network with axis symmetrical masks","text":"Automated banknote recognition system can be a very good utility in banking systems and other field of commerce. It can also aid visually impaired people. Although in Bangladesh, bill money recognition machines are not common but it is used in other countries. In this paper, for the first time, we have proposed a Neural Network based recognition scheme for Bangladeshi banknotes. The scheme can efficiently be implemented in cheap hardware which may be very useful in many places. The recognition system takes scanned images of banknotes which are scanned by low cost optoelectronic sensors and then fed into a Multilayer Perceptron, trained by Backpropagation algorithm, for recognition. Axis Symmetric Masks are used in preprocessing stage which reduces the network size and guarantees correct recognition even if the note is flipped. Experimental results are presented which show that this scheme can recognize currently available 8 notes (1, 2, 5, 10, 20, 50, 100 & 500 Taka) successfully with an average accuracy of 98.57%."}
{"_id":"1f3774fecbe0ee12a2135cd05a761a7d2b537e13","title":"Cowden syndrome.","text":"BACKGROUND\nCowden syndrome is a rare genodermatosis charactarized by presence of multiple hamartomas. The aim of the study was to specify the clinical, therapeutic and prognostic aspects of Cowden syndrome.\n\n\nCASES REPORT\nOur study included 4 patients with Cowden syndrome, 2 males and 2 females between 14 and 46 years old. Clinical examination of the skin revealed facials papules (4 cases), acral keratosis (1 case), translucent keratotic papules (2 cases). Oral examination revealed papules (4 cases), papillomatosis (4 cases), gingival hypertrophy (4 cases) and scrotal tongue (2 cases). Investigations revealed thyroid lesions (2 cases), fibrocystic disease and lipoma of the breast in 1 case, \"glycogenic acanthosis\" (1 case), macrocephaly (2 cases), dysmorphic face (1 case) and lichen nitidus (1 case). Oral etretinate and acitretine were temporary efficient in 2 patients. Topical treatment with tretinoin lotion resulted in some improvement in cutaneous, but not mucosal lesions in one patient. No cancer was revealed.\n\n\nCONCLUSION\nThe pathognomonic mucocutaneous lesions were found in all patients. However, no degenerative lesions have been revealed. A new association of Cowden syndrome with lichen nitidus was found. Treatment with oral retinoids was efficient on cutaneous lesions."}
{"_id":"122374a3baf1e0efde03301226344a2d728eafc3","title":"High resolution stationary digital breast tomosynthesis using distributed carbon nanotube x-ray source array.","text":"PURPOSE\nThe purpose of this study is to investigate the feasibility of increasing the system spatial resolution and scanning speed of Hologic Selenia Dimensions digital breast tomosynthesis (DBT) scanner by replacing the rotating mammography x-ray tube with a specially designed carbon nanotube (CNT) x-ray source array, which generates all the projection images needed for tomosynthesis reconstruction by electronically activating individual x-ray sources without any mechanical motion. The stationary digital breast tomosynthesis (s-DBT) design aims to (i) increase the system spatial resolution by eliminating image blurring due to x-ray tube motion and (ii) reduce the scanning time. Low spatial resolution and long scanning time are the two main technical limitations of current DBT technology.\n\n\nMETHODS\nA CNT x-ray source array was designed and evaluated against a set of targeted system performance parameters. Simulations were performed to determine the maximum anode heat load at the desired focal spot size and to design the electron focusing optics. Field emission current from CNT cathode was measured for an extended period of time to determine the stable life time of CNT cathode for an expected clinical operation scenario. The source array was manufactured, tested, and integrated with a Selenia scanner. An electronic control unit was developed to interface the source array with the detection system and to scan and regulate x-ray beams. The performance of the s-DBT system was evaluated using physical phantoms.\n\n\nRESULTS\nThe spatially distributed CNT x-ray source array comprised 31 individually addressable x-ray sources covering a 30 angular span with 1 pitch and an isotropic focal spot size of 0.6 mm at full width at half-maximum. Stable operation at 28 kV(peak) anode voltage and 38 mA tube current was demonstrated with extended lifetime and good source-to-source consistency. For the standard imaging protocol of 15 views over 14, 100 mAs dose, and 2\u2009\u00d7\u20092 detector binning, the projection resolution along the scanning direction increased from 4.0 cycles\/mm [at 10% modulation-transfer-function (MTF)] in DBT to 5.1 cycles\/mm in s-DBT at magnification factor of 1.08. The improvement is more pronounced for faster scanning speeds, wider angular coverage, and smaller detector pixel sizes. The scanning speed depends on the detector, the number of views, and the imaging dose. With 240 ms detector readout time, the s-DBT system scanning time is 6.3 s for a 15-view, 100 mAs scan regardless of the angular coverage. The scanning speed can be reduced to less than 4 s when detectors become faster. Initial phantom studies showed good quality reconstructed images.\n\n\nCONCLUSIONS\nA prototype s-DBT scanner has been developed and evaluated by retrofitting the Selenia rotating gantry DBT scanner with a spatially distributed CNT x-ray source array. Preliminary results show that it improves system spatial resolution substantially by eliminating image blur due to x-ray focal spot motion. The scanner speed of s-DBT system is independent of angular coverage and can be increased with faster detector without image degration. The accelerated lifetime measurement demonstrated the long term stability of CNT x-ray source array with typical clinical operation lifetime over 3 years."}
{"_id":"0fc73c4a6e537b6c718ad54f47ae8847115a5d17","title":"From Vision to NLP : A Merge","text":"The study of artificial intelligence can be simplified into one goal: trying to mimic\/enhance human senses. This paper attempts to combine computer vision and natural language processing to create a question answer system. This system takes a question and an image as input and outputs a response to the answer based on how the RCNN understands the question asked. The system correlates the question with the image by leveraging attention and memory mechanisms. Mentor: Arun Chaganty"}
{"_id":"b4729432b23842ff6b1b126572f8fa17aca14758","title":"Fast high-quality non-blind deconvolution using sparse adaptive priors","text":"We present an efficient approach for high-quality non-blind deconvolution based on the use of sparse adaptive priors. Its regularization term enforces preservation of strong edges while removing noise. We model the image-prior deconvolution problem as a linear system, which is solved in the frequency domain. This clean formulation lends to a simple and efficient implementation. We demonstrate its effectiveness by performing an extensive comparison with existing non-blind deconvolution methods, and by using it to deblur photographs degraded by camera shake. Our experiments show that our solution is faster and its results tend to have higher peak signal-to-noise ratio than the state-of-the-art techniques. Thus, it provides an attractive alternative to perform high-quality non-blind deconvolution of large images, as well as to be used as the final step of blind-deconvolution algorithms."}
{"_id":"41a5499a8e4a55a16c94b1944a74274f4340be74","title":"Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification","text":"Human identification using fingerprint impressions has been widely studied and employed for more than 2000\u00a0years. Despite new advancements in the 3D imaging technologies, widely accepted representation of 3D fingerprint features and matching methodology is yet to emerge. This paper investigates 3D representation of widely employed 2D minutiae features by recovering and incorporating (i) minutiae height z and (ii) its 3D orientation \u03c6 information and illustrates an effective matching strategy for matching popular minutiae features extended in 3D space. One of the obstacles of the emerging 3D fingerprint identification systems to replace the conventional 2D fingerprint system lies in their bulk and high cost, which is mainly contributed from the usage of structured lighting system or multiple cameras. This paper attempts to addresses such key limitations of the current 3D fingerprint technologies bydeveloping the single camera-based 3D fingerprint identification system. We develop a generalized 3D minutiae matching model and recover extended 3D fingerprint features from the reconstructed 3D fingerprints. 2D fingerprint images acquired for the 3D fingerprint reconstruction can themselves be employed for the performance improvement and have been illustrated in the work detailed in this paper. This paper also attempts to answer one of the most fundamental questions on the availability of inherent discriminableinformation from 3D fingerprints. The experimental results are presented on a database of 240 clients 3D fingerprints, which is made publicly available to further research efforts in this area, and illustrate the discriminant power of 3D minutiae representation andmatching to achieve performance improvement."}
{"_id":"8d7c2c3d03ae5360bb73ac818a0e36f324f1e8ce","title":"PiCANet: Learning Pixel-wise Contextual Attention in ConvNets and Its Application in Saliency Detection","text":"1Context plays an important role in many computer vision tasks. Previous models usually construct contextual information from the whole context region. However, not all context locations are helpful and some of them may be detrimental to the final task. To solve this problem, we propose a novel pixel-wise contextual attention network, i.e., the PiCANet, to learn to selectively attend to informative context locations for each pixel. Specifically, it can generate an attention map over the context region for each pixel, where each attention weight corresponds to the contextual relevance of each context location w.r.t. the specified pixel location. Thus, an attended contextual feature can be constructed by using the attention map to aggregate the contextual features. We formulate PiCANet in a global form and a local form to attend to global contexts and local contexts, respectively. Our designs for the two forms are both fully differentiable. Thus they can be embedded into any CNN architectures for various computer vision tasks in an end-to-end manner. We take saliency detection as an example application to demonstrate the effectiveness of the proposed PiCANets. Specifically, we embed global and local PiCANets into an encoder-decoder Convnet hierarchically. Thorough * This paper was previously submitted to CVPR 2017 and ICCV 2017. This is a slightly revised version based on our previous submission. analyses indicate that the global PiCANet helps to construct global contrast while the local PiCANets help to enhance the feature maps to be more homogenous, thus making saliency detection results more accurate and uniform. As a result, our proposed saliency model achieves state-of-the-art results on 4 benchmark datasets."}
{"_id":"35875600a30f89ea133ac06afeefc8cacec9fb3d","title":"Can virtual reality simulations be used as a research tool to study empathy, problems solving and perspective taking of educators?: theory, method and application","text":"Simulations in virtual environments are becoming an important research tool for educators. These simulations can be used in a variety of areas from the study of emotions and psychological disorders to more effective training. The current study uses animated narrative vignette simulation technology to observe a classroom situation depicting a low level aggressive peer-to-peer victim dyad. Participants were asked to respond to this situation as if they were the teacher, and these responses were then coded and analyzed. Consistent with other literature, the pre-service teachers expressed very little empathic concern, problem-solving or management of the situation with the victim. Future direction and educational implications are presented."}
{"_id":"5f17b4a08d14afaa14a695573ad598dcb763c623","title":"Comparison of SVM and ANN for classification of eye events in EEG","text":"The eye events (eye blink, eyes close and eyes open) are usually considered as biological artifacts in the electroencephalographic (EEG) signal. One can control his or her eye blink by proper training and hence can be used as a control signal in Brain Computer Interface (BCI) applications. Support vector machines (SVM) in recent years proved to be the best classification tool. A comparison of SVM with the Artificial Neural Network (ANN) always provides fruitful results. A one-against-all SVM and a multilayer ANN is trained to detect the eye events. A comparison of both is made in this paper."}
{"_id":"f971f858e59edbaeb0b75b7bcbca0d5c7b7d8065","title":"Effects of Fitness Applications with SNS: How Do They Influence Physical Activity","text":"Fitness applications with social network services (SNS) have emerged for physical activity management. However, there is little understanding of the effects of these applications on users\u2019 physical activity. Motivated thus, we develop a theoretical model based on social cognitive theory of self-regulation to explain the effects of goal setting, self-tracking, and SNS (through social comparison from browsing others\u2019 tracking data and social support from sharing one\u2019s own tracking data) on physical activity. The model was tested with objective data from 476 Runkeeper users. Our preliminary results show that goal setting, number of uses, the proportion of lower-performing friends, and number of likes positively influence users\u2019 physical activities, while the proportion of higher-performing friends has a negative effect. Moreover, the effect of the proportion of higher-performing friends is moderated by the average difference between the friends and the individual. The initial contributions of the study and remaining research plan are described."}
{"_id":"44911bdb33d0dc1781016e9afe605a9091ea908b","title":"Vehicle license plate detection and recognition using non-blind image de-blurring algorithm","text":"This paper proposes the method of vehicle license plate recognition, which is essential in the field of intelligent transportation system. The purpose of the study is to present a simple and effective vehicle license plate detection and recognition using non-bling image de-blurring algorithm. The sharpness of the edges in an image is restored by the prior information on images. The blue kernel is free of noise while using the non-blind image de-blurring algorithm. This non-blind image de-blurring (NBID) algorithm is involved in the process of removing the optimization difficulties with respect to unknown image and unknown blur. Estimation is carried out for the length of the motion kernel with Radon transform in Fourier domain. The proposed algorithm was tested on different vehicle images and achieved satisfactory results in license plate detection. The experimental results deal with the efficiency and robustness of the proposed algorithm in both synthesized and real images."}
{"_id":"36973330ae638571484e1f68aaf455e3e6f18ae9","title":"Scale-Aware Fast R-CNN for Pedestrian Detection","text":"In this paper, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intracategory variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in subnetworks which detect pedestrians with scales from disjoint ranges. Outputs from all of the subnetworks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech [P. Dollar, C. Wojek, B. Schiele, and P. Perona, \u201cPedestrian detection: An evaluation of the state of the art,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 743\u2013761, Apr. 2012], and obtains competitive results on INRIA [N. Dalal and B. Triggs, \u201cHistograms of oriented gradients for human detection,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2005, pp. 886\u2013893], ETH [A. Ess, B. Leibe, and L. V. Gool, \u201cDepth and appearance for mobile scene analysis,\u201d in Proc. Int. Conf. Comput. Vis., 2007, pp. 1\u20138], and KITTI [A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? The KITTI vision benchmark suite,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354\u20133361]."}
{"_id":"f1e1437ad6cada93dc8627f9c9679ffee02d921c","title":"Behavior-Based Telecommunication Churn Prediction with Neural Network Approach","text":"A behavior-based telecom customer churn prediction system is presented in this paper. Unlike conventional churn prediction methods, which use customer demographics, contractual data, customer service logs, call-details, complaint data, bill and payment as inputs and churn as target output, only customer service usage information is included in this system to predict customer churn using a clustering algorithm. It can solve the problems which traditional methods have to face, such as missing or non-reliable data and the correlation among inputs. This study provides a new way to solve traditional churn prediction problems."}
{"_id":"3769e65690e424808361e3eebfdec8ab91908aa9","title":"Affective Image Retrieval via Multi-Graph Learning","text":"Images can convey rich emotions to viewers. Recent research on image emotion analysis mainly focused on affective image classification, trying to find features that can classify emotions better. We concentrate on affective image retrieval and investigate the performance of different features on different kinds of images in a multi-graph learning framework. Firstly, we extract commonly used features of different levels for each image. Generic features and features derived from elements-of-art are extracted as low-level features. Attributes and interpretable principles-of-art based features are viewed as mid-level features, while semantic concepts described by adjective noun pairs and facial expressions are extracted as high-level features. Secondly, we construct single graph for each kind of feature to test the retrieval performance. Finally, we combine the multiple graphs together in a regularization framework to learn the optimized weights of each graph to efficiently explore the complementation of different features. Extensive experiments are conducted on five datasets and the results demonstrate the effectiveness of the proposed method."}
{"_id":"00c57bb8c7d2ce7c8f32aef062ef3d61b9961711","title":"Everyday dwelling with WhatsApp","text":"In this paper, we present a study of WhatsApp, an instant messaging smartphone application. Through our interviews with participants, we develop anthopologist Tim Ingold's notion of dwelling, and discuss how use of WhatsApp is constitutive of a felt-life of being together with those close by. We focus on the relationship \"doings\" in WhatsApp and how this togetherness and intimacy are enacted through small, continuous traces of narrative, of tellings and tidbits, noticings and thoughts, shared images and lingering pauses; this is constitutive of dwelling. Further, we discuss how an intimate knowing of others in these relationships, through past encounters and knowledge of coming together in the future, pertain to the particular forms of relationship engagements manifest through the possibilities presented in WhatsApp. We suggest that this form of sociality is likely to be manifest in other smartphone IM-like applications."}
{"_id":"41ca84736e55375c73416b7d698fb72ad3ccde67","title":"Enabling Real-Time Context-Aware Collaboration through 5G and Mobile Edge Computing","text":"Creating context-aware ad hoc collaborative systems remains to be one of the primary hurdles hampering the ubiquitous deployment of IT and communication services. Especially under mission-critical scenarios, these services must often adhere to strict timing deadlines. We believe empowering such realtime collaboration systems requires context-aware application platforms working in conjunction with ultra-low latency data transmissions. In this paper, we make a strong case that this could be accomplished by combining the novel communication architectures being proposed for 5G with the principles of Mobile Edge Computing (MEC). We show that combining 5G with MEC would enable inter- and intra-domain use cases that are otherwise not feasible."}
{"_id":"75c72ed46042f172d174afe106ce41ec8dee71ae","title":"Modeling of magnetically biased graphene patch frequency selective surface (FSS)","text":"A free-standing magnetically biased graphene patch frequency selective surface (FSS) is modelled in this paper. Its transmission coefficients of co- and cross-polarizations can be obtained with an equivalent tensorial surface conductivity. Then, the rotation angle for normal incidence is explored with different values of biasing magnetic field B0. The maximum rotation angle provided by the magnetically biased graphene patch FSS is 43\u00b0 at 4.7 THz with B0=2.5 T, which is much larger than the rotation angle provided by a graphene sheet. This is very promising for THz nano-devices based on the giant Faraday rotation of graphenes."}
{"_id":"1c28f6b59c209e4f73809d5a5d16d672d26ad1d8","title":"Cooperative Localization for Autonomous Underwater Vehicles","text":"Self-localization of an underwater vehicle is particularly challenging due to the absence of Global Positioning System (GPS) reception or features at known positions that could otherwise have been used for position computation. Thus Autonomous Underwater Vehicle (AUV) applications typically require the pre-deployment of a set of beacons. This thesis examines the scenario in which the members of a group of AUVs exchange navigation information with one another so as to improve their individual position estimates. We describe how the underwater environment poses unique challenges to vehicle navigation not encountered in other environments in which robots operate and how cooperation can improve the performance of self-localization. As intra-vehicle communication is crucial to cooperation, we also address the constraints of the communication channel and the effect that these constraints have on the design of cooperation strategies. The classical approaches to underwater self-localization of a single vehicle, as well as more recently developed techniques are presented. We then examine how methods used for cooperating land-vehicles can be transferred to the underwater domain. An algorithm for distributed self-localization, which is designed to take the specific characteristics of the environment into account, is proposed. We also address how correlated position estimates of cooperating vehicles can lead to overconfidence in individual position estimates. Finally, key to any successful cooperative navigation strategy is the incorporation of the relative positioning between vehicles. The performance of localization algorithms with different geometries is analyzed and a distributed algorithm for the dynamic positioning of vehicles, which serve as dedicated navigation beacons for a fleet of AUVs, is proposed. Thesis Supervisor: John J. Leonard Title: Professor of Mechanical and Ocean Engineering Massachusetts Institute of Technology Acknowledgments This thesis would not have been possible without the help and support of many friends and colleagues who made the last five years at MIT an exceptionally fulfilling experience. I would like to thank my advisor John Leonard who strongly supported me ever since our first email exchange in 2003. He guided me all the way through the application process, research, thesis and finding a post doc position. His broad range of research interests enabled me to find a thesis subject that exactly matched my interest. And I very much appreciated him allowing me to take a significant amount of time to pursue other projects as well as travel. I would also like to thank my other committee members: Henrik Schmidt for his continued support and great company during several research cruises, and Hanu Singh and Arjuna Balasuriya for their helpful suggestions during my thesis writing. Many thanks to David Battle who helped me through my first steps with Autonomous Underwater Vehicles and was a great mate to have around. Many of the experiments presented in this thesis would not have been possible without the support of Andrew Patrikalakis. The results owe a lot to his countless hours of coding assistance and his efforts to ensure that the kayaks would be ready when needed. They were also made possible by Joe Curcio, the builder of the kayaks, and the support of Jacques Leedekerken and Kevin Cockrell. The last years would not have been the same without the many great people I met at MIT. Most important was Matt Walter who convinced me in August 2003 that MIT is not only an interesting place, but that it can be very friendly as well. Throughout the years we shared many great personal and academic experiences. I wish him all the best, wherever life may take him. Patrycja unfortunately left our lab, but made up for it by taking me on a very memorable trip across the country. I wish Alec, Emma, Albert, Tom, Olivier and Aisha all the best for their remaining time and life beyond. Iuliu was always a welcome distraction in the United States and abroad and a great help with all hardware questions. I was glad to join Carrick, Marty and David on a number of exciting research and conference trips, as well as their advisor Daniela Rus. In the last months I was also very fortunate to meet a new group of people. First, I am very thankful to Maurice for carrying on what I started I cannot imagine somebody better suited for it and also Been, Georgios, Hordur and Rob. I would also like to thank the many people of the SEA 2007 cruise, particularly the B watch and Julian, Jamie, Heather, Chris, Toby and Jane. One of the most exciting things during my time at MIT was that I was not only able to pursue my thesis topic but also two other projects. First, the flood warning project introduced me to Elizabeth Basha. We shared many joyful moments as well as blood, sweat and tears in the Central American wilderness. I hope that the end of my PhD only marks the beginning of that partnership. Second, the harbor porpoise tag project led by Stacy DeRuiter was a great design challenge. It also provided an opportunity to reach into other areas of ocean sciences by contributing to marine biology research. Her dedication along with the support from Mark Johnson, Peter Tyack and Tom Hurst ensured the project\u2019s success. The exciting results and the process leading up to them rewarded me with a better experience at MIT than I could have ever hoped for. I would like to thank John Leonard for letting me take this scenic route. The path that led me to MIT would not have been possible without the support from people in the early stages of my engineering career who I would like to thank here: Raimund Eich for patiently answering my first electrical engineering questions; my best friends Jan Horn, Daniel Steffensky, Alexander Zimmer and Ulf Radenz for helping me through my university time in Germany; and John Peatman, Ludger Becks and Uwe Zimmer for their academic guidance. Finally, I would like to thank my parents for their continued support and especially Melissa Pitotti for her encouragement not only to start the work at this institution but also to finish it when the time had come. This work was funded by Office of Naval Research grants N00014-97-1-0202, N00014-05-1-0255, N00014-02-C-0210, N00014-07-1-1102 and the ASAP MURI program led by Naomi Leonard of Princeton University. \u201cOne degree is not a large distance. On a compass it is scarcely the thickness of a fingernail. But in certain conditions, one degree can be a very large distance. Enough to unmake a man.\u201d The Mysterious Geographic Explorations of Jasper Morello, c \u00a9 3D Films, Australia 2005 Meinen Eltern & meinem Bruder"}
{"_id":"d9ad9a0dc7c4032b085aa621bba108a9e14fd83f","title":"Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization","text":"By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose BLITZ, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to `1-regularized learning, BLITZ convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. BLITZ is not specific to `1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints."}
{"_id":"aab8c9514b473c4ec9c47d780b7c79112add9008","title":"Using Case Studies in Research","text":"Case study as a research strategy often emerges as an obvious option for students and other new researchers who are seeking to undertake a modest scale research project based on their workplace or the comparison of a limited number of organisations. The most challenging aspect of the application of case study research in this context is to lift the investigation from a descriptive account of \u2018what happens\u2019 to a piece of research that can lay claim to being a worthwhile, if modest addition to knowledge. This article draws heavily on established textbooks on case study research and related areas, such as Yin, 1994, Hamel et al., 1993, Eaton, 1992, Gomm, 2000, Perry, 1998, and Saunders et al., 2000 but seeks to distil key aspects of case study research in such a way as to encourage new researchers to grapple with and apply some of the key principles of this research approach. The article explains when case study research can be used, research design, data collection, and data analysis, and finally offers suggestions for drawing on the evidence in writing up a report or dissertation."}
{"_id":"d7f016f6ceb87092bc5cff84fafd94bfe3fa4adf","title":"Comparative Evaluation of Binary Features","text":"Performance evaluation of salient features has a long-standing tradition in computer vision. In this paper, we fill the gap of evaluation for the recent wave of binary feature descriptors, which aim to provide robustness while achieving high computational efficiency. We use established metrics to embed our assessment into the body of existing evaluations, allowing us to provide a novel taxonomy unifying both traditional and novel binary features. Moreover, we analyze the performance of different detector and descriptor pairings, which are often used in practice but have been infrequently analyzed. Additionally, we complement existing datasets with novel data testing for illumination change, pure camera rotation, pure scale change, and the variety present in photo-collections. Our performance analysis clearly demonstrates the power of the new class of features. To benefit the community, we also provide a website for the automatic testing of new description methods using our provided metrics and datasets (www.cs.unc.edu\/feature-evaluation)."}
{"_id":"f8c79719e877a9dec27f0fbfb0c3e5e4fd730304","title":"Appraisal of homogeneous techniques in Distributed Data Mining: classifier approach","text":"In recent years, Distributed Data Mining (DDM) has evolved in large space aiming at minimizing computation cost and memory overhead in processing huge geographically distributed data. There are two approaches of DDM -- homogeneous and heterogeneous classifier approach. This paper presents implementation of four homogeneous classifier techniques for DDM with different Electronic Health Records (EHRs) homogeneous datasets namely diabetes, hepatitis, hypothyroid and further analyzing results based on metric evaluation."}
{"_id":"26a1ef8133da61e162c2d8142d2691c2d89584f7","title":"Effects of loneliness and differential usage of Facebook on college adjustment of first-year students","text":"The popularity of social network sites (SNSs) among college students has stimulated scholarship examining the relationship between SNS use and college adjustment. The present research furthers our understanding of SNS use by studying the relationship between loneliness, varied dimensions of Facebook use, and college adjustment among first-year students. We looked at three facets of college adjustment: social adjustment, academic motivation, and perceived academic performance. Compulsive use of Facebook had a stronger association with academic motivation than habitual use of Facebook, but neither were directly correlated with academic performance. Too much time spent on Facebook was weakly but directly associated with poorer perceived academic performance. Loneliness was a stronger indicator of college adjustment than any dimension of Facebook usage. 2014 Elsevier Ltd. All rights reserved."}
{"_id":"1e5029cf2a120c0d7453f3ecbd059f97eebbbf6f","title":"From wireless sensor networks towards cyber physical systems","text":"In the past two decades, a lot of research activities have been dedicated to the fields of mobile ad hoc network (MANET) and wireless sensor networks (WSN). More recently, the cyber physical system (CPS) has emerged as a promising direction to enrich the interactions between physical and virtual worlds. In this article, we first review some research activities inWSN, including networking issues and coverage and deployment issues. Then,we review some CPS platforms and systems that have been developed recently, including health care, navigation, rescue, intelligent transportation, social networking, and gaming applications. Through these reviews, we hope to demonstrate how CPS applications exploit the physical information collected by WSNs to bridge real and cyber spaces and identify important research challenges related to CPS designs. \u00a9 2011 Elsevier B.V. All rights reserved."}
{"_id":"bb0c3045d839f5f74b9252581cf45faa8b3b5f7e","title":"Multi-Type Itemset Embedding for Learning Behavior Success","text":"Contextual behavior modeling uses data from multiple contexts to discover patterns for predictive analysis. However, existing behavior prediction models often face difficulties when scaling for massive datasets. In this work, we formulate a behavior as a set of context items of different types (such as decision makers, operators, goals and resources), consider an observable itemset as a behavior success, and propose a novel scalable method, \"multi-type itemset embedding\", to learn the context items' representations preserving the success structures. Unlike most of existing embedding methods that learn pair-wise proximity from connection between a behavior and one of its items, our method learns item embeddings collectively from interaction among all multi-type items of a behavior, based on which we develop a novel framework, LearnSuc, for (1) predicting the success rate of any set of items and (2) finding complementary items which maximize the probability of success when incorporated into an itemset. Extensive experiments demonstrate both effectiveness and efficency of the proposed framework."}
{"_id":"0f52a233d2e20e7b270a4eed9e06aff1840a46d6","title":"The mirror-neuron system.","text":"A category of stimuli of great importance for primates, humans in particular, is that formed by actions done by other individuals. If we want to survive, we must understand the actions of others. Furthermore, without action understanding, social organization is impossible. In the case of humans, there is another faculty that depends on the observation of others' actions: imitation learning. Unlike most species, we are able to learn by imitation, and this faculty is at the basis of human culture. In this review we present data on a neurophysiological mechanism--the mirror-neuron mechanism--that appears to play a fundamental role in both action understanding and imitation. We describe first the functional properties of mirror neurons in monkeys. We review next the characteristics of the mirror-neuron system in humans. We stress, in particular, those properties specific to the human mirror-neuron system that might explain the human capacity to learn by imitation. We conclude by discussing the relationship between the mirror-neuron system and language."}
{"_id":"1aa3e563e9179841a2b3a9b026c9f8a00c7fe664","title":"Automatic Medical Concept Extraction from Free Text Clinical Reports, a New Named Entity Recognition Approach","text":"Actually in the Hospital Information Systems, there is a wide range of clinical information representation from the Electronic Health Records (EHR), and most of the information contained in clinical reports is written in natural language free text. In this context, we are researching the problem of automatic clinical named entities recognition from free text clinical reports. We are using Snomed-CT (Systematized Nomenclature of Medicine \u2013 Clinical Terms) as dictionary to identify all kind of clinical concepts, and thus the problem we are considering is to map each clinical entity named in a free text report with its Snomed-CT unique ID. More in general, we are developed a new approach for the named entity recognition (NER) problem in specific domains, and we have applied it to recognize clinical concepts in free text clinical reports. In our approach we apply two types of NER approaches, dictionary-based and machine learning-based. We use a specific domain dictionary-based gazetteer (using Snomed-CT to get the standard clinical code for the clinical concept), and the main approach that we introduce is using a unsupervised shallow learning neural network, word2vec from Mikolov et al., to represent words as vectors, and then making the recognition based on the distance between candidates and dictionary terms. We have applied our approach on a Dataset with 318.585 clinical reports in Spanish from the emergency service of the Hospital \u201cRafael M\u00e9ndez\u201d from Lorca (Murcia) Spain, and preliminary results are encouraging. Key-Words: Snomed-CT, word2vec, doc2vec, clinical information extraction, skipgram, medical terminologies, search semantic, named entity recognition, ner, medical entity recognition"}
{"_id":"a72f412d336d69a82424cb7d610bb1bb9d81ef7c","title":"Avoiding monotony: improving the diversity of recommendation lists","text":"The primary premise upon which top-N recommender systems operate is that similar users are likely to have similar tastes with regard to their product choices. For this reason, recommender algorithms depend deeply on similarity metrics to build the recommendation lists for end-users.\n However, it has been noted that the products offered on recommendation lists are often too similar to each other and attention has been paid towards the goal of improving diversity to avoid monotonous recommendations.\n Noting that the retrieval of a set of items matching a user query is a common problem across many applications of information retrieval, we model the competing goals of maximizing the diversity of the retrieved list while maintaining adequate similarity to the user query as a binary optimization problem. We explore a solution strategy to this optimization problem by relaxing it to a trust-region problem.This leads to a parameterized eigenvalue problem whose solution is finally quantized to the required binary solution. We apply this approach to the top-N prediction problem, evaluate the system performance on the Movielens dataset and compare it with a standard item-based top-N algorithm. A new evaluation metric ItemNovelty is proposed in this work. Improvements on both diversity and accuracy are obtained compared to the benchmark algorithm."}
{"_id":"0015fa48e4ab633985df789920ef1e0c75d4b7a8","title":"Training Support Vector Machines: an Application to Face Detection","text":"Detection (To appear in the Proceedings of CVPR'97, June 17-19, 1997, Puerto Rico.) Edgar Osunay? Robert Freund? Federico Girosiy yCenter for Biological and Computational Learning and ?Operations Research Center Massachusetts Institute of Technology Cambridge, MA, 02139, U.S.A. Abstract We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs.) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classi ers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points."}
{"_id":"0224e11e8582dd35b32203e9da064d4a3935a792","title":"Fast Pose Estimation with Parameter-Sensitive Hashing","text":"Example-basedmethodsareeffectivefor parameterestimationproblemswhentheunderlyingsystemis simpleor thedimensionalityof the input is low. For complex andhigh-dimensional problemssuch asposeestimation,thenumberof required examplesand the computationalcomplexity rapidly becmeprohibitivelyhigh. We introducea new algorithm that learnsa setof hashingfunctionsthat efficiently index examplesrelevant to a particular estimationtask. Our algorithm extendsa recentlydevelopedmethodfor locality-sensitivehashing, which findsapproximateneighborsin timesublinearin thenumber of examples.Thismethoddependscritically on thechoiceof hashfunctions;weshowhowto find thesetof hashfunctions thatare optimallyrelevantto a particular estimationproblem.Experimentsdemonstr atethat theresultingalgorithm,which wecall Parameter -SensitiveHashing, canrapidlyandaccuratelyestimatethearticulatedposeof humanfiguresfroma large databaseof exampleimages. 0Part of thiswork wasdonewhenG.S.andP.V. werewith MitsubishiElectricResearchLabs,Cambridge,MA."}
{"_id":"0122e063ca5f0f9fb9d144d44d41421503252010","title":"Large Scale Distributed Deep Networks","text":"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestlysized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."}
{"_id":"15b45650fa30c56bdc4c595a5afd31663f7f3eb4","title":"Does Language Shape Thought?: Mandarin and English Speakers' Conceptions of Time","text":"Does the language you speak affect how you think about the world? This question is taken up in three experiments. English and Mandarin talk about time differently--English predominantly talks about time as if it were horizontal, while Mandarin also commonly describes time as vertical. This difference between the two languages is reflected in the way their speakers think about time. In one study, Mandarin speakers tended to think about time vertically even when they were thinking for English (Mandarin speakers were faster to confirm that March comes earlier than April if they had just seen a vertical array of objects than if they had just seen a horizontal array, and the reverse was true for English speakers). Another study showed that the extent to which Mandarin-English bilinguals think about time vertically is related to how old they were when they first began to learn English. In another experiment native English speakers were taught to talk about time using vertical spatial terms in a way similar to Mandarin. On a subsequent test, this group of English speakers showed the same bias to think about time vertically as was observed with Mandarin speakers. It is concluded that (1) language is a powerful tool in shaping thought about abstract domains and (2) one's native language plays an important role in shaping habitual thought (e.g., how one tends to think about time) but does not entirely determine one's thinking in the strong Whorfian sense."}
{"_id":"9005c34200880bc2ca0bad398d0a6391667a2dfc","title":"Disability studies as a source of critical inquiry for the field of assistive technology","text":"Disability studies and assistive technology are two related fields that have long shared common goals - understanding the experience of disability and identifying and addressing relevant issues. Despite these common goals, there are some important differences in what professionals in these fields consider problems, perhaps related to the lack of connection between the fields. To help bridge this gap, we review some of the key literature in disability studies. We present case studies of two research projects in assistive technology and discuss how the field of disability studies influenced that work, led us to identify new or different problems relevant to the field of assistive technology, and helped us to think in new ways about the research process and its impact on the experiences of individuals who live with disability. We also discuss how the field of disability studies has influenced our teaching and highlight some of the key publications and publication venues from which our community may want to draw more deeply in the future."}
{"_id":"0e2795b1329b25ba3709584b96fd5cb4c96f6f22","title":"A Systematic Comparison of Various Statistical Alignment Models","text":"We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."}
{"_id":"e92771cf6244a4b5965f3cae60d16131774b794c","title":"Linear codes over Z4+uZ4: MacWilliams identities, projections, and formally self-dual codes","text":"Linear codes are considered over the ring Z 4 + uZ 4 , a non-chain extension of Z 4. Lee weights, Gray maps for these codes are defined and MacWilliams identities for the complete, symmetrized and Lee weight enumer-ators are proved. Two projections from Z 4 + uZ 4 to the rings Z 4 and F 2 + uF 2 are considered and self-dual codes over Z 4 +uZ 4 are studied in connection with these projections. Finally three constructions are given for formally self-dual codes over Z 4 + uZ 4 and their Z 4-images together with some good examples of formally self-dual Z 4-codes obtained through these constructions."}
{"_id":"fc40ad1238fba787dd8a58a7aed57a8d020a6fdc","title":"Artificial neural networks: fundamentals, computing, design, and application.","text":"Artificial neural networks (ANNs) are relatively new computational tools that have found extensive utilization in solving many complex real-world problems. The attractiveness of ANNs comes from their remarkable information processing characteristics pertinent mainly to nonlinearity, high parallelism, fault and noise tolerance, and learning and generalization capabilities. This paper aims to familiarize the reader with ANN-based computing (neurocomputing) and to serve as a useful companion practical guide and toolkit for the ANNs modeler along the course of ANN project development. The history of the evolution of neurocomputing and its relation to the field of neurobiology is briefly discussed. ANNs are compared to both expert systems and statistical regression and their advantages and limitations are outlined. A bird's eye review of the various types of ANNs and the related learning rules is presented, with special emphasis on backpropagation (BP) ANNs theory and design. A generalized methodology for developing successful ANNs projects from conceptualization, to design, to implementation, is described. The most common problems that BPANNs developers face during training are summarized in conjunction with possible causes and remedies. Finally, as a practical application, BPANNs were used to model the microbial growth curves of S. flexneri. The developed model was reasonably accurate in simulating both training and test time-dependent growth curves as affected by temperature and pH."}
{"_id":"25406e6733a698bfc4ac836f8e74f458e75dad4f","title":"What Size Net Gives Valid Generalization?","text":"We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1\/8. We show that if m O(W\/\u220a log N\/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a\/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W\/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."}
{"_id":"656a33c1db546da8490d6eba259e2a849d73a001","title":"Learning in Artificial Neural Networks: A Statistical Perspective","text":"The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."}
{"_id":"fbe24a2d9598c620324e3bd51e2f817cd35e9c81","title":"Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights","text":"A two-layer neural network can be used to approximate any nonlinear function. T h e behavior of the hidden nodes tha t allows the network to do this is described. Networks w i th one input are analyzed first, and the analysis is then extended to networks w i t h mult iple inputs. T h e result of th is analysis is used to formulate a method for ini t ial izat ion o f the weights o f neural networks to reduce t ra in ing t ime. Training examples are given and the learning curve for these examples are shown to illustrate the decrease in necessary training t ime. Introduction Two-layer feed forward neural networks have been proven capable of approximating any arbitrary functions [l], given that they have sufficient numbers of nodes in their hidden layers. We offer a description of how this works, along with a method of speeding up the training process by choosing the networks\u2019 initial weights. The relationship between the inputs and the output of a two-layer neural network may be described by Equation (1) H l y = wi . sigmoid(LqX + W b i ) (1) i=O where y is the network\u2019s output, X is the input vector, H is the number of hidden nodes, Wi is the weight vector of the i th node of the hidden layer, Wbi is the bias weight of the ith hidden node, w i is the weight of the output layer which connects the i th hidden unit to the output. The behavior of hidden nodes in two-layer networks with one input To illustrate the behavior of the hidden nodes, a two-layer network with one input is trained to approximate a function of one variable d(z) . That is, the network is trained to produce d(z) given z as input using the back-propagation algorithm [2]. The output of the network is given as"}
{"_id":"f66696ce5c60b8fe71590521718213316fb2ea2f","title":"Large Margin Metric Learning for Multi-Label Prediction","text":"Canonical correlation analysis (CCA) and maximum margin output coding (MMOC) methods have shown promising results for multi-label prediction, where each instance is associated with multiple labels. However, these methods require an expensive decoding procedure to recover the multiple labels of each testing instance. The testing complexity becomes unacceptable when there are many labels. To avoid decoding completely, we present a novel large margin metric learning paradigm for multi-label prediction. In particular, the proposed method learns a distance metric to discover label dependency such that instances with very different multiple labels will be moved far away. To handle many labels, we present an accelerated proximal gradient procedure to speed up the learning process. Comprehensive experiments demonstrate that our proposed method is significantly faster than CCA and MMOC in terms of both training and testing complexities. Moreover, our method achieves superior prediction performance compared with state-of-the-art methods."}
{"_id":"4afa918506a45be77b0682156c3dfdc956272fa3","title":"Optimal Design of Energy-Efficient Multi-User MIMO Systems: Is Massive MIMO the Answer?","text":"Assume that a multi-user multiple-input multiple-output (MIMO) system is designed from scratch to uniformly cover a given area with maximal energy efficiency (EE). What are the optimal number of antennas, active users, and transmit power? The aim of this paper is to answer this fundamental question. We consider jointly the uplink and downlink with different processing schemes at the base station and propose a new realistic power consumption model that reveals how the above parameters affect the EE. Closed-form expressions for the EE-optimal value of each parameter, when the other two are fixed, are provided for zero-forcing (ZF) processing in single-cell scenarios. These expressions prove how the parameters interact. For example, in sharp contrast to common belief, the transmit power is found to increase (not to decrease) with the number of antennas. This implies that energy-efficient systems can operate in high signal-to-noise ratio regimes in which interference-suppressing signal processing is mandatory. Numerical and analytical results show that the maximal EE is achieved by a massive MIMO setup wherein hundreds of antennas are deployed to serve a relatively large number of users using ZF processing. The numerical results show the same behavior under imperfect channel state information and in symmetric multi-cell scenarios."}
{"_id":"0d94ee90e5d91fa1eb3d5e26d8a2cd9a1e57812c","title":"A systematic analysis of textual variability modeling languages","text":"Industrial variability models tend to grow in size and complexity due to ever-increasing functionality and complexity of software systems. Some authors report on variability models specifying several thousands of variabilities. However, traditional variability modeling approaches do not seem to scale adequately to cope with size and complexity of such models. Recently, textual variability modeling languages have been advocated as one scalable solution.\n In this paper, we provide a systematic analysis of the capabilities of current textual variability modeling languages, in particular regarding variability management in the large. Towards this aim, we define a classification schema consisting of five dimensions, classify ten different textual variability modeling languages using the classification schema and provide an analysis. In summary, some textual variability modeling languages go beyond textual representations of traditional variability modeling approaches and provide sophisticated modeling concepts and constraint languages. Three textual variability modeling approaches already support mechanisms for large-scale variability modeling such as model composition, modularization, or evolution support."}
{"_id":"0a70ea1496ccd01ea3e51afe60f508ee6c0984ec","title":"Cracking the Code of Biodiversity Responses to Past Climate Change.","text":"How individual species and entire ecosystems will respond to future climate change are among the most pressing questions facing ecologists. Past biodiversity dynamics recorded in the paleoecological archives show a broad array of responses, yet significant knowledge gaps remain. In particular, the relative roles of evolutionary adaptation, phenotypic plasticity, and dispersal in promoting survival during times of climate change have yet to be clarified. Investigating the paleo-archives offers great opportunities to understand biodiversity responses to future climate change. In this review we discuss the mechanisms by which biodiversity responds to environmental change, and identify gaps of knowledge on the role of range shifts and tolerance. We also outline approaches at the intersection of paleoecology, genomics, experiments, and predictive models that will elucidate the processes by which species have survived past climatic changes and enhance predictions of future changes in biological diversity."}
{"_id":"5dd5f3844c0402141e4083bccdde66303750f87c","title":"Application of Artificial Intelligence to Real-Time Fault Detection in Permanent-Magnet Synchronous Machines","text":"This paper discusses faults in rotating electrical machines in general and describes a fault detection technique using artificial neural network (ANN) which is an expert system to detect short-circuit fault currents in the stator windings of a permanent-magnet synchronous machine (PMSM). The experimental setup consists of PMSM coupled mechanically to a dc motor configured to run in torque mode. Particle swarm optimization is used to adjust the weights of the ANN. All simulations are carried out in MATLAB\/SIMULINK environment. The technique is shown to be effective and can be applied to real-time fault detection."}
{"_id":"38438103e787b7b2d112596fd14225872a5403f3","title":"Language Models with Pre-Trained (GloVe) Word Embeddings","text":"In this work we present a step-by-step implementation of training a Language Model (LM) , using Recurrent Neural Network (RNN) and pre-trained GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4]. The implementation presented is based on using keras1 [5]."}
{"_id":"e22be626987f1288744bb9f0ffc60806b4ed8bbc","title":"Comparison study of non-orthogonal multiple access schemes for 5G","text":"With the development of mobile Internet and Internet of things (IoT), the 5th generation (5G) wireless communications will foresee explosive increase in mobile traffic. To address challenges in 5G such as higher spectral efficiency, massive connectivity, and lower latency, some non-orthogonal multiple access (NOMA) schemes have been recently actively investigated, including power-domain NOMA, multiple access with low-density spreading (LDS), sparse code multiple access (SCMA), multiuser shared access (MUSA), pattern division multiple access (PDMA), etc. Different from conventional orthogonal multiple access (OMA) schemes, NOMA can realize overloading by introducing some controllable interferences at the cost of slightly increased receiver complexity, which can achieve significant gains in spectral efficiency and accommodate much more users. In this paper, we will discuss basic principles and key features of three typical NOMA schemes, i.e., SCMA, MUSA, and PDMA. What's more, their performance in terms of uplink bit error rate (BER) will be compared. Simulation results show that in typical Rayleigh fading channels, SCMA has the best performance, while the BER performance of MUSA and PDMA are very close to each other. In addition, we also analyze the performance of PDMA using the same factor graph as SCMA, which indicates that the performance gain of SCMA over PDMA comes from both the difference of factor graph and the codebook optimization."}
{"_id":"1ac8d6ec012f0bca057ea0ca71df8c8746cf39c5","title":"RPL-based multipath Routing Protocols for Internet of Things on Wireless Sensor Networks","text":"In the last few years, Wireless Sensor Network (WSN) emerges and appears as an essential platform for prominent concept of Internet of Things (IoT). Their application ranges from so-called \u201csmart cities\u201d, \u201csmart homes\u201d over environmental monitoring. The connectivity in IoT mainly relies on RPL (IPv6 Routing Protocol for Low Power and Lossy Network) - a routing algorithm that constructs and maintains DODAGs (Destination Oriented Directed Acyclic Graph) to transmit data from sensors to root over a single path. However, due to the resource constraints of sensor nodes and the unreliability of wireless links, single-path routing approaches cannot be considered effective techniques to meet the performance demands of various applications. In order to overcome these problems, many individuals and group research focuses on multi-path solutions for RPL routing protocol. In this paper, we propose three multipath schemes based on RPL (Energy Load Balancing-ELB, Fast Local Repair-FLR and theirs combination-ELB-FLR) and integrate them in a modified IPv6 communication stack for IoT. These schemes are implemented in OMNET++ simulator and the experiment outcomes show that our approaches have achieved better energy efficiency, better end-to-end delay, packet delivery rate and network load balance compared to traditional solution of RPL."}
{"_id":"041326c202655cd60df276bf7a148f2ecddfc479","title":"Cognitive architectures: Research issues and challenges","text":"In this paper, we examine the motivations for research on cognitive architectures and review some candidates that have been explored in the literature. After this, we consider the capabilities that a cognitive architecture should support, some properties that it should exhibit related to representation, organization, performance, and learning, and some criteria for evaluating such architectures at the systems level. In closing, we discuss some open issues that should drive future research in this important area. 2008 Published by Elsevier B.V."}
{"_id":"e2a3bbfd375811c5fef523be8623904455af1cec","title":"GRS: The green, reliability, and security of emerging machine to machine communications","text":"Machine-to-machine communications is characterized by involving a large number of intelligent machines sharing information and making collaborative decisions without direct human intervention. Due to its potential to support a large number of ubiquitous characteristics and achieving better cost efficiency, M2M communications has quickly become a market-changing force for a wide variety of real-time monitoring applications, such as remote e-healthcare, smart homes, environmental monitoring, and industrial automation. However, the flourishing of M2M communications still hinges on fully understanding and managing the existing challenges: energy efficiency (green), reliability, and security (GRS). Without guaranteed GRS, M2M communications cannot be widely accepted as a promising communication paradigm. In this article, we explore the emerging M2M communications in terms of the potential GRS issues, and aim to promote an energy-efficient, reliable, and secure M2M communications environment. Specifically, we first formalize M2M communications architecture to incorporate three domains - the M2M, network, and application domains - and accordingly define GRS requirements in a systematic manner. We then introduce a number of GRS enabling techniques by exploring activity scheduling, redundancy utilization, and cooperative security mechanisms. These techniques hold promise in propelling the development and deployment of M2M communications applications."}
{"_id":"b6bf558edda0378cd756a0267801164109f5f5c5","title":"Adverse drug reactions as cause of admission to hospital: prospective analysis of 18 820 patients.","text":"OBJECTIVE\nTo ascertain the current burden of adverse drug reactions (ADRs) through a prospective analysis of all admissions to hospital.\n\n\nDESIGN\nProspective observational study.\n\n\nSETTING\nTwo large general hospitals in Merseyside, England.\n\n\nPARTICIPANTS\n18 820 patients aged > 16 years admitted over six months and assessed for cause of admission.\n\n\nMAIN OUTCOME MEASURES\nPrevalence of admissions due to an ADR, length of stay, avoidability, and outcome.\n\n\nRESULTS\nThere were 1225 admissions related to an ADR, giving a prevalence of 6.5%, with the ADR directly leading to the admission in 80% of cases. The median bed stay was eight days, accounting for 4% of the hospital bed capacity. The projected annual cost of such admissions to the NHS is 466m pounds sterling (706m Euros, 847m dollars). The overall fatality was 0.15%. Most reactions were either definitely or possibly avoidable. Drugs most commonly implicated in causing these admissions included low dose aspirin, diuretics, warfarin, and non-steroidal anti-inflammatory drugs other than aspirin, the most common reaction being gastrointestinal bleeding.\n\n\nCONCLUSION\nThe burden of ADRs on the NHS is high, accounting for considerable morbidity, mortality, and extra costs. Although many of the implicated drugs have proved benefit, measures need to be put into place to reduce the burden of ADRs and thereby further improve the benefit:harm ratio of the drugs."}
{"_id":"ed6e0366584769a5b4c45b4fdf847583038611ae","title":"Health-related rumour detection on Twitter","text":"In the last years social networks have emerged as a critical mean for information spreading. In spite of all the positive consequences this phenomenon brings, unverified and instrumentally relevant information statements in circulation, named as rumours, are becoming a potential threat to the society. Recently, there have been several studies on topic-independent rumour detection on Twitter. In this paper we present a novel rumour detection system which focuses on a specific topic, that is health-related rumours on Twitter. To this aim, we constructed a new subset of features including influence potential and network characteristics features. We tested our approach on a real dataset observing promising results, as it is able to correctly detect about 89% of rumours, with acceptable levels of precision."}
{"_id":"cf1fb664b28fa5dd4952d08c1cb631c20a504b02","title":"Full bridge phase-shifted soft switching high-frequency inverter with boost PFC function for induction heating system","text":"This paper is mainly concerned with a high frequency soft-switching PWM inverter suitable for consumer induction heating system. Proposed system is composed of soft switching chopper based voltage boost PFC input stage and phase shifted PWM controlled full bridge ZCZVS high frequency inverter stage. Its fundamentals of operating performances are illustrated and evaluated on the experimental results. Its effectiveness is substantially proved on the basis of the experimental results from a practical point of view."}
{"_id":"68af6bcc37d08863af5bb081301e562b597cb4fc","title":"Automatic Modulation Classification of Overlapped Sources Using Multi-Gene Genetic Programming With Structural Risk Minimization Principle","text":"As the spectrum environment becomes increasingly crowded and complicated, primary users may be interfered by secondary users and other illegal users. Automatic modulation classification (AMC) of a single source cannot recognize the overlapped sources. Consequently, the AMC of overlapped sources attracts much attention. In this paper, we propose a genetic programming-based modulation classification method for overlapped sources (GPOS). The proposed GPOS consists of two stages, the training stage, and the classification stage. In the training stage, multi-gene genetic programming (MGP)-based feature engineering transforms sample estimates of cumulants into highly discriminative MGP-features iteratively, until optimal MGP-features (OMGP-features) are obtained, where the structural risk minimization principle (SRMP) is employed to evaluate the classification performance of MGP-features and train the classifier. Moreover, a self-adaptive genetic operation is designed to accelerate the feature engineering process. In the classification stage, the classification decision is made by the trained classifier using the OMGP-features. Through simulation results, we demonstrate that the proposed scheme outperforms other existing methods in terms of classification performance and robustness in case of varying power ratios and fading channel."}
{"_id":"87a7ccc5f37cd846a978ae17d60b6dcd923bd996","title":"Measuring geographical regularities of crowd behaviors for Twitter-based geo-social event detection","text":"Recently, microblogging sites such as Twitter have garnered a great deal of attention as an advanced form of location-aware social network services, whereby individuals can easily and instantly share their most recent updates from any place. In this study, we aim to develop a geo-social event detection system by monitoring crowd behaviors indirectly via Twitter. In particular, we attempt to find out the occurrence of local events such as local festivals; a considerable number of Twitter users probably write many posts about these events. To detect such unusual geo-social events, we depend on geographical regularities deduced from the usual behavior patterns of crowds with geo-tagged microblogs. By comparing these regularities with the estimated ones, we decide whether there are any unusual events happening in the monitored geographical area. Finally, we describe the experimental results to evaluate the proposed unusuality detection method on the basis of geographical regularities obtained from a large number of geo-tagged tweets around Japan via Twitter."}
{"_id":"bccda0e4b34cc1b73b50f0faeb1d340919619825","title":"Classic Hallucinogens and Mystical Experiences: Phenomenology and Neural Correlates.","text":"This chapter begins with a brief review of descriptions and definitions of mystical-type experiences and the historical connection between classic hallucinogens and mystical experiences. The chapter then explores the empirical literature on experiences with classic hallucinogens in which claims about mystical or religious experiences have been made. A psychometrically validated questionnaire is described for the reliable measurement of mystical-type experiences occasioned by classic hallucinogens. Controlled laboratory studies show that under double-blind conditions that provide significant controls for expectancy bias, psilocybin can occasion complete mystical experiences in the majority of people studied. These effects are dose-dependent, specific to psilocybin compared to placebo or a psychoactive control substance, and have enduring impact on the moods, attitudes, and behaviors of participants as assessed by self-report of participants and ratings by community observers. Other studies suggest that enduring personal meaning in healthy volunteers and therapeutic outcomes in patients, including reduction and cessation of substance abuse behaviors and reduction of anxiety and depression in patients with a life-threatening cancer diagnosis, are related to the occurrence of mystical experiences during drug sessions. The final sections of the chapter draw parallels in human neuroscience research between the neural bases of experiences with classic hallucinogens and the neural bases of meditative practices for which claims of mystical-type experience are sometimes made. From these parallels, a functional neural model of mystical experience is proposed, based on changes in the default mode network of the brain that have been observed after the administration of classic hallucinogens and during meditation practices for which mystical-type claims have been made."}
{"_id":"97626d505052d2eb6ebeb194d4ba3d993c320fe4","title":"Sparsely sampled Fourier ptychography.","text":"Fourier ptychography (FP) is an imaging technique that applies angular diversity functions for high-resolution complex image recovery. The FP recovery routine switches between two working domains: the spectral and spatial domains. In this paper, we investigate the spectral-spatial data redundancy requirement of the FP recovery process. We report a sparsely sampled FP scheme by exploring the sampling interplay between these two domains. We demonstrate the use of the reported scheme for bypassing the high-dynamic-range combination step in the original FP recovery routine. As such, it is able to shorten the acquisition time of the FP platform by ~50%. As a special case of the sparsely sample FP, we also discuss a sub-sampled scheme and demonstrate its application in solving the pixel aliasing problem plagued in the original FP algorithm. We validate the reported schemes with both simulations and experiments. This paper provides insights for the development of the FP approach."}
{"_id":"fdbf82918de37e14d047893c074f59deb2879f32","title":"Threats to Feminist Identity and Reactions to Gender Discrimination","text":"The aim of this research was to examine conditions that modify feminists' support for women as targets of gender discrimination. In an experimental study we tested a hypothesis that threatened feminist identity will lead to greater differentiation between feminists and conservative women as victims of discrimination and, in turn, a decrease in support for non-feminist victims. The study was conducted among 96 young Polish female professionals and graduate students from Gender Studies programs in Warsaw who self-identified as feminists (Mage \u2009=\u200922.23). Participants were presented with a case of workplace gender discrimination. Threat to feminist identity and worldview of the discrimination victim (feminist vs. conservative) were varied between research conditions. Results indicate that identity threat caused feminists to show conditional reactions to discrimination. Under identity threat, feminists perceived the situation as less discriminatory when the target held conservative views on gender relations than when the target was presented as feminist. This effect was not observed under conditions of no threat. Moreover, feminists showed an increase in compassion for the victim when she was portrayed as a feminist compared to when she was portrayed as conservative. Implications for the feminist movement are discussed."}
{"_id":"9b9906a2cf7fe150faed8d618def803232684719","title":"Dynamic Filters in Graph Convolutional Networks","text":"Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. While CNNs naturally extend to other domains, such as audio and video, where data is also organized in rectangular grids, they do not easily generalize to other types of data such as 3D shape meshes, social network graphs or molecular graphs. To handle such data, we propose a novel graph-convolutional network architecture that builds on a generic formulation that relaxes the 1-to-1 correspondence between filter weights and data elements around the center of the convolution. The main novelty of our architecture is that the shape of the filter is a function of the features in the previous network layer, which is learned as an integral part of the neural network. Experimental evaluations on digit recognition, semi-supervised document classification, and 3D shape correspondence yield state-of-the-art results, significantly improving over previous work for shape correspondence."}
{"_id":"50dad1b5f35c0ba613fd79fae91d7270c64cea0f","title":"BINSEC\/SE: A Dynamic Symbolic Execution Toolkit for Binary-Level Analysis","text":"When it comes to software analysis, several approaches exist from heuristic techniques to formal methods, which are helpful at solving different kinds ofproblems. Unfortunately very few initiative seek to aggregate this techniques in the same platform. BINSEC intend to fulfill this lack of binary analysis platform by allowing to perform modular analysis. This work focusses on BINSEC\/SE, the new dynamic symbolic execution engine (DSE) implemented in BINSEC. We will highlight the novelties of the engine, especially in terms of interactions between concrete and symbolic execution or optimization of formula generation. Finally, two reverse engineering applications are shown in order to emphasize the tool effectiveness."}
{"_id":"4a5f9152c66f61158abc8e9eaa8de743129cdbba","title":"Artificial intelligent firewall","text":"Firewalls are now an integral part of network security. An intelligent firewall that prevents unauthorized access to a system has been developed. Artificial intelligence applications are uniquely suited for the ever-changing, ever-evolving world of network security. Typical firewalls are only as good as the information provided by the Network Administrator. A new type of attack creates vulnerabilities, which a static firewall does not have the ability to avoid without human direction. An AI-managed firewall service, however, can protect a computer network from known and future threats. We report in this paper on research in progress concerning the integration of different security techniques. A main purpose of the project is to integrate a smart detection engine into a firewall. The smart detection engine will aim at not only detecting anomalous network traffic as in classical IDSs, but also detecting unusual structures data packets that suggest the presence of virus data. We will report in this paper on the concept of an intelligent firewall that contains a smart detection engine for potentially malicious data packets."}
{"_id":"bc12715a1ddf1a540dab06bf3ac4f3a32a26b135","title":"Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking","text":"Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research. We present a benchmark for Multiple Object Tracking launched in the late 2014, with the goal of creating a framework for the standardized evaluation of multiple object tracking methods. This paper collects the two releases of the benchmark made so far, and provides an in-depth analysis of almost 50 state-of-the-art trackers that were tested on over 11000 frames. We show the current trends and weaknesses of multiple people tracking methods, and provide pointers of what researchers should be focusing on to push the field forward."}
{"_id":"2e0db4d4c8bdc7e11541b362cb9f8972f66563ab","title":"Personality Analysis Through Handwriting","text":""}
{"_id":"02599a02d46ea2f1c00e14cac2a76dcb156df8ee","title":"Search Based Software Engineering","text":"The use of evolutionary algorithms for solving multi-objective optimization problems has become increasingly popular, mainly within the last 15 years. From among the several research trends that have originated in recent years, one of the most promising is the use of hybrid approaches that allow to improve the performance of multi-objective evolutionary algorithms (MOEAs). In this talk, some of the most representative research on the use of hybrid approaches in evolutionary multi-objective optimization will be discussed. The topics discussed will include multi-objective memetic algorithms, hybridization of MOEAs with gradient-based methods and with direct search methods, as well as multi-objective hyperheuristics. Some applications of these approaches as well as some potential paths for future research in this area will also be briefly discussed. Towards Evolutionary Multitasking: A New Paradigm"}
{"_id":"2b286ed9f36240e1d11b585d65133db84b52122c","title":"Real-time 3D eyelids tracking from semantic edges","text":"State-of-the-art real-time face tracking systems still lack the ability to realistically portray subtle details of various aspects of the face, particularly the region surrounding the eyes. To improve this situation, we propose a technique to reconstruct the 3D shape and motion of eyelids in real time. By combining these results with the full facial expression and gaze direction, our system generates complete face tracking sequences with more detailed eye regions than existing solutions in real-time. To achieve this goal, we propose a generative eyelid model which decomposes eyelid variation into two low-dimensional linear spaces which efficiently represent the shape and motion of eyelids. Then, we modify a holistically-nested DNN model to jointly perform semantic eyelid edge detection and identification on images. Next, we correspond vertices of the eyelid model to 2D image edges, and employ polynomial curve fitting and a search scheme to handle incorrect and partial edge detections. Finally, we use the correspondences in a 3D-to-2D edge fitting scheme to reconstruct eyelid shape and pose. By integrating our fast fitting method into a face tracking system, the estimated eyelid results are seamlessly fused with the face and eyeball results in real time. Experiments show that our technique applies to different human races, eyelid shapes, and eyelid motions, and is robust to changes in head pose, expression and gaze direction."}
{"_id":"1fdc785c0152d86d661213038150195058a24703","title":"Sparse deep belief net model for visual area V2","text":"Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or \u201cdeep,\u201d structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both colinear (\u201ccontour\u201d) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex \u201ccorner\u201d features matches well with the results from the Ito & Komatsu\u2019s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features."}
{"_id":"08bb1bc1cc40f3a0dfe264c80105fcd82e5a70d1","title":"New reverse-conducting IGBT (1200V) with revolutionary compact package","text":"Fuji Electric developed a 1200V class RC-IGBT based on our latest thin wafer process. The performance of this RC-IGBT shows the same relationship between conduction loss and switching loss as our 6th generation conventional IGBT and FWD. In addition its trade-off can be optimized for hard switching by lifetime killer. Calculations of the hard switching inverter loss and chip junction temperature (Tj) show that the optimized RC-IGBT can handle 35% larger current density per chip area. In order to utilize the high performance characteristics of the RC-IGBT, we assembled them in our newly developed compact package. This module can handle 58% higher current than conventional 100A modules at a 51% smaller footprint."}
{"_id":"65e9f4d7a80ea39a265b9d60d57397011395efcc","title":"Tracing Data Lineage Using Schema Transformation Pathways","text":"With the increasing amount and diversity of information available on the Internet, there has been a huge growth in information systems that need to integrate data from distributed, heterogeneous data sources. Tracing the lineage of the integrated data is one of the current problems being addressed in data warehouse research. In this paper, we propose a new approach for tracing data linage based on schema transformation pathways. We show how the individual transformation steps in a transformation pathway can be used to trace the derivation of the integrated data in a step-wise fashion, thus simplifying the lineage tracing process."}
{"_id":"8ef7b152c35434eba0c5f1cf03051a65426e3463","title":"Electromagnetic energy harvesting from train induced railway track vibrations","text":"Anelectromagnetic energy harvester is designed to harness the vibrational power from railroad track deflections due to passing trains. Whereas typical existing vibration energy harvester technologies are built for low power applications of milliwatts range, the proposed harvester will be designed for higher power applications for major track-side equipment such as warning signals, switches, and health monitoring sensors, which typically require a power supply of 10 Watts or more. To achieve this goal, we implement a new patent pending motion conversion mechanism which converts irregular pulse-like bidirectional linear vibration into regulated unidirectional rotational motion. Features of the motion mechanism include bidirectional to unidirectional conversion and flywheel speed regulation, with advantages of improved reliability, efficiency, and quality of output power. It also allows production of DC power directly from bidirectional vibration without electronic diodes. Preliminary harvester prototype testing results illustrate the features and benefits of the proposed motion mechanism, showing reduction of continual system loading, regulation of generator speed, and capability for continuous DC power generation."}
{"_id":"064d2ddd75aa931899136173c8364ced0c5cbea3","title":"Comparison of Measurement-Based Call Admission Control Algorithms for Controlled-Load Service","text":"We compare the performance of four admission control algorithms\u2014one parameter-based and three measurementbased\u2014for controlled-load service. The parameter-based admission control ensures that the sum of reserved resources is bounded by capacity. The three measurementbased algorithms are based on measured bandwidth, acceptance region [9], and equivalent bandwidth [7]. We use simulationon several network scenarios to evaluate the link utilization and adherence to service commitment achieved by these four algorithms."}
{"_id":"574ec01e67e69a072e1e7cf4d6138d43491170ea","title":"Planar Differential Elliptical UWB Antenna Optimization","text":"A recently proposed optimization procedure, based on the time domain characteristics of an antenna, is exploited to design a planar differential elliptical antenna for ultrawideband (UWB) applications. The optimization procedure aims at finding an antenna not only with low VSWR but also one exhibiting low-dispersion characteristics over the relevant frequency band. Furthermore, since in pulse communications systems the input signal is often of a given form, suited to a particular purpose, the optimization procedure also aims at finding the best antenna for the given input signal form. Specifically, the optimized antenna is designed for high temporal correlation between its electric field intensity signal and the given input signal. The optimization technique followed in this work makes use of genetic algorithm (GA) search concepts. The electromagnetic analysis of the antenna is done by means of a finite-difference time-domain method using the commercially available CST Microwave Studio software"}
{"_id":"db9bbebbe0c4dab997583b2a3de2bca6b2217af9","title":"A systemic and cognitive view on collaborative knowledge building with wikis","text":"Wikis provide new opportunities for learning and for collaborative knowledge building as well as for understanding these processes. This article presents a theoretical framework for describing how learning and collaborative knowledge building take place. In order to understand these processes, three aspects need to be considered: the social processes facilitated by a wiki, the cognitive processes of the users, and how both processes influence each other mutually. For this purpose, the model presented in this article borrows from the systemic approach of Luhmann as well as from Piaget\u2019s theory of equilibration and combines these approaches. The model analyzes processes which take place in the social system of a wiki as well as in the cognitive systems of the users. The model also describes learning activities as processes of externalization and internalization. Individual learning happens through internal processes of assimilation and accommodation, whereas changes in a wiki are due to activities of external assimilation and accommodation which in turn lead to collaborative knowledge building. This article provides empirical examples for these equilibration activities by analyzing Wikipedia articles. Equilibration activities are described as being caused by subjectively perceived incongruities between an individuals\u2019 knowledge and the information provided by a wiki. Incongruities of medium level cause cognitive conflicts which in turn activate the described processes of equilibration and facilitate individual learning and collaborative knowledge building."}
{"_id":"1005645c05585c2042e3410daeed638b55e2474d","title":"A Scalable Hierarchical Distributed Language Model","text":"Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models."}
{"_id":"0b47b6ffe714303973f40851d975c042ff4fcde1","title":"Distributional Clustering of English Words","text":"We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \u201csoft\u201d clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."}
{"_id":"0c9c4daa230bcc62cf3d78236ccf54ff686a36e0","title":"Biobanks: transnational, European and global networks.","text":"Biobanks contain biological samples and associated information that are essential raw materials for advancement of biotechnology, human health, and research and development in life sciences. Population-based and disease-oriented biobanks are major biobank formats to establish the disease relevance of human genes and provide opportunities to elucidate their interaction with environment and lifestyle. The developments in personalized medicine require molecular definition of new disease subentities and biomarkers for identification of relevant patient subgroups for drug development. These emerging demands can only be met if biobanks cooperate at the transnational or even global scale. Establishment of common standards and strategies to cope with the heterogeneous legal and ethical landscape in different countries are seen as major challenges for biobank networks. The Central Research Infrastructure for Molecular Pathology (CRIP), the concept for a pan-European Biobanking and Biomolecular Resources Research Infrastructure (BBMRI), and the Organization for Economic Co-operation and Development (OECD) global Biological Resources Centres network are examples for transnational, European and global biobank networks that are described in this article."}
{"_id":"449b47c55dac9cae588086dde9249caa230e01b1","title":"Indistinguishability of Random Systems","text":"An (X ,Y)-random system takes inputs X1, X2, . . . \u2208 X and generates, for each new input Xi, an output Yi \u2208 Y, depending probabilistically on X1, . . . , Xi and Y1, . . . , Yi\u22121. Many cryptographic systems like block ciphers, MAC-schemes, pseudo-random functions, etc., can be modeled as random systems, where in fact Yi often depends only on Xi, i.e., the system is stateless. The security proof of such a system (e.g. a block cipher) amounts to showing that it is indistinguishable from a certain perfect system (e.g. a random permutation). We propose a general framework for proving the indistinguishability of two random systems, based on the concept of the equivalence of two systems, conditioned on certain events. This abstraction demonstrates the common denominator among many security proofs in the literature, allows to unify, simplify, generalize, and in some cases strengthen them, and opens the door to proving new indistinguishability results. We also propose the previously implicit concept of quasi-randomness and give an efficient construction of a quasi-random function which can be used as a building block in cryptographic systems based on pseudorandom functions."}
{"_id":"65b6c31bccadf0bfb0427929439f2ae1dc2b45bc","title":"On the derivation of coseismic displacement fields using differential radar interferometry: The Landers earthquake","text":"We present a map of the coseismic displacement field resulting from the Landers, California, June 28, 1992, earthquake derived using data acquired from an orbiting high-resolution radar system. We achieve results more accurate than previous space studies and similar in accuracy to those obtained by conventional field survey techniques. Data from the ERS 1 synthetic aperture radar instrument acquired in April, July, and August 1992 are used to generate a high-resolution, wide area map of the displacements. The data represent the motion in the direction of the radar line of sight to centimeter level precision of each 30-m resolution element in a 113 km by 90 km image. Our coseismic displacement contour map gives a lobed pattern consistent with theoretical models of the displacement field from the earthquake. Fine structure observed as displacement tiling in regions several kilometers from the fault appears to be the result of local surface fracturing. Comparison of these data with Global Positioning System and electronic distance measurement survey data yield a correlation of 0.96; thus the radar measurements are a means to extend the point measurements acquired by traditional techniques to an area map format. The technique we use is (1) more automatic, (2) more precise, and (3) better validated than previous similar applications of differential radar interferometry. Since we require only remotely sensed satellite data with no additional requirements for ancillary information, the technique is well suited for global seismic monitoring and analysis."}
{"_id":"146bb2ea1fbdd86f81cd0dae7d3fd63decac9f5c","title":"Genetic Algorithms in Search Optimization and Machine Learning","text":"This book brings together-in an informal and tutorial fashion-the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems..."}
{"_id":"97d3a8863cf2ea6a4670a911cf2fbeca6e043b36","title":"Indian Sign Language gesture classification as single or double handed gestures","text":"The development of a sign language recognition system can have a great impact on the daily lives of humans with hearing disabilities. Recognizing gestures from the Indian Sign Language (ISL) with a camera can be difficult due to complexity of various gestures. The motivation behind the paper is to develop an approach to successfully classify gestures in the ISL under ambiguous conditions from static images. A novel approach involving the decomposition of gestures into single handed or double handed gesture has been presented in this paper. Classifying gesture into these subcategories simplifies the process of gesture recognition in the ISL due to presence of lesser number of gestures in each subcategory. Various approaches making use of Histogram of Gradients (HOG) features and geometric descriptors using KNN and SVM classifiers were tried on a dataset consisting of images of all 26 English alphabets present in the ISL under variable background. HOG features when classified with Support Vector Machine were found to be the most efficient approach resulting in an accuracy of 94.23%."}
{"_id":"4cd0ef755d5473415b5a99555c12f52ce7ce9329","title":"Modified Firefly Algorithm","text":"Firefly algorithm is one of the new metaheuristic algorithms for optimization problems. The algorithm is inspired by the flashing behavior of fireflies. In the algorithm, randomly generated solutions will be considered as fireflies, and brightness is assigned depending on their performance on the objective function. One of the rules used to construct the algorithm is, a firefly will be attracted to a brighter firefly, and if there is no brighter firefly, it will move randomly. In this paper we modify this random movement of the brighter firefly by generating random directions in order to determine the best direction in which the brightness increases. If such a direction is not generated, it will remain in its current position. Furthermore the assignment of attractiveness is modified in such a way that the effect of the objective function is magnified. From the simulation result it is shown that the modified firefly algorithm performs better than the standard one in finding the best solution with smaller CPU time."}
{"_id":"24c64dc1e20924d4c2f65bf1e71f59abe2195f2e","title":"The empathic brain: how, when and why?","text":"Recent imaging results suggest that individuals automatically share the emotions of others when exposed to their emotions. We question the assumption of the automaticity and propose a contextual approach, suggesting several modulatory factors that might influence empathic brain responses. Contextual appraisal could occur early in emotional cue evaluation, which then might or might not lead to an empathic brain response, or not until after an empathic brain response is automatically elicited. We propose two major roles for empathy; its epistemological role is to provide information about the future actions of other people, and important environmental properties. Its social role is to serve as the origin of the motivation for cooperative and prosocial behavior, as well as help for effective social communication."}
{"_id":"a204471ad4722a5e4ade844f8a25aa1c1037e1c1","title":"Brain regions with mirror properties: A meta-analysis of 125 human fMRI studies","text":"Mirror neurons in macaque area F5 fire when an animal performs an action, such as a mouth or limb movement, and also when the animal passively observes an identical or similar action performed by another individual. Brain-imaging studies in humans conducted over the last 20 years have repeatedly attempted to reveal analogous brain regions with mirror properties in humans, with broad and often speculative claims about their functional significance across a range of cognitive domains, from language to social cognition. Despite such concerted efforts, the likely neural substrates of these mirror regions have remained controversial, and indeed the very existence of a distinct subcategory of human neurons with mirroring properties has been questioned. Here we used activation likelihood estimation (ALE), to provide a quantitative index of the consistency of patterns of fMRI activity measured in human studies of action observation and action execution. From an initial sample of more than 300 published works, data from 125 papers met our strict inclusion and exclusion criteria. The analysis revealed 14 separate clusters in which activation has been consistently attributed to brain regions with mirror properties, encompassing 9 different Brodmann areas. These clusters were located in areas purported to show mirroring properties in the macaque, such as the inferior parietal lobule, inferior frontal gyrus and the adjacent ventral premotor cortex, but surprisingly also in regions such as the primary visual cortex, cerebellum and parts of the limbic system. Our findings suggest a core network of human brain regions that possess mirror properties associated with action observation and execution, with additional areas recruited during tasks that engage non-motor functions, such as auditory, somatosensory and affective components."}
{"_id":"fb4dcbd818e5839f025a6bc247b3bc5632be502f","title":"Immersion and Emotion: Their Impact on the Sense of Presence","text":"The present study is designed to test the role of immersion and media content in the sense of presence. Specifically, we are interested in the affective valence of the virtual environments. This paper describes an experiment that compares three immersive systems (a PC monitor, a rear projected video wall, and a head-mounted display) and two virtual environments, one involving emotional content and the other not. The purpose of the experiment was to test the interactive role of these two media characteristics (form and content). Scores on two self-report presence measurements were compared among six groups of 10 people each. The results suggest that both immersion and affective content have an impact on presence. However, immersion was more relevant for non-emotional environments than for emotional ones."}
{"_id":"1f315e1ba65cdce34d372ae445b737c0bcc4dac7","title":"Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders","text":"To examine mirror neuron abnormalities in autism, high-functioning children with autism and matched controls underwent fMRI while imitating and observing emotional expressions. Although both groups performed the tasks equally well, children with autism showed no mirror neuron activity in the inferior frontal gyrus (pars opercularis). Notably, activity in this area was inversely related to symptom severity in the social domain, suggesting that a dysfunctional 'mirror neuron system' may underlie the social deficits observed in autism."}
{"_id":"f7785ec6cc6d443151dcbb7560da7d8bdf4e9ed0","title":"The successive mean quantization transform","text":"This paper presents the successive mean quantization transform (SMQT). The transform reveals the organization or structure of the data and removes properties such as gain and bias. The transform is described and applied in speech processing and image processing. The SMQT is considered as an extra processing step for the mel frequency cepstral coefficients commonly used in speech recognition. In image processing the transform is applied in automatic image enhancement and dynamic range compression."}
{"_id":"b18796d9ecbc3371bea65b136f77b7d65f129390","title":"Global asymptotic and robust stability of recurrent neural networks with time delays","text":"In this paper, two related problems, global asymptotic stability (GAS) and global robust stability (GRS) of neural networks with time delays, are studied. First, GAS of delayed neural networks is discussed based on Lyapunov method and linear matrix inequality. New criteria are given to ascertain the GAS of delayed neural networks. In the designs and applications of neural networks, it is necessary to consider the deviation effects of bounded perturbations of network parameters. In this case, a delayed neural network must be formulated as a interval neural network model. Several sufficient conditions are derived for the existence, uniqueness, and GRS of equilibria for interval neural networks with time delays by use of a new Lyapunov function and matrix inequality. These results are less restrictive than those given in the earlier references."}
{"_id":"871314d440dc55cfd2198e229cde92584964be0f","title":"Evaluating iterative optimization across 1000 datasets","text":"While iterative optimization has become a popular compiler optimization approach, it is based on a premise which has never been truly evaluated: that it is possible to learn the best compiler optimizations across data sets. Up to now, most iterative optimization studies find the best optimizations through repeated runs on the same data set. Only a handful of studies have attempted to exercise iterative optimization on a few tens of data sets.\n In this paper, we truly put iterative compilation to the test for the first time by evaluating its effectiveness across a large number of data sets. We therefore compose KDataSets, a data set suite with 1000 data sets for 32 programs, which we release to the public. We characterize the diversity of KDataSets, and subsequently use it to evaluate iterative optimization.We demonstrate that it is possible to derive a robust iterative optimization strategy across data sets: for all 32 programs, we find that there exists at least one combination of compiler optimizations that achieves 86% or more of the best possible speedup across all data sets using Intel's ICC (83% for GNU's GCC). This optimal combination is program-specific and yields speedups up to 1.71 on ICC and 2.23 on GCC over the highest optimization level (-fast and -O3, respectively). This finding makes the task of optimizing programs across data sets much easier than previously anticipated, and it paves the way for the practical and reliable usage of iterative optimization. Finally, we derive pre-shipping and post-shipping optimization strategies for software vendors."}
{"_id":"85f9441dfd90b1143c6a9afcdca1f279a2e555c0","title":"SDN-Guard: DoS Attacks Mitigation in SDN Networks","text":"Software Defined Networking (SDN) has recently emerged as a new networking technology offering an unprecedented programmability that allows network operators to dynamically configure and manage their infrastructures. The main idea of SDN is to move the control plane into a central controller that is in charge of taking all routing decisions in the network. However, despite all the advantages offered by this technology, Deny-of-Service (DoS) attacks are considered a major threat to such networks as they can easily overload the controller processing and communication capacity and flood switch CAM tables, resulting in a critical degradation of the overall network performance. To address this issue, we propose in this paper SDN-Guard, a novel scheme able to efficiently protect SDN networks against DoS attacks by dynamically (1) rerouting potential malicious traffic, (2) adjusting flow timeouts and (3) aggregating flow rules. Realistic experiments using Mininet show that the proposed solution succeeds in minimizing by up to 32% the impact of DoS attacks on~the controller performance, switch memory usage and control plane bandwidth and thereby maintaining acceptable network performance during such attacks."}
{"_id":"92b89ddca96c90b56410396bedcfaec68bf13578","title":"Twitter Geolocation Prediction Shared Task of the 2016 Workshop on Noisy User-generated Text","text":"This paper describes the shared task for the English Twitter geolocation prediction associated with WNUT 2016. We discuss details of the task settings, data preparation and participant systems. The derived dataset and performance figures from each system provide baselines for future research in this realm."}
{"_id":"99fbfa347fa1ce0ac5828d5fde6177b9a2e3d47c","title":"Feasibility of a DC network for commercial facilities","text":"This paper analyzes the feasibility of direct current for the supply of offices and commercial facilities. This is done by analyzing a case study, i.e. the supply to a university department. Voltage drop calculations have been carried out for different voltage levels. A back-up system for reliable power supply is designed based on commercially available batteries. Finally, an economic evaluation of AC vs. DC is performed."}
{"_id":"7b0bc241de12eeeeaacefa8cd8b86a81cfc0a87d","title":"Sexual arousal: The correspondence of eyes and genitals","text":"Men's, more than women's, sexual responses may include a coordination of several physiological indices in order to build their sexual arousal to relevant targets. Here, for the first time, genital arousal and pupil dilation to sexual stimuli were simultaneously assessed. These measures corresponded more strongly with each other, subjective sexual arousal, and self-reported sexual orientation in men than women. Bisexual arousal is more prevalent in women than men. We therefore predicted that if bisexual-identified men show bisexual arousal, the correspondence of their arousal indices would be more female-typical, thus weaker, than for other men. Homosexual women show more male-typical arousal than other women; hence, their correspondence of arousal indices should be stronger than for other women. Findings, albeit weak in effect, supported these predictions. Thus, if sex-specific patterns are reversed within one sex, they might affect more than one aspect of sexual arousal. Because pupillary responses reflected sexual orientation similar to genital responses, they offer a less invasive alternative for the measurement of sexual arousal."}
{"_id":"dabb16e3bbcd382e2bc4c5f380d7bfa21b7cc9d6","title":"Personal Safety is More Important Than Cost of Damage During Robot Failure","text":"As robots become more common in everyday life it will be increasingly important to understand how non-experts will view robot failure. In this study, we found that severity of failure seems to be tightly coupled with perceived risk to self rather than risk to the robot's task and object. We initially thought perceived severity would be tied to the cost of damage. Instead, participants placed falling drinking glasses above a laptop when rating the severity of the failure. Related results reinforce the primacy of personal safety over the financial cost of damage and suggest the results were tied to proximity to breaking glass."}
{"_id":"b7682634c8633822145193242e7a3e3739042768","title":"MusicMixer: computer-aided DJ system based on an automatic song mixing","text":"In this paper, we present MusicMixer, a computer-aided DJ system that helps DJs, specifically with song mixing. MusicMixer continuously mixes and plays songs using an automatic music mixing method that employs audio similarity calculations. By calculating similarities between song sections that can be naturally mixed, MusicMixer enables seamless song transitions. Though song mixing is the most fundamental and important factor in DJ performance, it is difficult for untrained people to seamlessly connect songs. MusicMixer realizes automatic song mixing using an audio signal processing approach; therefore, users can perform DJ mixing simply by selecting a song from a list of songs suggested by the system, enabling effective DJ song mixing and lowering entry barriers for the inexperienced. We also propose personalization for song suggestions using a preference memorization function of MusicMixer."}
{"_id":"e7ee27816ade366584d411f4287e50bdc4771e56","title":"Fast Discovery of Association Rules","text":""}
{"_id":"38215c283ce4bf2c8edd597ab21410f99dc9b094","title":"The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent","text":"SEMAINE has created a large audiovisual database as a part of an iterative approach to building Sensitive Artificial Listener (SAL) agents that can engage a person in a sustained, emotionally colored conversation. Data used to build the agents came from interactions between users and an \"operator\u201d simulating a SAL agent, in different configurations: Solid SAL (designed so that operators displayed an appropriate nonverbal behavior) and Semi-automatic SAL (designed so that users' experience approximated interacting with a machine). We then recorded user interactions with the developed system, Automatic SAL, comparing the most communicatively competent version to versions with reduced nonverbal skills. High quality recording was provided by five high-resolution, high-framerate cameras, and four microphones, recorded synchronously. Recordings total 150 participants, for a total of 959 conversations with individual SAL characters, lasting approximately 5 minutes each. Solid SAL recordings are transcribed and extensively annotated: 6-8 raters per clip traced five affective dimensions and 27 associated categories. Other scenarios are labeled on the same pattern, but less fully. Additional information includes FACS annotation on selected extracts, identification of laughs, nods, and shakes, and measures of user engagement with the automatic system. The material is available through a web-accessible database."}
{"_id":"af77be128c7a0efb1e9c559b4e77fa0b1b1e77d6","title":"Analysis and Simulation of Theme Park Queuing System","text":"It has been an important issue to improve customers' satisfaction in theme parks for which become a major role of recreation in our daily life. Waiting for rides has been identified as a factor decreasing satisfaction. A previous study indicated that a virtual queuing system can reduce the total waiting time so the customer's satisfaction is improved. The results from a simulation tool Arena show that an index Satisfaction Value (SV) increases when the queuing system is introduced. In this study, a more complex scenario of theme park queuing system (TPQS) is first designed, followed by comparison of a number of combinations of the rides with various waiting time and distribution factors. Analysis is also carried out."}
{"_id":"802b80852996d87dc16082b86f6e77115eb6c9a6","title":"Reverse Engineering Flash EEPROM Memories Using Scanning Electron Microscopy","text":"In this article, a methodology to extract Flash EEPROM memory contents is presented. Samples are first backside prepared to expose the tunnel oxide of floating gate transistors. Then, a Scanning Electron Microscope (SEM) in the so called Passive Voltage Contrast (PVC) mode allows distinguishing \u20180\u2019 and \u20181\u2019 bit values stored in individual memory cell. Using SEM operator-free acquisition and standard image processing technique we demonstrate the possible automating of such technique over a full memory. The presented fast, efficient and low cost technique is successfully implemented on 0.35\u03bcm technology node microcontrollers and on a 0.21\u03bcm smart card type integrated circuit. The technique is at least two orders of magnitude faster than state-of-the-art Scanning Probe Microscopy (SPM) methods. Without adequate protection an adversary could obtain the full memory array content within minutes. The technique is a first step for reverse engineering secure embedded systems."}
{"_id":"b2c31108c126ec05d0741146dc5d639f30a12d11","title":"Multi-stage beamforming codebook for 60GHz WPAN","text":"Beamforming(BF) based on codebook is regarded as an attractive solution to resolve the poor link budget of millimeter-wave 60GHz wireless communication. Because the number of the elements of antenna array in 60GHz increases, the beam patterns generated are more than common BF, and it causes long set-up time during beam patterns searching. In order to reduce the set-up time, three stages protocol, namely the device (DEV) to DEV linking, sector-level searching and beam-level searching has been adopted by the IEEE 802.15.3c as an optional functionality to realize Gbps communication systems. However, it is still a challenge to create codebook of different patterns to support three stages protocol from common codebook of beam pattern. In this paper, we proposes a multi-stage codebook design and the realization architecture to support three stages BF. The multi-stage codebook can create different granularity of beam patterns and realize progressive searching. Simulation results for eight elements uniform linear array (ULA) show that this design can divide the beam searching to three stages searching without increasing the system complexity."}
{"_id":"ca036c1e6a18931386016df9733a8c1366098235","title":"Explanation of Two Anomalous Results in Statistical Mediation Analysis.","text":"Previous studies of different methods of testing mediation models have consistently found two anomalous results. The first result is elevated Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap tests not found in nonresampling tests or in resampling tests that did not include a bias correction. This is of special concern as the bias-corrected bootstrap is often recommended and used due to its higher statistical power compared with other tests. The second result is statistical power reaching an asymptote far below 1.0 and in some conditions even declining slightly as the size of the relationship between X and M, a, increased. Two computer simulations were conducted to examine these findings in greater detail. Results from the first simulation found that the increased Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap are a function of an interaction between the size of the individual paths making up the mediated effect and the sample size, such that elevated Type I error rates occur when the sample size is small and the effect size of the nonzero path is medium or larger. Results from the second simulation found that stagnation and decreases in statistical power as a function of the effect size of the a path occurred primarily when the path between M and Y, b, was small. Two empirical mediation examples are provided using data from a steroid prevention and health promotion program aimed at high school football players (Athletes Training and Learning to Avoid Steroids; Goldberg et al., 1996), one to illustrate a possible Type I error for the bias-corrected bootstrap test and a second to illustrate a loss in power related to the size of a. Implications of these findings are discussed."}
{"_id":"dd5a7641b70120813cff6f3a08573a2215f4ad40","title":"The Use Of Edmodo In Creating An Online Learning Community Of Practice For Learning To Teach Science","text":"This study aimed to create an online community of practice by creating a virtual classroom in the Edmodo application and ascertain the opinions of pre-service primary teachers about the effects of Edmodo on their learning to teach science and availability of Edmodo. The research used a case study, which is one method of descriptive research. During the implementation process, pre-service primary teachers used Edmodo to share activities they had designed that centred on the scientific concepts taught in primary science education programmes. They also shared their diary entries that outlined their experiences and views after they had practised their activities in a real classroom. 58 pre-service primary teachers participated in the study. The author developed a questionnaire and it included one closed-ended and six open-ended questions; the questionnaire was used as the data collection tool. The pre-service primary teachers used Edmodo for 12 weeks. Descriptive and content analysis methods were used to analyse the data obtained from the study. The results obtained from the data analysis showed that pre-service primary teachers generally had positive views about the use of Edmodo in teacher education programmes. Most pre-service primary teachers stated that Edmodo provides the possibility of sharing knowledge, experiences and views. However, some pre-service teachers stated that Edmodo has some limitations; for example, the fact that it requires the user to have internet access. As a result, it can be said that Edmodo can be used to create an online community of practice in teacher education programmes."}
{"_id":"3d1f7de876b57952fded28adfbb74d5cd4249b02","title":"QUANTUM COMPUTING : AN INTRODUCTION","text":"After some remarks on the fundamental physical nature of information, Bennett and Fredkin's ideas of reversible computation are introduced. This leads on to the suggestions of Benioff and Feynman as to the possibilit y of a new type of essentially \u2018quantum computers\u2019 . If we can build such devices, Deutsch showed that \u2018quantum paralleli sm\u2019 leads to new algorithms and new complexity classes. This is dramatically ill ustrated by Shor's quantum algorithm for factorization which is polynomial in time in contrast to algorithms for factorization on a classical Turing computer. This discovery has potentiall y important implications for the security of many modern cryptographic systems. The fundamentals of quantum computing are then introduced reversible logic gates, qubits and quantum registers. The key quantum property of \u2018 entanglement\u2019 is described, with due homage to Einstein and Bell . As an ill ustration of a quantum program, Grover's database search algorithm is described in some detail . After all this theory, the status of experimental attempts to build a quantum computer is reviewed: it will become evident that we have a long way to go before we can factorize even small numbers. Finally, we end with some thoughts about the process of \u2018quantum compilation\u2019 translating a quantum algorithm into actual physical operations on a quantum system and some comments on prospects for future progress."}
{"_id":"b155c6ddeba5d094207d6dce1b55893210b65c2d","title":"Classification methods for activity recognition","text":"Activity recognition is an important subject in different areas of research, like e-health and ubiquitous computing. In this paper we give an overview of recent work in the field of activity recognition from both body mounted sensors and ambient sensors. We discuss some of the differences among approaches and present guidelines for choosing a suitable approach under various circumstances."}
{"_id":"a23e7b612245366c3f7934eebf5bac6205ff023a","title":"Biocybernetic system evaluates indices of operator engagement in automated task","text":"A biocybernetic system has been developed as a method to evaluate automated flight deck concepts for compatibility with human capabilities. A biocybernetic loop is formed by adjusting the mode of operation of a task set (e.g., manual\/automated mix) based on electroencephalographic (EEG) signals reflecting an operator's engagement in the task set. A critical issue for the loop operation is the selection of features of the EEG to provide an index of engagement upon which to base decisions to adjust task mode. Subjects were run in the closed-loop feedback configuration under four candidate and three experimental control definitions of an engagement index. The temporal patterning of system mode switching was observed for both positive and negative feedback of the index. The indices were judged on the basis of their relative strength in exhibiting expected feedback control system phenomena (stable operation under negative feedback and unstable operation under positive feedback). Of the candidate indices evaluated in this study, an index constructed according to the formula, beta power\/(alpha power + theta power), reflected task engagement best."}
{"_id":"1a85b157bc6237aa1e556ccfb84a1e3c6c9d602f","title":"AmphiBot I: an amphibious snake-like robot","text":"This article presents a project that aims at constructing a biologically inspired amphibious snake-like robot. The robot is designed to be capable of anguilliform swimming like sea-snakes and lampreys in water and lateral undulatory locomotion like a snake on ground. Both the structure and the controller of the robot are inspired by elongate vertebrates. In particular, the locomotion of the robot is controlled by a central pattern generator (a system of coupled oscillators) that produces travelling waves of oscillations as limit cycle behavior. We present the design considerations behind the robot and its controller. Experiments are carried out to identify the types of travelling waves that optimize speed during lateral undulatory locomotion on ground. In particular, the optimal frequency, amplitude and wavelength are thus identified when the robot is crawling on a particular surface."}
{"_id":"47c80e2ba776d82edfe038cb0e132e20170a79e6","title":"Breakthrough in improving the skin sagging with focusing on the subcutaneous tissue structure , retinacula cutis","text":"Skin sagging is one of the most prominent aging signs and a concerning issue for people over middle age. Although many cosmetic products are challenging the reduction of the skin sagging by improving the dermal elasticity, which decreases with age, the effects are insufficient for giving drastic changes to the facial morphology. This study focused on subcutaneous tissue for investigating a skin sagging mechanism. Subcutaneous tissue consists of predominantly adipose tissue with fibrous network structures, called retinacula cutis (RC), which is reported to have a possibility to maintain the soft tissue structure and morphology. This study investigated the effect of subcutaneous tissue physical-property alteration due to RC-deterioration on the skin sagging. For evaluating RC structure noninvasively, the tomographic images of faces were obtained by magnetic resonance (MR) imaging. Subcutaneous tissue network structures observed by MR imaging were indicated to be RC by comparing MR images and the histological specimens of human skin. The density of RC was measured by image analysis. For evaluating sagging degree and physical properties of the skin, sagging scoring and the measurement of elasticity of deeper skin layers were performed. The density of RC was correlated with the elasticity data of deeper skin layers, and the sagging scores tended to increase with decreasing the density. These results suggested that the sparse RC structure gave a decrease in the elasticity of subcutaneous tissue layer, which consequently would be the cause of facial sagging. This study would be a pathfinder for the complete elimination of skin sagging."}
{"_id":"3cb35dda9637a582c650ecfd80214bfd2914c9f3","title":"Predicting defects using network analysis on dependency graphs","text":"In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10% points higher than for models built from complexity metrics. In addition, network measures could identify 60% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics."}
{"_id":"55289d3feef4bc1e4ff17008120e371eb7f55a24","title":"Conditional Generation and Snapshot Learning in Neural Dialogue Systems","text":"Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an endto-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used."}
{"_id":"027b0d240066f8d1560edcd75505f5650291cced","title":"Solving optimization problems using black hole algorithm","text":"Various meta-heuristic optimization approaches have been recently created and applied in different areas. Many of these approaches are inspired by swarm behaviors in the nature. This paper studies the solving optimization problems using Black Hole Algorithm (BHA) which is a population-based algorithm. Since the performance of this algorithm was not tested in mathematical functions, we have studied this issue using some standard functions. The results of the BHA are compared with the results of GA and PSO algorithms which indicate that the performance of BHA is better than the other two mentioned algorithms."}
{"_id":"8dc4025cfa05e6aab60ce578f9c9b55e356aeb79","title":"Active Magnetic Anomaly Detection Using Multiple Micro Aerial Vehicles","text":"Magnetic anomaly detection (MAD) is an important problem in applications ranging from geological surveillance to military reconnaissance. MAD sensors detect local disturbances in the magnetic field, which can be used to detect the existence of and to estimate the position of buried, hidden, or submerged objects, such as ore deposits or mines. These sensors may experience false positive and false negative detections and, without prior knowledge of the targets, can only determine proximity to a target. The uncertainty in the sensors, coupled with a lack of knowledge of even the existence of targets, makes the estimation and control problems challenging. We utilize a hierarchical decomposition of the environment, coupled with an estimation algorithm based on random finite sets, to determine the number of and the locations of targets in the environment. The small team of robots follow the gradient of mutual information between the estimated set of targets and the future measurements, locally maximizing the rate of information gain. We present experimental results of a team of quadrotor micro aerial vehicles discovering and localizing an unknown number of permanent magnets."}
{"_id":"8149e52a8c549aa9e1f4a93fe315263849bc92c1","title":"Dragonfly wing nodus: A one-way hinge contributing to the asymmetric wing deformation.","text":"Dragonfly wings are highly specialized locomotor systems, which are formed by a combination of several structural components. The wing components, also known as structural elements, are responsible for the various aspects of the wing functionality. Considering the complex interactions between the wing components, modelling of the wings as a whole is only possible with inevitable huge oversimplifications. In order to overcome this difficulty, we have recently proposed a new approach to model individual components of complex wings comparatively. Here, we use this approach to study nodus, a structural element of dragonfly wings which has been less studied to date. Using a combination of several imaging techniques including scanning electron microscopy (SEM), wide-field fluorescence microscopy (WFM), confocal laser scanning microscopy (CLSM) and micro-computed tomography (micro-CT) scanning, we aim to characterize the spatial morphology and material composition of fore- and hindwing nodi of the dragonfly Brachythemis contaminata. The microscopy results show the presence of resilin in the nodi, which is expected to help the deformability of the wings. The computational results based on three-dimensional (3D) structural data suggest that the specific geometry of the nodus restrains its displacements when subjected to pressure on the ventral side. This effect, resulting from an interlocking mechanism, is expected to contribute to the dorso-ventral asymmetry of wing deformation and to provide a higher resistance to aerodynamic forces during the downstroke. Our results provide an important step towards better understanding of the structure-property-function relationship in dragonfly wings.\n\n\nSTATEMENT OF SIGNIFICANCE\nIn this study, we investigate the wing nodus, a specialized wing component in dragonflies. Using a combination of modern imaging techniques, we demonstrate the presence of resilin in the nodus, which is expected to facilitate the wing deformability in flight. The specific geometry of the nodus, however, seems to restrain its displacements when subjected to pressure on the ventral side. This effect, resulting from an interlocking mechanism, is suggested to contribute to dorso-ventral asymmetry of wing deformations and to provide a higher resistance to aerodynamic forces during the downstroke. Our results provide an important step towards better understanding of the structure-property-function relationship in dragonfly wings and might help to design more efficient wings for biomimetic micro-air vehicles."}
{"_id":"0ae944eb32cdce405125f948b2eef2e7c0512fd3","title":"HF , VHF , and UHF Systems and Technology","text":"A wide variety of unique systems and components inhabits the HF, VHF, and UHF bands. Many communication systems (ionospheric, meteor-burst, and troposcatter) provide beyond-line-of-sight coverage and operate independently of external infrastructure. Broadcasting and over-the-horizon radar also operate in these bands. Magnetic-resonance imaging uses HF\/VHF signals to see the interior of a human body, and RF heating is used in a variety of medical and industrial applications. Receivers typically employ a mix of analog and digital-signal-processing techniques. Systems for these frequencies make use of RF-power MOSFETs, p-i-n diodes, and ferrite-loaded transmission-line transformers."}
{"_id":"722e2f7894a1b62e0ab09913ce9b98654733d98e","title":"Information overload and the message dynamics of online interaction spaces: a theoretical model and empirical exploration","text":"This publication contains reprint articles for which IEEE does not hold copyright. Full text is not available on IEEE Xplore for these articles."}
{"_id":"be48c3ac5eb233155158a3b8defed0e083cc2381","title":"Dry electrodes for electrocardiography.","text":"Patient biopotentials are usually measured with conventional disposable Ag\/AgCl electrodes. These electrodes provide excellent signal quality but are irritating for long-term use. Skin preparation is usually required prior to the application of electrodes such as shaving and cleansing with alcohol. To overcome these difficulties, researchers and caregivers seek alternative electrodes that would be acceptable in clinical and research environments. Dry electrodes that operate without gel, adhesive or even skin preparation have been studied for many decades. They are used in research applications, but they have yet to achieve acceptance for medical use. So far, a complete comparison and evaluation of dry electrodes is not well described in the literature. This work compares dry electrodes for biomedical use and physiological research, and reviews some novel systems developed for cardiac monitoring. Lastly, the paper provides suggestions to develop a dry-electrode-based system for mobile and long-term cardiac monitoring applications."}
{"_id":"118422012ca38272ac766294f27ca83b5319d3cb","title":"Forecasting the weather of Nevada: A deep learning approach","text":"This paper compares two approaches for predicting air temperature from historical pressure, humidity, and temperature data gathered from meteorological sensors in Northwestern Nevada. We describe our data and our representation and compare a standard neural network against a deep learning network. Our empirical results indicate that a deep neural network with Stacked Denoising Auto-Encoders (SDAE) outperforms a standard multilayer feed forward network on this noisy time series prediction task. In addition, predicting air temperature from historical air temperature data alone can be improved by employing related weather variables like barometric pressure, humidity and wind speed data in the training process."}
{"_id":"368f3dea4f12c77dfc9b7203f3ab2b9efaecb635","title":"Statistical Phrase-Based Translation","text":"We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems."}
{"_id":"02df3d50dbd1d15c38db62ff58a5601ebf815d59","title":"NLTK: The Natural Language Toolkit","text":"NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset."}
{"_id":"408e8eecc14c5cc60bbdfc486ba7a7fc97031788","title":"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks","text":"Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled \u2019seed\u2019 image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101)."}
{"_id":"53fab2769ef87e8153d905a5cf76ead9f9e46c22","title":"The American Sign Language Lexicon Video Dataset","text":"The lack of a written representation for American sign language (ASL) makes it difficult to do something as commonplace as looking up an unknown word in a dictionary. The majority of printed dictionaries organize ASL signs (represented in drawings or pictures) based on their nearest English translation; so unless one already knows the meaning of a sign, dictionary look-up is not a simple proposition. In this paper we introduce the ASL lexicon video dataset, a large and expanding public dataset containing video sequences of thousands of distinct ASL signs, as well as annotations of those sequences, including start\/end frames and class label of every sign. This dataset is being created as part of a project to develop a computer vision system that allows users to look up the meaning of an ASL sign. At the same time, the dataset can be useful for benchmarking a variety of computer vision and machine learning methods designed for learning and\/or indexing a large number of visual classes, and especially approaches for analyzing gestures and human communication."}
{"_id":"6ebdb88c39787f4242e92504b6d2c60b8421193a","title":"The association between psychological distance and construal level: evidence from an implicit association test.","text":"According to construal level theory (N. Liberman, Y. Trope, & E. Stephan, in press; Y. Trope & N. Liberman, 2003), people use a more abstract, high construal level when judging, perceiving, and predicting more psychologically distal targets, and they judge more abstract targets as being more psychologically distal. The present research demonstrated that associations between more distance and higher level of construal also exist on a pure conceptual level. Eight experiments used the Implicit Association Test (IAT; A. G. Greenwald, D. E. McGhee, & J. L. K. Schwartz, 1998) to demonstrate an association between words related to construal level (low vs. high) and words related to four dimensions of distance (proximal vs. distal): temporal distance, spatial distance, social distance, and hypotheticality. In addition to demonstrating an association between level of construal and psychological distance, these findings also corroborate the assumption that all 4 dimensions of psychological distance are related to level of construal in a similar way and support the notion that they all are forms of psychological distance."}
{"_id":"1f4412f8c0d2e491b2b4bf486d47d448d8f46858","title":"Implicit Association Test 1 The Implicit Association Test at Age 7 : A Methodological and Conceptual Review","text":"A mong earthly organisms, humans have a unique propensity to introspect or look inward into the contents of their own minds, and to share those observations with others. With the ability to introspect comes the palpable feeling of \" knowing, \" of being objective or certain, of being mentally in control of one's thoughts, aware of the causes of one's thoughts, feelings, and actions, and of making decisions deliberately and rationally. Among the noteworthy discoveries of 20th century psychology was a challenge posed to this assumption of rationality. From the groundbreaking theorizing of Herbert Simon (1955) and the mind-boggling problems posed by Kahneman, Slovik, and Tversky (1982) to striking demonstrations of illusions of control (Wegner, 2002), the paucity of introspection (Nisbett and Wilson, 1977), and the automaticity of everyday thought (Bargh, 1997), psychologists have shown the frailties of the minds of their species. As psychologists have come to grips with the limits of the mind, there has been an increased interest in measuring aspects of thinking and feeling that may not be easily accessed or available to consciousness. Innovations in measurement have been undertaken with the purpose of bringing under scrutiny new forms of cogni-tion and emotion that were previously undiscovered and especially by asking if traditional concepts such as attitude and preference, belief and stereotype, self-concept and self-esteem can be rethought based on what the new measures reveal. These newer measures do not require introspection on the part of the subject. For many constructs this is considered a valuable, if not essential, feature of measurement; for others, avoiding introspection is greeted with suspicion and skepticism. For example, one approach to measuring math ability would be to ask \" how good are you at math? \" whereas an alternative approach is to infer math ability via a performance on a math skills test. The former requires introspection to assess the relevant construct, the latter does not. And yet, the latter is accepted"}
{"_id":"2698f74468c49b29ac69e193d5aeaa09bb33faea","title":"Can language restructure cognition? The case for space","text":"Frames of reference are coordinate systems used to compute and specify the location of objects with respect to other objects. These have long been thought of as innate concepts, built into our neurocognition. However, recent work shows that the use of such frames in language, cognition and gesture varies cross-culturally, and that children can acquire different systems with comparable ease. We argue that language can play a significant role in structuring, or restructuring, a domain as fundamental as spatial cognition. This suggests we need to rethink the relation between the neurocognitive underpinnings of spatial cognition and the concepts we use in everyday thinking, and, more generally, to work out how to account for cross-cultural cognitive diversity in core cognitive domains."}
{"_id":"755b94b766dee3a34536f6b481a60f0d9f68aa0c","title":"The Role of Feasibility and Desirability Considerations in Near and Distant Future Decisions : A Test of Temporal Construal Theory","text":"Temporal construal theory states that distant future situations are construed on a higher level (i.e., using more abstract and central features) than near future situations. Accordingly, the theory suggests that the value associated with the high-level construal is enhanced over delay and that the value associated with the low-level construal is discounted over delay. In goal-directed activities, desirability of the activity's end state represents a high-level construal, whereas the feasibility of attaining this end state represents a low-level construal. Study 1 found that distant future activities were construed on a higher level than near future activities. Studies 2 and 3 showed that decisions regarding distant future activities, compared with decisions regarding near future activities, were more influenced by the desirability of the end state and less influenced by the feasibility of attaining the end state. Study 4 presented students with a real-life choice of academic assignments varying in difficulty (feasibility) and interest (desirability). In choosing a distant future assignment, students placed relatively more weight on the assignment's interest, whereas in choosing a near future assignment, they placed relatively more weight on difficulty. Study 5 found that distant future plans, compared with near future plans, were related to desirability of activities rather than to time constraints."}
{"_id":"848e2f107ed6abe3c32c6442bcef4f6215f3f426","title":"A Capacitor-DAC-Based Technique For Pre-Emphasis-Enabled Multilevel Transmitters","text":"This brief presents a capacitor digital-to-analog converter (DAC) based technique that is suitable for pre-emphasis-enabled multilevel wireline transmitter design in voltage mode. Detailed comparisons between the proposed technique and conventional direct-coupling-based as well as resistor-DAC-based multilevel transmitter design techniques are given, revealing potential benefits in terms of speed, linearity, implementation complexity, and also power consumption. A PAM-4 transmitter with 2-Tap feed-forward equalization adopting the proposed technique is implemented in 65-nm CMOS technology. It achieves a 25-Gb\/s data rate and energy efficiency of 2 mW\/Gb\/s."}
{"_id":"10b0ec0b2920a5e9d7b6527e5b87d8fde0b11e86","title":"Toward defining the preclinical stages of Alzheimer\u2019s disease: Recommendations from the National Institute on Aging-Alzheimer's Association workgroups on diagnostic guidelines for Alzheimer's disease","text":"The pathophysiological process of Alzheimer's disease (AD) is thought to begin many years before the diagnosis of AD dementia. This long \"preclinical\" phase of AD would provide a critical opportunity for therapeutic intervention; however, we need to further elucidate the link between the pathological cascade of AD and the emergence of clinical symptoms. The National Institute on Aging and the Alzheimer's Association convened an international workgroup to review the biomarker, epidemiological, and neuropsychological evidence, and to develop recommendations to determine the factors which best predict the risk of progression from \"normal\" cognition to mild cognitive impairment and AD dementia. We propose a conceptual framework and operational research criteria, based on the prevailing scientific evidence to date, to test and refine these models with longitudinal clinical research studies. These recommendations are solely intended for research purposes and do not have any clinical implications at this time. It is hoped that these recommendations will provide a common rubric to advance the study of preclinical AD, and ultimately, aid the field in moving toward earlier intervention at a stage of AD when some disease-modifying therapies may be most efficacious."}
{"_id":"6d2465be30dbcbf9b76509eed81cd5f32c4f8618","title":"Fully integrated 54nm STT-RAM with the smallest bit cell dimension for high density memory application","text":"A compact STT(Spin-Transfer Torque)-RAM with a 14F2 cell was integrated using modified DRAM processes at the 54nm technology node. The basic switching performance (R-H and R-V) of the MTJs and current drivability of the access transistors were characterized at the single bit cell level. Through the direct access capability and normal chip operation in our STT-RAM test blocks, the switching behavior of bit cell arrays was also analyzed statistically. From this data and from the scaling trend of STT-RAM, we estimate that the unit cell dimension below 30nm can be smaller than 8F2."}
{"_id":"f70c724a299025fa58127b4bcd3426c565e1e7be","title":"Single-Image Noise Level Estimation for Blind Denoising","text":"Noise level is an important parameter to many image processing applications. For example, the performance of an image denoising algorithm can be much degraded due to the poor noise level estimation. Most existing denoising algorithms simply assume the noise level is known that largely prevents them from practical use. Moreover, even with the given true noise level, these denoising algorithms still cannot achieve the best performance, especially for scenes with rich texture. In this paper, we propose a patch-based noise level estimation algorithm and suggest that the noise level parameter should be tuned according to the scene complexity. Our approach includes the process of selecting low-rank patches without high frequency components from a single noisy image. The selection is based on the gradients of the patches and their statistics. Then, the noise level is estimated from the selected patches using principal component analysis. Because the true noise level does not always provide the best performance for nonblind denoising algorithms, we further tune the noise level parameter for nonblind denoising. Experiments demonstrate that both the accuracy and stability are superior to the state of the art noise level estimation algorithm for various scenes and noise levels."}
{"_id":"16bf6a111ada81d837da3de7227def58baa6b95b","title":"Causal Structure Learning and Inference : A Selective Review","text":"In this paper we give a review of recent causal inference methods. First, we discuss methods for causal structure learning from observational data when confounders are not present and have a close look at methods for exact identifiability. We then turn to methods which allow for a mix of observational and interventional data, where we also touch on active learning strategies. We also discuss methods which allow arbitrarily complex structures of hidden variables. Second, we present approaches for estimating the interventional distribution and causal effects given the (true or estimated) causal structure. We close with a note on available software and two examples on real data."}
{"_id":"ec5bfa1d267d0797af46b6cc6f69f91748ae27c1","title":"Activity recognition using a single accelerometer placed at the wrist or ankle.","text":"PURPOSE\nLarge physical activity surveillance projects such as the UK Biobank and NHANES are using wrist-worn accelerometer-based activity monitors that collect raw data. The goal is to increase wear time by asking subjects to wear the monitors on the wrist instead of the hip, and then to use information in the raw signal to improve activity type and intensity estimation. The purposes of this work was to obtain an algorithm to process wrist and ankle raw data and to classify behavior into four broad activity classes: ambulation, cycling, sedentary, and other activities.\n\n\nMETHODS\nParticipants (N = 33) wearing accelerometers on the wrist and ankle performed 26 daily activities. The accelerometer data were collected, cleaned, and preprocessed to extract features that characterize 2-, 4-, and 12.8-s data windows. Feature vectors encoding information about frequency and intensity of motion extracted from analysis of the raw signal were used with a support vector machine classifier to identify a subject's activity. Results were compared with categories classified by a human observer. Algorithms were validated using a leave-one-subject-out strategy. The computational complexity of each processing step was also evaluated.\n\n\nRESULTS\nWith 12.8-s windows, the proposed strategy showed high classification accuracies for ankle data (95.0%) that decreased to 84.7% for wrist data. Shorter (4 s) windows only minimally decreased performances of the algorithm on the wrist to 84.2%.\n\n\nCONCLUSIONS\nA classification algorithm using 13 features shows good classification into the four classes given the complexity of the activities in the original data set. The algorithm is computationally efficient and could be implemented in real time on mobile devices with only 4-s latency."}
{"_id":"5cbf2f236ac35434eda20239237ed84db491a28d","title":"Toward Rapid Development of Multi-Party Virtual Human Negotiation Scenarios","text":"This paper reports on an ongoing effort to enable the rapid development of multi-party virtual human negotiation scenarios. We present a case study in which a new scenario supporting negotiation between two human role players and two virtual humans was developed over a period of 12 weeks. We discuss the methodology and development process that were employed, from storyline design through role play and iterative development of the virtual humans\u2019 semantic and task representations and natural language processing capabilities. We analyze the effort, expertise, and time required for each development step, and discuss opportunities to further streamline the development process."}
{"_id":"452bc3a5f547ba1c1678b20552e3cd465870a33a","title":"Deep Convolutional Neural Networks [Lecture Notes]","text":"Neural networks are a subset of the field of artificial intelligence (AI). The predominant types of neural networks used for multidimensional signal processing are deep convolutional neural networks (CNNs). The term deep refers generically to networks having from a \"few\" to several dozen or more convolution layers, and deep learning refers to methodologies for training these systems to automatically learn their functional parameters using data representative of a specific problem domain of interest. CNNs are currently being used in a broad spectrum of application areas, all of which share the common objective of being able to automatically learn features from (typically massive) data bases and to generalize their responses to circumstances not encountered during the learning phase. Ultimately, the learned features can be used for tasks such as classifying the types of signals the CNN is expected to process. The purpose of this \"Lecture Notes\" article is twofold: 1) to introduce the fundamental architecture of CNNs and 2) to illustrate, via a computational example, how CNNs are trained and used in practice to solve a specific class of problems."}
{"_id":"40385eb9d1464bea8f13fb24ca58aee3e36bd634","title":"Nearest prototype classifier designs: An experimental study","text":"We compare eleven methods for finding prototypes upon which to base the nearest \u017d prototype classifier. Four methods for prototype selection are discussed: Wilson Hart a . condensation error-editing method , and three types of combinatorial search random search, genetic algorithm, and tabu search. Seven methods for prototype extraction are discussed: unsupervised vector quantization, supervised learning vector quantization \u017d . with and without training counters , decision surface mapping, a fuzzy version of vector quantization, c-means clustering, and bootstrap editing. These eleven methods can be usefully divided two other ways: by whether they employ preor postsupervision; and by whether the number of prototypes found is user-defined or \u2018\u2018automatic.\u2019\u2019 Generalization error rates of the 11 methods are estimated on two synthetic and two real data sets. Offering the usual disclaimer that these are just a limited set of experiments, we feel confident in asserting that presupervised, extraction methods offer a better chance for success to the casual user than postsupervised, selection schemes. Finally, our calculations do not suggest that methods which find the \u2018\u2018best\u2019\u2019 number of prototypes \u2018\u2018automatically\u2019\u2019 are superior to methods for which the user simply specifies the number of prototypes. 2001 John Wiley & Sons, Inc."}
{"_id":"dbbbddfa44c0a091b7e15b7989fbf8af4fad3a78","title":"Synthesizing evidence in software engineering research","text":"Synthesizing the evidence from a set of studies that spans many countries and years, and that incorporates a wide variety of research methods and theoretical perspectives, is probably the single most challenging task of performing a systematic review. In this paper, we perform a tertiary review to assess the types and methods of research synthesis in systematic reviews in software engineering. Almost half of the 31 studies included in our review did not contain any synthesis; of the ones that did, two thirds performed a narrative or a thematic synthesis. The results show that, despite the focus on systematic reviews, there is, currently, limited attention to research synthesis in software engineering. This needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice."}
{"_id":"b5aa59085ebd6b0a23e0941efc2ab10efb7474bc","title":"Spectral\u2013Spatial Classification and Shape Features for Urban Road Centerline Extraction","text":"This letter presents a two-step method for urban main road extraction from high-resolution remotely sensed imagery by integrating spectral-spatial classification and shape features. In the first step, spectral-spatial classification segments the imagery into two classes, i.e., the road class and the nonroad class, using path openings and closings. The local homogeneity of the gray values obtained by local Geary's C is then fused with the road class. In the second step, the road class is refined by using shape features. The experimental results indicated that the proposed method was able to achieve a comparatively good performance in urban main road extraction."}
{"_id":"29500203bfb404e5f2808c0342410a6b91e75e31","title":"Automatic Optimization Computational Method for Unconventional S . W . A . T . H . Ships Resistance","text":"The paper illustrates the main theoretical and computational aspects of an automatic computer based procedure for the parametric shape optimization of a particular unconventional hull typology: that for a catamaran S.W.A.T.H. ship. The goal of the integrated computational procedure is to find the best shape of the submerged hulls of a new U.S.V. (Unmanned Surface Vehicle) S.W.A.T.H. (Small Waterplane Area Twin Hull) vessel, in terms of minimum wave pattern resistance. After dealing with the theoretical aspects the papers presents the numerical aspects of the main software module of the automatic procedure, which integrates a parametric generation routine for innovative and unconventional S.W.A.T.H. (Small Waterplane Area Twin Hull) vessel geometry, a multi-objective, globally convergent and constrained, optimization algorithm and a Computational Fluid Dynamic (C.F.D.) solver. The integrated process is able to find the best shape of the submerged hull of the vessel, subject to the total displaced volume constraint. The hydrodynamic computation is carried out by means of a free surface potential flow method and it is addressed to find the value of wave resistance of each hull variant. Results of the application of the described computational procedure are presented for two optimization cases and the obtained best shapes are compared with a conventional one, featuring a typical torpedo-shaped body, proving the effectiveness of the method in reducing the resistance by a considerable extent, in the order of 40 percent. Keywords\u2014S.W.A.T.H., B.E.M., Wave Resistance, Parametric Modeling, Optimization, Genetic Algorithms"}
{"_id":"244fa023adbbff31806ded21d5b2f36afd3ff988","title":"BRAD 1.0: Book reviews in Arabic dataset","text":"The availability of rich datasets is a pre-requisite for proposing robust sentiment analysis systems. A variety of such datasets exists in English language. However, it is rare or nonexistent for the Arabic language except for a recent LABR dataset, which consists of a little bit over 63,000 book reviews extracted from. Goodreads. com. We introduce BRAD 1.0, the largest Book Reviews in Arabic Dataset for sentiment analysis and machine language applications. BRAD comprises of almost 510,600 book records. Each record corresponds for a single review and has the review in Arabic language and the reviewer's rating on a scale of 1 to 5 stars. In this paper, we present and describe the properties of BRAD. Further, we provide two versions of BRAD: the complete unbalanced dataset and the balanced version of BRAD. Finally, we implement four sentiment analysis classifiers based on this dataset and report our findings. When training and testing the classifiers on BRAD as opposed to LABR, an improvement rate growth of 46% is reported. The highest accuracy attained is 91%. Our core contribution is to make this benchmark-dataset available and accessible to the research community on Arabic language."}
{"_id":"90a2a7a3d22c58c57e3b1a4248c7420933d7fe2f","title":"An integrated approach to testing complex systems","text":"The increasing complexity of today\u2019s testing scenarios for complex systems demands an integrated, open, and flexible approach to support the management of the overall test process. \u201cClassical\u201d model-based testing approaches, where a complete and precise formal specification serves as a reference for automatic test generation, are often impractical. Reasons are, on the one hand, the absence of a suitable formal specification. As complex systems are composed of several components, either hardware or software, often pre-built and third party, it is unrealistic to assume that a formal specification exists a priori. On the other hand, a sophisticated test execution environment is needed that can handle distributed test cases. This is because the test actions and observations can take place on different subsystems of the overall system. This thesis presents a novel approach to the integrated testing of complex systems. Our approach offers a coarse grained test environment, realized in terms of a component-based test design on top of a library of elementary but intuitively understandable test case fragments. The relations between the fragments are treated orthogonally, delivering a test design and execution environment enhanced by means of light-weight formal verification methods. In this way we are able to shift the test design issues from total experts of the system and the used test tools to experts of the system\u2019s logic only. We illustrate the practical usability of our approach by means of industrial case studies in two different application domains: Computer Telephony Integrated solutions and Web-based applications. As an enhancement of our integrated test approach we provide an algorithm for generating approximate models for complex systems a posteriori. This is done by optimizing a standard machine learning algorithm according to domain-specific structural properties, i.e. properties like prefix-closeness, input-determinism, as well as independency and symmetries of events. The resulting models can never be exact, i.e. reflect the complete and correct behaviour of the considered system. Nevertheless they can be useful in practice, to represent the cumulative knowledge of the system in a consistent description."}
{"_id":"8df383aae16ce1003d57184d8e4bf729f265ab40","title":"Axial-Ratio-Bandwidth Enhancement of a Microstrip-Line-Fed Circularly Polarized Annular-Ring Slot Antenna","text":"The design of a new microstrip-line-fed wideband circularly polarized (CP) annular-ring slot antenna (ARSA) is proposed. Compared with existing ring slot antennas, the ARSAs designed here possess much larger CP bandwidths. The main features of the proposed design include a wider ring slot, a pair of grounded hat-shaped patches, and a deformed bent feeding microstrip line. The ARSAs designed using FR4 substrates in the L and S bands have 3-dB axial-ratio bandwidths (ARBWs) of as large as 46% and 56%, respectively, whereas the one using an RT5880 substrate in the L band, 65%. In these 3-dB axial-ratio bands, impedance matching with VSWR \u2264 2 is also achieved."}
{"_id":"1b0af2ba6f22b43f9115c03a52e515b5d1e358d2","title":"A Practical Approach to Differential Private Learning","text":"Applying differential private learning to real-world data is currently unpractical. Differential privacy (DP) introduces extra hyper-parameters for which no thorough good practices exist, while manually tuning these hyper-parameters on private data results in low privacy guarantees. Furthermore, the exact guarantees provided by differential privacy for machine learning models are not well understood. Current approaches use undesirable post-hoc privacy attacks on models to assess privacy guarantees. To improve this situation, we introduce three tools to make DP machine learning more practical. First, two sanity checks for differential private learning are proposed. These sanity checks can be carried out in a centralized manner before training, do not involve training on the actual data and are easy to implement. Additionally, methods are proposed to reduce the effective number of tuneable privacy parameters by making use of an adaptive clipping bound. Lastly, existing methods regarding large batch training and differential private learning are combined. It is demonstrated that this combination improves model performance within a constant privacy budget."}
{"_id":"d82c363d46e2d49028776da1092674efe4282d39","title":"Privacy as a fuzzy concept: A new conceptualization of privacy for practitioners","text":"\u2022 Users may freely distribute the URL that is used to identify this publication. \u2022 Users may download and\/or print one copy of the publication from the University of Birmingham research portal for the purpose of private study or non-commercial research. \u2022 User may use extracts from the document in line with the concept of \u2018fair dealing\u2019 under the Copyright, Designs and Patents Act 1988 (?) \u2022 Users may not further distribute the material nor use it for the purposes of commercial gain."}
{"_id":"e65881a89633b8d4955e9314e84b943e155da6a9","title":"TG13 flowchart for the management of acute cholangitis and cholecystitis.","text":"We propose a management strategy for acute cholangitis and cholecystitis according to the severity assessment. For Grade I (mild) acute cholangitis, initial medical treatment including the use of antimicrobial agents may be sufficient for most cases. For non-responders to initial medical treatment, biliary drainage should be considered. For Grade II (moderate) acute cholangitis, early biliary drainage should be performed along with the administration of antibiotics. For Grade III (severe) acute cholangitis, appropriate organ support is required. After hemodynamic stabilization has been achieved, urgent endoscopic or percutaneous transhepatic biliary drainage should be performed. In patients with Grade II (moderate) and Grade III (severe) acute cholangitis, treatment for the underlying etiology including endoscopic, percutaneous, or surgical treatment should be performed after the patient's general condition has been improved. In patients with Grade I (mild) acute cholangitis, treatment for etiology such as endoscopic sphincterotomy for choledocholithiasis might be performed simultaneously, if possible, with biliary drainage. Early laparoscopic cholecystectomy is the first-line treatment in patients with Grade I (mild) acute cholecystitis while in patients with Grade II (moderate) acute cholecystitis, delayed\/elective laparoscopic cholecystectomy after initial medical treatment with antimicrobial agent is the first-line treatment. In non-responders to initial medical treatment, gallbladder drainage should be considered. In patients with Grade III (severe) acute cholecystitis, appropriate organ support in addition to initial medical treatment is necessary. Urgent or early gallbladder drainage is recommended. Elective cholecystectomy can be performed after the improvement of the acute inflammatory process. Free full-text articles and a mobile application of TG13 are available via http:\/\/www.jshbps.jp\/en\/guideline\/tg13.html."}
{"_id":"08e1f479de40d66711f89e0926bc3f6d14f3dbc0","title":"Disease Trajectory Maps","text":"Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Longitudinal data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of longitudinal EHR data. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled longitudinal data. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes."}
{"_id":"95e873c3f64a9bd8346f5b5da2e4f14774536834","title":"Wideband H-Plane Horn Antenna Based on Ridge Substrate Integrated Waveguide (RSIW)","text":"A substrate integrated waveguide (SIW) H-plane sectoral horn antenna, with significantly improved bandwidth, is presented. A tapered ridge, consisting of a simple arrangement of vias on the side flared wall within the multilayer substrate, is introduced to enlarge the operational bandwidth. A simple feed configuration is suggested to provide the propagating wave for the antenna structure. The proposed antenna is simulated by two well-known full-wave packages, Ansoft HFSS and CST Microwave Studio, based on segregate numerical methods. Close agreement between simulation results is reached. The designed antenna shows good radiation characteristics and low VSWR, lower than 2.5, for the whole frequency range of 18-40 GHz."}
{"_id":"8c030a736512456e9fd8d53763cbfcac0c014ab3","title":"Multiscale Approaches To Music Audio Feature Learning","text":"Content-based music information retrieval tasks are typically solved with a two-stage approach: features are extracted from music audio signals, and are then used as input to a regressor or classifier. These features can be engineered or learned from data. Although the former approach was dominant in the past, feature learning has started to receive more attention from the MIR community in recent years. Recent results in feature learning indicate that simple algorithms such as K-means can be very effective, sometimes surpassing more complicated approaches based on restricted Boltzmann machines, autoencoders or sparse coding. Furthermore, there has been increased interest in multiscale representations of music audio recently. Such representations are more versatile because music audio exhibits structure on multiple timescales, which are relevant for different MIR tasks to varying degrees. We develop and compare three approaches to multiscale audio feature learning using the spherical K-means algorithm. We evaluate them in an automatic tagging task and a similarity metric learning task on the Magnatagatune dataset."}
{"_id":"a4ac001d1a11df51e05b1651d497c4e56dec3f51","title":"Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method","text":"We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of time we only need to update fewer than 5% of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy."}
{"_id":"5aab22bf1f18e28fbb15af044f4dcf6f60524eb2","title":"SeemGo: Conditional Random Fields Labeling and Maximum Entropy Classification for Aspect Based Sentiment Analysis","text":"This paper describes our SeemGo system for the task of Aspect Based Sentiment Analysis in SemEval-2014. The subtask of aspect term extraction is cast as a sequence labeling problem modeled with Conditional Random Fields that obtains the F-score of 0.683 for Laptops and 0.791 for Restaurants by exploiting both word-based features and context features. The other three subtasks are solved by the Maximum Entropy model, with the occurrence counts of unigram and bigram words of each sentence as features. The subtask of aspect category detection obtains the best result when applying the Boosting method on the Maximum Entropy model, with the precision of 0.869 for Restaurants. The Maximum Entropy model also shows good performance in the subtasks of both aspect term and aspect category polarity classification."}
{"_id":"8188d1bac84d020595115a695ba436ceeb5437e0","title":"Machine learning approach for automated screening of malaria parasite using light microscopic images.","text":"The aim of this paper is to address the development of computer assisted malaria parasite characterization and classification using machine learning approach based on light microscopic images of peripheral blood smears. In doing this, microscopic image acquisition from stained slides, illumination correction and noise reduction, erythrocyte segmentation, feature extraction, feature selection and finally classification of different stages of malaria (Plasmodium vivax and Plasmodium falciparum) have been investigated. The erythrocytes are segmented using marker controlled watershed transformation and subsequently total ninety six features describing shape-size and texture of erythrocytes are extracted in respect to the parasitemia infected versus non-infected cells. Ninety four features are found to be statistically significant in discriminating six classes. Here a feature selection-cum-classification scheme has been devised by combining F-statistic, statistical learning techniques i.e., Bayesian learning and support vector machine (SVM) in order to provide the higher classification accuracy using best set of discriminating features. Results show that Bayesian approach provides the highest accuracy i.e., 84% for malaria classification by selecting 19 most significant features while SVM provides highest accuracy i.e., 83.5% with 9 most significant features. Finally, the performance of these two classifiers under feature selection framework has been compared toward malaria parasite classification."}
{"_id":"c29920f73686404a580fa2f9bff34548fd73125a","title":"Multi-factor Authentication Security Framework in Cloud Computing","text":"Data Security is the most critical issues in a cloud computing environment. Authentication is a key technology for information security, which is a mechanism to establish proof of identities to get access of information in the system. Traditional password authentication does not provide enough security for information in cloud computing environment to the most modern means of attacks. In this paper, we propose a new multi-factor authentication framework for cloud computing. In this paper the features of various access control mechanisms are discussed and a novel framework of access control is proposed for cloud computing, which provides a multi -step and multifactor authentication of a user. The model proposed is well-organized and provably secure solution of access control for externally hosted applications."}
{"_id":"12a376e621d690f3e94bce14cd03c2798a626a38","title":"Rapid Object Detection using a Boosted Cascade of Simple Features","text":"This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection."}
{"_id":"191b8537a875f0171593a4356f2535d5f1bbceac","title":"Tensor-Factorized Neural Networks","text":"The growing interests in multiway data analysis and deep learning have drawn tensor factorization (TF) and neural network (NN) as the crucial topics. Conventionally, the NN model is estimated from a set of one-way observations. Such a vectorized NN is not generalized for learning the representation from multiway observations. The classification performance using vectorized NN is constrained, because the temporal or spatial information in neighboring ways is disregarded. More parameters are required to learn the complicated data structure. This paper presents a new tensor-factorized NN (TFNN), which tightly integrates TF and NN for multiway feature extraction and classification under a unified discriminative objective. This TFNN is seen as a generalized NN, where the affine transformation in an NN is replaced by the multilinear and multiway factorization for tensor-based NN. The multiway information is preserved through layerwise factorization. Tucker decomposition and nonlinear activation are performed in each hidden layer. The tensor-factorized error backpropagation is developed to train TFNN with the limited parameter size and computation time. This TFNN can be further extended to realize the convolutional TFNN (CTFNN) by looking at small subtensors through the factorized convolution. Experiments on real-world classification tasks demonstrate that TFNN and CTFNN attain substantial improvement when compared with an NN and a convolutional NN, respectively."}
{"_id":"682c434becc69b9dc70a4c18305f9d733d03f581","title":"Users of the world , unite ! The challenges and opportunities of Social Media","text":"As of January 2009, the online social networking application Facebook registered more than 175 million active users. To put that number in perspective, this is only slightly less than the population of Brazil (190 million) and over twice the population of Germany (80 million)! At the same time, every minute, 10 hours of content were uploaded to the video sharing platform YouTube. And, the image hosting site Flickr provided access to over 3 billion photographs, making the world-famous Louvre Museum\u2019s collection of 300,000 objects seem tiny in comparison. According to Forrester Research, 75% of Internet surfers used \u2018\u2018Social Media\u2019\u2019 in the second quarter of 2008 by joining social networks, reading blogs, or contributing reviews to shopping sites; this represents a significant rise from 56% in 2007. The growth is not limited to teenagers, either; members of Generation X, now 35\u201444 years old, increasingly populate the ranks of joiners, spectators, and critics. It is therefore reasonable to say that Social Media represent a revolutionary new trend that should be of interest to companies operating in online space\u2013\u2014or any space, for that matter. Yet, not overly many firms seem to act comfortably in a world where consumers can speak so freely Business Horizons (2010) 53, 59\u201468"}
{"_id":"397e30b7a9fef1f210f29ab46eda013efb093fef","title":"SpatialHadoop: towards flexible and scalable spatial processing using mapreduce","text":"Recently, MapReduce frameworks, e.g., Hadoop, have been used extensively in different applications that include tera-byte sorting, machine learning, and graph processing. With the huge volumes of spatial data coming from different sources, there is an increasing demand to exploit the efficiency of Hadoop, coupled with the flexibility of the MapReduce framework, in spatial data processing. However, Hadoop falls short in supporting spatial data efficiently as the core is unaware of spatial data properties. This paper describes SpatialHadoop; a full-edged MapReduce framework with native support for spatial data. SpatialHadoop is a comprehensive extension to Hadoop that injects spatial data awareness in each Hadoop layer, namely, the language, storage, MapReduce, and operations layers. In the language layer, SpatialHadoop adds a simple and ex- pressive high level language for spatial data types and operations. In the storage layer, SpatialHadoop adapts traditional spatial index structures, Grid, R-tree and R+-tree, to form a two-level spatial index. SpatialHadoop enriches the MapReduce layer by two new components, SpatialFileSplitter and SpatialRecordReader, for efficient and scalable spatial data processing. In the operations layer, SpatialHadoop is already equipped with a dozen of operations, including range query, kNN, and spatial join. The flexibility and open source nature of SpatialHadoop allows more spatial operations to be implemented efficiently using MapReduce. Extensive experiments on a real system prototype and real datasets show that SpatialHadoop achieves orders of magnitude better performance than Hadoop for spatial data processing."}
{"_id":"9e549115438a49e468d2ee129c735058d1cc1623","title":"Treating the Banana Fold with the Dermotuberal Anchorage Technique: Case Report","text":"The banana fold, or the infragluteal fold, is a fat deposit on the posterior thigh close to the gluteal crease and parallel to it. A banana fold may form for different reasons, among which an iatrogenic cause is recurrent. Although banana fold is a common problem, unrelished by most women, few procedures are targeted specifically to fight it. This report presents a severe case of iatrogenic banana fold corrected by a modification of the dermotuberal anchorage buttock-lifting technique, as reported by the author for gluteal ptosis. The operation is performed by tucking in part of the banana fold tissue caudal to the gluteal crease, sliding that tissue, after depithelization, under the buttock, and pulling it up toward the ischial tuberosity until the redundant skin on the posterior thigh is tight and the banana fold is reduced. Assessment of the results 1 year after surgery showed that the technique provided a good scar kept within the subgluteal crease, and that it satisfactorily corrected the patient\u2019s major complaint: the banana-shaped fold."}
{"_id":"ee4ae9d87438e4b39eccd2ff3b509a489587c2b2","title":"Integrated microstrip and rectangular waveguide in planar form","text":"Usually transitions from microstrip line to rectangular waveguide are made with three-dimensional complex mounting structures. In this paper, a new planar platform is developed in which the microstrip line and rectangular waveguide are fully integrated on the same substrate, and they are interconnected via a simple taper. Our experiments at 28 GHz show that an effective bandwidth of 12% at 20 dB return loss is obtained with an in-band insertion loss better than 0.3 dB. The new transition allows a complete integration of waveguide components on substrate with MICs and MMICs."}
{"_id":"b9e78ee951ad2e4ba32a0861744bc4cea3821d50","title":"A 20 Gb\/s 0.4 pJ\/b Energy-Efficient Transmitter Driver Utilizing Constant- ${\\rm G}_{\\rm m}$  Bias","text":"This paper describes a transmitter driver based on a CMOS inverter with a resistive feedback. By employing the proposed driver topology, the pre-driver can be greatly simplified, resulting in a remarkable reduction of the overall driver power consumption. It also offers another advantage that the implementation of equalization is straightforward, compared with a conventional voltage-mode driver. Furthermore, the output impedance remains relatively constant while the data is being transmitted, resulting in good signal integrity. For evaluation of the driver performance, a fully functional 20 Gb\/s transmitter is implemented, including a PRBS generator, a serializer, and a half-rate clock generator. In order to enhance the overall speed of the digital circuits for 20 Gb\/s data transmission, the resistive feedback is applied to the time-critical inverters, which enables shorter rise\/fall times. The prototype chip is fabricated in a 65 nm CMOS technology. The implemented driver circuit operates up to the data rate of 20 Gb\/s, exhibiting an energy efficiency of 0.4 pJ\/b for the output swing of 250 mVpp,diff."}
{"_id":"178631e0f0e624b1607c7a7a2507ed30d4e83a42","title":"Speech recognition with deep recurrent neural networks","text":"Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."}
{"_id":"00a7370518a6174e078df1c22ad366a2188313b5","title":"Determining Optical Flow","text":"Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image."}
{"_id":"0c2764756299a82659605b132aef9159f61a4171","title":"Sarcasm Detection on Czech and English Twitter","text":"This paper presents a machine learning approach to sarcasm detection on Twitter in two languages \u2013 English and Czech. Although there has been some research in sarcasm detection in languages other than English (e.g., Dutch, Italian, and Brazilian Portuguese), our work is the first attempt at sarcasm detection in the Czech language. We created a large Czech Twitter corpus consisting of 7,000 manually-labeled tweets and provide it to the community. We evaluate two classifiers with various combinations of features on both the Czech and English datasets. Furthermore, we tackle the issues of rich Czech morphology by examining different preprocessing techniques. Experiments show that our language-independent approach significantly outperforms adapted state-of-the-art methods in English (F-measure 0.947) and also represents a strong baseline for further research in Czech (F-measure 0.582)."}
{"_id":"4140e7481c2599604b14fcd04625274022583631","title":"Availability : A Heuristic for Judging Frequency and Probability 122","text":"This paper explores a judgmental heuristic in which a person evaluates the frequency of classes or the probability of events by availability, i.e., by the ease with which relevant instances come to mind. In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatorial outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias. The effects of the availability of incidents and scenarios on subjective probability are discussed."}
{"_id":"2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","title":"ImageNet Classification with Deep Convolutional Neural Networks","text":"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."}
{"_id":"4691d99c6f08e719de161463c2b9210c6d8beecb","title":"Diabetes Data Analysis and Prediction Model Discovery Using RapidMiner","text":"Data mining techniques have been extensively applied in bioinformatics to analyze biomedical data. In this paper, we choose the Rapid-I\u00bfs RapidMiner as our tool to analyze a Pima Indians Diabetes Data Set, which collects the information of patients with and without developing diabetes. The discussion follows the data mining process. The focus will be on the data preprocessing, including attribute identification and selection, outlier removal, data normalization and numerical discretization, visual data analysis, hidden relationships discovery, and a diabetes prediction model construction."}
{"_id":"254ded254065f2d26ca24ec024cefd7604bd74e7","title":"Efficient Parallel Graph Exploration on Multi-Core CPU and GPU","text":"Graphs are a fundamental data representation that has been used extensively in various domains. In graph-based applications, a systematic exploration of the graph such as a breadth-first search (BFS) often serves as a key component in the processing of their massive data sets. In this paper, we present a new method for implementing the parallel BFS algorithm on multi-core CPUs which exploits a fundamental property of randomly shaped real-world graph instances. By utilizing memory bandwidth more efficiently, our method shows improved performance over the current state-of-the-art implementation and increases its advantage as the size of the graph increases. We then propose a hybrid method which, for each level of the BFS algorithm, dynamically chooses the best implementation from: a sequential execution, two different methods of multicore execution, and a GPU execution. Such a hybrid approach provides the best performance for each graph size while avoiding poor worst-case performance on high-diameter graphs. Finally, we study the effects of the underlying architecture on BFS performance by comparing multiple CPU and GPU systems, a high-end GPU system performed as well as a quad-socket high-end CPU system."}
{"_id":"01f187c3f0390123e70e01f824101bf771e76b8f","title":"Bitcoin and Beyond: A Technical Survey on Decentralized Digital Currencies","text":"Besides attracting a billion dollar economy, Bitcoin revolutionized the field of digital currencies and influenced many adjacent areas. This also induced significant scientific interest. In this survey, we unroll and structure the manyfold results and research directions. We start by introducing the Bitcoin protocol and its building blocks. From there we continue to explore the design space by discussing existing contributions and results. In the process, we deduce the fundamental structures and insights at the core of the Bitcoin protocol and its applications. As we show and discuss, many key ideas are likewise applicable in various other fields, so that their impact reaches far beyond Bitcoin itself."}
{"_id":"131b3fa7f7c1f35fb81e2860523650750d6ff10e","title":"Collaging on Internal Representations: An Intuitive Approach for Semantic Transfiguration","text":"We present a novel CNN-based image editing method that allows the user to change the semantic information of an image over a user-specified region. Our method makes this possible by combining the idea of manifold projection with spatial conditional batch normalization (sCBN), a version of conditional batch normalization with userspecifiable spatial weight maps. With sCBN and manifold projection, our method lets the user perform (1) spatial class translation that changes the class of an object over an arbitrary region of user\u2019s choice, and (2) semantic transplantation that transplants semantic information contained in an arbitrary region of the reference image to an arbitrary region in the target image. These two transformations can be used simultaneously, and can realize a complex composite image-editing task like \u201cchange the nose of a beagle to that of a bulldog, and open her mouth\u201d. The user can also use our method with intuitive copy-paste-style manipulations. We demonstrate the power of our method on various images. Code will be available at https:\/\/github. com\/pfnet-research\/neural-collage."}
{"_id":"dcbbb4509c7256f20ec2ec3c1450cd09290518f5","title":"Food, livestock production, energy, climate change, and health","text":"Food provides energy and nutrients, but its acquisition requires energy expenditure. In post-hunter-gatherer societies, extra-somatic energy has greatly expanded and intensified the catching, gathering, and production of food. Modern relations between energy, food, and health are very complex, raising serious, high-level policy challenges. Together with persistent widespread under-nutrition, over-nutrition (and sedentarism) is causing obesity and associated serious health consequences. Worldwide, agricultural activity, especially livestock production, accounts for about a fifth of total greenhouse-gas emissions, thus contributing to climate change and its adverse health consequences, including the threat to food yields in many regions. Particular policy attention should be paid to the health risks posed by the rapid worldwide growth in meat consumption, both by exacerbating climate change and by directly contributing to certain diseases. To prevent increased greenhouse-gas emissions from this production sector, both the average worldwide consumption level of animal products and the intensity of emissions from livestock production must be reduced. An international contraction and convergence strategy offers a feasible route to such a goal. The current global average meat consumption is 100 g per person per day, with about a ten-fold variation between high-consuming and low-consuming populations. 90 g per day is proposed as a working global target, shared more evenly, with not more than 50 g per day coming from red meat from ruminants (ie, cattle, sheep, goats, and other digastric grazers)."}
{"_id":"3a09d0f6cd5da2178419d7e6c346ef9f6a82863f","title":"Techniques to Detect Spammers in Twitter- A Survey","text":"With the rapid growth of social networking sites for communicating, sharing, storing and managing significant information, it is attracting cybercriminals who misuse the Web to exploit vulnerabilities for their illicit benefits. Forged online accounts crack up every day. Impersonators, phishers, scammers and spammers crop up all the time in Online Social Networks (OSNs), and are harder to identify. Spammers are the users who send unsolicited messages to a large audience with the intention of advertising some product or to lure victims to click on malicious links or infecting user&apos;s system just for the purpose of making money. A lot of research has been done to detect spam profiles in OSNs. In this paper we have reviewed the existing techniques for detecting spam users in Twitter social network. Features for the detection of spammers could be user based or content based or both. Current study provides an overview of the methods, features used, detection rate and their limitations (if any) for detecting spam profiles mainly in Twitter."}
{"_id":"5af12524db0186bcdcafb7a342556d421cf342bf","title":"Exploiting Passive Dynamics with Variable Stiffness Actuation in Robot Brachiation","text":"This paper explores a passive control strategy with variable stiffness actuation for swing movements. We consider brachiation as an example of a highly dynamic task which requires exploitation of gravity in an efficient manner for successful task execution. First, we present our passive control strateg y considering a pendulum with variable stiffness actuation. Then, we formulate the problem based an optimal control framework with temporal optimization in order to simultaneously find an appropriate stiffness profile and movement duration such that the resultant movement will be able to exploit the passive dynamics of the robot. Finally, numerical evaluations on a twolink brachiating robot with a variable stiffness actuator (VSA) model are provided to demonstrate the effectiveness of our approach under different task requirements, modelling errors and switching in the robot dynamics. In addition, we discuss the issue of task description in terms of the choice of cost function for successful task execution in optimal control."}
{"_id":"12f7b71324ee8e1796a9ef07af05b66674fe6af0","title":"Collective annotation of Wikipedia entities in web text","text":"To take the first step beyond keyword-based search toward entity-based search, suitable token spans (\"spots\") on documents must be identified as references to real-world entities from an entity catalog. Several systems have been proposed to link spots on Web pages to entities in Wikipedia. They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity. Two recent systems exploit inter-label dependencies, but in limited ways. We propose a general collective disambiguation approach. Our premise is that coherent documents refer to entities from one or a few related topics or domains. We give formulations for the trade-off between local spot-to-entity compatibility and measures of global coherence between entities. Optimizing the overall entity assignment is NP-hard. We investigate practical solutions based on local hill-climbing, rounding integer linear programs, and pre-clustering entities followed by local optimization within clusters. In experiments involving over a hundred manually-annotated Web pages and tens of thousands of spots, our approaches significantly outperform recently-proposed algorithms."}
{"_id":"77d2698e8efadda698b0edb457cd8de75224bfa0","title":"Knowledge Base Population: Successful Approaches and Challenges","text":"In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking \u2013 linking names in context to entities in the KB \u2013 and Slot Filling \u2013 adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes (\u201cslots\u201d) derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges."}
{"_id":"0638d1f7d37f6bda49f6ec951de37aca0e53b98a","title":"Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation","text":"Conversation between two people is usually of Mixed-Initiative, with Control over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles."}
{"_id":"1c909ac1c331c0c246a88da047cbdcca9ec9b7e7","title":"Large-Scale Named Entity Disambiguation Based on Wikipedia Data","text":"This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles."}
{"_id":"2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2","title":"DBpedia: A Nucleus for a Web of Open Data","text":"DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for humanand machineconsumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data."}
{"_id":"557e71f87073be437908c1d033500acbe7670712","title":"On error management: lessons from aviation.","text":"Copies of the full protocol and details of training programmes are available from Association of Litigation and Risk Management (ALARM), Royal Society of Medicine, 1 Wimpole Street, London W1. Contributors: CV and ST-A carried out the research on which the original protocol was based. All authors participated equally in the development of the protocol, in which successive versions were tested in clinical practice and refined in the light of experience. The writing of the original protocol and present paper was primarily carried out by CV, ST-A, EJC, and DH, but all authors contributed to the final version. CV and DH are the guarantors. Competing interests: CV received funding from Healthcare Risk Resources International to support the work of ST-A during the development of the protocol."}
{"_id":"3f007c43a7b4bb5a052b7a16d4e33c4842a8244a","title":"Self-assembly of neural networks viewed as swarm intelligence","text":"While self-assembly is a fairly active area of research in swarm intelligence, relatively little attention has been paid to the issues surrounding the construction of network structures. In this paper we extend methods developed previously for controlling collective movements of agent teams to serve as the basis for self-assembly or \u201cgrowth\u201d of networks, using neural networks as a concrete application to evaluate our approach. Our central innovation is having network connections arise as persistent \u201ctrails\u201d left behind moving agents, trails that are reminiscent of pheromone deposits made by agents in ant colony optimization models. The resulting network connections are thus essentially a record of agent movements. We demonstrate our model\u2019s effectiveness by using it to produce two large networks that support subsequent learning of topographic and feature maps. Improvements produced by the incorporation of collective movements are also examined through computational experiments. These results indicate that methods for directing collective movements can be adopted to facilitate network self-assembly."}
{"_id":"92930f4279b48f7e4e8ec2edc24e8aa65c5954fd","title":"Client Profiling for an Anti-Money Laundering System","text":"We present a data mining approach for profiling bank clients in order to support the process of detection of antimoney laundering operations. We first present the overall system architecture, and then focus on the relevant component for this paper. We detail the experiments performed on real world data from a financial institution, which allowed us to group clients in clusters and then generate a set of classification rules. We discuss the relevance of the founded client profiles and of the generated classification rules. According to the defined overall agent-based architecture, these rules will be incorporated in the knowledge base of the intelligent agents responsible for the signaling of suspicious transactions."}
{"_id":"f64d11fa836bf5bde5dc730f4311e899aea1047c","title":"Strength Training for Endurance Athletes: Theory to Practice","text":"THE PURPOSE OF THIS REVIEW IS TWOFOLD: TO ELUCIDATE THE UTILITY OF RESISTANCE TRAINING FOR ENDURANCE ATHLETES, AND PROVIDE THE PRACTITIONER WITH EVIDENCED-BASED PERIODIZATION STRATEGIES FOR CONCURRENT STRENGTH AND ENDURANCE TRAINING IN ATHLETIC POPULATIONS. BOTH LOW-INTENSITY EXERCISE ENDURANCE (LIEE) AND HIGH-INTENSITY EXERCISE ENDURANCE (HIEE) HAVE BEEN SHOWN TO IMPROVE AS A RESULT OF MAXIMAL, HIGH FORCE, LOW VELOCITY (HFLV) AND EXPLOSIVE, LOW-FORCE, HIGH-VELOCITY STRENGTH TRAINING. HFLV STRENGTH TRAINING IS RECOMMENDED INITIALLY TO DEVELOP A NEUROMUSCULAR BASE FOR ENDURANCE ATHLETES WITH LIMITED STRENGTH TRAINING EXPERIENCE. A SEQUENCED APPROACH TOSTRENGTHTRAINING INVOLVING PHASES OF STRENGTHENDURANCE, BASIC STRENGTH, STRENGTH, AND POWER WILL PROVIDE FURTHER ENHANCEMENTS IN LIEE AND HIEE FOR HIGHLEVEL ENDURANCE ATHLETES."}
{"_id":"526fb409521b6e7ef1afa771b30afe30e71e8c2b","title":"Dual Switches DC\/DC Converter With Three-Winding-Coupled Inductor and Charge Pump","text":"In order to obtain a high step-up voltage gain, high-efficiency converter, this paper proposed a dual switches dc\/dc converter with three-winding-coupled inductor and charge pump. The proposed converter composed of dual switches structure, three-winding-coupled inductor, and charge pump. This combination facilitates realization of high step-up voltage gain with a low voltage\/current stress on the power switches. Meanwhile, the voltage across the diodes is low and the diode reverse-recovery problem is alleviated by the leakage inductance of the three-winding-coupled inductor. Taking all these into consideration, the efficiency can be high. This paper illustrated the operation principle of the proposed converter; discussed the effect of leakage inductance on voltage gain; the conditions of zero current shutting off of the diodes are illustrated; the voltage and current stress of the power devices are shown; a comparison between the performance of the proposed converter and previous high step-up converters was conducted. Finally, a prototype rated at 500 W has been established, and the experimental results verify the correctness of the analysis."}
{"_id":"5b7addfb161b6e43937c9b8db3c85f10de671d0c","title":"Learning from positive and unlabeled examples","text":"In many machine learning settings, labeled examples are difficult to collect while unlabeled data are abundant. Also, for some binary classification problems, positive examples which are elements of the target concept are available. Can these additional data be used to improve accuracy of supervised learning algorithms? We investigate in this paper the design of learning algorithms from positive and unlabeled data only. Many machine learning and data mining algorithms, such as decision tree induction algorithms and naive Bayes algorithms, use examples only to evaluate statistical queries (SQ-like algorithms). Kearns designed the statistical query learning model in order to describe these algorithms. Here, we design an algorithm scheme which transforms any SQ-like algorithm into an algorithm based on positive statistical queries (estimate for probabilities over the set of positive instances) and instance statistical queries (estimate for probabilities over the instance space). We prove that any class learnable in the statistical query learning model is learnable from positive statistical queries and instance statistical queries only if a lower bound on the weight of any target concept f can be estimated in polynomial time. Then, we design a decision tree induction algorithm POSC4.5, based on C4.5, that uses only positive and unlabeled examples and we give experimental results for this algorithm. In the case of imbalanced classes in the sense that one of the two classes (say the positive class) is heavily underrepresented compared to the other class, the learning problem remains open. This problem is challenging because it is encountered in many real-world applications. \u00a9 2005 Elsevier B.V. All rights reserved."}
{"_id":"c2bd37348784b4e6c16c7ab5ca8317987f3a73dd","title":"Specificity of genetic and environmental risk factors for use and abuse\/dependence of cannabis, cocaine, hallucinogens, sedatives, stimulants, and opiates in male twins.","text":"OBJECTIVE\nData on use and misuse of six classes of illicit substances by male twin pairs were used to examine whether genetic and shared environmental risk factors for substance use disorders are substance-specific or -nonspecific in their effect.\n\n\nMETHOD\nLifetime history of use and abuse\/dependence of cannabis, cocaine, hallucinogens, sedatives, stimulants, and opiates was assessed at personal interview in both members of 1,196 male-male twin pairs ascertained by the Virginia Twin Registry. Multivariate twin modeling of substance-nonspecific (common) and substance-specific genetic, shared environmental, and unique environmental risk factors was performed by using the program Mx.\n\n\nRESULTS\nHigh levels of comorbidity involving the different substance categories were observed for both use and abuse\/dependence. One common genetic factor was found to have a strong influence on risk for illicit use and abuse\/dependence for all six substance classes. A modest influence of substance-specific genetic factors was seen for use but not for abuse\/dependence. Shared environmental factors were more important for use than for abuse\/dependence and were mediated entirely through a single common factor.\n\n\nCONCLUSIONS\nIn an adult population-based sample of male twins, both the genetic and the shared environmental effects on risk for the use and misuse of six classes of illicit substances were largely or entirely nonspecific in their effect. Environmental experiences unique to the person largely determine whether predisposed individuals will use or misuse one class of psychoactive substances rather than another."}
{"_id":"cae8f2af7a25480479811453f27b4189ba5cc801","title":"Question Answering on SQuAD","text":"In this project, we exploit several deep learning architectures in Question Answering field, based on the newly released Stanford Question Answering dataset (SQuAD)[7]. We introduce a multi-stage process that encodes context paragraphs at different levels of granularity, uses co-attention mechanism to fuse representations of questions and context paragraphs, and finally decodes the co-attention vectors to get the answers. Our best model gets 62.23% F1 score and 48.72% EM score on the test set."}
{"_id":"c8ea1664d0cf4823b5ffb61a0d2f6dbda2441c49","title":"Genetic Evidence That Carbohydrate-Stimulated Insulin Secretion Leads to Obesity.","text":"BACKGROUND\nA fundamental precept of the carbohydrate-insulin model of obesity is that insulin secretion drives weight gain. However, fasting hyperinsulinemia can also be driven by obesity-induced insulin resistance. We used genetic variation to isolate and estimate the potentially causal effect of insulin secretion on body weight.\n\n\nMETHODS\nGenetic instruments of variation of insulin secretion [assessed as insulin concentration 30 min after oral glucose (insulin-30)] were used to estimate the causal relationship between increased insulin secretion and body mass index (BMI), using bidirectional Mendelian randomization analysis of genome-wide association studies. Data sources included summary results from the largest published metaanalyses of predominantly European ancestry for insulin secretion (n = 26037) and BMI (n = 322154), as well as individual-level data from the UK Biobank (n = 138541). Data from the Cardiology and Metabolic Patient Cohort study at Massachusetts General Hospital (n = 1675) were used to validate genetic associations with insulin secretion and to test the observational association of insulin secretion and BMI.\n\n\nRESULTS\nHigher genetically determined insulin-30 was strongly associated with higher BMI (\u03b2 = 0.098, P = 2.2 \u00d7 10-21), consistent with a causal role in obesity. Similar positive associations were noted in sensitivity analyses using other genetic variants as instrumental variables. By contrast, higher genetically determined BMI was not associated with insulin-30.\n\n\nCONCLUSIONS\nMendelian randomization analyses provide evidence for a causal relationship of glucose-stimulated insulin secretion on body weight, consistent with the carbohydrate-insulin model of obesity."}
{"_id":"a52d4736bb9728e6993cd7b3190271f6728706fd","title":"Slip-aware Model Predictive optimal control for Path following","text":"Traditional control and planning algorithms for wheeled mobile robots (WMR) either totally ignore or make simplifying assumptions about the effects of wheel slip on the motion. While this approach works reasonably well in practice on benign terrain, it fails very quickly when the WMR is deployed in terrain that induces significant wheel slip. We contribute a novel control framework that predictively corrects for the wheel slip to effectively minimize path following errors. Our framework, the Receding Horizon Model Predictive Path Follower (RHMPPF), specifically addresses the problem of path following in challenging environments where the wheel slip substantially affects the vehicle mobility. We formulate the solution to the problem as an optimal controller that utilizes a slip-aware model predictive component to effectively correct the controls generated by a strictly geometric pure-pursuit path follower. We present extensive experimental validation of our approach using a simulated 6-wheel skid-steered robot in a high-fidelity data-driven simulator, and on a real 4-wheel skid-steered robot. Our results show substantial improvement in the path following performance in both simulation and real world experiments."}
{"_id":"c7b8ad27e2ddbabcc5d785b51b967f4ccb824bc0","title":"Datum: Managing Data Purchasing and Data Placement in a Geo-Distributed Data Market","text":"This paper studies two design tasks faced by a geo-distributed cloud data market: which data to purchase data purchasing and where to place\/replicate the data for delivery data placement. We show that the joint problem of data purchasing and data placement within a cloud data market can be viewed as a facility location problem and is thus NP-hard. However, we give a provably optimal algorithm for the case of a data market made up of a single data center and then generalize the structure from the single data center setting in order to develop a near-optimal, polynomial-time algorithm for a geo-distributed data market. The resulting design, $\\mathsf {Datum}$ , decomposes the joint purchasing and placement problem into two subproblems, one for data purchasing and one for data placement, using a transformation of the underlying bandwidth costs. We show, via a case study, that $\\mathsf {Datum}$ is near optimal within 1.6% in practical settings."}
{"_id":"0db7dcb8f91604a6b9f74bd789e70188377984ea","title":"Reducing Length of Stay Using a Robotic-assisted Approach for Retromuscular Ventral Hernia Repair: A Comparative Analysis From the Americas Hernia Society Quality Collaborative.","text":"OBJECTIVE\nThe aim of this study was to compare length of stay (LOS) after robotic-assisted and open retromuscular ventral hernia repair (RVHR).\n\n\nBACKGROUND\nRVHR has traditionally been performed by open techniques. Robotic-assisted surgery enables surgeons to perform minimally invasive RVHR, but with unknown benefit. Using real-world evidence, this study compared LOS after open (o-RVHR) and robotic-assisted (r-RVHR) approach.\n\n\nMETHODS\nMulti-institutional data from patients undergoing elective RVHR in the Americas Hernia Society Quality Collaborative between 2013 and 2016 were analyzed. Propensity score matching was used to compare median LOS between o-RVHR and r-RVHR groups. This work was supported by an unrestricted grant from Intuitive Surgical, and all clinical authors have declared direct or indirect relationships with Intuitive Surgical.\n\n\nRESULTS\nIn all, 333 patients met inclusion criteria for a 2:1 match performed on 111 r-RVHR patients using propensity scores, with 222 o-RVHR patients having similar characteristics as the robotic-assisted group. Median LOS [interquartile range (IQR)] was significantly decreased for r-RVHR patients [2 days (IQR 2)] compared with o-RVHR patients [3 days (IQR 3), P < 0.001]. No differences in 30-day readmissions or surgical site infections were observed. Higher surgical site occurrences were noted with r-RVHR, consisting mostly of seromas not requiring intervention.\n\n\nCONCLUSIONS\nUsing real-world evidence, a robotic-assisted approach to RVHR offers the clinical benefit of reduced postoperative LOS. Ongoing monitoring of this technique should be employed through continuous quality improvement to determine the long-term effect on hernia recurrence, complications, patient satisfaction, and overall cost."}
{"_id":"d9718e8745bf9bd70727dd0aec48b007593e00bc","title":"Possibilistic interest discovery from uncertain information in social networks","text":"User generated content on the microblogging social network Twitter continues to grow with significant amount of information. The semantic analysis offers the opportunity to discover and model latent interests\u2019 in the users\u2019 publications. This article focuses on the problem of uncertainty in the users\u2019 publications that has not been previously treated. It proposes a new approach for users\u2019 interest discovery from uncertain information that augments traditional methods using possibilistic logic. The possibility theory provides a solid theoretical base for the treatment of incomplete and imprecise information and inferring the reliable expressions from a knowledge base. More precisely, this approach used the product-based possibilistic network to model knowledge base and discovering possibilistic interests. DBpedia ontology is integrated into the interests\u2019 discovery process for selecting the significant topics. The empirical analysis and the comparison with the most known methods proves the significance of this approach."}
{"_id":"6b27f7ccbc68e6bf9bc1538f7ed8d1ca9d8e563a","title":"Mechanisms of emotional arousal and lasting declarative memory","text":"Neuroscience is witnessing growing interest in understanding brain mechanisms of memory formation for emotionally arousing events, a development closely related to renewed interest in the concept of memory consolidation. Extensive research in animals implicates stress hormones and the amygdaloid complex as key, interacting modulators of memory consolidation for emotional events. Considerable evidence suggests that the amygdala is not a site of long-term explicit or declarative memory storage, but serves to influence memory-storage processes in other brain regions, such as the hippocampus, striatum and neocortex. Human-subject studies confirm the prediction of animal work that the amygdala is involved with the formation of enhanced declarative memory for emotionally arousing events."}
{"_id":"8985000860dbb88a80736cac8efe30516e69ee3f","title":"Human Activity Recognition Using Recurrent Neural Networks","text":"Human activity recognition using smart home sensors is one of the bases of ubiquitous computing in smart environments and a topic undergoing intense research in the field of ambient assisted living. The increasingly large amount of data sets calls for machine learning methods. In this paper, we introduce a deep learning model that learns to classify human activities without using any prior knowledge. For this purpose, a Long Short Term Memory (LSTM) Recurrent Neural Network was applied to three real world smart home datasets. The results of these experiments show that the proposed approach outperforms the existing ones in terms of accuracy and performance."}
{"_id":"0658264c335017587a906ceb202da417b5521a92","title":"An Introductory Study on Time Series Modeling and Forecasting","text":"ACKNOWLEDGEMENT The timely and successful completion of the book could hardly be possible without the helps and supports from a lot of individuals. I will take this opportunity to thank all of them who helped me either directly or indirectly during this important work. First of all I wish to express my sincere gratitude and due respect to my supervisor Dr. R.K. Agrawal, Associate Professor SC & SS, JNU. I am immensely grateful to him for his valuable guidance, continuous encouragements and positive supports which helped me a lot during the period of my work. I would like to appreciate him for always showing keen interest in my queries and providing important suggestions. I also express whole hearted thanks to my friends and classmates for their care and moral supports. The moments, I enjoyed with them during my M.Tech course will always remain as a happy memory throughout my life. I owe a lot to my mother for her constant love and support. She always encouraged me to have positive and independent thinking, which really matter in my life. I would like to thank her very much and share this moment of happiness with her. Last but not the least I am also thankful to entire faculty and staff of SC & SS for their unselfish help, I got whenever needed during the course of my work.ABSTRACT Time series modeling and forecasting has fundamental importance to various practical domains. Thus a lot of active research works is going on in this subject during several years. Many important models have been proposed in literature for improving the accuracy and effeciency of time series modeling and forecasting. The aim of this book is to present a concise description of some popular time series forecasting models used in practice, with their salient features. In this book, we have described three important classes of time series models, viz. the stochastic, neural networks and SVM based models, together with their inherent forecasting strengths and weaknesses. We have also discussed about the basic issues related to time series modeling, such as stationarity, parsimony, overfitting, etc. Our discussion about different time series models is supported by giving the experimental forecast results, performed on six real time series datasets. While fitting a model to a dataset, special care is taken to select the most parsimonious one. To evaluate forecast accuracy as well as to compare among different models \u2026"}
{"_id":"3db9b57efd0e0c64e3fdc57afb1d8e018b30b0b6","title":"On the development of name search techniques for Arabic","text":"The need for effective identity matching systems has led to extensive research in the area of name search. For the most part, such work has been limited to English and other Latin-based languages. Consequently, algorithms such as Soundex and n-gram matching are of limited utility for languages such as Arabic, which has a vastly different morphology that relies heavily on phonetic information. The dearth of work in this field is partly due to the lack of standardized test data. Consequently, we built a collection of 7,939 Arabic names, along with 50 training queries and 111 test queries. We use this collection to evaluate a variety of algorithms, including a derivative of Soundex tailored to Arabic (ASOUNDEX), measuring effectiveness using standard information retrieval measures. Our results show an improvement of 70% over existing approaches. Introduction Identity matching systems frequently employ name search algorithms to effectively locate relevant information about a given person. Such systems are used for applications as diverse as tax fraud detection and immigration control. Using names to retrieve information makes such systems susceptible to problems arising from typographical errors. That is, exact match search approaches will not find instances of misspelled names or those names that have more than one accepted spelling. An example of the severity of the problem is noted in an NCR (1998) report that estimates that the state of Texas saved $43 million over 18 months in the field of tax compliance using an improved name search approach. Thus, the importance of such name-based search applications has resulted in improved name matching algorithms for English that make use of phonetic information, but these language-dependent techniques have not been extended to Arabic."}
{"_id":"becb5fbd24881dd78793686bbe30b153b5745fb8","title":"Tax Fraud Detection for Under-Reporting Declarations Using an Unsupervised Machine Learning Approach","text":"Tax fraud is the intentional act of lying on a tax return form with intent to lower one's tax liability. Under-reporting is one of the most common types of tax fraud, it consists in filling a tax return form with a lesser tax base. As a result of this act, fiscal revenues are reduced, undermining public investment.\n Detecting tax fraud is one of the main priorities of local tax authorities which are required to develop cost-efficient strategies to tackle this problem. Most of the recent works in tax fraud detection are based on supervised machine learning techniques that make use of labeled or audit-assisted data. Regrettably, auditing tax declarations is a slow and costly process, therefore access to labeled historical information is extremely limited. For this reason, the applicability of supervised machine learning techniques for tax fraud detection is severely hindered.\n Such limitations motivate the contribution of this work. We present a novel approach for the detection of potential fraudulent tax payers using only unsupervised learning techniques and allowing the future use of supervised learning techniques. We demonstrate the ability of our model to identify under-reporting taxpayers on real tax payment declarations, reducing the number of potential fraudulent tax payers to audit. The obtained results demonstrate that our model doesn't miss on marking declarations as suspicious and labels previously undetected tax declarations as suspicious, increasing the operational efficiency in the tax supervision process without needing historic labeled data."}
{"_id":"b31f0085b7dd24bdde1e5cec003589ce4bf4238c","title":"Discriminative Label Consistent Domain Adaptation","text":"Domain adaptation (DA) is transfer learning which aims to learn an effective predictor on target data from source data despite data distribution mismatch between source and target. We present in this paper a novel unsupervised DA method for cross-domain visual recognition which simultaneously optimizes the three terms of a theoretically established error bound. Specifically, the proposed DA method iteratively searches a latent shared feature subspace where not only the divergence of data distributions between the source domain and the target domain is decreased as most state-of-the-art DA methods do, but also the inter-class distances are increased to facilitate discriminative learning. Moreover, the proposed DA method sparsely regresses class labels from the features achieved in the shared subspace while minimizing the prediction errors on the source data and ensuring label consistency between source and target. Data outliers are also accounted for to further avoid negative knowledge transfer. Comprehensive experiments and in-depth analysis verify the effectiveness of the proposed DA method which consistently outperforms the state-of-the-art DA methods on standard DA benchmarks, i.e., 12 cross-domain image classification tasks."}
{"_id":"9bfb04bb15f7cc414108945571bd1d6d1f77b4ad","title":"Feature Selection by Joint Graph Sparse Coding","text":"This paper takes manifold learning and regression simultaneously into account to perform unsupervised spectral feature selection. We first extract the bases of the data, and then represent the data sparsely using the extracted bases by proposing a novel joint graph sparse coding model, JGSC for short. We design a new algorithm TOSC to compute the resulting objective function of JGSC, and then theoretically prove that the proposed objective function converges to its global optimum via the proposed TOSC algorithm. We repeat the extraction and the TOSC calculation until the value of the objective function of JGSC satisfies pre-defined conditions. Eventually the derived new representation of the data may only have a few non-zero rows, and we delete the zero rows (a.k.a. zero-valued features) to conduct feature selection on the new representation of the data. Our empirical studies demonstrate that the proposed method outperforms several state-of-the-art algorithms on real datasets in term of the kNN classification performance."}
{"_id":"0f03074cc5ef0e2bd0f16de62a1e170531474eae","title":"Discovering Available Drinks Through Natural , Robot-Led , Human-Robot Interaction Between a Waiter and a Bartender","text":"This research focuses on natural, robot-led, human-robot interaction that enables a robot to discover what drinks a barman can prepare through continuous application of speech recognition, understanding and generation. Speech was recognised using Google Cloud\u2019s speech to text API, understood by matching either the object or main verb of a sentence against a list of key words and, finally, generated using templates with variable parts. The difficulty lies in the large quantity of key words, as they are based on the properties of the ordered drinks. The results show that having the aforementioned interaction works well to some extent, i.e. the naturalness of the interaction was ranked 5.5 on average. Furthermore, the obtained precision when identifying the unavailable drinks was 0.625 and the obtained recall was 1, resulting in an F1 measure of 0.769."}
{"_id":"2d8d089d368f2982748fde93a959cf5944873673","title":"Visually Guided Spatial Relation Extraction from Text","text":"Extraction of spatial relations from sentences with complex\/nesting relationships is very challenging as often needs resolving inherent semantic ambiguities. We seek help from visual modality to fill the information gap in the text modality and resolve spatial semantic ambiguities. We use various recent vision and language datasets and techniques to train inter-modality alignment models, visual relationship classifiers and propose a novel global inference model to integrate these components into our structured output prediction model for spatial role and relation extraction. Our global inference model enables us to utilize the visual and geometric relationships between objects and improves the state-of-art results of spatial information extraction from text."}
{"_id":"852203bfc1694fc2ea4bbe0eea4c2f830df85d31","title":"Does Microfinance Really Help the Poor ? New Evidence from Flagship Programs in Bangladesh","text":"The microfinance movement has built on innovations in financial intermediation that reduce the costs and risks of lending to poor households. Replications of the movement\u2019s flagship, the Grameen Bank of Bangladesh, have now spread around the world. While programs aim to bring social and economic benefits to clients, few attempts have been made to quantify benefits rigorously. This paper draws on a new cross-sectional survey of nearly 1800 households, some of which are served by the Grameen Bank and two similar programs, and some of which have no access to programs. Households that are eligible to borrow and have access to the programs do not have notably higher consumption levels than control households, and, for the most part, their children are no more likely to be in school. Men also tend to work harder, and women less. More favorably, relative to controls, households eligible for programs have substantially (and significantly) lower variation in consumption and labor supply across seasons. The most important potential impacts are thus associated with the reduction of vulnerability, not of poverty per se. The consumption-smoothing appears to be driven largely by income-smoothing, not by borrowing and lending. The evaluation holds lessons for studies of other programs in low-income countries. While it is common to use fixed effects estimators to control for unobservable variables correlated with the placement of programs, using fixed effects estimators can exacerbate biases when, as here, programs target their programs to specific populations within larger communities."}
{"_id":"1cc920998208f988a873dbbfa0315274d0b51b57","title":"Introduction to Robotics: Mechanics and Control","text":"How can you change your mind to be more open? There many sources that can help you to improve your thoughts. It can be from the other experiences and also story from some people. Book is one of the trusted sources to get. You can find so many books that we share here in this website. And now, we show you one of the best, the introduction to robotics mechanics and control john j craig solution manual."}
{"_id":"5238190eb598fb3352c51ee07b7ee8ec714f3c38","title":"OPEM: A Static-Dynamic Approach for Machine-Learning-Based Malware Detection","text":"Malware is any computer software potentially harmful to both computers and networks. The amount of malware is growing every year and poses a serious global security threat. Signature-based detection is the most extended method in commercial antivirus software, however, it consistently fails to detect new malware. Supervised machine learning has been adopted to solve this issue. There are two types of features that supervised malware detectors use: (i) static features and (ii) dynamic features. Static features are extracted without executing the sample whereas dynamic ones requires an execution. Both approaches have their advantages and disadvantages. In this paper, we propose for the first time, OPEM, an hybrid unknown malware detector which combines the frequency of occurrence of operational codes (statically obtained) with the information of the execution trace of an executable (dynamically obtained). We show that this hybrid approach enhances the performance of both approaches when run separately."}
{"_id":"e5acdb0246b33d33c2a34a4a23faaf21e2f9b924","title":"An Efficient Non-Negative Matrix-Factorization-Based Approach to Collaborative Filtering for Recommender Systems","text":"Matrix-factorization (MF)-based approaches prove to be highly accurate and scalable in addressing collaborative filtering (CF) problems. During the MF process, the non-negativity, which ensures good representativeness of the learnt model, is critically important. However, current non-negative MF (NMF) models are mostly designed for problems in computer vision, while CF problems differ from them due to their extreme sparsity of the target rating-matrix. Currently available NMF-based CF models are based on matrix manipulation and lack practicability for industrial use. In this work, we focus on developing an NMF-based CF model with a single-element-based approach. The idea is to investigate the non-negative update process depending on each involved feature rather than on the whole feature matrices. With the non-negative single-element-based update rules, we subsequently integrate the Tikhonov regularizing terms, and propose the regularized single-element-based NMF (RSNMF) model. RSNMF is especially suitable for solving CF problems subject to the constraint of non-negativity. The experiments on large industrial datasets show high accuracy and low-computational complexity achieved by RSNMF."}
{"_id":"11efa6998c2cfd3de59cf0ec0321a9e17418915d","title":"Toward Automated Dynamic Malware Analysis Using CWSandbox","text":"Malware is notoriously difficult to combat because it appears and spreads so quickly. In this article, we describe the design and implementation of CWSandbox, a malware analysis tool that fulfills our three design criteria of automation, effectiveness, and correctness for the Win32 family of operating systems"}
{"_id":"129ed742b496b23efdf745aaf0c48958ef64d2c6","title":"Exploring Multiple Execution Paths for Malware Analysis","text":"Malicious code (or Malware) is defined as software that fulfills the deliberately harmful intent of an attacker. Malware analysis is the process of determining the behavior and purpose of a given Malware sample (such as a virus, worm, or Trojan horse). This process is a necessary step to be able to develop effective detection techniques and removal tools. Currently, Malware analysis is mostly a manual process that is tedious and time-intensive. To mitigate this problem, a number of analysis tools have been proposed that automatically extract the behavior of an unknown program by executing it in a restricted environment and recording the operating system calls that are invoked. The problem of dynamic analysis tools is that only a single program execution is observed. Unfortunately, however, it is possible that certain malicious actions are only triggered under specific circumstances (e.g., on a particular day, when a certain file is present, or when a certain command is received). In this paper, we propose a system that allows us to explore multiple execution paths and identify malicious actions that are executed only when certain conditions are met. This enables us to automatically extract a more complete view of the program under analysis and identify under which circumstances suspicious actions are carried out. Our experimental results demonstrate that many Malware samples show different behavior depending on input read from the environment. Thus, by exploring multiple execution paths, we can obtain a more complete picture of their actions."}
{"_id":"66651bb47e7d16b49f677decfb60cbe1939e7a91","title":"Optimizing number of hidden neurons in neural networks","text":"In this paper, a novel and effective criterion based on the estimation of the signal-to-noise-ratio figure (SNRF) is proposed to optimize the number of hidden neurons in neural networks to avoid overfitting in the function approximation. SNRF can quantitatively measure the useful information left unlearned so that overfitting can be automatically detected from the training error only without use of a separate validation set. It is illustrated by optimizing the number of hidden neurons in a multi-layer perceptron (MLP) using benchmark datasets. The criterion can be further utilized in the optimization of other parameters of neural networks when overfitting needs to be considered."}
{"_id":"03bb84352b89691110f112b5515766c55bcc5720","title":"Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning","text":"Extracting relations is critical for knowledge base completion and construction in which distant supervised methods are widely used to extract relational facts automatically with the existing knowledge bases. However, the automatically constructed datasets comprise amounts of low-quality sentences containing noisy words, which is neglected by current distant supervised methods resulting in unacceptable precisions. To mitigate this problem, we propose a novel word-level distant supervised approach for relation extraction. We first build Sub-Tree Parse (STP) to remove noisy words that are irrelevant to relations. Then we construct a neural network inputting the subtree while applying the entity-wise attention to identify the important semantic features of relational words in each instance. To make our model more robust against noisy words, we initialize our network with a priori knowledge learned from the relevant task of entity classification by transfer learning. We conduct extensive experiments using the corpora of New York Times (NYT) and Freebase. Experiments show that our approach is effective and improves the area of Precision\/Recall (PR) from 0.35 to 0.39 over the state-of-the-art work."}
{"_id":"5c7f700d28c9e5b5ac115c1409262ecfe89812db","title":"An Evaluation of Aggregation Techniques in Crowdsourcing","text":"As the volumes of AI problems involving human knowledge are likely to soar, crowdsourcing has become essential in a wide range of world-wide-web applications. One of the biggest challenges of crowdsourcing is aggregating the answers collected from the crowd since the workers might have wide-ranging levels of expertise. In order to tackle this challenge, many aggregation techniques have been proposed. These techniques, however, have never been compared and analyzed under the same setting, rendering a \u2018right\u2019 choice for a particular application very difficult. Addressing this problem, this paper presents a benchmark that offers a comprehensive empirical study on the performance comparison of the aggregation techniques. Specifically, we integrated several stateof-the-art methods in a comparable manner, and measured various performance metrics with our benchmark, including computation time, accuracy, robustness to spammers, and adaptivity to multi-labeling. We then provide in-depth analysis of benchmarking results, obtained by simulating the crowdsourcing process with different types of workers. We believe that the findings from the benchmark will be able to serve as a practical guideline for crowdsourcing applications."}
{"_id":"6ebd2b2822bcd32b58af033a26fd08a39ba777f7","title":"ONTOLOGY ALIGNMENT USING MACHINE LEARNING TECHNIQUES","text":"In semantic web, ontology plays an important role to provide formal definitions of concepts and relationships. Therefore, communicating similar ontologies becomes essential to provide ontologies interpretability and extendibility. Thus, it is inevitable to have similar but not the same ontologies in a particular domain, since there might be several definitions for a given concept. This paper presents a method to combine similarity measures of different categories without having ontology instances or any user feedbacks towards aligning two given ontologies. To align different ontologies efficiently, K Nearest Neighbor (KNN), Support Vector Machine (SVM), Decision Tree (DT) and AdaBoost classifiers are investigated. Each classifier is optimized based on the lower cost and better classification rate. Experimental results demonstrate that the F-measure criterion improves to 99% using feature selection and combination of AdaBoost and DT classifiers, which is highly comparable, and outperforms the previous reported F-measures. Computer Engineering Department, Shahid Chamran University, Ahvaz, Iran."}
{"_id":"d09b70d5029f60bef888a8e73f9a18e2fc2db2db","title":"Pictogrammar: an AAC device based on a semantic grammar","text":"As many as two-thirds of individuals with an Autism Spectrum Disorder (ASD) also have language impairments, which can range from mild limitations to complete non-verbal behavior. For such cases, there are several Augmentative and Alternative Communication (AAC) devices available. These are computer-designed tools in order to help people with ASD to palliate or overcome such limitations, at least partially. Some of the most popular AAC devices are based on pictograms, so that a pictogram is the graphical representation of a simple concept and sentences are composed by concatenating a number of such pictograms. Usually, these tools have to manage a vocabulary made up of hundreds of pictograms\/concepts, with no or very poor knowledge of the language at semantic and pragmatic level. In this paper we present Pictogrammar, an AAC system which takes advantage of SUpO and PictOntology. SUpO (Simple Upper Ontology) is a formal semantic ontology which is made up of detailed knowledge of facts of everyday life such as simple words, with special interest in linguistic issues, allowing automated grammatical supervision. PictOntology is an ontology developed to manage sets of pictograms, linked to SUpO. Both ontologies make possible the development of tools which are able to take advantage of a formal semantics."}
{"_id":"b0a1f562a55aae189d6a5cb826582b2e7fb06d3c","title":"Multi-modal Mean-Fields via Cardinality-Based Clamping","text":"Mean Field inference is central to statistical physics. It has attracted much interest in the Computer Vision community to efficiently solve problems expressible in terms of large Conditional Random Fields. However, since it models the posterior probability distribution as a product of marginal probabilities, it may fail to properly account for important dependencies between variables. We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields."}
{"_id":"aded779fe2ec65b9d723435182e093f9f18ad80f","title":"Design and Implementation of a Reliable Wireless Real-Time Home Automation System Based on Arduino Uno Single-Board Microcontroller","text":"\u2014This paper presents design and implementation concepts for a wireless real-time home automation system based on Arduino Uno microcontroller as central controllers. The proposed system has two operational modes. The first one is denoted as a manually\u2013automated mode in which the user can monitor and control the home appliances from anywhere over the world using the cellular phone through Wi-Fi communication technology. The second one is referred to a self-automated mode that makes the controllers to be capable of monitoring and controlling different appliances in the home automatically in response to the signals comes from the related sensors. To support the usefulness of the proposed technique, a hardware implementation with Matlab-GUI platform for the proposed system is carried out and the reliability of the system is introduced. The proposed system is shown to be a simple, cost effective and flexible that making it a suitable and a good candidate for the smart home future. I. INTRODUCTION ecently, man's work and life are increasingly tight with the rapid growth in communications and information technology. The informationized society has changed human being's way of life as well as challenged the traditional residence. Followed by the rapid economic expansion, living standard keeps raising up day by day that people have a higher requirement for dwelling functions. The intellectualized society brings diversified information where safe, economic, comfortable and convenient life has become the ideal for every modern family [1]."}
{"_id":"b6741f7862be64a5435d2625ea46f0508b9d3fee","title":"Security Keys: Practical Cryptographic Second Factors for the Modern Web","text":"\u201cSecurity Keys\u201d are second-factor devices that protect users against phishing and man-in-the-middle attacks. Users carry a single device and can self-register it with any online service that supports the protocol. The devices are simple to implement and deploy, simple to use, privacy preserving, and secure against strong attackers. We have shipped support for Security Keys in the Chrome web browser and in Google\u2019s online services. We show that Security Keys lead to both an increased level of security and user satisfaction by analyzing a two year deployment which began within Google and has extended to our consumer-facing web applications. The Security Key design has been standardized by the FIDO Alliance, an organization with more than 250 member companies spanning the industry. Currently, Security Keys have been deployed by Google, Dropbox, and GitHub. An updated and extended tech report is available at https:\/\/github.com\/google\/u2fref-code\/docs\/SecurityKeys_TechReport.pdf."}
{"_id":"f98d91492335e74621837c01c860cbc801a2acbb","title":"A web-based bayesian intelligent tutoring system for computer programming","text":"In this paper, we present a Web-based intelligent tutoring system, called BITS. The decision making process conducted in our intelligent system is guided by a Bayesian network approach to support students in learning computer programming. Our system takes full advantage of Bayesian networks, which are a formal framework for uncertainty management in Artificial Intelligence based on probability theory. We discuss how to employ Bayesian networks as an inference engine to guide the students\u2019 learning processes . In addition, we describe the architecture of BITS and the role of each module in the system. Whereas many tutoring systems are static HTML Web pages of a class textbook or lecture notes, our intelligent system can help a student nav igate through the online course materials, recommend learning goals, and generate appropriate reading sequences."}
{"_id":"0204dee746e1f55d87409c5f482d1e7c74c48baa","title":"Adjustable Autonomy : From Theory to Implementation","text":"Recent exciting, ambitious applications in agent technology involve agents acting individually or in teams in support of critical activities of individual humans or entire human organizations. Applications range from intelligent homes [13], to \"routine\" organizational coordination[16], to electronic commerce[4] to long-term space missions[12, 6]. These new applications have brought forth an increasing interest in agents\u2019 adjustable autonomy (AA), i.e., in agents\u2019 dynamically adjusting their own level of autonomy based on the situation[8]. In fact, many of these applications will not be deployed, unless reliable AA reasoning is a central component. At the heart of AA is the question of whether and when agents should make autonomous decisions and when they should transfer decision-making control to other entities (e.g., human users). Unfortunately, previous work in adjustable autonomy has focused on individual agent-human interactions and tile techniques developed fail to scale-up to complex heterogeneous organizations. Indeed, as a first step, we focused on a smallscale, but real-world agent-human organization called Electric Elves, where an individual agent and human worked together within a larger multiagent context. Although\u2019the application limits the interactions among entities, key weaknesses of previous approaches to adjustable autonomy are readily apparent. In particular, previous approaches to transferof-control are seen to be too rigid, employing one-shot transfersof-control that can result in unacceptable coordination failures. Furthermore, the previous approaches ignore potential costs (e.g., from delays) to an agent\u2019s team due to such transfers of control. To remedy such problems, we propose a novel approach to AA, based on the notion of a transfer-of-control strategy. A transfer-of-control strategy consists of a conditional sequence of two types of actions: (i) actions to transfer decision-making control (e.g., from the agent to the user or vice versa) and (ii) actions to change an agent\u2019s pre-specified coordination constraints with team members, aimed at minimizing miscoordination costs. The goal is for high quality individual decisions to be made with minimal disruption to the coordination of the team. We operationalize such strategies via Markov decision processes (MDPs) which select the optimal strategy given an uncertain environment and costs to individuals and teams. We have developed a general reward function and state representation for such an MDP, to facilitate application of the approach to different domains. We present results from a careful evaluation of this approach, including via its use in our real-world, deployed Electric Elves system."}
{"_id":"697754f7e62236f6a2a069134cbc62e3138ac89f","title":"The Granular Tabu Search and Its Application to the Vehicle-Routing Problem","text":""}
{"_id":"8d3c794bf910f9630e9516f3ebb1fa4995a5c0c1","title":"An Item-based Collaborative Filtering Recommendation Algorithm Using Slope One Scheme Smoothing","text":"Collaborative filtering is one of the most important technologies in electronic commerce. With the development of recommender systems, the magnitudes of users and items grow rapidly, resulted in the extreme sparsity of user rating data set. Traditional similarity measure methods work poor in this situation, make the quality of recommendation system decreased dramatically. Poor quality is one major challenge in collaborative filtering recommender systems. Sparsity of users\u2019 ratings is the major reason causing the poor quality. To address this issue, an item-based collaborative filtering recommendation algorithm using slope one scheme smoothing is presented. This approach predicts item ratings that users have not rated by the employ of slope one scheme, and then uses Pearson correlation similarity measurement to find the target items\u2019 neighbors, lastly produces the recommendations. The experiments are made on a common data set using different recommender algorithms. The results show that the proposed approach can improve the accuracy of the collaborative filtering recommender system."}
{"_id":"2f56548a5a8d849e17e4186393c698e5a735aa91","title":"Design methodology using inversion coefficient for low-voltage low-power CMOS voltage reference","text":"This paper presents an analog design methodology, using the selection of inversion coefficient of MOS devices, to design low voltage and low-power (LVLP) CMOS voltage references. These circuits often work under subthreshold operation. Hence, there is a demand for analog design methods that optimize the sizing process of transistors working in weak and moderate inversion. The advantage of the presented method -- compared with the traditional approach to design circuits -- is the reduction of design cycle time and minimization of trial-and-error simulations, if the proposed equations are used. As a case study, a LVLP voltage reference based on subthreshold MOSFETs with supply voltage of 0.7 V was designed for 0.18-\u00bcm CMOS technology."}
{"_id":"42dcf7c039bf8bb36b8fa3a658bd66e834128c1b","title":"Exploring commonalities across participants in the neural representation of objects.","text":"The question of whether the neural encodings of objects are similar across different people is one of the key questions in cognitive neuroscience. This article examines the commonalities in the internal representation of objects, as measured with fMRI, across individuals in two complementary ways. First, we examine the commonalities in the internal representation of objects across people at the level of interobject distances, derived from whole brain fMRI data, and second, at the level of spatially localized anatomical brain regions that contain sufficient information for identification of object categories, without making the assumption that their voxel patterns are spatially matched in a common space. We examine the commonalities in internal representation of objects on 3T fMRI data collected while participants viewed line drawings depicting various tools and dwellings. This exploratory study revealed the extent to which the representation of individual concepts, and their mutual similarity, is shared across participants."}
{"_id":"ee654db227dcb7b39d26bec7cc06e2b43b525826","title":"Hierarchy of fibrillar organization levels in the polytene interphase chromosomes of Chironomus.","text":""}
{"_id":"579b2962ac567a39742601cafe3fc43cf7a7109c","title":"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks","text":"We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video. Our hierarchical framework contains a sentence generator and a paragraph generator. The sentence generator produces one simple short sentence that describes a specific short video interval. It exploits both temporal-and spatial-attention mechanisms to selectively focus on visual elements during generation. The paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator. We evaluate our approach on two large-scale benchmark datasets: YouTubeClips and TACoS-MultiLevel. The experiments demonstrate that our approach significantly outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and 0.305 respectively."}
{"_id":"8772e0057d0adc7b1e8d9f09473fdc1de05a8148","title":"Autonomous Self-Assembly in Swarm-Bots","text":"In this paper, we discuss the self-assembling capabilities of the swarm-bot, a distributed robotics concept that lies at the intersection between collective and self-reconfigurable robotics. A swarm-bot is comprised of autonomous mobile robots called s-bots. S-bots can either act independently or self-assemble into a swarm-bot by using their grippers. We report on experiments in which we study the process that leads a group of s-bots to self-assemble. In particular, we present results of experiments in which we vary the number of s-bots (up to 16 physical robots), their starting configurations, and the properties of the terrain on which self-assembly takes place. In view of the very successful experimental results, swarm-bot qualifies as the current state of the art in autonomous self-assembly"}
{"_id":"926e97d5ce2a6e070f8ec07c5aa7f91d3df90ba0","title":"Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural Networks","text":"Deep Neural Networks (DNNs) have shown to outperform traditional methods in various visual recognition tasks including Facial Expression Recognition (FER). In spite of efforts made to improve the accuracy of FER systems using DNN, existing methods still are not generalizable enough in practical applications. This paper proposes a 3D Convolutional Neural Network method for FER in videos. This new network architecture consists of 3D Inception-ResNet layers followed by an LSTM unit that together extracts the spatial relations within facial images as well as the temporal relations between different frames in the video. Facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions. Our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods."}
{"_id":"41c419e663aa9b301a951bab245a7a6543a3e5f6","title":"A Java API and Web Service Gateway for wireless M-Bus","text":"M-Bus Systems are used to support the remote data exchange with meter units through a network. They provide an easy extend able and cost effective method to connect many meter devices to one coherent network. Master applications operate as concentrated data collectors and are used to process measured values further. As modern approaches facilitate the possibility of a Smart Grid, M-Bus can be seen as the foundation of this technology. With the current focus on a more effective power grid, Smart Meters and Smart Grids are an important research topic. This bachelor thesis first gives an overview of the M-Bus standard and then presents a Java library and API to access M-Bus devices remotely in a standardized way through Web Services. Integration into common IT applications requires interoperable interfaces which also facilitate automated machine-to-machine communication. The oBIX (Open Building Information Exchange) standard provides such standardized objects and thus is used to create a Web Service gateway for the Java API."}
{"_id":"97398e974e6965161ad9b9f85e00fd7a8e7b3d83","title":"AWEsome: An open-source test platform for airborne wind energy systems","text":"In this paper we present AWEsome (Airborne Wind Energy Standardized Open-source Model Environment), a test platform for airborne wind energy systems that consists of low-cost hardware and is entirely based on open-source software. It can hence be used without the need of large financial investments, in particular by research groups and startups to acquire first experiences in their flight operations, to test novel control strategies or technical designs, or for usage in public relations. Our system consists of a modified off-the-shelf model aircraft that is controlled by the pixhawk autopilot hardware and the ardupilot software for fixed wing aircraft. The aircraft is attached to the ground by a tether. We have implemented new flight modes for the autonomous tethered flight of the aircraft along periodic patterns. We present the principal functionality of our algorithms. We report on first successful tests of these modes in real flights. \u2217Author names are in alphabetical order. a Universit\u00e4t Bonn, bechtle@physik.uni-bonn.de b Universit\u00e4t Bonn, thomasgehrmann@gmx.net c Humboldt-Universit\u00e4t zu Berlin, csieg@physik.hu-berlin.de d Daidalos Capital, zillmann@daidalos-capital.com awesome.physik.uni-bonn.de 1 ar X iv :1 70 4. 08 69 5v 1 [ cs .S Y ] 2 7 A pr 2 01 7 Section"}
{"_id":"e93d2e8066087b9756a585b28244e7335229f1e4","title":"Optimization of Belt-Type Electrostatic Separation of Triboaerodynamically Charged Granular Plastic Mixtures","text":"Electrostatic separation is frequently employed for the selective sorting of conducting and insulating materials for waste electric and electronic equipment. In a series of recent papers, the authors have presented several novel triboelectrostatic separation installations for the recycling of the main categories of plastics contained in information technology wastes. The aim of the present work is to optimize the design and the operation of such an installation, composed of a belt-type electrostatic separator associated to a propeller-type tribocharging device. The experimental design method is employed for modeling and optimizing the separation of a mixture of high-impact polystyrene and acrylonitrile butadiene styrene granules originating from shredded out-of-use computer cases. A distinct experiment is carried out on three synthetic mixtures of virgin plastic granules: 75% polyamide (PA)+ 25% polycarbonate (PC), 50% PA+ 50% PC, and 25% PA + 75% PC. The best results are obtained for the mixtures containing equal quantities of the two constituents."}
{"_id":"aeddb6d9568171cc9ca0297b2adfd6c8e23fe797","title":"Transformerless micro-inverter for grid-connected photovoltaic systems","text":"The leakage currents caused by high-frequency common-mode (CM) voltage have become a major concern in transformerless photovoltaic (PV) inverters. This paper addresses to a review on dc-ac converters applied to PV systems that can avoid the circulation of leakage currents. Looking for a lower cost and higher reliability solution, a 250 W PV transformerless micro-inverter prototype based on the bipolar full-bridge topology was built and tested. As it is confirmed by experimental results, this topology is able to maintain the CM voltage constant and prevent the circulation of CM currents through the circuit."}
{"_id":"08f991c35f2b3d67cfe219f19e4e791b049f398d","title":"GQ ( \u03bb ) : A general gradient algorithm for temporal-difference prediction learning with eligibility traces","text":"A new family of gradient temporal-difference learning algorithms have recently been introduced by Sutton, Maei and others in which function approximation is much more straightforward. In this paper, we introduce the GQ(\u03bb) algorithm which can be seen as extension of that work to a more general setting including eligibility traces and off-policy learning of temporally abstract predictions. These extensions bring us closer to the ultimate goal of this work\u2014the development of a universal prediction learning algorithm suitable for learning experientially grounded knowledge of the world. Eligibility traces are essential to this goal because they bridge the temporal gaps in cause and effect when experience is processed at a temporally fine resolution. Temporally abstract predictions are also essential as the means for representing abstract, higher-level knowledge about courses of action, or options. GQ(\u03bb) can be thought of as an extension of Q-learning. We extend existing convergence results for policy evaluation to this setting and carry out a forward-view\/backwardview analysis to derive and prove the validity of the new algorithm. Introduction One of the main challenges in artificial intelligence (AI) is to connect the low-level experience to high-level representations (grounded world knowledge). Low-level experience refers to rich signals received back and forth between the agent and the world. Recent theoretical developments in temporal-difference learning combined with mathematical ideas developed for temporally abstract options, known as intra-option learning, can be used to address this challenge (Sutton, 2009). Intra-option learning (Sutton, Precup, and Singh, 1998) is seen as a potential method for temporalabstraction in reinforcement learning. Intra-option learning is a type of off-policy learning. Off-policy learning refers to learning about a target policy while following another policy, known as behavior policy. Offpolicy learning arises in Q-learning where the target policy is a greedy optimal policy while the behavior policy is exploratory. It is also needed for intra-option learning. Intra-option methods look inside options and allow AI agent to learn about multiple different options simultaneously from a single stream of received data. Option refers to a temporally course of actions with a termination condition. Options are ubiquitous in our everyday life. For example, to go for hiking, we need to consider and evaluate multiple options such as transportation options to the hiking trail. Each option includes a course of primitive actions and only is excited in particular states. The main feature of intra-option learning is its ability to predict the consequences of each option policy without executing it while data is received from a different policy. Temporal difference (TD) methods in reinforcement learning are considered as powerful techniques for prediction problems. In this paper, we consider predictions always in the form of answers to the questions. Questions are like \u201cIf of follow this trail, would I see a creek?\u201d The answers to such questions are in the form of a single scalar (value function) that tells us about the expected future consequences given the current state. In general, due to the large number of states, it is not feasible to compute the exact value of each state entry. One of the key features of TD methods is their ability to generalize predictions to states that may not have visited; this is known as function approximation. Recently, Sutton et al. (2009b) and Maei et al. (2009) introduced a new family of gradient TD methods in which function approximation is much more straightforward than conventional methods. Prior to their work, the existing classical TD algorithms (e.g.; TD(\u03bb) and Q-learning) with function approximation could become unstable and diverge (Baird, 1995; Tsitsiklis and Van Roy, 1997). In this paper, we extend their work to a more general setting that includes off-policy learning of temporally abstract predictions and eligibility traces. Temporally abstract predictions are essential for representing higher-level knowledge about the course of actions, or options (Sutton et al., 1998). Eligibility traces bridge between the temporal gaps when experience is processes at a temporally fine resolution. In this paper, we introduce the GQ(\u03bb) algorithm that can be thought of as an extension to Q-learning (Watkins and Dayan, 1989); one of the most popular off-policy learning algorithms in reinforcement learning. Our algorithm incorporates gradient-descent ideas originally developed by Sutton et al. (2009a,b), for option conditional predictions with varying eligibility traces. We extend existing convergence results for policy evaluation to this setting and carry forward-view\/backwardview analysis and prove the validity of the new algorithm. The organization of the paper is as follows: First, we describe the problem setting and define our notations. Then we introduce the GQ(\u03bb) algorithm and describe how to use it. In the next sections we provide derivation of the algorithm and carry out analytical analysis on the equivalence of TD forward-view\/backward-view. We finish the paper with convergence proof and conclusion section. Notation and background We consider the problem of policy evaluation in finite state-action Markov Decision Process (MDP). Under standard conditions, however, our results can be extended to MDPs with infinite state\u2013action pairs. We use a standard reinforcement learning (RL) framework. In this setting, data is obtained from a continually evolving MDP with states st \u2208 S, actions at \u2208 A, and rewards rt \u2208 <, for t = 1, 2, . . ., with each state and reward as a function of the preceding state and action. Actions are chosen according to the behavior policy b, which is assumed fixed and exciting, b(s, a) > 0,\u2200s, a. We consider the transition probabilities between state\u2013 action pairs, and for simplicity we assume there is a finite number N of state\u2013action pairs. Suppose the agent find itself at time t in a state\u2013 action pair st, at. The agent likes its answer at that time to tell something about the future sequence st+1, at+1, . . . , st+k if actions from t + 1 on were taken according to the option until it terminated at time t+k. The option policy is denoted \u03c0 : S \u00d7 A \u2192 [0, 1] and whose termination condition is denoted \u03b2 : S \u2192 [0, 1]. The answer is always in the form of a single number, and of course we have to be more specific about what we are trying to predict. There are two common cases: 1) we are trying to predict the outcome of the option; we want to know about the expected value of some function of the state at the time the option terminates. We call this function the outcome target function, and denote it z : S \u2192 <, 2) we are trying to predict the transient; that is, what happens during the option rather than its end. The most common thing to predict about the transient is the total or discounted reward during the option. We denote the reward function r : S \u00d7A \u2192 <. Finally, the answer could conceivably be a mixture of both a transient and an outcome. Here we will present the algorithm for answering questions with both an outcome part z and a transient part r, with the two added together. In the common place where one wants only one of the two, the other is set to zero. Now we can start to state the goal of learning more precisely. In particular, we would like our answer to be equal to the expected value of the outcome target function at termination plus the cumulative sum of the transient reward function along the way: Q(st, at) (1) \u2261 E [ rt+1 + \u03b3rt+2 + \u00b7 \u00b7 \u00b7+ \u03b3rt+k + zt+k | \u03c0, \u03b2 ] , where \u03b3 \u2208 (0, 1] is discount factor and Q(s, a) denotes action value function that evaluates policy \u03c0 given state-action pair s, a. To simplify the notation, from now on, we drop the superscript \u03c0 on action values. In many problems the number of state-action pairs is large and therefore it is not feasible to compute the action values for each state-action entry. Therefore, we need to approximate the action values through generalization techniques. Here, we use linear function approximation; that is, the answer to a question is always formed linearly as Q\u03b8(s, a) = \u03b8>\u03c6(s, a) \u2248 Q(s, a) for all s \u2208 S and a \u2208 A, where \u03b8 \u2208 < is a learned weight vector and \u03c6(s, a) \u2208 < indicates a state\u2013action feature vector. The goal is to learn parameter vector \u03b8 through a learning method such as TD learning. The above (1) describes the target in a Monte Carlo sense, but of course we want to include the possibility of temporal-difference learning; one of the widely used techniques in reinforcement learning. To do this, we provide an eligibility-trace function \u03bb : S \u2192 [0, 1] as described in Sutton and Barto (1998). We let eligibilitytrace function, \u03bb, to vary over different states. In the next section, first we introduce GQ(\u03bb); a general temporal-difference learning algorithm that is stable under off-policy training, and show how to use it. Then in later sections we provide the derivation of the algorithm and convergence proof. The GQ(\u03bb) algorithm In this section we introduce the GQ(\u03bb) algorithm for off-policy learning about the outcomes and transients of options, in other words, intra-option GQ(\u03bb) for learning the answer to a question chosen from a wide (possibly universal) class of option-conditional predictive questions. To specify the question one provides four functions: \u03c0 and \u03b2, for the option, and z and r, for the target functions. To specify how the answers will be formed one provides their functional form (here in linear form), the feature vectors \u03c6(s, a) for all state\u2013action pairs, and the eligibility-trace function \u03bb. The discount factor \u03b3 can be taken to be 1, and thus ignored; the same effect as discounting can be achieved through the choice of \u03b2. Now, we specify the GQ(\u03bb) algorithm as follows: The weight vector \u03b8 \u2208 < is initialized arbitrarily. The secondary weight vector w \u2208 < is init"}
{"_id":"1721801db3a467adf7a10c69f21c21896a80c6dd","title":"Efficient Program Analyses Using Deductive and Semantic Methodologies","text":"Program analysis is the process of gathering deeper insights about a source code and analysing them to resolve software problems of arbitrary complexity. The key challenge in program analysis is to keep it fast, precise and straightforward. This research focuses on three key objectives to achieve an efficient program analysis: (i) expressive data representation, (ii) optimised data structure and (iii) fast data processing mechanisms. State of the art technologies such as Resource Description Framework (RDF) as data representation format, triplestores as the storage &amp; processing layer, and datalog to represent program analysis rules are considered in our research. diagram(BDD) to be embedded in the triplestore. Additionally, an ontology is being designed to standardise the definitions of concepts and representation of the knowledge in the program analysis domain."}
{"_id":"2902e0a4b12cf8269bb32ef6a4ebb3f054cd087e","title":"A Bridging Framework for Model Optimization and Deep Propagation","text":"Optimizing task-related mathematical model is one of the most fundamental methodologies in statistic and learning areas. However, generally designed schematic iterations may hard to investigate complex data distributions in real-world applications. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular tasks. Unfortunately, existing networks are often built in heuristic manners, thus lack of principled interpretations and solid theoretical supports. In this work, we provide a new paradigm, named Propagation and Optimization based Deep Model (PODM), to bridge the gaps between these different mechanisms (i.e., model optimization and deep propagation). On the one hand, we utilize PODM as a deeply trained solver for model optimization. Different from these existing network based iterations, which often lack theoretical investigations, we provide strict convergence analysis for PODM in the challenging nonconvex and nonsmooth scenarios. On the other hand, by relaxing the model constraints and performing end-to-end training, we also develop a PODM based strategy to integrate domain knowledge (formulated as models) and real data distributions (learned by networks), resulting in a generic ensemble framework for challenging real-world applications. Extensive experiments verify our theoretical results and demonstrate the superiority of PODM against these state-of-the-art approaches."}
{"_id":"2327ad6f237b37150e84f0d745a05565ebf0b24d","title":"Zerocash: Decentralized Anonymous Payments from Bitcoin","text":"Bit coin is the first digital currency to see widespread adoption. While payments are conducted between pseudonyms, Bit coin cannot offer strong privacy guarantees: payment transactions are recorded in a public decentralized ledger, from which much information can be deduced. Zero coin (Miers et al., IEEE S&P 2013) tackles some of these privacy issues by unlinking transactions from the payment's origin. Yet, it still reveals payments' destinations and amounts, and is limited in functionality. In this paper, we construct a full-fledged ledger-based digital currency with strong privacy guarantees. Our results leverage recent advances in zero-knowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARKs). First, we formulate and construct decentralized anonymous payment schemes (DAP schemes). A DAP scheme enables users to directly pay each other privately: the corresponding transaction hides the payment's origin, destination, and transferred amount. We provide formal definitions and proofs of the construction's security. Second, we build Zero cash, a practical instantiation of our DAP scheme construction. In Zero cash, transactions are less than 1 kB and take under 6 ms to verify - orders of magnitude more efficient than the less-anonymous Zero coin and competitive with plain Bit coin."}
{"_id":"3d08280ae82c2044c8dcc66d2be5a72c738e9cf9","title":"Metadata Embeddings for User and Item Cold-start Recommendations","text":"I present a hybrid matrix factorisation model representing users and items as linear combinations of their content features\u2019 latent factors. The model outperforms both collaborative and content-based models in cold-start or sparse interaction data scenarios (using both user and item metadata), and performs at least as well as a pure collaborative matrix factorisation model where interaction data is abundant. Additionally, feature embeddings produced by the model encode semantic information in a way reminiscent of word embedding approaches, making them useful for a range of related tasks such as tag recommendations."}
{"_id":"6659c47db31129a60df2b113b46f45614197d5d8","title":"Extremely Low-Profile, Single-Arm, Wideband Spiral Antenna Radiating a Circularly Polarized Wave","text":"The antenna characteristics are described of a single-arm spiral (SAS) antenna. A balun circuit, required for a conventional two-arm balanced-mode spiral (TAS), is not necessary for feeding this SAS. First, the radiation pattern of an SAS having a disc\/ground-plane is investigated. When the radius of the disc is smaller than the radius of the first-mode active region on the spiral and the spacing between the disc and spiral plane is small, the SAS is found to radiate a circularly polarized (CP) bidirectional beam, whose radiation pattern is almost symmetric with respect to the antenna axis normal to the spiral plane. Second, from a practical standpoint, the CP bidirectional beam is transformed into a CP unidirectional beam by placing a conducting cavity behind an SAS with a disc. It is revealed that the SAS has a good VSWR (less than 2) and a good axial ratio (less than 3 dB) over a design frequency range of 3 GHz to 10 GHz, where a cavity\/antenna height of 7 mm (0.07 wavelength at the lowest design frequency) is chosen. The frequency response of the gain for the SAS is found to be similar to that for a conventional TAS, and the radiation efficiency for the SAS is slightly larger than that for the TAS. It is concluded that the SAS with a small disc backed by a cavity realizes a circularly polarized, low-profile, wideband antenna with a simple feed system that does not require a balun circuit."}
{"_id":"3692d1c5e36145a5783135f3f077f6486e263d23","title":"Vocabulary acquisition from extensive reading : A case study","text":"A number of studies have shown that second language learners acquire vocabulary through reading, but only relatively small amounts. However, most of these studies used only short texts, measured only the acquisition of meaning, and did not credit partial learning of words. This case study of a learner of French explores whether an extensive reading program can enhance lexical knowledge. The study assessed a relatively large number of words (133), and examined whether one month of extensive reading enhanced knowledge of these target words' spelling, meaning, and grammatical characteristics. The measurement procedure was a one-on-one interview that allowed a very good indication of whether learning occurred. The study also explores how vocabulary acquisition varies according to how often words are encountered in the texts. The results showed that knowledge of 65% of the target words was enhanced in some way, for a pickup rate of about 1 of every 1.5 words tested. Spelling was strongly enhanced, even from a small number of exposures. Meaning and grammatical knowledge were also enhanced, but not to the same extent. Overall, the study indicates that more vocabulary acquisition is possible from extensive reading than previous studies have suggested."}
{"_id":"46977c2e7a812e37f32eb05ba6ad16e03ee52906","title":"Gated End-to-End Memory Networks","text":"Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks (MemN2N) have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact questionanswering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-toEnd trainable Memory Network architecture (GMemN2N). From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the Dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art. \u2217work done as an Intern at XRCE"}
{"_id":"a3348ee4cc93e68485df4d06e205e064f12c0239","title":"Feature Selection for Maximizing the Area Under the ROC Curve","text":"Feature selection is an important pre-processing step for solving classification problems. A good feature selection method may not only improve the performance of the final classifier, but also reduce the computational complexity of it. Traditionally, feature selection methods were developed to maximize the classification accuracy of a classifier. Recently, both theoretical and experimental studies revealed that a classifier with the highest accuracy might not be ideal in real-world problems. Instead, the Area Under the ROC Curve (AUC) has been suggested as the alternative metric, and many existing learning algorithms have been modified in order to seek the classifier with maximum AUC. However, little work was done to develop new feature selection methods to suit the requirement of AUC maximization. To fill this gap in the literature, we propose in this paper a novel algorithm, called AUC and Rank Correlation coefficient Optimization (ARCO) algorithm. ARCO adopts the general framework of a well-known method, namely minimal redundancy- maximal-relevance (mRMR) criterion, but defines the terms \u201drelevance\u201d and \u201dredundancy\u201d in totally different ways. Such a modification looks trivial from the perspective of algorithmic design. Nevertheless, experimental study on four gene expression data sets showed that feature subsets obtained by ARCO resulted in classifiers with significantly larger AUC than the feature subsets obtained by mRMR. Moreover, ARCO also outperformed the Feature Assessment by Sliding Thresholds algorithm, which was recently proposed for AUC maximization, and thus the efficacy of ARCO was validated."}
{"_id":"49f5671cdf3520e04104891265c74b34c22ebccc","title":"Overconfidence and Trading Volume","text":"Theoretical models predict that overconfident investors will trade more than rational investors. We directly test this hypothesis by correlating individual overconfidence scores with several measures of trading volume of individual investors. Approximately 3,000 online broker investors were asked to answer an internet questionnaire which was designed to measure various facets of overconfidence (miscalibration, volatility estimates, better than average effect). The measures of trading volume were calculated by the trades of 215 individual investors who answered the questionnaire. We find that investors who think that they are above average in terms of investment skills or past performance (but who did not have above average performance in the past) trade more. Measures of miscalibration are, contrary to theory, unrelated to measures of trading volume. This result is striking as theoretical models that incorporate overconfident investors mainly motivate this assumption by the calibration literature and model overconfidence as underestimation of the variance of signals. In connection with other recent findings, we conclude that the usual way of motivating and modeling overconfidence which is mainly based on the calibration literature has to be treated with caution. Moreover, our way of empirically evaluating behavioral finance models the correlation of economic and psychological variables and the combination of psychometric measures of judgment biases (such as overconfidence scores) and field data seems to be a promising way to better understand which psychological phenomena actually drive economic behavior."}
{"_id":"5703617b9d9d40e90b6c8ffa21a52734d9822d60","title":"Defining Computational Thinking for Mathematics and Science Classrooms","text":"Science and mathematics are becoming computational endeavors. This fact is reflected in the recently released Next Generation Science Standards and the decision to include \u2018\u2018computational thinking\u2019\u2019 as a core scientific practice. With this addition, and the increased presence of computation in mathematics and scientific contexts, a new urgency has come to the challenge of defining computational thinking and providing a theoretical grounding for what form it should take in school science and mathematics classrooms. This paper presents a response to this challenge by proposing a definition of computational thinking for mathematics and science in the form of a taxonomy consisting of four main categories: data practices, modeling and simulation practices, computational problem solving practices, and systems thinking practices. In formulating this taxonomy, we draw on the existing computational thinking literature, interviews with mathematicians and scientists, and exemplary computational thinking instructional materials. This work was undertaken as part of a larger effort to infuse computational thinking into high school science and mathematics curricular materials. In this paper, we argue for the approach of embedding computational thinking in mathematics and science contexts, present the taxonomy, and discuss how we envision the taxonomy being used to bring current educational efforts in line with the increasingly computational nature of modern science and mathematics."}
{"_id":"5fee30aa381e5382ac844c48cdc7bf65f316b3cb","title":"Flexible Learning on the Sphere Via Adaptive Needlet Shrinkage and Selection","text":"This paper introduces an approach for flexible, robust Bayesian modeling of structure in spherical data sets. Our method is based upon a recent construction called the needlet, which is a particular form of spherical wavelet with many favorable statistical and computational properties. We perform shrinkage and selection of needlet coefficients, focusing on two main alternatives: empirical-Bayes thresholding for selection, and the horseshoe prior for shrinkage. We study the performance of the proposed methodology both on simulated data and on a real data set involving the cosmic microwave background radiation. Horseshoe shrinkage of needlet coefficients is shown to yield the best overall performance against some common benchmarks."}
{"_id":"129298d1dd8a9ec398d7f14a706e5d06d3aead01","title":"A privacy-compliant fingerprint recognition system based on homomorphic encryption and Fingercode templates","text":"The privacy protection of the biometric data is an important research topic, especially in the case of distributed biometric systems. In this scenario, it is very important to guarantee that biometric data cannot be steeled by anyone, and that the biometric clients are unable to gather any information different from the single user verification\/identification. In a biom\u00e9trie system with high level of privacy compliance, also the server that processes the biom\u00e9trie matching should not learn anything on the database and it should be impossible for the server to exploit the resulting matching values in order to extract any knowledge about the user presence or behavior. Within this conceptual framework, in this paper we propose a novel complete demonstrator based on a distributed biom\u00e9trie system that is capable to protect the privacy of the individuals by exploiting cryptosystems. The implemented system computes the matching task in the encrypted domain by exploiting homomorphic encryption and using Fingercode templates. The paper describes the design methodology of the demonstrator and the obtained results. The demonstrator has been fully implemented and tested in real applicative conditions. Experimental results show that this method is feasible in the cases where the privacy of the data is more important than the accuracy of the system and the obtained computational time is satisfactory."}
{"_id":"69852901bb1d411717d59f7647b1c332008393c1","title":"Extended Kalman filter based grid synchronization in the presence of voltage unbalance for smart grid","text":"In this paper, grid synchronization for gird-connected power generation systems in the presence of voltage unbalance and frequency variation is considered. A new extended Kalman filter based synchronization algorithm is proposed to track the phase angle of the utility network. Instead of processing the three-phase voltage signal in the abc natural reference frame and resorting to the symmetrical component transformation as in the traditional way, the proposed algorithm separates the positive and negative sequences in the transformed \u03b1\u03b2 stationary reference frame. Based on the obtained expressions in \u03b1\u03b2 domain, an EKF is developed to track both the in-phase and quadrature sinusoidal signals together with the unknown frequency. An estimate of the phase angle of the positive sequence is then obtained. As a by-product, estimates of phase angle of negative sequence and grid frequency are also computed. Compared to the commonly used scheme, the proposed algorithm has a simplified structure. The good performance is supported by computer simulations."}
{"_id":"54efb068debeea58fd05951f86db797b5b5e4788","title":"Frequency split metal artifact reduction (FSMAR) in computed tomography.","text":"PURPOSE\nThe problem of metal artifact reduction (MAR) is almost as old as the clinical use of computed tomography itself. When metal implants are present in the field of measurement, severe artifacts degrade the image quality and the diagnostic value of CT images. Up to now, no generally accepted solution to this issue has been found. In this work, a method based on a new MAR concept is presented: frequency split metal artifact reduction (FSMAR). It ensures efficient reduction of metal artifacts at high image quality with enhanced preservation of details close to metal implants.\n\n\nMETHODS\nFSMAR combines a raw data inpainting-based MAR method with an image-based frequency split approach. Many typical methods for metal artifact reduction are inpainting-based MAR methods and simply replace unreliable parts of the projection data, for example, by linear interpolation. Frequency split approaches were used in CT, for example, by combining two reconstruction methods in order to reduce cone-beam artifacts. FSMAR combines the high frequencies of an uncorrected image, where all available data were used for the reconstruction with the more reliable low frequencies of an image which was corrected with an inpainting-based MAR method. The algorithm is tested in combination with normalized metal artifact reduction (NMAR) and with a standard inpainting-based MAR approach. NMAR is a more sophisticated inpainting-based MAR method, which introduces less new artifacts which may result from interpolation errors. A quantitative evaluation was performed using the examples of a simulation of the XCAT phantom and a scan of a spine phantom. Further evaluation includes patients with different types of metal implants: hip prostheses, dental fillings, neurocoil, and spine fixation, which were scanned with a modern clinical dual source CT scanner.\n\n\nRESULTS\nFSMAR ensures sharp edges and a preservation of anatomical details which is in many cases better than after applying an inpainting-based MAR method only. In contrast to other MAR methods, FSMAR yields images without the usual blurring close to implants.\n\n\nCONCLUSIONS\nFSMAR should be used together with NMAR, a combination which ensures an accurate correction of both high and low frequencies. The algorithm is computationally inexpensive compared to iterative methods and methods with complex inpainting schemes. No parameters were chosen manually; it is ready for an application in clinical routine."}
{"_id":"7a3f1ea18bf3e8223890b122bc31fb79db758c6e","title":"Tagging Urdu Sentences from English POS Taggers","text":"Being a global language, English has attracted a majority of researchers and academia to work on several Natural Language Processing (NLP) applications. The rest of the languages are not focused as much as English. Part-of-speech (POS) Tagging is a necessary component for several NLP applications. An accurate POS Tagger for a particular language is not easy to construct due to the diversity of that language. The global language English, POS Taggers are more focused and widely used by the researchers and academia for NLP processing. In this paper, an idea of reusing English POS Taggers for tagging non-English sentences is proposed. On exemplary basis, Urdu sentences are processed to tagged from 11 famous English POS Taggers. State-of-the-art English POS Taggers were explored from the literature, however, 11 famous POS Taggers were being input to Urdu sentences for tagging. A famous Google translator is used to translate the sentences across the languages. Data from twitter.com is extracted for evaluation perspective. Confusion matrix with kappa statistic is used to measure the accuracy of actual Vs predicted tagging. The two best English POS Taggers which tagged Urdu sentences were Stanford POS Tagger and MBSP POS Tagger with an accuracy of 96.4% and 95.7%, respectively. The system can be generalized for multilingual sentence tagging. Keywords\u2014Standford part-of-speech (POS) tagger; Google translator; Urdu POS tagging; kappa statistic"}
{"_id":"21677e1dda607f649cd63b2311795ce6e2653b33","title":"IR sensitivity enhancement of CMOS Image Sensor with diffractive light trapping pixels","text":"We report on the IR sensitivity enhancement of back-illuminated CMOS Image Sensor (BI-CIS) with 2-dimensional diffractive inverted pyramid array structure (IPA) on crystalline silicon (c-Si) and deep trench isolation (DTI). FDTD simulations of semi-infinite thick c-Si having 2D IPAs on its surface whose pitches over 400\u2009nm shows more than 30% improvement of light absorption at \u03bb\u2009=\u2009850\u2009nm and the maximum enhancement of 43% with the 540\u2009nm pitch at the wavelength is confirmed. A prototype BI-CIS sample with pixel size of 1.2\u2009\u03bcm square containing 400\u2009nm pitch IPAs shows 80% sensitivity enhancement at \u03bb\u2009=\u2009850\u2009nm compared to the reference sample with flat surface. This is due to diffraction with the IPA and total reflection at the pixel boundary. The NIR images taken by the demo camera equip with a C-mount lens show 75% sensitivity enhancement in the \u03bb\u2009=\u2009700\u20131200\u2009nm wavelength range with negligible spatial resolution degradation. Light trapping CIS pixel technology promises to improve NIR sensitivity and appears to be applicable to many different image sensor applications including security camera, personal authentication, and range finding Time-of-Flight camera with IR illuminations."}
{"_id":"45a1a5dd0b9186d6476b56ab10bc582f003da2c5","title":"The Gambler's Ruin Problem, Genetic Algorithms, and the Sizing of Populations","text":"This paper presents a model to predict the convergence quality of genetic algorithms based on the size of the population. The model is based on an analogy between selection in GAs and one-dimensional random walks. Using the solution to a classic random walk problemthe gambler's ruinthe model naturally incorporates previous knowledge about the initial supply of building blocks (BBs) and correct selection of the best BB over its competitors. The result is an equation that relates the size of the population with the desired quality of the solution, as well as the problem size and difficulty. The accuracy of the model is verified with experiments using additively decomposable functions of varying difficulty. The paper demonstrates how to adjust the model to account for noise present in the fitness evaluation and for different tournament sizes."}
{"_id":"79ce9533944cdee059232495fc9f94f1d47eb900","title":"Deep Learning Approaches to Semantic Relevance Modeling for Chinese Question-Answer Pairs","text":"The human-generated question-answer pairs in the Web social communities are of great value for the research of automatic question-answering technique. Due to the large amount of noise information involved in such corpora, it is still a problem to detect the answers even though the questions are exactly located. Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. Since both the questions and their answers usually contain a small number of sentences, the relevance modeling methods have to overcome the problem of word feature sparsity. In this article, the deep learning principle is introduced to address the semantic relevance modeling task. Two deep belief networks with different architectures are proposed by us to model the semantic relevance for the question-answer pairs. According to the investigation of the textual similarity between the community-driven question-answering (cQA) dataset and the forum dataset, a learning strategy is adopted to promote our models\u2019 performance on the social community corpora without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora."}
{"_id":"bf4aa4acdfa83586bbdbd6351b1a96dbb9672c4c","title":"JINS MEME algorithm for estimation and tracking of concentration of users","text":"Activity tracking using a wearable device is an emerging research field. Large-scale studies on activity tracking performed with eyewear-type wearable devices remains a challenging area owing to the negative effect such devices have on users' looks. To cope with this challenge, JINS Inc., an eyewear retailer in Japan, has developed a state-of-the-art smart eyewear called JINS MEME. The design of JINS MEME is almost the same as that of Wellington-type eyeglasses so that people can use it in their daily lives. JINS MEME is equipped with sensors to detect a user's eye movement and head motion. In addition to these functions of JINS MEME, JINS developed an application to measure concentration levels of users. In this demonstration, users will experience wearing the JINS MEME glasses and their concentration will be measured while they perform a certain task at our booth."}
{"_id":"75a4944af32cd53aa30bf3af533556401b9f3759","title":"A 5GS\/s voltage-to-time converter in 90nm CMOS","text":"A voltage-to-time converter (VTC) is presented for use in a time-based analog-to-digital converter (ADC). The converter runs with a 5 GHz input clock to provide a maximum conversion rate of 5 GS\/s. A novel architecture enables the VTC to provide an adjustable linear delay versus input voltage characteristic. The circuit is realized in a 90nm CMOS process. After calibration, the converter achieves better than 3.7 effective bits for input frequencies up to 1.75 GHz, making it suitable for use in a time-based ADC with up to 4-bit resolution."}
{"_id":"03809a85789f7aeb39002fdcd7c3cdf33cc7370f","title":"A Client-Driven Approach for Channel Management in Wireless LANs","text":"We propose an efficient client-based approach for channel management (channel assignment and load balancing) in 802.11-based WLANs that lead to better usage of the wireless spectrum. This approach is based on a \u201cconflict set coloring\u201d formulation that jointly performs load balancing along with channel assignment. Such a formulation has a number of advantages. First, it explicitly captures interference effects at clients. Next, it intrinsically exposes opportunities for better channel re-use. Finally, algorithms based on this formulation do not depend on specific physical RF models and hence can be applied efficiently to a wide-range of inbuilding as well as outdoor scenarios. We have performed extensive packet-level simulations and measurements on a deployed wireless testbed of 70 APs to validate the performance of our proposed algorithms. We show that in addition to single network scenarios, the conflict set coloring formulation is well suited for channel assignment where multiple wireless networks share and contend for spectrum in the same physical space. Our results over a wide range of both simulated topologies and in-building testbed experiments indicate that our approach improves application level performance at the clients by upto three times (and atleast 50%) in comparison to current best-known techniques."}
{"_id":"cfb3d339a6b369144c356a93e3b519f22928e238","title":"Meta-Learning with Hessian Free Approach in Deep Neural Nets Training","text":"Meta-learning is a promising method to achieve efficient training method towards deep neural net and has been attracting increases interests in recent years. But most of the current methods are still not capable to train complex neuron net model with long-time training process. In this paper, a novel second-order meta-optimizer, named Meta-learning with Hessian-Free(MLHF) approach, is proposed based on the Hessian-Free approach. Two recurrent neural networks are established to generate the damping and the precondition matrix of this Hessian-Free framework. A series of techniques to meta-train the MLHF towards stable and reinforce the meta-training of this optimizer, including the gradient calculation of H . Numerical experiments on deep convolution neural nets, including CUDA-convnet and ResNet18(v2), with datasets of CIFAR10 and ILSVRC2012, indicate that the MLHF shows good and continuous training performance during the whole long-time training process, i.e., both the rapiddecreasing early stage and the steadily-deceasing later stage, and so is a promising meta-learning framework towards elevating the training efficiency in real-world deep neural nets."}
{"_id":"2449a067370ca24353ee8e9fd5e8187cf08ca8f7","title":"Thoth: Comprehensive Policy Compliance in Data Retrieval Systems","text":"Data retrieval systems process data from many sources, each subject to its own data use policy. Ensuring compliance with these policies despite bugs, misconfiguration, or operator error in a large, complex, and fast evolving system is a major challenge. Thoth provides an efficient, kernel-level compliance layer for data use policies. Declarative policies are attached to the systems\u2019 input and output files, key-value tuples, and network connections, and specify the data\u2019s integrity and confidentiality requirements. Thoth tracks the flow of data through the system, and enforces policy regardless of bugs, misconfigurations, compromises in application code, or actions by unprivileged operators. Thoth requires minimal changes to an existing system and has modest overhead, as we show using a prototype Thoth-enabled data retrieval system based on the popular Apache Lucene."}
{"_id":"7112dd4ab26fe7d0f8908f1352e4ba279b2f521d","title":"A multiparadigm intelligent tutoring system for robotic arm training","text":"To assist learners during problem-solving activities, an intelligent tutoring system (ITS) has to be equipped with domain knowledge that can support appropriate tutoring services. Providing domain knowledge is usually done by adopting one of the following paradigms: building a cognitive model, specifying constraints, integrating an expert system, and using data mining algorithms to learn domain knowledge. However, for some ill-defined domains, each single paradigm may present some advantages and limitations in terms of the required resources for deploying it, and tutoring support that can be offered. To address this issue, we propose using a multiparadigm approach. In this paper, we explain how we have applied this idea in CanadarmTutor, an ITS for learning to operate the Canadarm2 robotic arm. To support tutoring services in this ill-defined domain, we have developed a multiparadigm model combining: 1) a cognitive model to cover well-defined parts of the task and spatial reasoning, 2) a data mining approach for automatically building a task model from user solutions for ill-defined parts of the task, and 3) a 3D path-planner to cover other parts of the task for which no user data are available. The multiparadigm version of CanadarmTutor allows providing a richer set of tutoring services than what could be offered with previous single paradigm versions of CanadarmTutor."}
{"_id":"014004e84fbc6e4c7c4aced2e69fc4a5d28daabf","title":"Intellectual Capital (IC) Measurement in the Mass Media Context","text":"Mass media is the key in\u00b0uencer of public opinion. The in\u00b0uence is not only limited to political and social, but also relates to organisational and economical reputation and brands. Within public opinion, organisations must manage how they are represented competitively within mass media so that they can develop their brand strategically to grow and compete in the current global knowledge economy. This is where the link to Intellectual Capital (IC) Measurement is signi \u0304cant. IC, as the sum of all an organisation's intangible assets drives a company's presence and value within the media, albeit related to human, structural or relational capital attributes. The measurement, therefore, of IC in the mass media context is invaluable to understand how a company is placed strategically and competitively in the external space, and how this links to internal activities, goals and outcomes. This paper is an attempt to address some of the issues related to IC measurements in the mass media context by suggesting a framework that provides a multidisciplinary and holistic approach to the understanding and contextualising of the organisation's presence in the public space."}
{"_id":"25d1a2c364b05e0db056846ec397fbf0eacdca5c","title":"Orthogonal nonnegative matrix tri-factorization for co-clustering: Multiplicative updates on Stiefel manifolds","text":"Matrix factorization-based methods become popular in dyadic data analysis, where a fundamental problem, for example, is to perform document clustering or co-clustering words and documents given a term-document matrix. Nonnegative matrix tri-factorization (NMTF) emerges as a promising tool for co-clustering, seeking a 3-factor decomposition X USV with all factor matrices restricted to be nonnegative, i.e., U P 0; S P 0;V P 0: In this paper we develop multiplicative updates for orthogonal NMTF where X USV is pursued with orthogonality constraints, UU 1\u20444 I; and VV 1\u20444 I, exploiting true gradients on Stiefel manifolds. Experiments on various document data sets demonstrate that our method works well for document clustering and is useful in revealing polysemous words via co-clustering words and documents. 2010 Elsevier Ltd. All rights reserved."}
{"_id":"2abf2c3e7ebed04e8c09e478157372dda5cb8bc5","title":"Real-time rigid-body visual tracking in a scanning electron microscope","text":"Robotics continues to provide researchers with an increasing ability to interact with objects at the nano scale. As micro- and nanorobotic technologies mature, more interest is given to computer-assisted or automated approaches to manipulation at these scales. Although actuators are currently available that enable displacements resolutions in the subnanometer range, improvements in feedback technologies have not kept pace. Thus, many actuators that are capable of performing nanometer displacements are limited in automated tasks by the lack of suitable feedback mechanisms. This paper proposes the use of a rigid-model-based method for end effector tracking in a scanning electron microscope to aid in enabling more precise automated manipulations and measurements. These models allow the system to leverage domain-specific knowledge to increase performance in a challenging tracking environment."}
{"_id":"2c6c6d3c94322e9ff75ff2143f7028bfab2b3c5f","title":"Extension of phase correlation to subpixel registration","text":"In this paper, we have derived analytic expressions for the phase correlation of downsampled images. We have shown that for downsampled images the signal power in the phase correlation is not concentrated in a single peak, but rather in several coherent peaks mostly adjacent to each other. These coherent peaks correspond to the polyphase transform of a filtered unit impulse centered at the point of registration. The analytic results provide a closed-form solution to subpixel translation estimation, and are used for detailed error analysis. Excellent results have been obtained for subpixel translation estimation of images of different nature and across different spectral bands."}
{"_id":"42d60f7faaa2f6fdd2b928c352d65eb57b4791aa","title":"Improving resolution by image registration","text":"Image resolution can be improved when the relative displacements in image sequences are known accurately, and some knowledge of the imaging process is available. The proposed approach is similar to back-projection used in tomography. Examples of improved image resolution are given for gray-level and color images, when the unknown image displacements are computed from the image sequence."}
{"_id":"90614cea8c2ab2bff0343231a26d6d0c9315d6c7","title":"A Comparison of Affine Region Detectors","text":"The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris\u00a0 (Mikolajczyk and \u00a0Schmid, 2002; Schaffalitzky and \u00a0Zisserman, 2002) and Hessian points\u00a0 (Mikolajczyk and \u00a0Schmid, 2002), a detector of \u2018maximally stable extremal regions', proposed by Matas et al.\u00a0(2002); an edge-based region detector\u00a0 (Tuytelaars and Van\u00a0Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van\u00a0Gool, 2000), and a detector of \u2018salient regions', proposed by Kadir, Zisserman and Brady\u00a0(2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework."}
{"_id":"db0851926cabc588e1cecb879ed7ce0ce4b9a298","title":"PROCEDURAL GENERATION OF IMAGINATIVE TREES USING A SPACE COLONIZATION ALGORITHM","text":"The modeling of trees is challenging due to their complex branching structures. Three different ways to generate trees are using real world data for reconstruction, interactive modeling methods and modeling with procedural or rule-based systems. Procedural content generation is the idea of using algorithms to automate content creation processes, and it is useful in plant modeling since it can generate a wide variety of plants that can adapt and react to the environment and changing conditions. This thesis focuses on and extends a procedural tree generation technique that uses a space colonization algorithm to model the tree branches\u2019 competition for space, and shifts the previous works\u2019 focus from realism to fantasy. The technique satisfied the idea of using interaction between the tree\u2019s internal and external factors to determine its final shape, by letting the designer control the where and the how of the tree\u2019s growth process. The implementation resulted in a tree generation application where the user\u2019s imagination decides the limit of what can be produced, and if that limit is reached can the application be used to randomly generate a wide variety of trees and tree-like structures. A motivation for many researchers in the procedural content generation area is how it can be used to augment human imagination. The result of this thesis can be used for that, by stepping away from the restrictions of realism, and with ease let the user generate widely diverse trees, that are not necessarily realistic but, in most cases, adapts to the idea of a tree."}
{"_id":"a6f72b3dabe64d1ede9e1f68d1bb11750bcc5166","title":"Thin dual-resonant stacked shorted patch antenna for mobile communications","text":"Introduction: Short-circuited microstrip patch antennas, or PIFAs, can be used as directive internal handset antennas in various communication systems. They provide advantages over traditional external whip and helix antennas in terms of increased total efficiency (when the handset is near the user\u2019s head), decreased radiation towards the user, and increased mechanical reliability. The main disadvantage is their narrow impedance bandwidth compared to the volume occupied by the antenna structure. Typical requirements for directive internal antennas designed for cellular handsets (with a thickness \u2264 20mm) are antenna thickness \u2264 5mm and bandwidth \u2265 10%. It is difficult to design such a small, directive, low-profile handset antenna with high radiation efficiency and an impedance bandwidth greater than 10%. Small antenna design is always a compromise between size, bandwidth, and efficiency. Often, the price for improved performance is increased complexity. If the total dimensions of a patch antenna element are fixed, there are two effective ways to enhance its bandwidth: either dissipative loading [1], which reduces the radiation efficiency, or the addition of more resonators into the antenna structure (matching networks or parasitic elements). The latter method may increase the manufacturing complexity of the antenna. In this Letter, a thin dual-resonant stacked shorted patch antenna is presented. Previously, stacked short-circuited patch antenna elements have been reported in [2, 3]. More recently, similar elements have also been discussed in [4, 5]. The presented antenna is dual-resonant and has a very low profile. The thickness is only one fifth of that of the antenna reported in [5], whereas the surface area reserved for the patches is approximately equal. The small size of the antenna presented in this Letter makes it relatively easy to fit inside a cellular handset. The bandwidth is sufficient for many communication systems. With a common short circuit element which connects both patches to the ground plane, the presented antenna is the simplest example of a stacked shorted microstrip patch antenna. No extra tuning or shorting posts inside the patch area are needed, which makes its manufacturing easier."}
{"_id":"8cc7f8891cf1ed5d22e74ff6fc57d1c23faf9048","title":"Mapping continued brain growth and gray matter density reduction in dorsal frontal cortex: Inverse relationships during postadolescent brain maturation.","text":"Recent in vivo structural imaging studies have shown spatial and temporal patterns of brain maturation between childhood, adolescence, and young adulthood that are generally consistent with postmortem studies of cellular maturational events such as increased myelination and synaptic pruning. In this study, we conducted detailed spatial and temporal analyses of growth and gray matter density at the cortical surface of the brain in a group of 35 normally developing children, adolescents, and young adults. To accomplish this, we used high-resolution magnetic resonance imaging and novel computational image analysis techniques. For the first time, in this report we have mapped the continued postadolescent brain growth that occurs primarily in the dorsal aspects of the frontal lobe bilaterally and in the posterior temporo-occipital junction bilaterally. Notably, maps of the spatial distribution of postadolescent cortical gray matter density reduction are highly consistent with maps of the spatial distribution of postadolescent brain growth, showing an inverse relationship between cortical gray matter density reduction and brain growth primarily in the superior frontal regions that control executive cognitive functioning. Inverse relationships are not as robust in the posterior temporo-occipital junction where gray matter density reduction is much less prominent despite late brain growth in these regions between adolescence and adulthood. Overall brain growth is not significant between childhood and adolescence, but close spatial relationships between gray matter density reduction and brain growth are observed in the dorsal parietal and frontal cortex. These results suggest that progressive cellular maturational events, such as increased myelination, may play as prominent a role during the postadolescent years as regressive events, such as synaptic pruning, in determining the ultimate density of mature frontal lobe cortical gray matter."}
{"_id":"4b8b23c8b2364e25f2bc4e2ad7c8737797f59d19","title":"Broadband antenna design using different 3D printing technologies and metallization processes","text":"The purpose of this paper is to provide a comprehensive evaluation of how plastic materials, different metallization process, and thickness of the metal can influence the performance of 3D printed broadband antenna structures. A set of antennas were manufactured using Fused Deposition Modeling technology and Polyjet technology. Three different plastic materials ABS, PLA and Vero were employed in the tests. The antenna structures were metallized using three common metallization processes: vacuum metallization, electroplating, and conductive paint. In this project, the broadband performances of the metallized plastic antennas are compared to an original metal antenna by measuring VSWR, radiation patterns (H- and E-Planes), and gain. Measurements show that the performances of these plastic structures, regardless of production method, are very similar to the original metal antenna."}
{"_id":"99ad0533f84c110da2d0713d5798e6e14080b159","title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences","text":"We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains \u223c6k questions for +800 paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 86.4%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills."}
{"_id":"25eb08e6985ded20ae723ec668014a2bad789e0f","title":"Kd-Jump: a Path-Preserving Stackless Traversal for Faster Isosurface Raytracing on GPUs","text":"Stackless traversal techniques are often used to circumvent memory bottlenecks by avoiding a stack and replacing return traversal with extra computation. This paper addresses whether the stackless traversal approaches are useful on newer hardware and technology (such as CUDA). To this end, we present a novel stackless approach for implicit kd-trees, which exploits the benefits of index-based node traversal, without incurring extra node visitation. This approach, which we term Kd-Jump, enables the traversal to immediately return to the next valid node, like a stack, without incurring extra node visitation (kd-restart). Also, Kd-Jump does not require global memory (stack) at all and only requires a small matrix in fast constant-memory. We report that Kd-Jump outperforms a stack by 10 to 20% and kd-restar t by 100%. We also present a Hybrid Kd-Jump, which utilizes a volume stepper for leaf testing and a run-time depth threshold to define where kd-tree traversal stops and volume-stepping occurs. By using both methods, we gain the benefits of empty space removal, fast texture-caching and realtime ability to determine the best threshold for current isosurface and view direction."}
{"_id":"03bf26d72d8cc0cf401c31e31c242e1894bd0890","title":"Focused Trajectory Planning for autonomous on-road driving","text":"On-road motion planning for autonomous vehicles is in general a challenging problem. Past efforts have proposed solutions for urban and highway environments individually. We identify the key advantages\/shortcomings of prior solutions, and propose a novel two-step motion planning system that addresses both urban and highway driving in a single framework. Reference Trajectory Planning (I) makes use of dense lattice sampling and optimization techniques to generate an easy-to-tune and human-like reference trajectory accounting for road geometry, obstacles and high-level directives. By focused sampling around the reference trajectory, Tracking Trajectory Planning (II) generates, evaluates and selects parametric trajectories that further satisfy kinodynamic constraints for execution. The described method retains most of the performance advantages of an exhaustive spatiotemporal planner while significantly reducing computation."}
{"_id":"86b164233e22e9edbe56ba32c48e20e53c9a71b6","title":"Pattern Recognition for Downhole Dynamometer Card in Oil Rod Pump System using Artificial Neural Networks","text":"This paper presents the development of an Artificial Neural Network system for Dynamometer Card pattern recognition in oil well rod pump systems. It covers the establishment of pattern classes and a set of standards for training and validation, the study of descriptors which allow the design and the implementation of features extractor, training, analysis and finally the validation and performance test with"}
{"_id":"3bb0a4630a7055858d669b9be194c643a95af3e2","title":"Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment","text":"We consider the task of predicting lexical entailment using distributional vectors. We focus experiments on one previous classifier which was shown to only learn to detect prototypicality of a word pair. Analysis shows that the model single-mindedly learns to detect Hearst Patterns, which are well known to be predictive of lexical relations. We present a new model which exploits this Hearst Detector functionality, matching or outperforming prior work on multiple data sets."}
{"_id":"9006a6b2544162c4bb66cc7f041208e0fe4c0359","title":"A Subsequence Interleaving Model for Sequential Pattern Mining","text":"Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms."}
{"_id":"58a28a12d9cf44bf7a0504b6a9c3194ca210659a","title":"Towards a theory of supply chain management : the constructs and measurements","text":"Rising international cooperation, vertical disintegration, along with a focus on core activities have led to the notion that firms are links in a networked supply chain. This novel perspective has created the challenge of designing and managing a network of interdependent relationships developed and fostered through strategic collaboration. Although research interests in supply chain management (SCM) are growing, no research has been directed towards a systematic development of SCM instruments. This study identifies and consolidates various supply chain initiatives and factors to develop key SCM constructs conducive to advancing the field. To this end, we analyzed over 400 articles and synthesized the large, fragmented body of work dispersed across many disciplines. The result of this study, through successive stages of measurement analysis and refinement, is a set of reliable, valid, and unidimensional measurements that can be subsequently used in different contexts to refine or extend conceptualization and measurements or to test various theoretical models, paving the way for theory building in SCM. \u00a9 2004 Elsevier B.V. All rights reserved."}
{"_id":"14b6b458a931888c7629d47d3e158acd0da13f02","title":"Sieve: Cryptographically Enforced Access Control for User Data in Untrusted Clouds","text":"Modern web services rob users of low-level control over cloud storage; a user\u2019s single logical data set is scattered across multiple storage silos whose access controls are set by the web services, not users. The result is that users lack the ultimate authority to determine how their data is shared with other web services. In this thesis, we introduce Sieve, a new architecture for selectively exposing user data to third party web services in a provably secure manner. Sieve starts with a user-centric storage model: each user uploads encrypted data to a single cloud store, and by default, only the user knows the decryption keys. Given this storage model, Sieve defines an infrastructure to support rich, legacy web applications. Using attribute-based encryption, Sieve allows users to define intuitive, understandable access policies that are cryptographically enforceable. Using key homomorphism, Sieve can re-encrypt user data on storage providers in situ, revoking decryption keys from web services without revealing new ones to the storage provider. Using secret sharing and two-factor authentication, Sieve protects against the loss of user devices like smartphones and laptops. The result is that users can enjoy rich, legacy web applications, while benefitting from cryptographically strong controls over what data the services can access. Thesis Supervisor: Nickolai Zeldovich Title: Associate Professor Thesis Supervisor: James Mickens Title: Associate Professor"}
{"_id":"a6bff7b89af697508d23353148530ea43c6c36e1","title":"Segmentation of Urban Areas Using Road Networks","text":"Region-based analysis is fundamental and crucial in many geospatialrelated applications and research themes, such as trajectory analysis, human mobility study and urban planning. In this paper, we report on an image-processing-based approach to segment urban areas into regions by road networks. Here, each segmented region is bounded by the high-level road segments, covering some neighborhoods and low-level streets. Typically, road segments are classified into different levels (e.g., highways and expressways are usually high-level roads), providing us with a more natural and semantic segmentation of urban spaces than the grid-based partition method. We show that through simple morphological operators, an urban road network can be efficiently segmented into regions. In addition, we present a case study in trajectory mining to demonstrate the usability of the proposed segmentation method."}
{"_id":"00ae3f736b28e2050e23acc65fcac1a516635425","title":"Collaborative Deep Learning Across Multiple Data Centers","text":"Valuable training data is often owned by independent organizations and located in multiple data centers. Most deep learning approaches require to centralize the multi-datacenter data for performance purpose. In practice, however, it is often infeasible to transfer all data to a centralized data center due to not only bandwidth limitation but also the constraints of privacy regulations. Model averaging is a conventional choice for data parallelized training, but its ineffectiveness is claimed by previous studies as deep neural networks are often non-convex. In this paper, we argue that model averaging can be effective in the decentralized environment by using two strategies, namely, the cyclical learning rate and the increased number of epochs for local model training. With the two strategies, we show that model averaging can provide competitive performance in the decentralized mode compared to the data-centralized one. In a practical environment with multiple data centers, we conduct extensive experiments using state-of-the-art deep network architectures on different types of data. Results demonstrate the effectiveness and robustness of the proposed method."}
{"_id":"2abcd3b82f51722a1ef25d7f408c06f43210c809","title":"An improved box-counting method for image fractal dimension estimation","text":"Article history: Received 6 September 2007 Received in revised form 14 January 2009 Accepted 2 March 2009"}
{"_id":"d05e42fa59d1f20f0d60ae89225b826204ef1216","title":"BanditSum: Extractive Summarization as a Contextual Bandit","text":"In this work, we propose a novel method for training neural networks to perform singledocument extractive summarization without heuristically-generated extractive labels. We call our approach BANDITSUM as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score. We perform a series of experiments demonstrating that BANDITSUM is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches. In addition, we show empirically that BANDITSUM performs significantly better than competing approaches when good summary sentences appear late in the source document."}
{"_id":"5dc612abb0b26c734047af8ae8a764819a6f579e","title":"Introduction to the special section on educational data mining","text":"Educational Data Mining (EDM) is an emerging multidisciplinary research area, in which methods and techniques for exploring data originating from various educational information systems have been developed. EDM is both a learning science, as well as a rich application area for data mining, due to the growing availability of educational data. EDM contributes to the study of how students learn, and the settings in which they learn. It enables data-driven decision making for improving the current educational practice and learning material. We present a brief overview of EDM and introduce four selected EDM papers representing a crosscut of different application areas for data mining in education."}
{"_id":"34dadeab830f902edb1213f81336f5cfbf753dcf","title":"Polynomial method for Procedural Terrain Generation","text":"A systematic fractal brownian motion approach is proposed for generating coherent noise, aiming at procedurally generating realistic terrain and textures. Two models are tested and compared to Perlin noise method for two-dimensional height map generation. A fractal analysis is performed in order to compare fractal behaviour of generated data to real terrain coastlines from the point of view of fractal dimension. Performance analysis show that one of the described schemes requires half as many primitive operations than Perlin noise while producing data of equivalent quality."}
{"_id":"9b678aa28facf4f90081d41c2c484c6addddb86d","title":"Fully Convolutional Attention Networks for Fine-Grained Recognition","text":"Fine-grained recognition is challenging due to its subtle local inter-class differences versus large intra-class variations such as poses. A key to address this problem is to localize discriminative parts to extract pose-invariant features. However, ground-truth part annotations can be expensive to acquire. Moreover, it is hard to define parts for many fine-grained classes. This work introduces Fully Convolutional Attention Networks (FCANs), a reinforcement learning framework to optimally glimpse local discriminative regions adaptive to different fine-grained domains. Compared to previous methods, our approach enjoys three advantages: 1) the weakly-supervised reinforcement learning procedure requires no expensive part annotations; 2) the fully-convolutional architecture speeds up both training and testing; 3) the greedy reward strategy accelerates the convergence of the learning. We demonstrate the effectiveness of our method with extensive experiments on four challenging fine-grained benchmark datasets, including CUB-200-2011, Stanford Dogs, Stanford Cars and Food-101."}
{"_id":"18163ea60e83cd3266150bd4b7d7fd3a849ee2db","title":"Consumption of ultra-processed foods and likely impact on human health. Evidence from Canada.","text":"OBJECTIVE\nTo investigate consumption of ultra-processed products in Canada and to assess their association with dietary quality.\n\n\nDESIGN\nApplication of a classification of foodstuffs based on the nature, extent and purpose of food processing to data from a national household food budget survey. Foods are classified as unprocessed\/minimally processed foods (Group 1), processed culinary ingredients (Group 2) or ultra-processed products (Group 3).\n\n\nSETTING\nAll provinces and territories of Canada, 2001.\n\n\nSUBJECTS\nHouseholds (n 5643).\n\n\nRESULTS\nFood purchases provided a mean per capita energy availability of 8908 (se 81) kJ\/d (2129 (se 19) kcal\/d). Over 61\u00b77 % of dietary energy came from ultra-processed products (Group 3), 25\u00b76 % from Group 1 and 12\u00b77 % from Group 2. The overall diet exceeded WHO upper limits for fat, saturated fat, free sugars and Na density, with less fibre than recommended. It also exceeded the average energy density target of the World Cancer Research Fund\/American Institute for Cancer Research. Group 3 products taken together are more fatty, sugary, salty and energy-dense than a combination of Group 1 and Group 2 items. Only the 20 % lowest consumers of ultra-processed products (who consumed 33\u00b72 % of energy from these products) were anywhere near reaching all nutrient goals for the prevention of obesity and chronic non-communicable diseases.\n\n\nCONCLUSIONS\nThe 2001 Canadian diet was dominated by ultra-processed products. As a group, these products are unhealthy. The present analysis indicates that any substantial improvement of the diet would involve much lower consumption of ultra-processed products and much higher consumption of meals and dishes prepared from minimally processed foods and processed culinary ingredients."}
{"_id":"4157e45f616233a0874f54a59c3df001b9646cd7","title":"Diagnostically relevant facial gestalt information from ordinary photos","text":"Craniofacial characteristics are highly informative for clinical geneticists when diagnosing genetic diseases. As a first step towards the high-throughput diagnosis of ultra-rare developmental diseases we introduce an automatic approach that implements recent developments in computer vision. This algorithm extracts phenotypic information from ordinary non-clinical photographs and, using machine learning, models human facial dysmorphisms in a multidimensional 'Clinical Face Phenotype Space'. The space locates patients in the context of known syndromes and thereby facilitates the generation of diagnostic hypotheses. Consequently, the approach will aid clinicians by greatly narrowing (by 27.6-fold) the search space of potential diagnoses for patients with suspected developmental disorders. Furthermore, this Clinical Face Phenotype Space allows the clustering of patients by phenotype even when no known syndrome diagnosis exists, thereby aiding disease identification. We demonstrate that this approach provides a novel method for inferring causative genetic variants from clinical sequencing data through functional genetic pathway comparisons.DOI: http:\/\/dx.doi.org\/10.7554\/eLife.02020.001."}
{"_id":"fef80706b29fc2345c8b7aa53af1076a6afff5f1","title":"Effect of Control-Loops Interactions on Power Stability Limits of VSC Integrated to AC System","text":"This paper investigates the effect of control-loops interactions on power stability limits of the voltage-source converter (VSC) as connected to an ac system. The focus is put on the physical mechanism of the control-loops interactions in the VSC, revealing that interactions among the control loops result in the production of an additional loop. The open-loop gain of the additional loop is employed to quantify the severity of the control-loop interactions. Furthermore, the power current sensitivity, closely related to control-loops interactions, is applied to estimate the maximum transferrable power of the VSC connected to an ac grid. On that basis, stability analysis results show that interactions between dc-link voltage control and phase-locked loop restrict the power angle to about 51\u00b0 for stable operation with no dynamic reactive power supported. Conversely, the system is capable of approaching the ac-side maximum power transfer limits with alternating voltage control included. Simulations in MATLAB\/Simulink are conducted to validate the stability analysis."}
{"_id":"ddf69c7126022bf99f6d0d5f8b4f272915e232cc","title":"A meta-analysis of human-system interfaces in unmanned aerial vehicle (UAV) swarm management.","text":"A meta-analysis was conducted to systematically evaluate the current state of research on human-system interfaces for users controlling semi-autonomous swarms composed of groups of drones or unmanned aerial vehicles (UAVs). UAV swarms pose several human factors challenges, such as high cognitive demands, non-intuitive behavior, and serious consequences for errors. This article presents findings from a meta-analysis of 27 UAV swarm management papers focused on the human-system interface and human factors concerns, providing an overview of the advantages, challenges, and limitations of current UAV management interfaces, as well as information on how these interfaces are currently evaluated. In general allowing user and mission-specific customization to user interfaces and raising the swarm's level of autonomy to reduce operator cognitive workload are beneficial and improve situation awareness (SA). It is clear more research is needed in this rapidly evolving field."}
{"_id":"8a5e5aaf26db3acdcafbc2cd5f44680d4f84d4fc","title":"Stealthy Denial of Service Strategy in Cloud Computing","text":"The success of the cloud computing paradigm is due to its on-demand, self-service, and pay-by-use nature. According to this paradigm, the effects of Denial of Service (DoS) attacks involve not only the quality of the delivered service, but also the service maintenance costs in terms of resource consumption. Specifically, the longer the detection delay is, the higher the costs to be incurred. Therefore, a particular attention has to be paid for stealthy DoS attacks. They aim at minimizing their visibility, and at the same time, they can be as harmful as the brute-force attacks. They are sophisticated attacks tailored to leverage the worst-case performance of the target system through specific periodic, pulsing, and low-rate traffic patterns. In this paper, we propose a strategy to orchestrate stealthy attack patterns, which exhibit a slowly-increasing-intensity trend designed to inflict the maximum financial cost to the cloud customer, while respecting the job size and the service arrival rate imposed by the detection mechanisms. We describe both how to apply the proposed strategy, and its effects on the target system deployed in the cloud."}
{"_id":"69544f17534db93e48d06b84e8f7cb611bd88c69","title":"Mapping CMMI Project Management Process Areas to SCRUM Practices","text":"Over the past years, the capability maturity model (CMM) and capability maturity model integration (CMMI) have been broadly used for assessing organizational maturity and process capability throughout the world. However, the rapid pace of change in information technology has caused increasing frustration to the heavyweight plans, specifications, and other documentation imposed by contractual inertia and maturity model compliance criteria. In light of that, agile methodologies have been adopted to tackle this challenge. The aim of our paper is to present mapping between CMMI and one of these methodologies, Scrum. It shows how Scrum addresses the Project Management Process Areas of CMMI. This is useful for organizations that have their plan-driven process based on the CMMI model and are planning to improve its processes toward agility or to help organizations to define a new project management framework based on both CMMI and Scrum practices."}
{"_id":"b6c6fda47921d7b6bf76c8cb28c316d9ee0c2b64","title":"10 user interface elements for mobile learning application development","text":"Mobile learning application is a new fashionable trend. More than a thousand learning applications have been designed and developed. Therefore, the focus on mobile learning applications needs to be refined and improved in order to facilitate users' interaction and response towards the learning process. To develop an effective mobile learning application, one should start from the User Interface (UI) because an effective UI can play a role in constantly making the user focus on an object and subject. The purpose of this study is to investigate and identify the best UI elements to use in mobile learning application development. Four existing UI guidelines and 12 selected learning applications were analysed, and we identified 10 UI Elements in order to develop the next mobile learning applications. All the 10 elements are described accordingly and they have implications for those designing and implementing UI in mobile learning applications."}
{"_id":"34bbe565d9538ffdf4c8ef4e891411edf8d29447","title":"Robust entropy-based endpoint detection for speech recognition in noisy environments","text":"This paper presents an entropy-based algorithm for accurate and robust endpoint detection for speech recognition under noisy environments. Instead of using the conventional energy-based features, the spectral entropy is developed to identify the speech segments accurately. Experimental results show that this algorithm outperforms the energy-based algorithms in both detection accuracy and recognition performance under noisy environments, with an average error rate reduction of more than 16%."}
{"_id":"bae5575284776cabed101750ac41848c700af431","title":"The social structure of free and open source software development","text":"Metaphors, such as the Cathedral and Bazaar, used to describe the organization of FLOSS projects typically place them in sharp contrast to proprietary development by emphasizing FLOSS\u2019s distinctive social and communications structures. But what do we really know about the communication patterns of FLOSS projects? How generalizable are the projects that have been studied? Is there consistency across FLOSS projects? Questioning the assumption of distinctiveness is important because practitioner-advocates from within the FLOSS community rely on features of social structure to describe and account for some of the advantages of FLOSS production. To address this question, we examined 120 project teams from SourceForge, representing a wide range of FLOSS project types, for their communications centralization as revealed in the interactions in the bug tracking system. We found that FLOSS development teams vary widely in their communications centralization, from projects completely centered on one developer to projects that are highly decentralized and exhibit a distributed pattern of conversation between developers and active users. We suggest, therefore, that it is wrong to assume that FLOSS projects are distinguished by a particular social structure merely because they are FLOSS. Our findings suggest that FLOSS projects might have to work hard to achieve the expected development advantages which have been assumed to flow from \u201cgoing open.\u201d In addition, the variation in communications structure across projects means that communications centralization is useful for comparisons between FLOSS teams. We"}
{"_id":"231878207a8641e605dc255f2c557fa4e8bb99bf","title":"The Cathedral and the Bazaar","text":"Permission is granted to copy, distribute and\/or modify this document under the terms of the Open Publication License, version 2.0. $Date: 2002\/08\/02 09:02:14 $ Revision History Revision 1.57 11 September 2000 esr New major section \u201cHow Many Eyeballs Tame Complexity\u201d. Revision 1.52 28 August 2000 esr MATLAB is a reinforcing parallel to Emacs. Corbato\u00f3 & Vyssotsky got it in 1965. Revision 1.51 24 August 2000 esr First DocBook version. Minor updates to Fall 2000 on the time-sensitive material. Revision 1.49 5 May 2000 esr Added the HBS note on deadlines and scheduling. Revision 1.51 31 August 1999 esr This the version that O\u2019Reilly printed in the first edition of the book. Revision 1.45 8 August 1999 esr Added the endnotes on the Snafu Principle, (pre)historical examples of bazaar development, and originality in the bazaar. Revision 1.44 29 July 1999 esr Added the \u201cOn Management and the Maginot Line\u201d section, some insights about the usefulness of bazaars for exploring design space, and substantially improved the Epilog. Revision 1.40 20 Nov 1998 esr Added a correction of Brooks based on the Halloween Documents. Revision 1.39 28 July 1998 esr I removed Paul Eggert\u2019s \u2019graph on GPL vs. bazaar in response to cogent aguments from RMS on Revision 1.31 February 1"}
{"_id":"2469fd136aaf16c49bbe6814d6153da1dc6c7c23","title":"Social translucence: an approach to designing systems that support social processes","text":"We are interested in desiging systems that support communication and collaboration among large groups of people over computing networks. We begin by asking what properties of the physical world support graceful human-human communication in face-to-face situations, and argue that it is possible to design digital systems that support coherent behavior by making participants and their activites visible to one another. We call such systems \u201csocially translucent systems\u201d and suggest that they have three characteristics\u2014visbility, awareness, and accountability\u2014which enable people to draw upon their experience and expertise to structure their interactions with one another. To motivate and focus our ideas we develop a vision of knowledge communities, conversationally based systems that support the creation, management and reuse of knowledge in a social context. We describe our experience in designing and deploying one layer of functionality for knowledge communities, embodied in a working system called \u201cBarbie\u201d and discuss research issues raised by a socially translucent approach to design."}
{"_id":"2fc0516f700b490b7e13db0f0d73d05afa5e346c","title":"Cave or Community? An Empirical Examination of 100 Mature Open Source Projects","text":"Starting with Eric Raymond\u2019s groundbreaking work, The Cathedral and the Bazaar, open-source software (OSS) has commonly been regarded as work produced by a community of developers. Yet, given the nature of software programs, one also hears of developers with no lives that work very hard to achieve great product results. In this paper, I sought empirical evidence that would help us understand which is more commonthe cave (i.e., lone producer) or the community. Based on a study of the top 100 mature products on Sourceforge, I find a few surprising things. First, most OSS programs are developed by individuals, rather than communities. The median number of developers in the 100 projects I looked at was 4 and the mode was 1numbers much lower than previous ones reported for highly successful projects! Second, most OSS programs do not generate a lot of discussion. Third, products with more developers tend to be viewed and downloaded more often. Fourth, the number of developers associated with a project is unrelated to the age of the project. Fifth, the larger the project, the smaller the percent of project administrators."}
{"_id":"4282abe7e08bcfb2d282c063428fb187b2802e9c","title":"Case Reports of Adipose-derived Stem Cell Therapy for Nasal Skin Necrosis after Filler Injection","text":"With the gradual increase of cases using fillers, cases of patients treated by non-medical professionals or inexperienced physicians resulting in complications are also increasing. We herein report 2 patients who experienced acute complications after receiving filler injections and were successfully treated with adipose-derived stem cell (ADSCs) therapy. Case 1 was a 23-year-old female patient who received a filler (Restylane) injection in her forehead, glabella, and nose by a non-medical professional. The day after her injection, inflammation was observed with a 3\u00d73 cm skin necrosis. Case 2 was a 30-year-old woman who received a filler injection of hyaluronic acid gel (Juvederm) on her nasal dorsum and tip at a private clinic. She developed erythema and swelling in the filler-injected area A solution containing ADSCs harvested from each patient's abdominal subcutaneous tissue was injected into the lesion at the subcutaneous and dermis levels. The wounds healed without additional treatment. With continuous follow-up, both patients experienced only fine linear scars 6 months postoperatively. By using adipose-derived stem cells, we successfully treated the acute complications of skin necrosis after the filler injection, resulting in much less scarring, and more satisfactory results were achieved not only in wound healing, but also in esthetics."}
{"_id":"c160ae4b1eed860e96250df2d7ecd86a0120c0a2","title":"Peer support services for individuals with serious mental illnesses: assessing the evidence.","text":"OBJECTIVE\nThis review assessed the level of evidence and effectiveness of peer support services delivered by individuals in recovery to those with serious mental illnesses or co-occurring mental and substance use disorders.\n\n\nMETHODS\nAuthors searched PubMed, PsycINFO, Applied Social Sciences Index and Abstracts, Sociological Abstracts, Social Services Abstracts, Published International Literature on Traumatic Stress, the Educational Resources Information Center, and the Cumulative Index to Nursing and Allied Health Literature for outcome studies of peer support services from 1995 through 2012. They found 20 studies across three service types: peers added to traditional services, peers in existing clinical roles, and peers delivering structured curricula. Authors judged the methodological quality of the studies using three levels of evidence (high, moderate, and low). They also described the evidence of service effectiveness.\n\n\nRESULTS\nThe level of evidence for each type of peer support service was moderate. Many studies had methodological shortcomings, and outcome measures varied. The effectiveness varied by service type. Across the range of methodological rigor, a majority of studies of two service types--peers added and peers delivering curricula--showed some improvement favoring peers. Compared with professional staff, peers were better able to reduce inpatient use and improve a range of recovery outcomes, although one study found a negative impact. Effectiveness of peers in existing clinical roles was mixed.\n\n\nCONCLUSIONS\nPeer support services have demonstrated many notable outcomes. However, studies that better differentiate the contributions of the peer role and are conducted with greater specificity, consistency, and rigor would strengthen the evidence."}
{"_id":"33224ad0cdf6e2dc4893194dd587309c7887f0ba","title":"Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)","text":"The simple, but general formal theory of fun and intrinsic motivation and creativity (1990-2010) is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional field of active learning, and is related to old, but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, and humor. This overview first describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown, but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical, but nonoptimal implementations (1991, 1995, and 1997-2002) are reviewed, as well as several recent variants by others (2005-2010). A simplified typology addresses current confusion concerning the precise nature of intrinsic motivation."}
{"_id":"e88929f3b08e8c64f81aa9475bfab5429b7cb051","title":"Deep neural network for manufacturing quality prediction","text":"Expected product quality is affected by multi-parameter in complex manufacturing processes. Product quality prediction can offer the possibility of designing better system parameters at the early production stage. Many existing approaches fail at providing favorable results duo to shallow architecture in prediction model that can not learn multi-parameter's features insufficiently. To address this issue, a deep neural network (DNN), consisting of a deep belief network (DBN) in the bottom and a regression layer on the top, is proposed in this paper. The DBN uses a greedy algorithm for unsupervised feature learning. It could learn effective features for manufacturing quality prediction in an unsupervised pattern which has been proven to be effective for many fields. Then the learned features are inputted into the regression tool, and the quality predictions are obtained. One type of manufacturing system with multi-parameter is investigated by the proposed DNN model. The experiments show that the DNN has good performance of the deep architecture, and overwhelms the peer shallow models. It is recommended from this study that the deep learning technique is more promising in manufacturing quality prediction."}
{"_id":"946dabbc13f06070f7618cd4ca6733a95b4b03c3","title":"A linguistic ontology of space for natural language processing","text":"a r t i c l e i n f o a b s t r a c t We present a detailed semantics for linguistic spatial expressions supportive of computational processing that draws substantially on the principles and tools of ontological engineering and formal ontology. We cover language concerned with space, actions in space and spatial relationships and develop an ontological organization that relates such expressions to general classes of fixed semantic import. The result is given as an extension of a linguistic ontology, the Generalized Upper Model, an organization which has been used for over a decade in natural language processing applications. We describe the general nature and features of this ontology and show how we have extended it for working particularly with space. Treaitng the semantics of natural language expressions concerning space in this way offers a substantial simplification of the general problem of relating natural spatial language to its contextualized interpretation. Example specifications based on natural language examples are presented, as well as an evaluation of the ontology's coverage, consistency, predictive power, and applicability."}
{"_id":"d95a8c90256a4f3008e3b1b8d089e5fbf46eb5e8","title":"A Survey of Intrusion Detection Systems for Mobile Ad Hoc Networks","text":"Tactical Mobile Ad-hoc Networks (MANETs) are widely deployed in military organizations that have critical needs of communication even with the absence of fixed infrastructure. The lack of communication infrastructure and dynamic topology makes MANETs vulnerable to a wide range attacks. Tactical military operations are among the prime users of ad-hoc networks. The military holds higher standards of security requirements and thus require special intrusion detection applications. Conventional Intrusion Detection System (IDS) require central control and monitoring entities and thus cannot be applied to MANETs. Solutions to secure these networks are based on distributed and cooperative security. This paper presents a survey of IDS specifically for MANETs and also highlights the strengths and weakness of each model. We present a unique evaluation matrix for measuring the effectiveness of IDS for MANETs in an emergency response scenario."}
{"_id":"e3b2990079f630e0821f38714dbc1bfd1f3e9c87","title":"Enabling agricultural automation to optimize utilization of water, fertilizer and insecticides by implementing Internet of Things (IoT)","text":"With the proliferation of smart devices, Internet can be extended into the physical realm of Internet-of-Things (IoT) by deploying them into a communicating-actuating network. In Ion, sensors and actuators blend seamlessly with the environment; collaborate globally with each other through internet to accomplish a specific task. Wireless Sensor Network (WSN) can be integrated into Ion to meet the challenges of seamless communication between any things (e.g., humans or objects). The potentialities of IoT can be brought to the benefit of society by developing novel applications in transportation and logistics, healthcare, agriculture, smart environment (home, office or plant). This research gives a framework of optimizing resources (water, fertilizers, insecticides and manual labour) in agriculture through the use of IoT. The issues involved in the implementation of applications are also investigated in the paper. This frame work is named as AgriTech."}
{"_id":"56c9c6b4e7bc658e065f80617e4e0278f40d6b26","title":"A review on stress inducement stimuli for assessing human stress using physiological signals","text":"Assessing human stress in real-time is more difficult and challenging today. The present review deals about the measurement of stress in laboratory environment using different stress inducement stimuli by the help of physiological signals. Previous researchers have been used different stress inducement stimuli such as stroop colour word test (CWT), mental arithmetic test, public speaking task, cold pressor test, computer games and works used to induce the stress. Most of the researchers have been analyzed stress using questionnaire based approach and physiological signals. The several physiological signals like Electrocardiogram (ECG), Electromyogram (EMG), Galvanic Skin Response (GSR), Blood Pressure (BP), Skin Temperature (ST), Blood Volume Pulse (BVP), respiration rate (RIP) and Electroencephalogram (EEG) were briefly investigated to identify the stress. Different statistical methods like Analysis of variance (ANOVA), two-way ANOVA, Multivariate analysis of variance (MANOVA), t-test, paired t-tests and student t-tests have used to describe the correlation between stress inducement stimuli, subjective parameters (age, gender and etc.,) and physiological signals. This present works aims to find the most appropriate stress inducement stimuli, physiological signals and statistical method to efficiently asses the human stress."}
{"_id":"9a700c7a7e7468e436f00c34551fbe3e0f70e42f","title":"Towards Principled Methods for Training Generative Adversarial Networks","text":"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them."}
{"_id":"0cf61f46f76e24beec8a1fe38fc4ab9dd6ac5abd","title":"Design Exploration of Hybrid CMOS and Memristor Circuit by New Modified Nodal Analysis","text":"Design of hybrid circuits and systems based on CMOS and nano-device requires rethinking of fundamental circuit analysis to aid design exploration. Conventional circuit analysis with modified nodal analysis (MNA) cannot consider new nano-devices such as memristor together with the traditional CMOS devices. This paper has introduced a new MNA method with magnetic flux (\u03a6) as new state variable. New SPICE-like circuit simulator is thereby developed for the design of hybrid CMOS and memristor circuits. A number of CMOS and memristor-based designs are explored, such as oscillator, chaotic circuit, programmable logic, analog-learning circuit, and crossbar memory, where their functionality, performance, reliability and power can be efficiently verified by the newly developed simulator. Specifically, one new 3-D-crossbar architecture with diode-added memristor is also proposed to improve integration density and to avoid sneak path during read-write operation."}
{"_id":"947b9250c2f6c41203468506255c3257d3decef3","title":"Facial Expression Recognition with Convolutional Neural Networks","text":"Facial expression recognition systems have attracted much research interest within the field of artificial intelligence. Many established facial expression recognition (FER) systems apply standard machine learning to extracted image features, and these methods generalize poorly to previously unseen data. This project builds upon recent research to classify images of human faces into discrete emotion categories using convolutional neural networks (CNNs). We experimented with different architectures and methods such as fractional max-pooling and finetuning, ultimately achieving an accuracy of 0.48 in a sevenclass classification task."}
{"_id":"5941bf3d86ebb8347dfa0b40af028e7f61339501","title":"Object Constraint Language (OCL): A Definitive Guide","text":"The Object Constraint Language (OCL) started as a complement of the UML notation with the goal to overcome the limitations of UML (and in general, any graphical notation) in terms of precisely specifying detailed aspects of a system design. Since then, OCL has become a key component of any model-driven engineering (MDE) technique as the default language for expressing all kinds of (meta)model query, manipulation and specification requirements. Among many other applications, OCL is frequently used to express model transformations (as part of the source and target patterns of transformation rules), well-formedness rules (as part of the definition of new domain-specific languages), or code-generation templates (as a way to express the generation patterns and rules). This chapter pretends to provide a comprehensive view of this language, its many applications and available tool support as well as the latest research developments and open challenges around it."}
{"_id":"3198e5de8eb9edfd92e5f9c2cb325846e25f22aa","title":"Topic models in information retrieval","text":""}
{"_id":"3c8f916264e8d15ba1bc618c6adf395e86dd7b40","title":"Generating Descriptions with Grounded and Co-referenced People","text":"Learning how to generate descriptions of images or videos received major interest both in the Computer Vision and Natural Language Processing communities. While a few works have proposed to learn a grounding during the generation process in an unsupervised way (via an attention mechanism), it remains unclear how good the quality of the grounding is and whether it benefits the description quality. In this work we propose a movie description model which learns to generate description and jointly ground (localize) the mentioned characters as well as do visual co-reference resolution between pairs of consecutive sentences\/clips. We also propose to use weak localization supervision through character mentions provided in movie descriptions to learn the character grounding. At training time, we first learn how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised approach. We then provide this (noisy) supervision into our description model which greatly improves its performance. Our proposed description model improves over prior work w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly collected grounding and co-reference data for characters."}
{"_id":"96dc6e4960eaa41b0089a0315fe598afacc52470","title":"Water content of latent fingerprints - Dispelling the myth.","text":"Changing procedures in the handling of rare and precious documents in museums and elsewhere, based on assumptions about constituents of latent fingerprints, have led the author to an examination of available data. These changes appear to have been triggered by one paper using general biological data regarding eccrine sweat production to infer that deposited fingerprints are mostly water. Searching the fingerprint literature has revealed a number of reference works similarly quoting figures for average water content of deposited fingerprints of 98% or more. Whilst accurate estimation is difficult there is no evidence that the residue on fingers could be anything like 98% water, even if there were no contamination from sebaceous glands. Consideration of published analytical data of real fingerprints, and several theoretical considerations regarding evaporation and replenishment rates, indicates a probable initial average water content of a fingerprint, soon after deposition, of 20% or less."}
{"_id":"9f22a67fa2d6272cec590d4e8ec2ba75cf41df9a","title":"A multiscale measure for mixing","text":"We present a multiscale measure for mixing that is based on the concept of weak convergence and averages the \u201cmixedness\u201d of an advected scalar field at various scales. This new measure, referred to as the Mix-Norm, resolves the inability of the L2 variance of the scalar density field to capture small-scale variations when advected by chaotic maps or flows. In addition, the Mix-Norm succeeds in capturing the efficiency of a mixing protocol in the context of a particular initial scalar field, wherein Lyapunov-exponent based measures fail to do so. We relate the Mix-Norm to the classical ergodic theoretic notion of mixing and present its formulation in terms of the power spectrum of the scalar field. We demonstrate the utility of the Mix-Norm by showing how it measures the efficiency of mixing due to various discrete dynamical systems and to diffusion. In particular, we show that the Mix-Norm can capture known exponential and algebraic mixing properties of certain maps. We also analyze numerically the behaviour of scalar fields evolved by the Standard Map using the Mix-Norm. \u00a9 2005 Elsevier B.V. All rights reserved."}
{"_id":"943d17f36d320ad9fcc3ae82c78914c0111cef1d","title":"Artificial cooperative search algorithm for numerical optimization problems","text":"In this paper, a new two-population based global search algorithm, the Artificial Cooperative Search Algorithm (ACS), is introduced. ACS algorithm has been developed to be used in solving real-valued numerical optimization problems. For purposes of examining the success of ACS algorithm in solving numerical optimization problems, 91 benchmark problems that have different specifications were used in the detailed tests. The success of ACS algorithm in solving the related benchmark problems was compared to the successes obtained by PSO, SADE, CLPSO, BBO, CMA-ES, CK and DSA algorithms in solving the related benchmark problems by using Wilcoxon Signed-Rank Statistical Test with Bonferroni-Holm correction. The results obtained in the statistical analysis demonstrate that the success achieved by ACS algorithm in solving numerical optimization problems is better in comparison to the other computational intelligence algorithms used in this paper. 2012 Elsevier Inc. All rights reserved."}
{"_id":"a9166e3223daed5655f4f57e911a8e5f91c6ec37","title":"Abstraction Refinement for Probabilistic Software","text":"ion Refinement for Probabilistic Software Mark Kattenbelt, Marta Kwiatkowska, Gethin Norman, and David Parker Oxford University Computing Laboratory, Parks Road, Oxford, OX1 3QD Abstract. We present a methodology and implementation for verifying ANSI-C programs that exhibit probabilistic behaviour, such as failures or randomisation. We use abstraction-refinement techniques that represent We present a methodology and implementation for verifying ANSI-C programs that exhibit probabilistic behaviour, such as failures or randomisation. We use abstraction-refinement techniques that represent probabilistic programs as Markov decision processes and their abstractions as stochastic two-player games. Our techniques target quantitative properties of software such as \u201cthe maximum probability of file-transfer failure\u201d or \u201cthe minimum expected number of loop iterations\u201d and the abstractions we construct yield lower and upper bounds on these properties, which then guide the refinement process. We build upon stateof-the-art techniques and tools, using SAT-based predicate abstraction, symbolic implementations of probabilistic model checking and components from GOTO-CC, SATABS and PRISM. Experimental results show that our approach performs very well in practice, successfully verifying actual networking software whose complexity is significantly beyond the scope of existing probabilistic verification tools."}
{"_id":"a70bbc4c6c3ac0c77526a64bf11073bc8f45bd48","title":"A study of SSL Proxy attacks on Android and iOS mobile applications","text":"According to recent articles in popular technology websites, some mobile applications function in an insecure manner when presented with untrusted SSL certificates. These non-browser based applications seem to, in the absence of a standard way of alerting a user of an SSL error, accept any certificate presented to it. This paper intends to research these claims and show whether or not an invisible proxy based SSL attack can indeed steal user's credentials from mobile applications, and which types applications are most likely to be vulnerable to this attack vector. To ensure coverage of the most popular platforms, applications on both Android 4.2 and iOS 6 are tested. The results of our study showed that stealing credentials is indeed possible using invisible proxy man in the middle attacks."}
{"_id":"a1221b0fd74212382c7387e6f6fd957918576dda","title":"Transient effects in application of PWM inverters to induction motors","text":"Standard squirrel cage induction motors are subjected to nonsinusoidal wave shapes, when supplied from adjustable frequency inverters. In addition to causing increased heating, these wave patterns can be destructive to the insulation. Pulse width modulated (PWM) inverter output amplitudes and risetimes are investigated, and motor insulation capabilities are discussed. Voltage reflections are simulated for various cable lengths and risetimes and are presented graphically. Simulations confirm potential problems with long cables and short risetimes. Application precautions are also discussed.<<ETX>>"}
{"_id":"bdf434f475654ee0a99fe11fd63405b038244f69","title":"Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction","text":"Recidivism prediction scores are used across the USA to determine sentencing and supervision for hundreds of thousands of inmates. One such generator of recidivism prediction scores is Northpointe's Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) score, used in states like California and Florida, which past research has shown to be biased against black inmates according to certain measures of fairness. To counteract this racial bias, we present an adversarially-trained neural network that predicts recidivism and is trained to remove racial bias. When comparing the results of our model to COMPAS, we gain predictive accuracy and get closer to achieving two out of three measures of fairness: parity and equality of odds. Our model can be generalized to any prediction and demographic. This piece of research contributes an example of scientific replication and simplification in a high-stakes real-world application like recidivism prediction."}
{"_id":"eb7cf446e983e98c1400c8181949f038caf0c8a8","title":"Perpetual development: A model of the Linux kernel life cycle","text":"Software evolution is widely recognized as an important and common phenomenon, whereby the system follows an ever-extending development trajectory w ith intermittent releases. Nevertheless there have been only few lifecycle models that attempt to por tray such evolution. We use the evolution of the Linux kernel as the basis for the formulation of such a model, integrating the progress in time with growth of the codebase, and differentiating bet ween development of new functionality and maintenance of production versions. A unique elem nt of the model is the sequence of activities involved in releasing new production versions, and how this has changed with the growth of Linux. In particular, the release follow-up phase before th forking of a new development version, which was prominent in early releases of production ve rsions, has been eliminated in favor of a concurrent merge window in the release of 2.6.x versions . We also show that a piecewise linear model with increasing slopes provides the best descr iption of the growth of Linux. The perpetual development model is used as a framework in which c ommonly recognized benefits of incremental and evolutionary development may be demonstra ted, nd to comment on issues such as architecture, conservation of familiarity, and failed p rojects. We suggest that this model and variants thereof may apply to many other projects in additio n to Linux."}
{"_id":"cbb866b10674bf7461b768ec154d4b9478e32e82","title":"Comparative study on power conversion methods for wireless battery charging platform","text":"In this paper, four different power conversion methods (voltage control, duty-cycle control, frequency control and phase-shift control) are compared for wireless power transfer applications by considering the energy transfer efficiency, electromagnetic interference, stability, and implementation complexity. The phase-shift control is found to be the optimal scheme with good efficiency and low electromagnetic interference. Its constant frequency feature is also within the framework of the new international wireless charging standard called \u2018Qi\u2019. A high system efficiency of 72% for 5W wireless charging applications has been achieved practically."}
{"_id":"280cd4a04cf7ecc36f84a7172483916a41403f5e","title":"Multi-class AdaBoost \u2217","text":"Boosting has been a very successful technique for solving the two-class classification problem. In going from two-class to multi-class classification, most algorithms have been restricted to reducing the multi-class classification problem to multiple two-class problems. In this paper, we develop a new algorithm that directly extends the AdaBoost algorithm to the multi-class case without reducing it to multiple two-class problems. We show that the proposed multi-class AdaBoost algorithm is equivalent to a forward stagewise additive modeling algorithm that minimizes a novel exponential loss for multi-class classification. Furthermore, we show that the exponential loss is a member of a class of Fisher-consistent loss functions for multi-class classification. As shown in the paper, the new algorithm is extremely easy to implement and is highly competitive in terms of misclassification error rate."}
{"_id":"3215a900e4fb9499c8904bfe662c59de042da67d","title":"Predicting Movie Sales from Blogger Sentiment","text":"The volume of discussion about a product in weblogs has recently been shown to correlate with the product\u2019s financial performance. In this paper, we study whether applying sentiment analysis methods to weblog data results in better correlation than volume only, in the domain of movies. Our main finding is that positive sentiment is indeed a better predictor for movie success when applied to a limited context around references to the movie in weblogs, posted prior to its release. If my film makes one more person miserable, I\u2019ve done my job."}
{"_id":"46a7464a8926241c8ed78b243ca0bf24253f8786","title":"Early Prediction of Movie Box Office Success Based on Wikipedia Activity Big Data","text":"Use of socially generated \"big data\" to access information about collective states of the minds in human societies has become a new paradigm in the emerging field of computational social science. A natural application of this would be the prediction of the society's reaction to a new product in the sense of popularity and adoption rate. However, bridging the gap between \"real time monitoring\" and \"early predicting\" remains a big challenge. Here we report on an endeavor to build a minimalistic predictive model for the financial success of movies based on collective activity data of online users. We show that the popularity of a movie can be predicted much before its release by measuring and analyzing the activity level of editors and viewers of the corresponding entry to the movie in Wikipedia, the well-known online encyclopedia."}
{"_id":"38d76b64705d193ee8017993f7fc4c6e8d4bdbc8","title":"A Live-User Study of Opinionated Explanations for Recommender Systems","text":"This paper describes an approach for generating rich and compelling explanations in recommender systems, based on opinions mined from user-generated reviews. The explanations highlight the features of a recommended item that matter most to the user and also relate them to other recommendation alternatives and the user's past activities to provide a context."}
{"_id":"2d81296e2894baaade499d9f1ed163f339943ddc","title":"MO-SLAM: Multi object SLAM with run-time object discovery through duplicates","text":"In this paper, we present MO-SLAM, a novel visual SLAM system that is capable of detecting duplicate objects in the scene during run-time without requiring an offline training stage to pre-populate a database of objects. Instead, we propose a novel method to detect landmarks that belong to duplicate objects. Further, we show how landmarks belonging to duplicate objects can be converted to first-order entities which generate additional constraints for optimizing the map. We evaluate the performance of MO-SLAM with extensive experiments on both synthetic and real data, where the experimental results verify the capabilities of MO-SLAM in detecting duplicate objects and using these constraints to improve the accuracy of the map."}
{"_id":"2172134ed38d28fb910879b39a13832c5fb7998b","title":"Speed planning for solar-powered electric vehicles","text":"Electric vehicles (EVs) are the trend for future transportation. The major obstacle is range anxiety due to poor availability of charging stations and long charging time. Solar-powered EVs, which mostly rely on solar energy, are free of charging limitations. However, the range anxiety problem is more severe due to the availability of sun light. For example, shadings of buildings or trees may cause a solar-powered EV to stop halfway in a trip. In this paper, we show that by optimally planning the speed on different road segments and thus balancing energy harvesting and consumption, we can enable a solar-powered EV to successfully reach the destination using the shortest travel time. The speed planning problem is essentially a constrained non-linear programming problem, which is generally difficult to solve. We have identified an optimality property that allows us to compute an optimal speed assignment for a partition of the path; then, a dynamic programming method is developed to efficiently compute the optimal speed assignment for the whole trip with significantly low computation overhead compared to the state-of-the-art non-linear programming solver. To evaluate the usability of the proposed method, we have also developed a solar-powered EV prototype. Experiments show that the predictions by the proposed technique match well with the data collected from the physical EV. Issues on practical implementation are also discussed."}
{"_id":"f3396763e2c3ec1e0c80fddad8b08177960ce34d","title":"Increase Physical Fitness and Create Health Awareness through Exergames and Gamification - The Role of Individual Factors, Motivation and Acceptance","text":"Demographic change and the aging population push health and welfare system to its limits. Increased physical fitness and increased awareness for health issues will help elderly to live independently for longer and will thereby reduce the costs in the health care system. Exergames seem to be a promising solution for promoting physical fitness. Still, there is little evidence under what conditions Exergames will be accepted and used by elderly. To investigate promoting and hindering factors we conducted a user study with a prototype of an Exergame. We contrasted young vs. elderly players and investigated the role of gamer types, personality factors and technical expertise on the performance within the game and changes in the attitude towards individual health after the game. Surprisingly, performance within the game is not affected by performance motivation but by gamer type. More importantly, a universal positive effect on perceived pain is detected after the Exergame"}
{"_id":"4e71af17f3b0ec59aa3a1c7ea3f2680a1c9d9f6f","title":"A CNN-based segmentation model for segmenting foreground by a probability map","text":"This paper proposes a CNN-based segmentation model to segment foreground from an image and a prior probability map. Our model is constructed based on the FCN model that we simply replace the original RGB-based three channel input layer by a four channel, i.e., RGB and prior probability map. We then train the model by constructing various image, prior probability maps and the groundtruths from the PASCAL VOC dataset, and finally obtain a CNN-based foreground segmentation model that is suitable for general images. Our proposed method is motivated by the observation that the classical graphcut algorithm using GMM for modeling the priors can not capture the semantic segmentation from the prior probability, and thus leads to low segmentation performance. Furthermore, the efficient FCN segmentation model is for specific objects rather than general objects. We therefore improve the graph-cut like foreground segmentation by extending FCN segmentation model. We verify the proposed model by various prior probability maps such as artifical maps, saliency maps, and discriminative maps. The ICoseg dataset that is different from the PASCAL Voc dataset is used for the verification. Experimental results demonstrates the fact that our method obviously outperforms the graphcut algorithms and FCN models."}
{"_id":"5906297bd4108376a032cb4c610d3e2926750d47","title":"Clothes Co-Parsing Via Joint Image Segmentation and Labeling With Application to Clothing Retrieval","text":"This paper aims at developing an integrated system for clothing co-parsing (CCP), in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. A novel data-driven system consisting of two phases of inference is proposed. The first phase, referred as \u201cimage cosegmentation,\u201d iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM technique [1]. In the second phase (i.e., \u201cregion colabeling\u201d), we construct a multiimage graphical model by taking the segmented regions as vertices, and incorporating several contexts of clothing configuration (e.g., item locations and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [2], we construct a dataset called the SYSU-Clothes dataset consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29%\/88.23% segmentation accuracy and 65.52%\/63.89% recognition rate on the Fashionista and the SYSU-Clothes datasets, respectively, which are superior compared with the previous methods. Furthermore, we apply our method on a challenging task, i.e., cross-domain clothing retrieval: given user photo depicting a clothing image, retrieving the same clothing items from online shopping stores based on the fine-grained parsing results."}
{"_id":"ea5fdb7f6c2e94d1a67bff622c566ddc66be19ab","title":"Face recognition based on convolutional neural network and support vector machine","text":"Face recognition is an important embodiment of human-computer interaction, which has been widely used in access control system, monitoring system and identity verification. However, since face images vary with expressions, ages, as well as poses of people and illumination conditions, the face images of the same sample might be different, which makes face recognition difficult. There are two main requirements in face recognition, the high recognition rate and less training time. In this paper, we combine Convolutional Neural Network (CNN) and Support Vector Machine (SVM) to recognize face images. CNN is used as a feature extractor to acquire remarkable features automatically. We first pre-train our CNN by ancillary data to get the updated weights, and then train the CNN by the target dataset to extract more hidden facial features. Finally we use SVM as our classifier instead of CNN to recognize all the classes. With the input of facial features extracted from CNN, SVM will recognize face images more accurately. In our experiments, some face images in the Casia-Webfaces database are used for pre-training, and FERET database is used for training and testing. The results in experiments demonstrate the efficiency with high recognition rate and less training time."}
{"_id":"45e3d69e23bf3b3583a8a71022a8e161a37e9571","title":"Progress on a cognitive-motivational-relational theory of emotion.","text":"The 2 main tasks of this article are 1st, to examine what a theory of emotion must do and basic issues that it must address. These include definitional issues, whether or not physiological activity should be a defining attribute, categorical versus dimensional strategies, the reconciliation of biological universals with sociocultural sources of variability, and a classification of the emotions. The 2nd main task is to apply an analysis of appraisal patterns and the core relational themes that they produce to a number of commonly identified emotions. Anger, anxiety, sadness, and pride (to include 1 positive emotion) are used as illustrations. The purpose is to show the capability of a cognitive-motivational-relational theory to explain and predict the emotions. The role of coping in emotion is also discussed, and the article ends with a response to criticisms of a phenomenological, folk-theory outlook."}
{"_id":"6ee9fcef938389c4cc48fd3b1875d680161ffe0a","title":"A Survey of Logical Models for OLAP Databases","text":"In this paper, we present different proposals for multidimensional data cubes, which are the basic logical model for OLAP applications. We have grouped the work in the field in two categories: commercial tools (presented along with terminology and standards) and academic efforts. We further divide the academic efforts in two subcategories: the relational model extensions and the cube-oriented approaches. Finally, we attempt a comparative analysis of the various efforts."}
{"_id":"a9f1838de25c17a38fae9738d137d7c7644b3be1","title":"Who Is Going to Get Hurt? Predicting Injuries in Professional Soccer","text":"Injury prevention has a fundamental role in professional soccer due to the high cost of recovery for players and the strong influence of injuries on a club\u2019s performance. In this paper we provide a predictive model to prevent injuries of soccer players using a multidimensional approach based on GPS measurements and machine learning. In an evolutive scenario, where a soccer club starts collecting the data for the first time and updates the predictive model as the season goes by, our approach can detect around half of the injuries, allowing the soccer club to save 70% of a season\u2019s economic costs related to injuries. The proposed approach can be a valuable support for coaches, helping the soccer club to reduce injury incidence, save money and increase team performance."}
{"_id":"cbf4040cb14a019ff3556fad5c455e99737f169f","title":"Answering Schr\u00f6dinger's question: A free-energy formulation","text":"The free-energy principle (FEP) is a formal model of neuronal processes that is widely recognised in neuroscience as a unifying theory of the brain and biobehaviour. More recently, however, it has been extended beyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay. The aim of this review is to synthesise these advances with a meta-theoretical ontology of biological systems called variational neuroethology, which integrates the FEP with Tinbergen's four research questions to explain biological systems across spatial and temporal scales. We exemplify this framework by applying it to Homo sapiens, before translating variational neuroethology into a systematic research heuristic that supplies the biological, cognitive, and social sciences with a computationally tractable guide to discovery."}
{"_id":"ab71da348979c50d33700bc2f6ddcf25b4c8cfd0","title":"Reconnaissance with ultra wideband UHF synthetic aperture radar","text":""}
{"_id":"ecb4d6621662f6c8eecfe9aab366d36146f0a6da","title":"Unseen Noise Estimation Using Separable Deep Auto Encoder for Speech Enhancement","text":"Unseen noise estimation is a key yet challenging step to make a speech enhancement algorithm work in adverse environments. At worst, the only prior knowledge we know about the encountered noise is that it is different from the involved speech. Therefore, by subtracting the components which cannot be adequately represented by a well defined speech model, the noises can be estimated and removed. Given the good performance of deep learning in signal representation, a deep auto encoder (DAE) is employed in this work for accurately modeling the clean speech spectrum. In the subsequent stage of speech enhancement, an extra DAE is introduced to represent the residual part obtained by subtracting the estimated clean speech spectrum (by using the pre-trained DAE) from the noisy speech spectrum. By adjusting the estimated clean speech spectrum and the unknown parameters of the noise DAE, one can reach a stationary point to minimize the total reconstruction error of the noisy speech spectrum. The enhanced speech signal is thus obtained by transforming the estimated clean speech spectrum back into time domain. The above proposed technique is called separable deep auto encoder (SDAE). Given the under-determined nature of the above optimization problem, the clean speech reconstruction is confined in the convex hull spanned by a pre-trained speech dictionary. New learning algorithms are investigated to respect the non-negativity of the parameters in the SDAE. Experimental results on TIMIT with 20 noise types at various noise levels demonstrate the superiority of the proposed method over the conventional baselines."}
{"_id":"488fc01e8663d67de2cb76b33167a60906b81eba","title":"A variable splitting augmented Lagrangian approach to linear spectral unmixing","text":"This paper presents a new linear hyperspectral unmixing method of the minimum volume class, termed simplex identification via split augmented Lagrangian (SISAL). Following Craig's seminal ideas, hyperspectral linear unmixing amounts to finding the minimum volume simplex containing the hyperspectral vectors. This is a nonconvex optimization problem with convex constraints. In the proposed approach, the positivity constraints, forcing the spectral vectors to belong to the convex hull of the endmember signatures, are replaced by soft constraints. The obtained problem is solved by a sequence of augmented Lagrangian optimizations. The resulting algorithm is very fast and able so solve problems far beyond the reach of the current state-of-the art algorithms. The effectiveness of SISAL is illustrated with simulated data."}
{"_id":"0d21449cba8735032af2f6dfc46e18641e6fa3d3","title":"Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation","text":"We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes."}
{"_id":"5fecf8411f1063a08af028effae9d3d208ef1101","title":"How to put probabilities on homographies","text":"We present a family of \"normal\" distributions over a matrix group together with a simple method for estimating its parameters. In particular, the mean of a set of elements can be calculated. The approach is applied to planar projective homographies, showing that using priors defined in this way improves object recognition."}
{"_id":"6d74c216d8246c2a356b00426af715102af2a172","title":"From Skimming to Reading: A Two-stage Examination Model for Web Search","text":"User's examination of search results is a key concept involved in all the click models. However, most studies assumed that eye fixation means examination and no further study has been carried out to better understand user's examination behavior. In this study, we design an experimental search engine to collect both the user's feedback on their examinations and the eye-tracking\/click-through data. To our surprise, a large proportion (45.8%) of the results fixated by users are not recognized as being \"read\". Looking into the tracking data, we found that before the user actually \"reads\" the result, there is often a \"skimming\" step in which the user quickly looks at the result without reading it. We thus propose a two-stage examination model which composes of a first \"from skimming to reading\" stage (Stage 1) and a second \"from reading to clicking\" stage (Stage 2). We found that the biases (e.g. position bias, domain bias, attractiveness bias) considered in many studies impact in different ways in Stage 1 and Stage 2, which suggests that users make judgments according to different signals in different stages. We also show that the two-stage examination behaviors can be predicted with mouse movement behavior, which can be collected at large scale. Relevance estimation with the two-stage examination model also outperforms that with a single-stage examination model. This study shows that the user's examination of search results is a complex cognitive process that needs to be investigated in greater depth and this may have a significant impact on Web search."}
{"_id":"638867fba638d4088b83e58215ac7682f1c55699","title":"Artificial Intelligence Technique Applied to Intrusion Detection","text":"Communication network is facilitated with different protocol .Each protocol supported to increase the network performance in a secured manner. In communication process, user\u2019s connectivity, violations of policy on access of information are handles through intrusion. Intrusion prevention is the process of performing intrusion detection and attempting to stop detected possible incidents. It focused on identifying possible incidents, logging information about them, attempting to stop them, and reporting them to security administrators. However, organizations use Intrusion detection and prevention system (IDPS) for other purposes, such as identifying problems with security policies, documenting existing threats, and determining individuals from violating security policies. Communication architecture is built up on IP.Internet Control Message Protocol (ICMP ) protocol is tightly integrated with IP. ICMP messages, delivered in IP packets, are used for out-of-band messages related to network operation or mis-operation.In this paper describes about HDLC , ICMP protocol sequences which is used to detect the intrusion on hybrid network and its attributes are recommend the standardized protocol for the intrusion detection process. This standardization protocol compare the previous part of the research of HDLC and ICMP protocols."}
{"_id":"ab81be51a3a96237ea24375d7b4fd0c244e5ffa8","title":"Enhanced LSB technique for audio steganography","text":"The idea of this paper is to invent a new strategy in Steganography to get the minimum effect in audio which is used to hide data into it. \u201cProgress always involves risk\u201d Fredrick Wilcox observed that technological progress of computer science and the Internet altered the way we lived, and will continue to cast our life[1] . In this paper we have presented a Steganography method of embedding text data in an audio file. The basic approach behind this paper is to provide a good, well-organized method for hiding the data and sent to the destination in safer manner. In the proposed technique first the audio file is sampled and then appropriate bit is modified. In selected sample one bit is modified at least significant bit .The remaining bits may be used but it may be cause noise. We have attempted to provide an overview, theoretical framework about audio Steganography techniques and a novel approach to hide data in an audio using least significant bit (LSB)."}
{"_id":"85d31ba076c2620fbea3c86595fb0ff6c44b1efa","title":"A stable-isotope dilution GC-MS approach for the analysis of DFRC (derivatization followed by reductive cleavage) monomers from low-lignin plant materials.","text":"The derivatization followed by reductive cleavage (DFRC) method is a well-established tool to characterize the lignin composition of plant materials. However, the application of the original procedure, especially the chromatographic determination of the DFRC monomers, is problematic for low-lignin foods. To overcome these problems a modified sample cleanup and a stable-isotope dilution approach were developed and validated. To quantitate the diacetylated DFRC monomers, their corresponding hexadeuterated analogs were synthesized and used as internal standards. By using the selected-ion monitoring mode, matrix-associated interferences can be minimized resulting in higher selectivity and sensitivity. The modified method was applied to four low-lignin samples. Lignin from carrot fibers was classified as guaiacyl-rich whereas the lignins from radish, pear, and asparagus fibers where classified as balanced lignins (guaiacyl\/syringyl ratio=1-2)."}
{"_id":"8eb62f0e3528f39e93f4073226c044235b45dce8","title":"Action Design Research and Visualization Design","text":"In applied visualization research, artifacts are shaped by a series of small design decisions, many of which are evaluated quickly and informally via methods that often go unreported and unverified. Such design decisions are influenced not only by visualization theory, but also by the people and context of the research. While existing applied visualization models support a level of reliability throughout the design process, they fail to explicitly account for the influence of the research context in shaping the resulting design artifacts. In this work, we look to action design research (ADR) for insight into addressing this issue. In particular, ADR offers a framework along with a set of guiding principles for navigating and capitalizing on the disruptive, subjective, human-centered nature of applied design work, while aiming to ensure reliability of the process and design, and emphasizing opportunities for conducting research. We explore the utility of ADR in increasing the reliability of applied visualization design research by: describing ADR in the language and constructs developed within the visualization community; comparing ADR to existing visualization methodologies; and analyzing a recent design study retrospectively through the lens of ADR's framework and principles."}
{"_id":"6cc4a3d0d8a278d30e05418afeaf6b8e5d04d3d0","title":"Econometric Modelling of Markov-Switching Vector Autoregressions using MSVAR for Ox","text":""}
{"_id":"2050e3ecf3919b05aacf53ab6bed8c004c2b6872","title":"An X-band to Ka-band SPDT switch using 200 nm SiGe HBTs","text":"This paper presents the design and measured performance of an X-band to Ka-band SiGe HBT SPDT switch. The proposed SPDT switch was fabricated using a 200 nm, 150 GHz peak fT silicon-germanium (SiGe) heterojunction bipolar transistor (HBT) BiCMOS technology. The SPDT switch design uses diode-connected SiGe HBTs in a series-shunt configuration to improve the switch bandwidth and isolation. Between 8 and 40 GHz, this SPDT switch achieves an insertion loss of less than 4.3 dB, an isolation of more than 20.3 dB, and a return loss of more than 9 dB."}
{"_id":"a2eb1abd58e1554dc6bac5a8ea2f9876e9f4d36b","title":"Measuring dimensions of intergenerational contact: factor analysis of the Queen's University Scale.","text":"OBJECTIVES\nIntergenerational contact has been linked to a range of health outcomes, including greater engagement and lower depression. Measures of contact are limited. Informed by Allport's contact theory, the Queen's University Scale consists of items rating contact with elders. We administered the survey to a young adult sample (N = 606) to identify factors that may optimize intervention programming and enhance young persons' health as they age.\n\n\nMETHODS\nWe conducted exploratory factor analysis (EFA) in the structural equation modeling framework and then confirmatory factor analysis with items pertaining to the general elder population.\n\n\nRESULTS\nEFAs did not yield an adequate factor structure. We tested two alternative confirmatory models based on findings from the EFA. Neither a second-order model nor a first-order model allowing double loadings and correlated errors proved adequate.\n\n\nCONCLUSION\nDifficulty finding an adequate factor solution reflects challenges to measuring intergenerational contact with this scale. Items reflect relevant topics but subscale models are limited in interpretability. Knox and colleagues' analyses led them to recommend a brief, global scale, but we did not find empirical support for such a measure. Next steps include development and testing of a reliable, valid scale measuring dimensions of contact as perceived by both youth and elders."}
{"_id":"699dab450bd2a34594c17a08dd01149fa2610e60","title":"An international qualitative study of ability and disability in ADHD using the WHO-ICF framework","text":"This is the third in a series of four cross-cultural empirical studies designed to develop International Classification of Functioning, Disability and Health (ICF, and Children and Youth version, ICF(-CY) Core Sets for Attention-Deficit Hyperactivity Disorder (ADHD). To explore the perspectives of individuals diagnosed with ADHD, self-advocates, immediate family members and professional caregivers on relevant areas of impairment and functional abilities typical for ADHD across the lifespan as operationalized by the ICF(-CY). A qualitative study using focus group discussions or semi-structured interviews of 76 participants, divided into 16 stakeholder groups. Participants from five countries (Brazil, India, Saudi Arabia, South Africa and Sweden) were included. A deductive qualitative content analysis was conducted to extract meaningful functioning and disability concepts from verbatim material. Extracted concepts were then linked to ICF(-CY) categories by independent researchers using a standardized linking procedure. In total, 82 ICF(-CY) categories were identified, of which 32 were related to activities and participation, 25 to environmental factors, 23 to body functions and 2 to body structures. Participants also provided opinions on experienced positive sides to ADHD. A high level of energy and drive, creativity, hyper-focus, agreeableness, empathy, and willingness to assist others were the most consistently reported strengths associated with ADHD. Stakeholder perspectives highlighted the need to appraise ADHD in a broader context, extending beyond diagnostic criteria into many areas of ability and disability as well as environmental facilitators and barriers. This qualitative study, along with three other studies (comprehensive scoping review, expert survey and clinical study), will provide the scientific basis to define ICF(-CY) Core Sets for ADHD, from which assessment tools can be derived for use in clinical and research setting, as well as in health care administration."}
{"_id":"2ec2038f229c40bd37552c26545743f02fe1715d","title":"Confidence sets for persistence diagrams","text":"Persistent homology is a method for probing topological properties of point clouds and functions. The method involves tracking the birth and death of topological features as one varies a tuning parameter. Features with short lifetimes are informally considered to be \u201ctopological noise,\u201d and those with a long lifetime are considered to be \u201ctopological signal.\u201d In this paper, we bring some statistical ideas to persistent homology. In particular, we derive confidence sets that allow us to separate topological signal from topological noise."}
{"_id":"75cbc0eec23375df69de6c64e2f48689dde417c5","title":"Enhanced Computer Vision With Microsoft Kinect Sensor: A Review","text":"With the invention of the low-cost Microsoft Kinect sensor, high-resolution depth and visual (RGB) sensing has become available for widespread use. The complementary nature of the depth and visual information provided by the Kinect sensor opens up new opportunities to solve fundamental problems in computer vision. This paper presents a comprehensive review of recent Kinect-based computer vision algorithms and applications. The reviewed approaches are classified according to the type of vision problems that can be addressed or enhanced by means of the Kinect sensor. The covered topics include preprocessing, object tracking and recognition, human activity analysis, hand gesture analysis, and indoor 3-D mapping. For each category of methods, we outline their main algorithmic contributions and summarize their advantages\/differences compared to their RGB counterparts. Finally, we give an overview of the challenges in this field and future research trends. This paper is expected to serve as a tutorial and source of references for Kinect-based computer vision researchers."}
{"_id":"cf5495cc7d7b361ca6577df62db063b0ba449c37","title":"New clock-gating techniques for low-power flip-flops","text":"Two novel low power flip-flops are presented in the paper. Proposed flip-flops use new gating techniques that reduce power dissipation deactivating the clock signal. Presented circuits overcome the clock duty-cycle limitation of previously reported gated flip-flops.\nCircuit simulations with the inclusion of parasitics show that sensible power dissipation reduction is possible if input signal has reduced switching activity. A 16-bit counter is presented as a simple low power application."}
{"_id":"e893b706c3d9e68fc978ec41fb17d757ec85ee1e","title":"Addressing the Winograd Schema Challenge as a Sequence Ranking Task","text":"The Winograd Schema Challenge targets pronominal anaphora resolution problems which require the application of cognitive inference in combination with world knowledge. These problems are easy to solve for humans but most difficult to solve for machines. Computational models that previously addressed this task rely on syntactic preprocessing and incorporation of external knowledge by manually crafted features. We address the Winograd Schema Challenge from a new perspective as a sequence ranking task, and design a Siamese neural sequence ranking model which performs significantly better than a random baseline, even when solely trained on sequences of words. We evaluate against a baseline and a state-of-the-art system on two data sets and show that anonymization of noun phrase candidates strongly helps our model to generalize."}
{"_id":"a6eee06341987faeb8b6135d00d578d0d8893162","title":"An industrial study on the risk of software changes","text":"Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and\/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects."}
{"_id":"3e1c07970fe976269ac5c9904b7b5651e8786c51","title":"Simulation Model Verification and Validation: Increasing the Users' Confidence","text":"This paper sets simulation model verification and validation (V&V) in the context of the process of performing a simulation study. Various different forms of V&V need to take place depending on the stage that has been reached. Since the phases of a study are performed in an iterative manner, so too are the various forms of V&V. A number of difficulties with verifying and validating models are discussed, after which a series of V&V methods are described. V&V is seen as a process of increasing confidence in a model, and not one of demonstrating absolute accuracy."}
{"_id":"4c2bbcb3e897e927cd390517b2036b0b9123953c","title":"BeAware! - Situation awareness, the ontology-driven way","text":"Available online 18 July 2010 Information overload is a severe problem for human operators of large-scale control systems as, for example, encountered in the domain of road traffic management. Operators of such systems are at risk to lack situation awareness, because existing systems focus on themere presentation of the available information on graphical user interfaces\u2014thus endangering the timely and correct identification, resolution, and prevention of critical situations. In recent years, ontology-based approaches to situation awareness featuring a semantically richer knowledge model have emerged. However, current approaches are either highly domain-specific or have, in case they are domain-independent, shortcomings regarding their reusability. In this paper, we present our experience gained from the development of BeAware!, a framework for ontology-driven information systems aiming at increasing anoperator's situation awareness. In contrast to existing domain-independent approaches, BeAware!'s ontology introduces the concept of spatio-temporal primitive relations betweenobserved real-world objects thereby improving the reusability of the framework. To show its applicability, a prototype of BeAware! has been implemented in thedomain of road trafficmanagement. An overviewof this prototype and lessons learned for the development of ontology-driven information systems complete our contribution. \u00a9 2010 Elsevier B.V. All rights reserved."}
{"_id":"66a7cfaca67cc69b6b08397a884e10ff374d710c","title":"Menu-Match: Restaurant-Specific Food Logging from Images","text":"Logging food and calorie intake has been shown to facilitate weight management. Unfortunately, current food logging methods are time-consuming and cumbersome, which limits their effectiveness. To address this limitation, we present an automated computer vision system for logging food and calorie intake using images. We focus on the \"restaurant\" scenario, which is often a challenging aspect of diet management. We introduce a key insight that addresses this problem specifically: restaurant plates are often both nutritionally and visually consistent across many servings. This insight provides a path to robust calorie estimation from a single RGB photograph: using a database of known food items together with restaurant-specific classifiers, calorie estimation can be achieved through identification followed by calorie lookup. As demonstrated on a challenging Menu-Match dataset and an existing third party dataset, our approach outperforms previous computer vision methods and a commercial calorie estimation app. Our Menu-Match dataset of realistic restaurant meals is made publicly available."}
{"_id":"b3d7371522a7a68137df2cb005ca9683f3436bd7","title":"Multivalued logics: a uniform approach to reasoning in artificial intelligence","text":"This paper describes a uniform formalization of much of the current work in artificial intelligence on inference systems. We show that many of these systems, including first-order theorem provers, assumption-based truth maintenance systems (ATMSS), and unimplemented formal systems such as default logic or circumscription, can be subsumed under a single general framework. We begin by defining this framework, which is based on a mathematical structure known as a bilattice. We present a formal definition of inference using this structure and show that this definition generalizes work involving ATMSS and some simple nonmonotonic logics. Following the theoretical description, we describe a constructive approach to inference in this setting; the resulting generalization of both conventional inference and ATMSS is achieved without incurring any substantial computational overhead. We show that our approach can also be used to implement a default reasoner, and discuss a combination of default and ATMS methods that enables us to formally describe an \u201cincremental\u201d default reasoning system. This incremental system does not need to perform consistency checks before drawing tentative conclusions, but can instead adjust its beliefs when a default premise or conclusion is overturned in the face of convincing contradictory evidence. The system is therefore much more computationally viable than earlier approaches. Finally, we discuss the implementation of our ideas. We begin by considering general issues that need to be addressed when implementing a multivalued approach such as that we are proposing, and then turn to specific examples showing the results of an existing implementation. This single implementation is used to solve a digital simulation task using first-order logic, a diagnostic task using ATMSS as suggested by de Kleer and Williams, a problem in default reasoning as in Reiter\u2019s default logic or McCarthy\u2019s circumscription, and to solve the same problem more efficiently by combining default methods with justification information. All of these applications use the same general-purpose bilattice theorem prover and differ only in the choice of bilattice being considered."}
{"_id":"070096ce36bba240b39b5ddb7bc6071311478843","title":"Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments","text":"In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers."}
{"_id":"21dd2790b76a57b42191b19a54505837f3969141","title":"Tuned Models of Peer Assessment in MOOCs","text":"In massive open-access online courses (MOOCs), peer grading serves as a critical tool for scaling the grading of complex, open-ended assignments to courses with tens or hundreds of thousands of students. But despite promising initial trials, it does not always deliver accurate results compared to human experts. In this paper, we develop algorithms for estimating and correcting for grader biases and reliabilities, showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from Coursera\u2019s HCI course offerings \u2014 the largest peer grading networks analysed to date. We relate grader biases and reliabilities to other student factors such as engagement, performance as well as commenting style. We also show that our model can lead to more intelligent assignment of graders to gradees."}
{"_id":"3b073bf632aa91628d134a828911ff82706b8a32","title":"The critical importance of retrieval for learning.","text":"Learning is often considered complete when a student can produce the correct answer to a question. In our research, students in one condition learned foreign language vocabulary words in the standard paradigm of repeated study-test trials. In three other conditions, once a student had correctly produced the vocabulary item, it was repeatedly studied but dropped from further testing, repeatedly tested but dropped from further study, or dropped from both study and test. Repeated studying after learning had no effect on delayed recall, but repeated testing produced a large positive effect. In addition, students' predictions of their performance were uncorrelated with actual performance. The results demonstrate the critical role of retrieval practice in consolidating learning and show that even university students seem unaware of this fact."}
{"_id":"50fcb0e5f921357b2ec96be9a75bfd3169e8f8da","title":"Personalized Online Education - A Crowdsourcing Challenge","text":"Interest in online education is surging, as dramatized by the success of Khan Academy and recent Stanford online courses, but the technology for online education is in its infancy. Crowdsourcing mechanisms will likely be essential in order to reach the full potential of this medium. This paper sketches some of the challenges and directions we hope HCOMP researchers will ad-"}
{"_id":"7545f90299a10dae1968681f6bd268b9b5ab2c37","title":"Powergrading: a Clustering Approach to Amplify Human Effort for Short Answer Grading","text":"We introduce a new approach to the machine-assisted grading of short answer questions. We follow past work in automated grading by first training a similarity metric between student responses, but then go on to use this metric to group responses into clusters and subclusters. The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as \u201cpowergrading.\u201d We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available. We show results in terms of grading progress with a small \u201cbudget\u201d of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents."}
{"_id":"e638bad00cbe2467dacc1b69876c21b776ad8d3b","title":"EARLY DEVELOPMENTS OF A PARALLELLY ACTUATED HUMANOID , SAFFIR","text":"This paper presents the design of our new 33 degree of freedom full size humanoid robot, SAFFiR (Shipboard Autonomous Fire Fighting Robot). The goal of this research project is to realize a high performance mixed force and position controlled robot with parallel actuation. The robot has two 6 DOF legs and arms, a waist, neck, and 3 DOF hands\/fingers. The design is characterized by a central lightweight skeleton actuated with modular ballscrew driven force controllable linear actuators arranged in a parallel fashion around the joints. Sensory feedback on board the robot includes an inertial measurement unit, force and position output of each actuator, as well as 6 axis force\/torque measurements from the feet. The lower body of the robot has been fabricated and a rudimentary walking algorithm implemented while the upper body fabrication is completed. Preliminary walking experiments show that parallel actuation successfully minimizes the loads through individual actuators."}
{"_id":"526f6208e5d0c9ef3eaa491b6a650110876ab574","title":"QuizRDF: search technology for the semantic Web","text":"An information-seeking system is described which combines traditional keyword querying of WWW resources with the ability to browse and query against RDF annotations of those resources. RDF(S) and RDF are used to specify and populate an ontology and the resultant RDF annotations are then indexed along with the full text of the annotated resources. The resultant index allows both keyword querying against the full text of the document and the literal values occurring in the RDF annotations, along with the ability to browse and query the ontology. We motivate our approach as a key enabler for fully exploiting the semantic Web in the area of knowledge management and argue that the ability to combine searching and browsing behaviours more fully supports a typical information-seeking task. The approach is characterised as \"low threshold, high ceiling\" in the sense that where RDF annotations exist they are exploited for an improved information-seeking experience but where they do not yet exist, a search capability is still available."}
{"_id":"cb615377f990c0b43cfa6fae3755d5a0da418b2f","title":"Squadron: Incentivizing Quality-Aware Mission-Driven Crowd Sensing","text":"Recent years have witnessed the success of mobile crowd sensing systems, which outsource sensory data collection to the public crowd equipped with various mobile devices in a wide spectrum of civilian applications. We envision that crowd sensing could as well be very useful in a whole host of mission-driven scenarios, such as peacekeeping operations, non-combatant evacuations, and humanitarian missions. However, the power of crowd sensing could not be fully unleashed in mission-driven crowd sensing (MiCS) systems, unless workers are effectively incentivized to participate. Therefore, in this paper, taking into consideration workers' diverse quality of information (QoI), we propose Squadron, a quality-aware incentive mechanism for MiCS systems. Squadron adopts the reverse auction framework. It approximately minimizes the platform's total payment for worker recruiting in a computationally efficient manner, and recruits workers who potentially could provide high quality data. Furthermore, it also satisfies the desirable properties of truth-fulness and individual rationality. Through rigorous theoretical analysis, as well as extensive simulations, we validate the various aforementioned desirable properties held by Squadron."}
{"_id":"a7b9af6fe95f0c17f85a940b1a71d1e3cdfa2109","title":"Where to Add Actions in Human-in-the-Loop Reinforcement Learning","text":"In order for reinforcement learning systems to learn quickly in vast action spaces such as the space of all possible pieces of text or the space of all images, leveraging human intuition and creativity is key. However, a human-designed action space is likely to be initially imperfect and limited; furthermore, humans may improve at creating useful actions with practice or new information. Therefore, we propose a framework in which a human adds actions to a reinforcement learning system over time to boost performance. In this setting, however, it is key that we use human effort as efficiently as possible, and one significant danger is that humans waste effort adding actions at places (states) that aren\u2019t very important. Therefore, we propose Expected Local Improvement (ELI), an automated method which selects states at which to query humans for a new action. We evaluate ELI on a variety of simulated domains adapted from the literature, including domains with over a million actions and domains where the simulated experts change over time. We find ELI demonstrates excellent empirical performance, even in settings where the synthetic \u201cexperts\u201d are quite poor."}
{"_id":"f6bb1c45e63783785e97ef99b4fe718847d10261","title":"Bad Subsequences of Well-Known Linear Congruential Pseudo-Random Number Generators","text":"We present a spectral test analysis of full-period subsequences with small step sizes generated by well-known linear congruential pseudorandom number generators. Subsequences may occur in certain simulation problems or as a method to get parallel streams of pseudorandom numbers. Applying the spectral test, it is possible to find bad subsequences with small step sizes for almost all linear pseudorandom number generators currently in use."}
{"_id":"fd4f24af30d64ca6375016249dc145b1f114ddc9","title":"An Introduction to Temporal Graphs: An Algorithmic Perspective","text":"A temporal graph is, informally speaking, a graph that changes with time. When time is discrete and only the relationships between the participating entities may change and not the entities themselves, a temporal graph may be viewed as a sequence G1, G2 . . . , Gl of static graphs over the same (static) set of nodes V . Though static graphs have been extensively studied, for their temporal generalization we are still far from having a concrete set of structural and algorithmic principles. Recent research shows that many graph properties and problems become radically different and usually substantially more difficult when an extra time dimension is added to them. Moreover, there is already a rich and rapidly growing set of modern systems and applications that can be naturally modeled and studied via temporal graphs. This, further motivates the need for the development of a temporal extension of graph theory. We survey here recent results on temporal graphs and temporal graph problems that have appeared in the Computer Science community."}
{"_id":"c4622d4a8d582c887904a9d0f2714a1dae794c1b","title":"Analytical Solution of Air-Gap Field in Permanent-Magnet Motors Taking Into Account the Effect of Pole Transition Over Slots","text":"We present an analytical method to study magnetic fields in permanent-magnet brushless motors, taking into consideration the effect of stator slotting. Our attention concentrates particularly on the instantaneous field distribution in the slot regions where the magnet pole transition passes over the slot opening. The accuracy in the flux density vector distribution in such regions plays a critical role in the prediction of the magnetic forces, i.e., the cogging torque and unbalanced magnetic pull. However, the currently available analytical solutions for calculating air-gap fields in permanent magnet motors can estimate only the distribution of the flux density component in the radial direction. Magnetic field and forces computed by the new analytical method agree well with those obtained by the finite-element method. The analytical method provides a useful tool for design and optimization of permanent-magnet motors."}
{"_id":"1e627fb686eaacaf66452f9ecad066a4311abfb4","title":"Learning to learn with the informative vector machine","text":"This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task."}
{"_id":"7a36bcc25b605394c8a61d39b6e4187653aacf98","title":"Test setup for multi-finger gripper control based on robot operating system (ROS)","text":"This paper presents the concept for a test setup to prototype control algorithms for a multi-finger gripper. The human-robot interface has to provide enough degrees of freedom (DOF) to intuitively control advanced gripper and to accomplish this task a simple sensor glove equipped with flex and force sensors has been prepared for this project. The software architecture has to support both real hardware and simulation, as well as flexible communication standards, therefore, ROS architecture was employed in this matter. Paper presents some preliminary results for using sensor glove and simulated model of the three finger gripper."}
{"_id":"a63c3f53584fd50e27ac0f2dcbe28c7361b5adff","title":"Integrated Phased Array Systems in Silicon","text":"Silicon offers a new set of possibilities and challenges for RF, microwave, and millimeter-wave applications. While the high cutoff frequencies of the SiGe heterojunction bipolar transistors and the ever-shrinking feature sizes of MOSFETs hold a lot of promise, new design techniques need to be devised to deal with the realities of these technologies, such as low breakdown voltages, lossy substrates, low-Q passives, long interconnect parasitics, and high-frequency coupling issues. As an example of complete system integration in silicon, this paper presents the first fully integrated 24-GHz eight-element phased array receiver in 0.18-\/spl mu\/m silicon-germanium and the first fully integrated 24-GHz four-element phased array transmitter with integrated power amplifiers in 0.18-\/spl mu\/m CMOS. The transmitter and receiver are capable of beam forming and can be used for communication, ranging, positioning, and sensing applications."}
{"_id":"64da24aad2e99514ab26d093c19cebec07350099","title":"Low cost Ka-band transmitter for CubeSat systems","text":"CubeSat platforms grow increasingly popular in commercial ventures as alternative solutions for global Internet networks deep space exploration, and aerospace research endeavors. Many technology companies and system engineers plan to implement small satellite systems as part of global Low Earth Orbit (LEO) inter-satellite constellations. High performing low cost hardware is of key importance in driving these efforts. This paper presents the heterodyne architecture and performance of Ka-Band Integrated Transmitter Assembly (ITA) Module, which could be implemented in nano\/microsatellite or other satellite systems as a low-cost solution for high data rate space communication systems. The module converts a 0.9 to 1.1 GHz IF input signal to deliver linear transmission of +29 dBm at 26.7 to 26.9 GHz frequency range with a built-in phase locked oscillator, integrated transmitter, polarizer, and lens corrected antenna."}
{"_id":"ed202833d8c1f5c432c1e24abf3945c2e0ef91c5","title":"Identifying Harm Events in Clinical Care through Medical Narratives","text":"Preventable medical errors are estimated to be among the leading causes of injury and death in the United States. To prevent such errors, healthcare systems have implemented patient safety and incident reporting systems. These systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care. These reports are narratives in natural language and while they provide detailed information about the situation, it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients. In this work, we present a method for identifying harm events in patient care and categorize the harm event types based on their severity level. We show that our method which is based on convolutional and recurrent networks with an attention mechanism is able to significantly improve over the existing methods on two large scale datasets of patient reports."}
{"_id":"891df42f3b1284e93128e5de23bcdf0b329700f4","title":"Quality of life in the anxiety disorders: a meta-analytic review.","text":"There has been significant interest in the impact of anxiety disorders on quality of life. In this meta-analytic review, we empirically evaluate differences in quality of life between patients with anxiety disorders and nonclinical controls. Thirty-two patient samples from 23 separate studies (N=2892) were included in the analysis. The results yielded a large effect size indicating poorer quality of life among anxiety disorder patients vs. controls and this effect was observed across all anxiety disorders. Compared to control samples, no anxiety disorder diagnosis was associated with significantly poorer overall quality of life than was any other anxiety disorder diagnosis. Examination of specific domains of QOL suggests that impairments may be particularly prominent among patients with post-traumatic stress disorder. QOL domains of mental health and social functioning were associated with the highest levels of impairment among anxiety disorder patients. These findings are discussed in the context of future research on the assessment of quality of life in the anxiety disorders."}
{"_id":"56ca21e44120ee31dcc2c7fd963a3567a037ca6f","title":"An Investigation into the Use of Common Libraries in Android Apps","text":"The packaging model of Android apps requires the entire code necessary for the execution of an app to be shipped into one single apk file. Thus, an analysis of Android apps often visits code which is not part of the functionality delivered by the app. Such code is often contributed by the common libraries which are used pervasively by all apps. Unfortunately, Android analyses, e.g., for piggybacking detection and malware detection, can produce inaccurate results if they do not take into account the case of library code, which constitute noise in app features. Despite some efforts on investigating Android libraries, the momentum of Android research has not yet produced a complete set of common libraries to further support in-depth analysis of Android apps. In this paper, we leverage a dataset of about 1.5 million apps from Google Play to harvest potential common libraries, including advertisement libraries. With several steps of refinements, we finally collect by far the largest set of 1,113 libraries supporting common functionality and 240 libraries for advertisement. We use the dataset to investigates several aspects of Android libraries, including their popularity and their proportion in Android app code. Based on these datasets, we have further performed several empirical investigations to confirm the motivations behind our work."}
{"_id":"6b13a159caebd098aa0f448ae24e541e58319a64","title":"' s personal copy Core , animal reminder , and contamination disgust : Three kinds of disgust with distinct personality , behavioral , physiological , and clinical correlates","text":"We examined the relationships between sensitivity to three kinds of disgust (core, animalreminder, and contamination) and personality traits, behavioral avoidance, physiological responding, and anxiety disorder symptoms. Study 1 revealed that these disgusts are particularly associated with neuroticism and behavioral inhibition. Moreover, the three disgusts showed a theoretically consistent pattern of relations on four disgust-relevant behavioral avoidance tasks in Study 2. Similar results were found in Study 3 such that core disgust was significantly related to increased physiological responding during exposure to vomit, while animal-reminder disgust was specifically related to physiological responding during exposure to blood. Lastly, Study 4 revealed that each of the three disgusts showed a different pattern of relations with fear of contamination, fear of animals, and fear of blood\u2013 injury relevant stimuli. These findings provide support for the convergent and divergent validity of core, animal-reminder, and contamination disgust. These findings also highlight the possibility that the three kinds of disgust may manifest as a function of different psychological mechanisms (i.e., oral incorporation, mortality defense, disease avoidance) that may give rise to different clinical conditions. However, empirical examination of the mechanisms that underlie the three disgusts will require further refinement of the psychometric properties of the disgust scale. 2008 Elsevier Inc. All rights reserved."}
{"_id":"38732356b452e098d30026d036622461c6e8a3f5","title":"A primer on spatial modeling and analysis in wireless networks","text":"The performance of wireless networks depends critically on their spatial configuration, because received signal power and interference depend critically on the distances between numerous transmitters and receivers. This is particularly true in emerging network paradigms that may include femtocells, hotspots, relays, white space harvesters, and meshing approaches, which are often overlaid with traditional cellular networks. These heterogeneous approaches to providing high-capacity network access are characterized by randomly located nodes, irregularly deployed infrastructure, and uncertain spatial configurations due to factors like mobility and unplanned user-installed access points. This major shift is just beginning, and it requires new design approaches that are robust to spatial randomness, just as wireless links have long been designed to be robust to fading. The objective of this article is to illustrate the power of spatial models and analytical techniques in the design of wireless networks, and to provide an entry-level tutorial."}
{"_id":"72d078429890dcf213d5e959d21fb84adc99d4df","title":"Digital Literacy: A Conceptual Framework for Survival Skills in the Digital Era","text":"Digital literacy involves more than the mere ability to use software or operate a digital device; it includes a large variety of complex cognitive, motor, sociological, and emotional skills, which users need in order to function effectively in digital environments. The tasks required in this context include, for example, \u201creading\u201d instructions from graphical displays in user interfaces; using digital reproduction to create new, meaningful materials from existing ones; constructing knowledge from a nonlinear, hypertextual navigation; evaluating the quality and validity of information; and have a mature and realistic understanding of the \u201crules\u201d that prevail in the cyberspace. This newly emerging concept of digital literacy may be used as a measure of the quality of learners\u2019 work in digital environments, and provide scholars and developers with a more effective means of communication in designing better user-oriented environments. This article proposes a holistic, refined conceptual framework for digital literacy, which includes photo-visual literacy; reproduction literacy; branching literacy; information literacy; and socioemotional literacy."}
{"_id":"5ed7b57af3976e635a08f4dc13bb2a2aca760dbf","title":"A 0.003 mm$^{2}$ 10 b 240 MS\/s 0.7 mW SAR ADC in 28 nm CMOS With Digital Error Correction and Correlated-Reversed Switching","text":"This paper describes a single-channel, calibration-free Successive-Approximation-Register (SAR) ADC with a resolution of 10 bits at 240 MS\/s. A DAC switching technique and an addition-only digital error correction technique based on the non-binary search are proposed to tackle the static and dynamic non-idealities attributed to capacitor mismatch and insufficient DAC settling. The conversion speed is enhanced, and the power and area of the DAC are also reduced by 40% as a result. In addition, a switching scheme lifting the input common mode of the comparator is proposed to further enhance the speed. Moreover, the comparator employs multiple feedback paths for an enhanced regeneration strength to alleviate the metastable problem. Occupying an active area of 0.003 mm 2 and dissipating 0.68 mW from 1 V supply at 240 MS\/s in 28 nm CMOS, the proposed design achieves an SNDR of 57 dB with low-frequency inputs and 53 dB at the Nyquist input. This corresponds to a conversion efficiency of 4.8 fJ\/c.-s. and 7.8 fJ\/c.-s. respectively. The DAC switching technique improves the INL and DNL from +1.15\/-1.01 LSB and +0.92\/-0.28 LSB to within +0.55\/-0.45 LSB and +0.45\/-0.23 LSB, respectively. This ADC is at least 80% smaller and 32% more power efficient than reported state-of-the-art ADCs of similar resolutions and Nyquist bandwidths larger than 75 MHz."}
{"_id":"39339e14ae221cd154354ec1d30d23c8681348c5","title":"Adhesive capsulitis: review of imaging findings, pathophysiology, clinical presentation, and treatment options","text":"Adhesive capsulitis, commonly referred to as \u201cfrozen shoulder,\u201d is a debilitating condition characterized by progressive pain and limited range of motion about the glenohumeral joint. It is a condition that typically affects middle-aged women, with some evidence for an association with endocrinological, rheumatological, and autoimmune disease states. Management tends to be conservative, as most cases resolve spontaneously, although a subset of patients progress to permanent disability. Conventional arthrographic findings include decreased capsular distension and volume of the axillary recess when compared with the normal glenohumeral joint, in spite of the fact that fluoroscopic visualization alone is rarely carried out today in favor of magnetic resonance imaging (MRI). MRI and MR arthrography (MRA) have, in recent years, allowed for the visualization of several characteristic signs seen with this condition, including thickening of the coracohumeral ligament, axillary pouch and rotator interval joint capsule, in addition to the obliteration of the subcoracoid fat triangle. Additional findings include T2 signal hyperintensity and post-contrast enhancement of the joint capsule. Similar changes are observable on ultrasound. However, the use of ultrasound is most clearly established for image-guided injection therapy. More aggressive therapies, including arthroscopic release and open capsulotomy, may be indicated for refractory disease, with arthroscopic procedures favored because of their less invasive nature and relatively high success rate."}
{"_id":"21d6baca37dbcb35cf263dd93ce306f6013a1ff5","title":"Issues in building general letter to sound rules","text":"In generaltext-to-speechsystems, it is notpossibleto guaranteethat a lexiconwill containall wordsfoundin a text, thereforesomesystemfor predictingpronunciationfrom theword itself is necessary . Herewe presenta generalframework for building letter to sound (LTS) rulesfrom a word list in a language.The techniquecanbe fully automatic,thougha small amountof handseedingcangive betterresults.We have appliedthis techniqueto English(UK and US), Frenchand German. The generatedmodelsachieve, 75%, 58%, 93% and89%, respecti vely, wordscorrectfor held out data from theword lists. To testour modelson moretypical datawe alsoanalyzedgeneral text, to find which wordsdo not appearin our lexicon. Theseunknown wordswereusedasamorerealistictestcorpusfor ourmodels. We also discussthe distribution and type of suchunknown words."}
{"_id":"d7e58d05232ed02d4bffba943e4523706971913b","title":"Telecommunication Fraud Detection Using Data Mining techniques","text":"This document presents the final report of the thesis \u201cTelecommunication Fraud Detection Using Data Mining Techniques\u201d, were a study is made over the effect of the unbalanced data, generated by the Telecommunications Industry, in the construction and performance of classifiers that allows the detection and prevention of frauds. In this subject, an unbalanced data set is characterized by an uneven class distribution where the amount of fraudulent instances (positive) is substantially smaller than the amount normal instances (negative). This will result in a classifier which is most likely to classify data has belonging to the normal class then to the fraud class. At first, an overall inspection is made over the data characteristics and the Naive Bayes model, which is the classifier selected to do the anomaly detection on these experiments. After the characteristics are presented, a feature engineering stage is done with the intent to extend the information contained in the data creating a depper relation with the data itself and model characteristics. A previously proposed solution that consists on undersampling the most abundant class (normal) before building the model, is presented and tested. In the end, the new proposals are presented. The first proposal is to study the effects of changing the intrinsic class distribution parameter in the Naive Bayes model and evaluate its performance. The second proposal consists in estimating margin values that when applied to the model output, attempt to bring more positive instances from previous negative classification. All of these suggested models are validated over a monte-carlo experiment, using data with and without the engineered features."}
{"_id":"8124c8f871c400dcbdba87aeb16938c86b068688","title":"An Analysis of Buck Converter Efficiency in PWM \/ PFM Mode with Simulink","text":"This technical paper takes a study into efficiency comparison between PWM and PFM control modes in DC-DC buck converters. Matlab Simulink Models are built to facilitate the analysis of various effects on power loss and converting efficiency, including different load conditions, gate switching frequency, setting of voltage and current thresholds, etc. From efficiency vs. load graph, a best switching frequency is found to achieve a good efficiency throughout the wide load range. This simulation point is then compared to theoretical predictions, justifying the effectiveness of computer based simulation. Efficiencies at two different control modes are compared to verify the improvement of PFM scheme."}
{"_id":"25523cc0bbe43885f0247398dcbf3aecf9538ce2","title":"Robust Unit Commitment Problem with Demand Response and Wind Energy","text":"To improve the efficiency in power generation and to reduce the greenhouse gas emission, both Demand Response (DR) strategy and intermittent renewable energy have been proposed or applied in electric power systems. However, the uncertainty and the generation pattern in wind farms and the complexity of demand side management pose huge challenges in power system operations. In this paper, we analytically investigate how to integrate DR and wind energy with fossil fuel generators to (i) minimize power generation cost; (2) fully take advantage wind energy with managed demand to reduce greenhouse emission. We first build a two-stage robust unit commitment model to obtain day-ahead generator schedules where wind uncertainty is captured by a polyhedron. Then, we extend our model to include DR strategy such that both price levels and generator schedule will be derived for the next day. For these two NP-hard problems, we derive their mathematical properties and develop a novel and analytical solution method. Our computational study on a IEEE 118 system with 36 units shows that (i) the robust unit commitment model can significantly reduce total cost and fully make use of wind energy; (ii) the cutting plane method is computationally superior to known algorithms."}
{"_id":"2e478ae969db8ff0b23e309a9ce46bcaf20c36b3","title":"Learner Modeling for Integration Skills","text":"Complex skill mastery requires not only acquiring individual basic component skills, but also practicing integrating such basic skills. However, traditional approaches to knowledge modeling, such as Bayesian knowledge tracing, only trace knowledge of each decomposed basic component skill. This risks early assertion of mastery or ineffective remediation failing to address skill integration. We introduce a novel integration-level approach to model learners' knowledge and provide fine-grained diagnosis: a Bayesian network based on a new kind of knowledge graph with progressive integration skills. We assess the value of such a model from multifaceted aspects: performance prediction, parameter plausibility, expected instructional effectiveness, and real-world recommendation helpfulness. Our experiments based on a Java programming tutor show that proposed model significantly improves two popular multiple-skill knowledge tracing models on all these four aspects."}
{"_id":"831ed2a5f40861866b4ebfe60257b997701e38e2","title":"ESPRIT-estimation of signal parameters via rotational invariance techniques","text":"High-resolution signal parameter estimation is a problem of significance in many signal processing applications. Such applications include direction-of-arrival (DOA) estimation, system identification, and time series analysis. A novel approach to the general problem of signal parameter estimation is described. Although discussed in the context of direction-of-arrival estimation, ESPRIT can be applied to a wide variety of problems including accurate detection and estimation of sinusoids in noise. It exploits an underlying rotational invariance among signal subspaces induced by an array of sensors with a translational invariance structure. The technique, when applicable, manifests significant performance and computational advantages over previous algorithms such as MEM, Capon's MLM, and MUSIC."}
{"_id":"65f55691cc3bad6ca224e2144083c9deb2b2cd1d","title":"A Look into 30 Years of Malware Development from a Software Metrics Perspective","text":"During the last decades, the problem of malicious and unwanted software (malware) has surged in numbers and sophistication. Malware plays a key role in most of today\u2019s cyber attacks and has consolidated as a commodity in the underground economy. In this work, we analyze the evolution of malware since the early 1980s to date from a software engineering perspective. We analyze the source code of 151 malware samples and obtain measures of their size, code quality, and estimates of the development costs (effort, time, and number of people). Our results suggest an exponential increment of nearly one order of magnitude per decade in aspects such as size and estimated effort, with code quality metrics similar to those of regular software. Overall, this supports otherwise confirmed claims about the increasing complexity of malware and its production progressively becoming an industry."}
{"_id":"63138eee4572ffee1d3395866da69c61be453a26","title":"Driver fatigue detection based on eye tracking and dynamk, template matching","text":"A vision-based real-time driver fatigue detection system is proposed for driving safely. The driver's face is located, from color images captured in a car, by using the characteristic of skin colors. Then, edge detection is used to locate the regions of eyes. In addition to being used as the dynamic templates for eye tracking in the next frame, the obtained eyes' images are also used for fatigue detection in order to generate some warning alarms for driving safety. The system is tested on a Pentium III 550 CPU with 128 MB RAM. The experiment results seem quite encouraging andpromising. The system can reach 20 frames per second for eye tracking, and the average correct rate for eye location and tracking can achieve 99.1% on four test videos. The correct rate for fatigue detection is l00%, but the average precision rate is 88.9% on the test videos."}
{"_id":"4e3a22ed94c260b9143eee9fdf6d5d6e892ecd8f","title":"A Performance Evaluation of Local Descriptors","text":""}
{"_id":"f8c90c6549b97934da4fcdafe0012cea95cc443c","title":"State-of-the-Art Predictive Maintenance Techniques*","text":"This paper discusses the limitations of time-based equipment maintenance methods and the advantages of predictive or online maintenance techniques in identifying the onset of equipment failure. The three major predictive maintenance techniques, defined in terms of their source of data, are described as follows: 1) the existing sensor-based technique; 2) the test-sensor-based technique (including wireless sensors); and 3) the test-signal-based technique (including the loop current step response method, the time-domain reflectrometry test, and the inductance-capacitance-resistance test). Examples of detecting blockages in pressure sensing lines using existing sensor-based techniques and of verifying calibration using existing-sensor direct current output are given. Three Department of Energy (DOE)-sponsored projects, whose aim is to develop online and wireless hardware and software systems for performing predictive maintenance on critical equipment in nuclear power plants, DOE research reactors, and general industrial applications, are described."}
{"_id":"0bc46478051356455facc79f216a00b896c2dc5f","title":"ORTHONORMAL BASES OF COMPACTLY SUPPORTED WAVELETS","text":"We construct orthonormal bases of compactly supported wavelets, with arbitrarily high regularity. The order of regularity increases linearly with the support width. We start by reviewing the concept of multiresolution analysis as well as several algorithms in vision decomposition and reconstruction. The construction then follows from a synthesis of these different approaches."}
{"_id":"877091fdec0c58d28b7d81094bc73e135a63fe60","title":"A quantitative analysis of the speedup factors of FPGAs over processors","text":"The speedup over a microprocessor that can be achieved by implementing some programs on an FPGA has been extensively reported. This paper presents an analysis, both quantitative and qualitative, at the architecture level of the components of this speedup. Obviously, the spatial parallelism that can be exploited on the FPGA is a big component. By itself, however, it does not account for the whole speedup.In this paper we experimentally analyze the remaining components of the speedup. We compare the performance of image processing application programs executing in hardware on a Xilinx Virtex E2000 FPGA to that on three general-purpose processor platforms: MIPS, Pentium III and VLIW. The question we set out to answer is what is the inherent advantage of a hardware implementation over a von Neumann platform. On the one hand, the clock frequency of general-purpose processors is about 20 times that of typical FPGA implementations. On the other hand, the iteration level parallelism on the FPGA is one to two orders of magnitude that on the CPUs. In addition to these two factors, we identify the efficiency advantage of FPGAs as an important factor and show that it ranges from 6 to 47 on our test benchmarks. We also identify some of the components of this factor: the streaming of data from memory, the overlap of control and data flow and the elimination of some instruction on the FPGA. The results provide a deeper understanding of the tradeoff between system complexity and performance when designing Configurable SoC as well as designing software for CSoC. They also help understand the one to two orders of magnitude in speedup of FPGAs over CPU after accounting for clock frequencies."}
{"_id":"11385b86db4e49d85abaf59940c536aa17ff44b0","title":"Dynamic program slicing methods","text":"A dynamic program slice is that part of a program that \u2018\u2018affects\u2019\u2019 the computation of a variable of interest during program execution on a specific program input. Dynamic program slicing refers to a collection of program slicing methods that are based on program execution and may significantly reduce the size of a program slice because run-time information, collected during program execution, is used to compute program slices. Dynamic program slicing was originally proposed only for program debugging, but its application has been extended to program comprehension, software testing, and software maintenance. Different types of dynamic program slices, together with algorithms to compute them, have been proposed in the literature. In this paper we present a classification of existing dynamic slicing methods and discuss the algorithms to compute dynamic slices. In the second part of the paper, we compare the existing methods of dynamic slice computation. \uf6d9 1998 Elsevier Science B.V. All rights reserved."}
{"_id":"090d25f94cb021bdd3400a2f547f989a6a5e07ec","title":"Direct least squares fitting of ellipses","text":"This work presents a new efficient method for fitting ellipses to scattered data. Previous algorithms either fitted general conics or were computationally expensive. By minimizing the algebraic distance subject to the constraint 4 2 1 the new method incorporates the ellipticity constraint into the normalization factor. The new method combines several advantages: (i) It is ellipse-specific so that even bad data will always return an ellipse; (ii) It can be solved naturally by a generalized eigensystem and (iii) it is extremely robust, efficient and easy to implement. We compare the proposed method to other approaches and show its robustness on several examples in which other non-ellipse-specific approaches would fail or require computationally expensive iterative refinements. Source code for the algorithm is supplied and a demonstration is available on ! \" ! $#% '& () \"*) & +, .\/10 0 32, . 4) \"*) \"*) % 5* 0"}
{"_id":"4c4387afaeadda64d8183d7aba19574a9b757a6a","title":"SUITOR: an attentive information system","text":"Attentive systems pay attention to what users do so that they can attend to what users need. Such systems track user behavior, model user interests, and anticipate user desires and actions. Because the general class of attentive systems is broad \u2014 ranging from human butlers to web sites that profile users \u2014 we have focused specifically on attentive information systems, which observe user actions with information resources, model user information states, and suggest information that might be helpful to users. In particular, we describe an implemented system, Simple User Interest Tracker (Suitor), that tracks computer users through multiple channels \u2014 gaze, web browsing, application focus \u2014 to determine their interests and to satisfy their information needs. By observing behavior and modeling users, Suitor finds and displays potentially relevant information that is both timely and non-disruptive to the users' ongoing activities."}
{"_id":"002aaf4412f91d0828b79511f35c0863a1a32c47","title":"A real-time face tracker","text":"We present a real-time face tracker in this paper The system has achieved a rate of 30% frameshecond using an HP-9000 workstation with a framegrabber and a Canon VC-CI camera. It can track a person 'sface while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, we present a stochastic model to characterize skin-color distributions of human faces. The information provided by the model is sufJicient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second, a motion model e's used to estimate image motion and to predict search window. Third, a camera model is used toprediet and to compensate for camera motion. The system can be applied to tele-conferencing and many HCI applications including lip-reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand."}
{"_id":"36bb4352891209ba0a7df150c74cd4db6d603ca5","title":"Single Image Super-Resolution via Multiple Mixture Prior Models","text":"Example learning-based single image super-resolution (SR) is a promising method for reconstructing a high-resolution (HR) image from a single-input low-resolution (LR) image. Lots of popular SR approaches are more likely either time-or space-intensive, which limit their practical applications. Hence, some research has focused on a subspace view and delivered state-of-the-art results. In this paper, we utilize an effective way with mixture prior models to transform the large nonlinear feature space of LR images into a group of linear subspaces in the training phase. In particular, we first partition image patches into several groups by a novel selective patch processing method based on difference curvature of LR patches, and then learning the mixture prior models in each group. Moreover, different prior distributions have various effectiveness in SR, and in this case, we find that student-t prior shows stronger performance than the well-known Gaussian prior. In the testing phase, we adopt the learned multiple mixture prior models to map the input LR features into the appropriate subspace, and finally reconstruct the corresponding HR image in a novel mixed matching way. Experimental results indicate that the proposed approach is both quantitatively and qualitatively superior to some state-of-the-art SR methods."}
{"_id":"7cecad0af237dcd7760364e1ae0d03dea362d7cd","title":"Pharmacogenetics and adverse drug reactions","text":"Polymorphisms in the genes that code for drug-metabolising enzymes, drug transporters, drug receptors, and ion channels can affect an individual's risk of having an adverse drug reaction, or can alter the efficacy of drug treatment in that individual. Mutant alleles at a single gene locus are the best studied individual risk factors for adverse drug reactions, and include many genes coding for drug-metabolising enzymes. These genetic polymorphisms of drug metabolism produce the phenotypes of \"poor metabolisers\" or \"ultrarapid metabolisers\" of numerous drugs. Together, such phenotypes make up a substantial proportion of the population. Pharmacogenomic techniques allow efficient analysis of these risk factors, and genotyping tests have the potential to optimise drug therapy in the future."}
{"_id":"794435fe025ac480dbdb6218866e1d30d5a786c8","title":"High performance work systems: the gap between policy and practice in health care reform.","text":"PURPOSE\nStudies of high-performing organisations have consistently reported a positive relationship between high performance work systems (HPWS) and performance outcomes. Although many of these studies have been conducted in manufacturing, similar findings of a positive correlation between aspects of HPWS and improved care delivery and patient outcomes have been reported in international health care studies. The purpose of this paper is to bring together the results from a series of studies conducted within Australian health care organisations. First, the authors seek to demonstrate the link found between high performance work systems and organisational performance, including the perceived quality of patient care. Second, the paper aims to show that the hospitals studied do not have the necessary aspects of HPWS in place and that there has been little consideration of HPWS in health system reform.\n\n\nDESIGN\/METHODOLOGY\/APPROACH\nThe paper draws on a series of correlation studies using survey data from hospitals in Australia, supplemented by qualitative data collection and analysis. To demonstrate the link between HPWS and perceived quality of care delivery the authors conducted regression analysis with tests of mediation and moderation to analyse survey responses of 201 nurses in a large regional Australian health service and explored HRM and HPWS in detail in three casestudy organisations. To achieve the second aim, the authors surveyed human resource and other senior managers in all Victorian health sector organisations and reviewed policy documents related to health system reform planned for Australia.\n\n\nFINDINGS\nThe findings suggest that there is a relationship between HPWS and the perceived quality of care that is mediated by human resource management (HRM) outcomes, such as psychological empowerment. It is also found that health care organisations in Australia generally do not have the necessary aspects of HPWS in place, creating a policy and practice gap. Although the chief executive officers of health service organisations reported high levels of strategic HRM, the human resource and other managers reported a distinct lack of HPWS from their perspectives. The authors discuss why health care organisations may have difficulty in achieving HPWS.\n\n\nORIGINALITY\/VALUE\nLeaders in health care organisations should focus on ensuring human resource management systems, structures and processes that support HPWS. Policy makers need to consider HPWS as a necessary component of health system reform. There is a strong need to reorient organisational human resource management policies and procedures in public health care organisations towards high performing work systems."}
{"_id":"da2a956676b59b5237bde62a308fa604215d6a55","title":"Analysis and design of HBT Cherry-Hooper amplifiers with emitter-follower feedback for optical communications","text":"In this article, the large-signal, small-signal, and noise performance of the Cherry-Hooper amplifier with emitter-follower feedback are analyzed from a design perspective. A method for choosing the component values to obtain a low group delay distortion or Bessel transfer function is given. The design theory is illustrated with an implementation of the circuit in a 47-GHz SiGe process. The amplifier has 19.7-dB gain, 13.7-GHz bandwidth, and \/spl plusmn\/10-ps group delay distortion. The amplifier core consumes 34 mW from a -3.3-V supply."}
{"_id":"d5673c53b3643372dd8d35136769ecd73a6dede3","title":"A Deep Learning Framework for Smart Street Cleaning","text":"Conventional street cleaning methods include street sweepers going to various spots in the city and manually verifying if the street needs cleaning and taking action if required. However, this method is not optimized and demands a huge investment in terms of time and money. This paper introduces an automated framework which addresses street cleaning problem in a better way by making use of modern equipment with cameras and computational techniques to analyze, find and efficiently schedule clean-up crews for the areas requiring more attention. Deep learning-based neural network techniques can be used to achieve better accuracy and performance for object detection and classification than conventional machine learning algorithms for large volume of images. The proposed framework for street cleaning leverages the deep learning algorithm pipeline to analyze the street photographs and determines if the streets are dirty by detecting litter objects. The pipeline further determines the the degree to which the streets are littered by classifying the litter objects detected in earlier stages. The framework also provides information on the cleanliness status of the streets on a dashboard updated in real-time. Such framework can prove effective in reducing resource consumption and overall operational cost involved in street cleaning."}
{"_id":"189a391b217387514bfe599a0b6c1bbc1ccc94bb","title":"A New Paradigm for Collision-free Hashing: Incrementality at Reduced Cost","text":"We present a simple, new paradigm for the design of collision-free hash functions. Any function emanating from this paradigm is incremental. (This means that if a message x which I have previously hashed is modi ed to x0 then rather than having to re-compute the hash of x 0 from scratch, I can quickly \\update\" the old hash value to the new one, in time proportional to the amount of modi cation made in x to get x.) Also any function emanating from this paradigm is parallelizable, useful for hardware implementation. We derive several speci c functions from our paradigm. All use a standard hash function, assumed ideal, and some algebraic operations. The rst function, MuHASH, uses one modular multiplication per block of the message, making it reasonably e cient, and signi cantly faster than previous incremental hash functions. Its security is proven, based on the hardness of the discrete logarithm problem. A second function, AdHASH, is even faster, using additions instead of multiplications, with security proven given either that approximation of the length of shortest lattice vectors is hard or that the weighted subset sum problem is hard. A third function, LtHASH, is a practical variant of recent lattice based functions, with security proven based, again on the hardness of shortest lattice vector approximation. Dept. of Computer Science & Engineering, University of California at San Diego, 9500 Gilman Drive, La Jolla, California 92093, USA. E-Mail: mihir@cs.ucsd.edu. URL: http:\/\/www-cse.ucsd.edu\/users\/mihir. Supported in part by NSF CAREER Award CCR-9624439 and a Packard Foundation Fellowship in Science and Engineering. yMIT Laboratory for Computer Science, 545 Technology Square, Cambridge, MA 02139, USA. E-Mail: miccianc@theory.lcs.mit.edu. Supported in part by DARPA contract DABT63-96-C-0018."}
{"_id":"185aa5c29a1bce3cd5c806cd68c6518b65ba3d75","title":"Bayesian Rose Trees","text":"Hierarchical structure is ubiquitous in data across many domains. There are many hierarchical clustering methods, frequently used by domain experts, which strive to discover this structure. However, most of these methods limit discoverable hierarchies to those with binary branching structure. This limitation, while computationally convenient, is often undesirable. In this paper we explore a Bayesian hierarchical clustering algorithm that can produce trees with arbitrary branching structure at each node, known as rose trees. We interpret these trees as mixtures over partitions of a data set, and use a computationally efficient, greedy agglomerative algorithm to find the rose trees which have high marginal likelihood given the data. Lastly, we perform experiments which demonstrate that rose trees are better models of data than the typical binary trees returned by other hierarchical clustering algorithms."}
{"_id":"940173d9b880defde6c5f171579cddbba8288dc6","title":"Digital Business Strategy and Value Creation: Framing the Dynamic Cycle of Control Points","text":"Within changing value networks, the profits and competitive advantages of participation reside dynamically at control points that are the positions of greatest value and\/or power. The enterprises that hold these positions have a great deal of control over how the network operates, how the benefits are redistributed, and how this influences the execution of a digital business strategy. This article is based on a field study that provides preliminary, yet promising, empirical evidence that sheds light on the dynamic cycle of value creation and value capture points in digitally enabled networks in response to triggers related to technology and business strategy. We use the context of the European and U.S. broadcasting industry. Specifically, we illustrate how incremental innovations may shift value networks from static, vertically integrated networks to more loosely coupled networks, and how cross-boundary industry disruptions may then, in turn, shift those to two-sided markets. Based on our analysis we provide insights and implications for digital business strategy research and practice."}
{"_id":"f39d45aeaf8a5ace793fb8c8495eb0c7598f5e23","title":"Attitude Detection for One-Round Conversation: Jointly Extracting Target-Polarity Pairs","text":"We tackle Attitude Detection, which we define as the task of extracting the replier's attitude, i.e., a target-polarity pair, from a given one-round conversation. While previous studies considered Target Extraction and Polarity Classification separately, we regard them as subtasks of Attitude Detection. Our experimental results show that treating the two subtasks independently is not the optimal solution for Attitude Detection, as achieving high performance in each subtask is not sufficient for obtaining correct target-polarity pairs. Our jointly trained model AD-NET substantially outperforms the separately trained models by alleviating the target-polarity mismatch problem. Moreover, we proposed a method utilising the attitude detection model to improve retrieval-based chatbots by re-ranking the response candidates with attitude features. Human evaluation indicates that with attitude detection integrated, the new responses to the sampled queries from are statistically significantly more consistent, coherent, engaging and informative than the original ones obtained from a commercial chatbot."}
{"_id":"b81ef3e2185b79843fce53bc682a6f9fa44f2485","title":"Robust misinterpretation of confidence intervals.","text":"Null hypothesis significance testing (NHST) is undoubtedly the most common inferential technique used to justify claims in the social sciences. However, even staunch defenders of NHST agree that its outcomes are often misinterpreted. Confidence intervals (CIs) have frequently been proposed as a more useful alternative to NHST, and their use is strongly encouraged in the APA Manual. Nevertheless, little is known about how researchers interpret CIs. In this study, 120 researchers and 442 students-all in the field of psychology-were asked to assess the truth value of six particular statements involving different interpretations of a CI. Although all six statements were false, both researchers and students endorsed, on average, more than three statements, indicating a gross misunderstanding of CIs. Self-declared experience with statistics was not related to researchers' performance, and, even more surprisingly, researchers hardly outperformed the students, even though the students had not received any education on statistical inference whatsoever. Our findings suggest that many researchers do not know the correct interpretation of a CI. The misunderstandings surrounding p-values and CIs are particularly unfortunate because they constitute the main tools by which psychologists draw conclusions from data."}
{"_id":"a9267dd3407e346aa08cd4018fbde92018230eb3","title":"Low-complexity image denoising based on statistical modeling of wavelet coefficients","text":"We introduce a simple spatially adaptive statistical model for wavelet image coefficients and apply it to image denoising. Our model is inspired by a recent wavelet image compression algorithm, the estimation-quantization (EQ) coder. We model wavelet image coefficients as zero-mean Gaussian random variables with high local correlation. We assume a marginal prior distribution on wavelet coefficients variances and estimate them using an approximate maximum a posteriori probability rule. Then we apply an approximate minimum mean squared error estimation procedure to restore the noisy wavelet image coefficients. Despite the simplicity of our method, both in its concept and implementation, our denoising results are among the best reported in the literature."}
{"_id":"aead2b2d26ab62f6229048b912b7ad313187008c","title":"Crowdlet: Optimal worker recruitment for self-organized mobile crowdsourcing","text":"In this paper, we advocate Crowdlet, a novel self-organized mobile crowdsourcing paradigm, in which a mobile task requester can proactively exploit a massive crowd of encountered mobile workers at real-time for quick and high-quality results. We present a comprehensive system model of Crowdlet that defines task, worker arrival and worker ability models. Further, we introduce a service quality concept to indicate the expected service gain that a requester can enjoy when he recruits an encountered worker, by jointly taking into account worker ability, real-timeness and task reward. Based on the models, we formulate an online worker recruitment problem to maximize the expected sum of service quality. We derive an optimal worker recruitment policy through the dynamic programming principle, and show that it exhibits a nice threshold based structure. We conduct extensive performance evaluation based on real traces, and numerical results demonstrate that our policy can achieve superior performance and improve more than 30% performance gain over classic policies. Besides, our Android prototype shows that Crowdlet is cost-efficient, requiring less than 7 seconds and 6 Joule in terms of time and energy cost for policy computation in most cases."}
{"_id":"f39322a314bf96dbaf482b7f86fdf99853fca468","title":"The population genetics of the Jewish people","text":"Adherents to the Jewish faith have resided in numerous geographic locations over the course of three millennia. Progressively more detailed population genetic analysis carried out independently by multiple research groups over the past two decades has revealed a pattern for the population genetic architecture of contemporary Jews descendant from globally dispersed Diaspora communities. This pattern is consistent with a major, but variable component of shared Near East ancestry, together with variable degrees of admixture and introgression from the corresponding host Diaspora populations. By combining analysis of monoallelic markers with recent genome-wide variation analysis of simple tandem repeats, copy number variations, and single-nucleotide polymorphisms at high density, it has been possible to determine the relative contribution of sex-specific migration and introgression to map founder events and to suggest demographic histories corresponding to western and eastern Diaspora migrations, as well as subsequent microevolutionary events. These patterns have been congruous with the inferences of many, but not of all historians using more traditional tools such as archeology, archival records, linguistics, comparative analysis of religious narrative, liturgy and practices. Importantly, the population genetic architecture of Jews helps to explain the observed patterns of health and disease-relevant mutations and phenotypes which continue to be carefully studied and catalogued, and represent an important resource for human medical genetics research. The current review attempts to provide a succinct update of the more recent developments in a historical and human health context."}
{"_id":"2ab66c0f02521f4e5d42a6b29b08eb815504818b","title":"Learning deep physiological models of affect","text":"More than 15 years after the early studies in Affective Computing (AC), [1] the problem of detecting and modeling emotions in the context of human-computer interaction (HCI) remains complex and largely unexplored. The detection and modeling of emotion is, primarily, the study and use of artificial intelligence (AI) techniques for the construction of computational models of emotion. The key challenges one faces when attempting to model emotion [2] are inherent in the vague definitions and fuzzy boundaries of emotion, and in the modeling methodology followed. In this context, open research questions are still present in all key components of the modeling process. These include, first, the appropriateness of the modeling tool employed to map emotional manifestations and responses to annotated affective states; second, the processing of signals that express these manifestations (i.e., model input); and third, the way affective annotation (i.e., model output) is handled. This paper touches upon all three key components of an affective model (i.e., input, model, output) and introduces the use of deep learning (DL) [3], [4], [5] methodologies for affective modeling from multiple physiological signals."}
{"_id":"b3e6312fb37a99336f7780b1c69badd899858b41","title":"Three Implementations of SquishQL, a Simple RDF Query Language","text":"RDF provides a basic way to represent data for the Semantic Web. We have been experimenting with the query paradigm for working with RDF data in semantic web applications. Query of RDF data provides a declarative access mechanism that is suitable for application usage and remote access. We describe work on a conceptual model for querying RDF data that refines ideas first presented in at the W3C workshop on Query Languages [14] and the design of one possible syntax, derived from [7], that is suitable for application programmers. Further, we present experience gained in three implementations of the query language."}
{"_id":"060d1b412dc3fb7d5f5d952adc8a4a8ecc4bd3fa","title":"Introduction to Statistical Learning Theory","text":"The goal of statistical learning theory is to study, in a statistical framework, the properties of learning algorithms. In particular, most results take the form of so-called error bounds. This tutorial introduces the techniques that are used to obtain such results."}
{"_id":"9b9c9cc72ebc16596a618d5b78972437c9c569f6","title":"Fast Texture Transfer","text":""}
{"_id":"5de1dec847e812b7d7f2776a756ee17b2d1791cd","title":"Social Media: The Good, the Bad, and the Ugly","text":"Companies when strategizing are looking for innovative ways to have a competitive advantage over their competitors. One way in which they compete is by the adoption of social media. Social media has evolved over the years and as a result, new concepts and applications are developed which promises to provide business value to a company. However, despite the usefulness of social media, many businesses fail to reap its full benefits. The current literature shows evidence of lack of strategically designed process for companies to successfully implement social media. The purpose of this study is to suggest a framework which provides the necessary alignment between social media goals with business objectives. From the literature review, a social media strategy framework was derived to offer an effective step by step approach to the development and imple\u2010 mentation of social media goals aligned with a company\u2019s business objectives. The contribution to this study is the development of a social media strategy framework that can be used by organisations for business value."}
{"_id":"a70ba6daebb31461480fe3a369af6c7658ccdec0","title":"Web Mining for Information Retrieval","text":"The present era is engulfed in data and it is quite difficult to churn the emergence of unstructured data in order to mine relevant information. In this paper we present a basic outline of web mining techniques which changed the scenario of today world and help in retrieval of information. Web Mining actually referred as mining of interesting pattern by using set of tools and techniques from the vast pool of web. It focuses on different aspects of web mining referred as Web Content Mining, Web Structure Mining and Web Usage Mining. The overview of these mining techniques which help in retrieving desired information are covered along with the approaches used and algorithm employed."}
{"_id":"6de1fbfe86ddfef72d5fcfdb6e7e140be0ab195e","title":"Argumentation Mining: Where are we now, where do we want to be and how do we get there?","text":"This paper gives a short overview of the state-of-the-art and goals of argumentation mining and it provides ideas for further research. Its content is based on two invited lectures on argumentation mining respectively at the FIRE 2013 conference at the India International Center in New Delhi, India and a lecture given as SICSA distinguished visitor at the University of Dundee, UK in the summer of 2014."}
{"_id":"9abba0edb3d631b6cca51a346d494b911e5342fa","title":"1 Unsupervised Feature Learning in Time Series 2 Prediction Using Continuous Deep Belief Network 3","text":"A continuous Deep Belief Network (cDBN) with two hidden layers is proposed in this 18 paper, focusing on the problem of weak feature learning ability when dealing with continuous data. 19 In cDBN, the input data is trained in an unsupervised way by using continuous version of transfer 20 functions, the contrastive divergence is designed in hidden layer training process to raise 21 convergence speed, an improved dropout strategy is then implemented in unsupervised training to 22 realize features learning by de-cooperating between the units, and then the network is fine-tuned 23 using back propagation algorithm. Besides, hyper-parameters are analysed through stability 24 analysis to assure the network can find the optimal. Finally, the experiments on Lorenz chaos series, 25 CATS benchmark and other real world like CO2 and waste water parameters forecasting show that 26 cDBN has the advantage of higher accuracy, simpler structure and faster convergence speed than 27 other methods. 28"}
{"_id":"63cffc8d517259e5b4669e122190dfe515ee5508","title":"Influence of Some Parameters on the Effectiveness of Induction Heating","text":"We have investigated the effectiveness of heating conductive plates by induction in low frequencies analytically and numerically, in relation to the shape of the plate, the area of the region exposed to the magnetic flux, and the position of the exposed region with respect to the center of the plate. We considered both uniform and nonuniform magnetic fields. For plates with equivalent area exposed to the same uniform magnetic flux, the one with the most symmetrical shape is heated most effectively. If a coil is employed for the excitation, the results depend on the shape of the plate, the shape of the coil section, and the coil lift-off. When only the central region of a plate is exposed to a variable magnetic flux, there is a specific value of the exposed area for which the power dissipated in the plate material reaches a maximum. The most effective heating of a plate partially exposed occurs when the axis of the exciting coil is at the plate center."}
{"_id":"4ad7f58bde3358082cc922da3e726571fab453e7","title":"A Model of a Localized Cross-Border E-Commerce","text":"By the explosive growth of B2B e-commerce transactions in international supply chains and the rapid increase of business documents in Iran\u2019s cross-border trading, effective management of trade processes over borders is vital in B2B e-commerce systems. Structure of the localized model in this paper is based on three major layers of a B2B e-commerce infrastructure, which are messaging layer, business process layer and content layer. For each of these layers proper standards and solutions are chosen due to Iran\u2019s e-commerce requirements. As it is needed to move smoothly towards electronic documents in Iran, UNedocs standard is suggested to support the contents of both paper and electronic documents. The verification of the suggested model is done by presenting a four phase scenario through case study method. The localized model in this paper tries to make a strategic view of business documents exchange in trade processes, and getting closer to the key target of regional single windows establishment in global trade e-supply chains."}
{"_id":"9583ac53a19cdf0db81fef6eb0b63e66adbe2324","title":"Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent","text":"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum."}
{"_id":"1d13f96a076cb0e3590f13209c66ad26aa33792f","title":"Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation","text":"Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms \u2014 especially the collaborative filtering (CF)-based approaches with shallow or deep models \u2014 usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users\u2019 historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines."}
{"_id":"4279602fdbe037b935b63bcadbbfbe503a7c30ee","title":"You are what you like! Information leakage through users' Interests","text":"Suppose that a Facebook user, whose age is hidden or missing, likes Britney Spears. Can you guess his\/her age? Knowing that most Britney fans are teenagers, it is fairly easy for humans to answer this question. Interests (or \u201clikes\u201d) of users is one of the highly-available on-line information. In this paper, we show how these seemingly harmless interests (e.g., music interests) can leak privacysensitive information about users. In particular, we infer their undisclosed (private) attributes using the public attributes of other users sharing similar interests. In order to compare user-defined interest names, we extract their semantics using an ontologized version of Wikipedia and measure their similarity by applying a statistical learning method. Besides self-declared interests in music, our technique does not rely on any further information about users such as friend relationships or group belongings. Our experiments, based on more than 104K public profiles collected from Facebook and more than 2000 private profiles provided by volunteers, show that our inference technique efficiently predicts attributes that are very often hidden by users. To the best of our knowledge, this is the first time that user interests are used for profiling, and more generally, semantics-driven inference of private data is addressed."}
{"_id":"021f37e9da69ea46fba9d2bf4e7ca3e8ba7b3448","title":"Amorphous Silicon Solar Vivaldi Antenna","text":"An ultrawideband solar Vivaldi antenna is proposed. Cut from amorphous silicon cells, it maintains a peak power at 4.25 V, which overcomes a need for lossy power management components. The wireless communications device can yield solar energy or function as a rectenna for dual-source energy harvesting. The solar Vivaldi performs with 0.5-2.8 dBi gain from 0.95-2.45 GHz , and in rectenna mode, it covers three bands for wireless energy scavenging."}
{"_id":"69296a15df81fd853e648d160a329cbd9c0050c8","title":"Integrating perceived playfulness into expectation-confirmation model for web portal context","text":"This paper investigated the value of including \"playfulness\" in expectation-confirmation theory (ECT) when studying continued use of a web site. Original models examined cognitive beliefs and effects that influence a person's intention to continue to use an information system. Here, an extended ECT model (with an additional relationship between perceived playfulness and satisfaction) was shown to provide a better fit than a simple path from perceived usefulness to satisfaction. The results indicated that perceived playfulness, confirmation to satisfaction, and perceived usefulness all contributed significantly to the users' intent to reuse a web site. Thus, we believe that the extended ECT model is an appropriate tool for the study of web site effects."}
{"_id":"6eedf0a4fe861335f7f7664c14de7f71c00b7932","title":"Neural Turing Machines","text":"We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."}
{"_id":"3215cd14ee1a559eec1513b97ab1c7e6318bd5af","title":"Global Linear Neighborhoods for Efficient Label Propagation","text":"Graph-based semi-supervised learning improves classification by combining labeled and unlabeled data through label propagation. It was shown that the sparse representation of graph by weighted local neighbors provides a better similarity measure between data points for label propagation. However, selecting local neighbors can lead to disjoint components and incorrect neighbors in graph, and thus, fail to capture the underlying global structure. In this paper, we propose to learn a nonnegative low-rank graph to capture global linear neighborhoods, under the assumption that each data point can be linearly reconstructed from weighted combinations of its direct neighbors and reachable indirect neighbors. The global linear neighborhoods utilize information from both direct and indirect neighbors to preserve the global cluster structures, while the low-rank property retains a compressed representation of the graph. An efficient algorithm based on a multiplicative update rule is designed to learn a nonnegative low-rank factorization matrix minimizing the neighborhood reconstruction error. Large scale simulations and experiments on UCI datasets and high-dimensional gene expression datasets showed that label propagation based on global linear neighborhoods captures the global cluster structures better and achieved more accurate classification results."}
{"_id":"5efbd02d154fb76506caa9cf3e9333c89f24ccc9","title":"A new approach to identify power transformer criticality and asset management decision based on dissolved gas-in-oil analysis","text":"Dissolved gas analysis (DGA) of transformer oil is one of the most effective power transformer condition monitoring tools. There are many interpretation techniques for DGA results. However, all of these techniques rely on personnel experience more than standard mathematical formulation and significant number of DGA results fall outside the proposed codes of the current methods and cannot be diagnosed by these methods. To overcome these limitations, this paper introduces a novel approach using Gene Expression Programming (GEP) to help in standardizing DGA interpretation techniques, identify transformer critical ranking based on DGA results and propose a proper maintenance action. DGA has been performed on 338 oil samples that have been collected from different transformers of different rating and different life span. Traditional DGA interpretation techniques are used in analyzing the results to measure its consistency. These data are then used to develop the new GEP model. Results show that all current traditional techniques do not necessarily lead to the same conclusion for the same oil sample. The new approach using GEP is easy to implement and it does not call for any expert personnel to interpret the DGA results and to provide a proper asset management decision on the transformer based on DGA analysis."}
{"_id":"096e07ced8d32fc9a3617ff1f725efe45507ede8","title":"Learning methods for generic object recognition with invariance to pose and lighting","text":"We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation\/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16\/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."}
{"_id":"03184ac97ebf0724c45a29ab49f2a8ce59ac2de3","title":"Evaluation of output embeddings for fine-grained image classification","text":"Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with finegrained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results."}
{"_id":"24be6d5e698e32a72b709cb3eb50f9da0f616027","title":"Maladaptive and adaptive emotion regulation through music: a behavioral and neuroimaging study of males and females","text":"Music therapists use guided affect regulation in the treatment of mood disorders. However, self-directed uses of music in affect regulation are not fully understood. Some uses of music may have negative effects on mental health, as can non-music regulation strategies, such as rumination. Psychological testing and functional magnetic resonance imaging (fMRI) were used explore music listening strategies in relation to mental health. Participants (n = 123) were assessed for depression, anxiety and Neuroticism, and uses of Music in Mood Regulation (MMR). Neural responses to music were measured in the medial prefrontal cortex (mPFC) in a subset of participants (n = 56). Discharge, using music to express negative emotions, related to increased anxiety and Neuroticism in all participants and particularly in males. Males high in Discharge showed decreased activity of mPFC during music listening compared with those using less Discharge. Females high in Diversion, using music to distract from negative emotions, showed more mPFC activity than females using less Diversion. These results suggest that the use of Discharge strategy can be associated with maladaptive patterns of emotional regulation, and may even have long-term negative effects on mental health. This finding has real-world applications in psychotherapy and particularly in clinical music therapy."}
{"_id":"08f4d8e7626e55b7c4ffe1fd12eb034bc8022a43","title":"Aspect Extraction with Automated Prior Knowledge Learning","text":"Aspect extraction is an important task in sentiment analysis. Topic modeling is a popular method for the task. However, unsupervised topic models often generate incoherent aspects. To address the issue, several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling. In this paper, we take a major step forward and show that in the big data era, without any user input, it is possible to learn prior knowledge automatically from a large amount of review data available on the Web. Such knowledge can then be used by a topic model to discover more coherent aspects. There are two key challenges: (1) learning quality knowledge from reviews of diverse domains, and (2) making the model fault-tolerant to handle possibly wrong knowledge. A novel approach is proposed to solve these problems. Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines."}
{"_id":"f77f1d1d274042105cceebf17a57e680dfe1ce03","title":"Ant colony optimization for routing and load-balancing: survey and new directions","text":"Although an ant is a simple creature, collectively a colony of ants performs useful tasks such as finding the shortest path to a food source and sharing this information with other ants by depositing pheromone. In the field ofant colony optimization (ACO), models ofcollective intelligenceof ants are transformed into useful optimization techniques that find applications in computer networking. In this survey, the problem-solving paradigm of ACO is explicated and compared to traditional routing algorithms along the issues of routing information, routing overhead and adaptivity. The contributions of this survey include 1) providing a comparison and critique of the state-of-the-art approaches for mitigatingstagnation (a major problem in many ACO algorithms), 2) surveying and comparing three major research in applying ACO in routing and load-balancing, and 3) discussing new directions and identifying open problems. The approaches for mitigating stagnation discussed include:evaporation, aging, pheromone smoothingand limiting, privileged pheromone layingandpheromone-heuristic control . The survey on ACO in routing\/load-balancing includes comparison and critique of ant-based control and its ramifications, AntNetand its extensions, as well as ASGAand SynthECA. Discussions on new directions include an ongoing work of the authors in applyingmultiple ant colony optimizationin load-balancing."}
{"_id":"f26a8dcfbaf9f46c021c41a3545fcfa845660c47","title":"Human Pose Regression by Combining Indirect Part Detection and Contextual Information","text":"In this paper, we propose an end-to-end trainable regression approach for human pose estimation from still images. We use the proposed Soft-argmax function to convert feature maps directly to joint coordinates, resulting in a fully differentiable framework. Our method is able to learn heat maps representations indirectly, without additional steps of artificial ground truth generation. Consequently, contextual information can be included to the pose predictions in a seamless way. We evaluated our method on two very challenging datasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets, reaching the best performance among all the existing regression methods and comparable results to the state-of-the-art detection based approaches."}
{"_id":"db4bfae21d57a1583effad1b2952f78daece2454","title":"New Fast and Accurate Jacobi SVD Algorithm. I","text":"This paper is the result of contrived efforts to break the barrier between numerical accuracy and run time efficiency in computing the fundamental decomposition of numerical linear algebra \u2013 the singular value decomposition (SVD) of a general dense matrix. It is an unfortunate fact that the numerically most accurate one\u2013sided Jacobi SVD algorithm is several times slower than generally less accurate bidiagonalization based methods such as the QR or the divide and conquer algorithm. Despite its sound numerical qualities, the Jacobi SVD is not included in the state of the art matrix computation libraries and it is even considered obsolete by some leading researches. Our quest for a highly accurate and efficient SVD algorithm has led us to a new, superior variant of the Jacobi algorithm. The new algorithm has inherited all good high accuracy properties, and it outperforms not only the best implementations of the one\u2013sided Jacobi algorithm but also the QR algorithm. Moreover, it seems that the potential of the new approach is yet to be fully exploited."}
{"_id":"587f6b97f6c75d7bfaf2c04be8d9b4ad28ee1b0a","title":"DIFusion: Fast Skip-Scan with Zero Space Overhead","text":"Scan is a crucial operation in main-memory column-stores. It scans a column and returns a result bit vector indicating which records satisfy a filter predicate. ByteSlice is an in-memory data layout that chops data into multiple bytes and exploits early-stop capability by high-order bytes comparisons. As column widths are usually not multiples of byte, the last-byte of ByteSlice is padded with 0's, wasting memory bandwidth and computation power. To fully leverage the resources, we propose to weave a secondary index into the vacant bits (i.e., bits originally padded with 0's), forming our new layout coined DIFusion (Data Index Fusion). DIFusion enables skip-scan, a new fast scan that inherits the early-stopping capability from ByteSlice and at the same time possesses the data-skipping ability of index with zero space overhead. Empirical results show that skip-scan on DIFusion outperforms scan on ByteSlice."}
{"_id":"431551e2626973b150d982563a175ef80b56ce98","title":"An annotation tool for automatically triangulating individuals' psychophysiological emotional reactions to digital media stimuli","text":"Current affective user experience studies require both laborious and time-consuming data analysis, as well as dedicated affective classification algorithms. Moreover, the high technical complexity and lack of general guidelines for developing these affective classification algorithms further limits the comparability of the obtained results. In this paper we target this issue by presenting a tool capable of automatically annotating and triangulating players' physiologically interpreted emotional reactions to in-game events. This tool was initially motivated by an experimental psychology study regarding the emotional habituation effects of audio-visual stimuli in digital games and we expect it to contribute in future similar studies by providing both a deeper and more objective analysis on the affective aspects of user experience. We also hope it will contribute towards the rapid implementation and accessibility of this type of studies by open-sourcing it. Throughout this paper we describe the development and benefits presented by our tool, which include: enabling researchers to conduct objective a posteriori analyses without disturbing the gameplay experience, automating the annotation and emotional response identification process, and formatted data exporting for further analysis in third-party statistical software applications."}
{"_id":"bff381287fcd0ae45cae0ad2fab03bceb04b1428","title":"PKLot - A robust dataset for parking lot classification","text":"http:\/\/dx.doi.org\/10.1016\/j.eswa.2015.02.009 0957-4174\/ 2015 Elsevier Ltd. All rights reserved. \u21d1 Corresponding author. E-mail addresses: prlalmeida@inf.ufpr.br (P.R.L. de Almeida), lesoliveira@inf. ufpr.br (L.S. Oliveira), alceu@ppgia.pucpr.br (A.S. Britto Jr.), eunelson@ppgia.pucpr. br (E.J. Silva Jr.), alessandro.koerich@etsmtl.ca (A.L. Koerich). Paulo R.L. de Almeida , Luiz S. Oliveira , Alceu S. Britto Jr. b,\u21d1, Eunelson J. Silva Jr. b Alessandro L. Koerich b,c"}
{"_id":"97325a06de27b59c431a37b03c88f33e3a789f31","title":"Applying data mining techniques in job recommender system for considering candidate job preferences","text":"Job recommender systems are desired to attain a high level of accuracy while making the predictions which are relevant to the customer, as it becomes a very tedious task to explore thousands of jobs, posted on the web, periodically. Although a lot of job recommender systems exist that uses different strategies , here efforts have been put to make the job recommendations on the basis of candidate's profile matching as well as preserving candidate's job behavior or preferences. Firstly, rules predicting the general preferences of the different user groups are mined. Then the job recommendations to the target candidate are made on the basis of content based matching as well as candidate preferences, which are preserved either in the form of mined rules or obtained by candidate's own applied jobs history. Through this technique a significant level of accuracy has been achieved over other basic methods of job recommendations."}
{"_id":"e3530c67ed2294ce1a72b424d5e38c95552ba15f","title":"Neural scene representation and rendering","text":"Scene representation\u2014the process of converting visual sensory data into concise descriptions\u2014is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them."}
{"_id":"073daaf4f6fa972d3bdee3c4e4510d21dc934dfb","title":"Machine learning - a probabilistic perspective","text":"\u201cKevin Murphy excels at unraveling the complexities of machine learning methods while motivating the reader with a stream of illustrated examples and real-world case studies. The accompanying software package includes source code for many of the figures, making it both easy and very tempting to dive in and explore these methods for yourself. A must-buy for anyone interested in machine learning or curious about how to extract useful knowledge from big data.\u201d John Winn, Microsoft Research"}
{"_id":"e6bdbe24f5cd7195af0bb7db343a77f7534b2491","title":"Illuminating Light: An Optical Design Tool with a Luminous-Tangible Interface","text":"We describe a novel system for rapid prototyping of laserbased optical and holographic layouts. Users of this optical prototyping tool \u2013 called the Illuminating Light system \u2013 move physical representations of various optical elements about a workspace, while the system tracks these components and projects back onto the workspace surface the simulated propagation of laser light through the evolving layout. This application is built atop the Luminous Room infrastructure, an aggregate of interlinked, computer-controlled projector-camera units called I\/O Bulbs. Philosophically, the work embodies the emerging ideas of the Luminous Room and builds on the notions of \u2018graspable media\u2019. We briefly introduce the I\/O Bulb and Luminous Room concepts and discuss their current implementations. After an overview of the optical domain that the Illuminating Light system is designed to address, we present the overall system design and implementation, including that of an intermediary toolkit called voodoo which provides a general facility for object identification and tracking."}
{"_id":"0c56c03a65a9c38561b2d6fd01e55dc80df357c3","title":"Improved ID3 algorithm","text":"as the classical algorithm of the decision tree classification algorithm, ID3 is famous for the merits of high classifying speed easy, strong learning ability and easy construction. But when use it to classify, there does exist the problem of inclining to chose attributions which has many values, which affects its practicality. This paper for solving the problem a decision tree algorithm based on attribute-importance is proposed. The improved algorithm uses attribute-importance to increase information gain of attribution which has fewer attributions and compares ID3 with improved ID3 by an example. The experimental analysis of the data show that the improved ID3 algorithm can get more reasonable and more effective rules."}
{"_id":"394ccc9c3f51bb537161f2fdb581f85259872928","title":"Research of security metric architecture for Next Generation Network","text":"Constructing a security metric architecture in Next Generation Network is a critical part for network security assessment. In this paper, a three-dimensional security metric architecture model is given. In this model, four comprehensive metrics, which are vulnerability, threat, stability and survivability, are first presented. Then based on these four metrics, a hierarchical security metric architecture is devised, and instanced in a prototype system of NGN. Compared with qualitative metrics, the presented metrics are not only accurate, but also measurable and easy to collect based on the quantitative parameters. Since there is limited research result of the security metric architecture for NGN at present, this model can be referenced to a certain extent."}
{"_id":"fb5d1bb23724d9a5a5eae036a2e3cf291cac2c1b","title":"Cognitive radio: making software radios more personal","text":"Software radios arc cmcrging as platforms for multiband multimode personal communications systems. Radio etiqucttc is the set of RF bands, air interfaces, protocols, and spatial and temporal patterns that modcrate the use of the radio spectrum. Cognitive radio extends the software radio with radio-domain model-based rcasoning about such ctiquettes. Cognitivc radio enhances the flexibility of personal services through a Radio Knowlcdge Representation Language. This language reprcsents knowledge of radio etiquettc, devices, software modules, propagation, nctworks, user needs, and application scenarios in a way that supports automated reasoning about thc needs of the user. This empowers software radios to conduct expressivc ncgotiations among peers about the use of radio spectrum across fluents of space, time, and uscr context. With RKRL, cognitivc radio agents may actively manipulate the protocol stack to adapt known etiquettes to better satis$ the LISCT\u2019S nccds. This transforms radio nodes from blind executors of predefined protocols to radio-domain-aware intclligent agents that scarch out ways to dclivcr the services thc uscr wants even if that uscr docs not know how to obtain them. Softwarc radio [l] provides an idcal platform for thc rcalization of cognitive radio. Cognitive Radio: Making Software Radios More Personal J O S E P H MITOLA 111 A N D GERALD Q. M A G U I R E , J R . R O Y A L INSTITUTE OF TECHNOLOGY Global System for Mobile Communications (GSM) radio\u2019s equalizer taps reflcct the channel multipath structure. A network might want to ask a handset, \u201cHow many distinguishable multipath components are you seeing?\u201d Knowledge of thc internal states of the equalizer could be useful bccause in some reception areas, thcrc may be little or no multipath and 20 dB of extra signal-to-noisc ratio (SNR). Software radio processing capacity is wasted running a computationally intcnsive equalizer algorithm when no cqualizer is necessary. That processing capacity could be diverted to better use, or part of the processor might be put to sleep, saving battery life. In addition, the radio and network could agree to put data bits in the superfluous embedded training scquence, enhancing the payload data rate accordingly.\u2019 Two problems arise. First, the network has no standard language with which to posc a question about cqualizer taps. Sccand, the handset has the answer in the time-domain structure of its equalizer taps, but cannot access this information. It has no computational description of its own structure. Thus, it does not \u201cknow what it knows.\u201d Standards-setting bodies have been gradually making such internal data available to networks through specific air interfaces, as the needs of the technology dictate. This labor-intensive process takes ycars to accomplish. Radio Knowledge Represcntation Language (RKRL), on the other hand, provides a standard languagc within which such unanticipated data exchanges can be defined dynamically. Why might the need for such unanticipatcd exchanges arise? Debugging new software radio downloads might require access to internal software parameters. Creating personal services that diffcrentiate one servicc provider from another might be enhanced if the provider does not need to expose new ideas to the competition in the standards-setting process. And the time to deploy those personalized services could be reduced. Cognitive radio, through RKRL, knows that the natural lanThis raise.$ a host of questions about the control of such complex adaptive agents, network ,stabiliQ, and the like. guage phrase equalizer taps refers to specific parameters of a tapped delay-line structure. This structure may be implemented in an application-specific integrated circuit (ASIC), a field programmable gate array (FPGA), or an algorithm in a software radio. Since a cognitive radio has a model of its own internal structurc, it can check the model to find out how the equalizer has been implemented. It then may retrieve thc register values from the ASIC (e.g., using a JTAG port) or find the taps in the proper memory location of its software implementation. A radio that knows its own internal structure to this degree does not have to wait for a consortium, forum, or standards body to define a level H33492.x7 radio as one that can access its equalizcr taps. The network can pose such an unanticipated question in (a standard) RKRL, and any RKRL-capable radio can answer it. To enable such a scenario, cognitive radio has an RKRL model of itself that includes the equalizer\u2019s structure and function, as illustrated in Fig. 1. In this example, the radio hardware consists of the antenna, the radio frequency (RF) conversion module, the modem, and the other modules shown in the hardware part of the figure. The baseband processor includes a baseband modem and a back-end control protocol stack. In addition, this processor contains a cognition cngine and a set of computational models. The models consist of RKRL frames that describe the radio itself, including the equalizer, in the context of a comprehensive ontology, also written in RKRL. Using this ontology, the radio can track the user\u2019s environment over time and space. Cognitive radio, then, matches its internal models to cxternal observations to understand what it means to commute to and from work, take a business trip to Europe, go on vacation, and so on. Clearly, significant memory, computational resources, and communications bandwidth are needed for cognitive radio, so this technology might not be deployable for somc time. In addition, a collcction of cognitivc radios may not require human intervention to develop their own protocols. Initially, intervention will be required in order to ensurd that networks of such radios remain stable (or that we know who to blame if this is not the case). Networks of such radios are complex Authorized licensed use limited to: CHONGQING UNIVERSITY. Downloaded on April 8, 2009 at 21:50 from IEEE Xplore. Restrictions apply. ___ ................. ............ .. Cognition -."}
{"_id":"e202c80c42b625ab54b44e1bbced7cfda134dbd6","title":"Comdb2: Bloomberg's Highly Available Relational Database System","text":"Comdb2 is a distributed database system designed for geographical replication and high availability. In contrast with the latest trends in this field, Comdb2 o\u21b5ers full transactional support, a standard relational model, and the expressivity of SQL. Moreover, the system allows for rich stored procedures using a dialect of Lua. Comdb2 implements a serializable system in which reads from any node always return current values. Comdb2 provides transparent High Availability through built-in service discovery and sophisticated retry logic embedded in the standard API. In addition to the relational data model, Comdb2 implements queues for publisher-to-subscriber message delivery. Queues can be combined with table triggers for timeconsistent log distribution, providing functionality commonly needed in modern OLTP. In this paper we give an overview of our last twelve years of work. We focus on the design choices that have made Comdb2 the primary database solution within our company, Bloomberg LP (BLP)."}
{"_id":"7e894811db92d2df7842eb5d4e517b36249f9100","title":"Thermal management consumption and its effect on remaining range estimation of electric vehicles","text":"This paper investigates the significance of thermal management energy consumption on the range of electric vehicles. It is necessary to combine consumption models of powertrain components such as the electric machine, power electronics and high voltage battery that take into account trip-based information containing a predicted speed profile and elevation data with a thermal management system that has the ability to incorporate predicted thermal losses of the powertrain. The thermal management system refers to the sum of all components within the refrigeration cycle of the vehicle as well as the associated control strategy. Based on the results of a real world drive study in an electric vehicle, it is demonstrated, that the range uncertainties introduced by the consumption of the thermal management are significant and should be considered when developing predictive range estimation algorithms. These uncertainties are further investigated by an energy flow based thermal system model. In order to minimize range uncertainties, a system architecture for the prediction of thermal management consumption is proposed."}
{"_id":"0cbc08b2e318133653448214d2b4fbbd7f812136","title":"Model-Driven Data Acquisition in Sensor Networks","text":"Declarative queries are proving to be an attractive paradigm for interacting with networks of wireless sensors. The metaphor that \u201cthe sensornet is a database\u201d is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a modelof that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques."}
{"_id":"69eaf306803c7eeedba0933244bc7fb105ea1a01","title":"Data augmentation for deep convolutional neural network acoustic modeling","text":"This paper investigates data augmentation based on label-preserving transformations for deep convolutional neural network (CNN) acoustic modeling to deal with limited training data. We show how stochastic feature mapping (SFM) can be carried out when training CNN models with log-Mel features as input and compare it with vocal tract length perturbation (VTLP). Furthermore, a two-stage data augmentation scheme with a stacked architecture is proposed to combine VTLP and SFM as complementary approaches. Improved performance has been observed in experiments conducted on the limited language pack (LLP) of Haitian Creole in the IARPA Babel program."}
{"_id":"8ecc044d920df247fbd455b752fd7cc0f7363ad7","title":"On the importance of initialization and momentum in deep learning","text":"Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods."}
{"_id":"9ed8e2f6c338f4e0d1ab0d8e6ab8b836ea66ae95","title":"A Fully Convolutional Neural Network for Speech Enhancement","text":"In hearing aids, the presence of babble noise degrades hearing intelligibility of human speech greatly. However, removing the babble without creating artifacts in human speech is a challenging task in a low SNR environment. Here, we sought to solve the problem by finding a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra via supervised learning. Specifically, we propose using fully Convolutional Neural Networks, which consist of lesser number of parameters than fully connected networks. The proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates that a convolutional network can be 12 times smaller than a recurrent network and yet achieves better performance, which shows its applicability for an embedded system: the hearing aids."}
{"_id":"0e400ee7aa7e53777596eaae63c4062a657d60f5","title":"Suppression of acoustic noise in speech using spectral subtraction","text":"A new technique for the modelling of perceptual systems called formal modelling is developed. This technique begins with qualitative observations about the perceptual system, the so-called perceptual symmetries, to obtain through mathematical analysis certain model structures which may then be calibrated by experiment. The analysis proceeds in two different ways depending upon the choice of linear or nonlinear models. For the linear case, the analysis proceeds through the methods of unitary representation theory. It begins with a unitary group representation on the image space and produces what we have called the fundamental structure theorem. For the nonlinear case, the analysis makes essential use of infinite-dimensional manifold theory. It ber ins with a Lie group action on an image manifold and produces the fundamental structure formula. These techniques will be used to study the brightness perception mechanism of the human visual system. Several visual groups are defined and their corresponding structures for visual system models are obtained. A new transform called the Mandala transform will be deduced from a certain visual group and its implications for image processing will be discussed. Several new phenomena of brightness perception will be presented. New facts about the Mach band illusion along with new adaptation phenomena will be presented. Also a new visual illusion will be presented. A visual model based on the above techniques will be presented. It will also be shown how use of statistical estimation theory can be made in the study of contrast adaptation."}
{"_id":"158fd3c7537d9d0af4c828cc0e3948e157287f83","title":"Dictionary Learning Algorithms for Sparse Representation","text":"Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave\/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial 25 words or less), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error)."}
{"_id":"becddb754d41fa2055be5d50eac0cade5b4004ce","title":"Tracing traitors","text":"We give cryptographic schemes that help trace the source of leaks when sensitive or proprietary data is made available to a large set of parties. A very relevant application is in the context of pay television, where only paying customers should be able to view certain programs. In this application, the programs are normally encrypted, and then the sensitive data is the decryption keys that are given to paying customers. If a pirate decoder is found, it is desirable to reveal the source of its decryption keys. We describe fully resilient schemes which can be used against any decoder which decrypts with nonnegligible probability. Since there is typically little demand for decoders which decrypt only a small fraction of the transmissions (even if it is nonnegligible), we further introduce threshold tracing schemes which can only be used against decoders which succeed in decryption with probability greater than some threshold. Threshold schemes are considerably more efficient than fully resilient schemes."}
{"_id":"903683bd91d0c67f566121a7351569edd7a4cb1a","title":"Efficacy and safety of a hyaluronic acid filler in subjects treated for correction of midface volume deficiency: a 24 month study","text":"BACKGROUND\nHyaluronic acid (HA) fillers are an established intervention for correcting facial volume deficiency. Few studies have evaluated treatment outcomes for longer than 6 months. The purpose of this study was to determine the durability of an HA filler in the correction of midface volume deficiency over 24 months, as independently evaluated by physician investigators and subjects.\n\n\nMETHODS\nSubjects received treatment with Juv\u00e9derm(\u2122) Voluma(\u2122) to the malar area, based on the investigators' determination of baseline severity and aesthetic goals. The treatment was administered in one or two sessions over an initial 4-week period. Supplementary treatment was permissible at week 78, based on protocol-defined criteria. A clinically meaningful response was predefined as at least a one-point improvement on the MidFace Volume Deficit Scale (MFVDS) and on the Global Aesthetic Improvement Scale (GAIS).\n\n\nRESULTS\nOf the 103 subjects enrolled, 84% had moderate or significant volume deficiency at baseline. At the first post-treatment evaluation (week 8), 96% were documented to be MFVDS responders, with 98% and 100% graded as GAIS responders when assessed by the subjects and investigators, respectively. At week 78, 81.7% of subjects were still MFVDS responders, with 73.2% and 78.1% being GAIS responders, respectively. Seventy-two subjects completed the 24-month study, of whom 45 did not receive supplementary Voluma(\u2122) at week 78. Forty-three of the 45 (95.6%) subjects were MFVDS responders, with 82.2% and 91.1% being GAIS responders, respectively. At end of the study, 66\/72 subjects were either satisfied or very satisfied with Voluma(\u2122), with 70\/72 indicating that they would recommend the product to others. Adverse events were transient and infrequent, with injection site bruising and swelling being the most commonly reported.\n\n\nCONCLUSION\nVoluma(\u2122) is safe and effective in the correction of mild to severe facial volume deficiency, achieving long-term clinically meaningful results. There was a high degree of satisfaction with the treatment outcome over the 24 months of the study."}
{"_id":"50240c32420328e877a81d0e9d9ac32527248966","title":"Validity in Quantitative Content Analysis","text":"Over the past 15 years, educational technologists have been dabbling with a research technique known as quantitative content analysis (QCA). Although it is characterized as a systematic and objective procedure for describing communication, readers find insufficient evidence of either quality in published reports. In this paper, it is argued that QCA should be conceived of as a form of testing and measurement. If this argument is successful, it becomes possible to frame many of the problems associated with QCA studies under the well-articulated rubric of test validity. Two sets of procedures for developing the validity of a QCA coding protocol are provided, (a) one for developing a protocol that is theoretically valid and (b) one for establishing its validity empirically. The paper is concerned specifically with the use of QCA to study educational applications of computer-mediated communication. The primary role of networked computers in higher education has shifted from presenting structured, preprogrammed learning materials to facilitating communication. In turn, the role of educational technology researchers has expanded to include the role of communication researcher. In the late 1980s, studies began to appear that incorporated new perspectives, new methods, and new techniques. One of the most promising was quantitative content analysis (QCA). Berelson (1952) defined QCA as \u201ca research technique for the systematic, objective, and quantitative description of the manifest content of communication\u201d (p. 18). In this context, description is a process that includes segmenting communication content into units, assigning each unit to a category, and providing tallies for each category. Bullen (1998), for instance, studied participation and critical thinking in an online discussion by counting the number of times each student contributed to the discussion and by assigning each contribution to one of three categories of critical thinking. By 1999, enough of these types of studies had been conducted in the field of educational technology that we were able to prepare a literature review (Rourke, Anderson, Garrison, & Archer, 2001). We used published reports to illustrate many of the basic issues of QCA, including objectivity, reliability, units of analysis, types of content, research designs, and ethics. Our intent was to summarize the experiences of researchers like us who were struggling to apply this unfamiliar technique in a disciplined and efficient manner. We found the technique promising but chided researchers on the rigor of their reports, particularly on the lack of reliability data. Those who persisted with the technique found their own way to these conclusions, and today"}
{"_id":"ce6403e99465e5e8a48d5c2017fc23976e29fe59","title":"FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks","text":"Convolutional Neural Networks (CNN) are verycomputation-intensive. Recently, a lot of CNN accelerators based on the CNN intrinsic parallelism are proposed. However, we observed that there is a big mismatch between the parallel types supported by computing engine and the dominant parallel types of CNN workloads. This mismatch seriously degrades resource utilization of existing accelerators. In this paper, we propose aflexible dataflow architecture (FlexFlow) that can leverage the complementary effects among feature map, neuron, and synapse parallelism to mitigate the mismatch. We evaluated our design with six typical practical workloads, it acquires 2-10x performance speedup and 2.5-10x power efficiency improvement compared with three state-of-the-art accelerator architectures. Meanwhile, FlexFlow is highly scalable with growing computing engine scale."}
{"_id":"1f3e3872e1c28d76da663164c431929aeacc57a2","title":"Robust Mapping and Localization in Indoor Environments Using Sonar Data","text":"In this paper we describe a new technique for the creation of featurebased stochastic maps using standard Polaroid sonar sensors. The fundamental contributions of our proposal are: (1) a perceptual grouping process that permits the robust identification and localization of environmental features, such as straight segments and corners, from the sparse and noisy sonar data; (2) a map joining technique that allows the system to build a sequence of independent limited-size stochastic maps and join them in a globally consistent way; (3) a robust mechanism to determine which features in a stochastic map correspond to the same environment feature, allowing the system to update the stochastic map accordingly, and perform tasks such as revisiting and loop closing. We demonstrate the practicality of this approach by building a geometric map of a medium size, real indoor environment, with several people moving around the robot. Maps built from laser data for the same experiment are provided for comparison. KEY WORDS\u2014map building, local maps, data association, sonar sensors, Hough transform"}
{"_id":"332fc63569a1fb07f9c61d886ba1b1328da7638b","title":"A design process for the development of innovative smart clothing that addresses end-user needs from technical, functional, aesthetic and cultural view points","text":"The research and development of attractive smart clothing, with truly embedded technologies, demands the merging of science and technology with art and design. This paper looks at a comparatively new and unique design discipline that has been given little prior consideration. The concept of 'wearables' crosses the boundaries between many disciplines. A gap exists for a common 'language' that facilitates creativity and a systematic design process. Designers of smart clothing require guidance in their enquiry, as gaining an understanding of issues such as; usability, manufacture, fashion, consumer culture, recycling and end user needs can seem forbidding and difficult to prioritise. This paper presents a design tool in a format that can be understood by practitioners who come from a range of backgrounds. The representation of the 'critical path' is intended as a tool to guide the design research and development process in the application of smart technologies."}
{"_id":"a30390887b9f791da26a9973e7569876d18f2d74","title":"The Runtime System Problem Identification Method Based on Log Analysis","text":"Currently system logs are an important source of information for system administrators to monitor system behaviors and to identify system problems. The manual examining is infeasible for the complex system and the existing automated methods for identifying system problems have different disadvantages such as the extreme dependency on the source code of the system, the low accuracy of predicting or identifying the system problems, or the requirement of the balanced and labeled training data set. This paper proposes a one- class Support Vector Machine (OCSVM) based method to identify the runtime system problems. Firstly, log sequences are generated for describing the running trajectories of the monitored system by parsing log messages; Secondly, variable length n-gram features are extracted, and moreover, the log sequences are represented as feature vectors based on these variable length n-gram features and Vector Space Model (VSM). Finally, all the feature vectors of the training log sequence set, which only includes the labeled normal log sequences, are input into OCSVM. Experimental results show that it performs better to use linear kernel to train OCSVM on our feature vectors than Gaussian kernel and the size of the sliding window hardly affects the performance of our method. Moreover, the proposed method achieves better performance on unbalanced training dataset than the method based on Random Indexing (RI) and weighted SVM."}
{"_id":"e355d6f9be3d62459688270b8213508add9e7420","title":"Medical Sentiment Analysis using Social Media: Towards building a Patient Assisted System","text":"With the enormous growth of Internet, more users have engaged in health communities such as medical forums to gather health-related information, to share experiences about drugs, treatments, diagnosis or to interact with other users with similar condition in communities. Monitoring social media platforms has recently fascinated medical natural language processing researchers to detect various medical abnormalities such as adverse drug reaction. In this paper, we present a benchmark setup for analyzing the sentiment with respect to users\u2019 medical condition considering the information, available in social media in particular. To this end, we have crawled the medical forum website \u2018patient.info\u2019 with opinions about medical condition self narrated by the users. We constrained ourselves to some of the popular domains such as depression, anxiety, asthma, and allergy. The focus is given on the identification of multiple forms of medical sentiments which can be inferred from users\u2019 medical condition, treatment, and medication. Thereafter, a deep Convolutional Neural Network (CNN) based medical sentiment analysis system is developed for the purpose of evaluation. The resources are made available to the community through LRE map for further research."}
{"_id":"7285ef77e66c7198c956e226ab837ba0b54f235b","title":"Kilobot: A low cost robot with scalable operations designed for collective behaviors","text":"In current robotics research there is a vast body of work on algorithms and control methods for groups of decentralized cooperating robots, called a swarm or collective. These algorithms are generally meant to control collectives of hundreds or even thousands of robots; however, for reasons of cost, time, or complexity, they are generally validated in simulation only, or on a group of a few tens of robots. To address this issue, this paper presents Kilobot, an open-source, low cost robot designed to make testing collective algorithms on hundreds or thousands of robots accessible to robotics researchers. To enable the possibility of large Kilobot collectives where the number of robots is an order of magnitude larger than the largest that exist today, each robot is made with only $14 worth of parts and takes 5 min to assemble. Furthermore, the robot design allows a single user to easily operate a large Kilobot collective, such as programming, powering on, and charging all robots, which would be difficult or impossible to do with many existing robotic systems. We demonstrate the capabilities of the Kilobot as a collective robot, by using a small robot test collective to implement four popular swarm behaviors: foraging, formation control, phototaxis, and synchronization. \u00a9 2013 Elsevier B.V. All rights reserved."}
{"_id":"3ca95b670f7e83b1a8096067a5da834da386fe08","title":"Design and Analysis of Energy-Efficient Reconfigurable Pre-Emphasis Voltage-Mode Transmitters","text":"This paper analyzes the signaling and digital power overhead of pre-emphasis voltage-mode transmitters. Utilizing a shunt branch in parallel with the differential channel to implement pre-emphasis is shown to have the best overall energy-efficiency. Leveraging this technique, an efficient pre-emphasis voltage mode transmitter architecture with output amplitude control, pre-emphasis coefficient control, and online impedance calibration is proposed and demonstrated. A 65 nm LP CMOS implementation of this architecture dissipates ~ 10 mW from a 1.2 V supply when transmitting 10 Gb\/s 200 mV differential amplitude data with 2-tap pre-emphasis, achieving 1 pJ\/bit energy efficiency."}
{"_id":"b30301ffa960ed17f16824e9badd94d82f40c5f9","title":"Affective style, psychopathology, and resilience: brain mechanisms and plasticity.","text":"The brain circuitry underlying emotion includes several territories of the prefrontal cortex (PFC), the amygdala, hippocampus, anterior cingulate, and related structures. In general, the PFC represents emotion in the absence of immediately present incentives and thus plays a crucial role in the anticipation of the future affective consequences of action, as well as in the persistence of emotion following the offset of an elicitor. The functions of the other structures in this circuit are also considered. Individual differences in this circuitry are reviewed, with an emphasis on asymmetries within the PFC and activation of the amygdala as 2 key components of affective style. These individual differences are related to both behavioral and biological variables associated with affective style and emotion regulation. Plasticity in this circuitry and its implications for transforming emotion and cultivating positive affect and resilience are considered."}
{"_id":"78b541f72ff31bf9f27c8b2a37560f4b45b6fe81","title":"MobileCDP: A mobile framework for the consumer decision process","text":"............................................................................................................. iii \u00d6Z ............................................................................................................................ iv ACKNOWLEDGEMENTS ........................................................................................ vi TABLE OF CONTENTS .......................................................................................... vii LIST OF TABLES ................................................................................................... xii LIST OF FIGURES ................................................................................................ xiii ABBREVIATIONS ................................................................................................... xv"}
{"_id":"952e3e6d16c1ac67ee1f63e3e728d5f52378b217","title":"Recognizing Textual Entailment Using a Subsequence Kernel Method","text":"We present a novel approach to recognizing Textual Entailment. Structural features are constructed from abstract tree descriptions, which are automatically extracted from syntactic dependency trees. These features are then applied in a subsequence-kernel-based classifier to learn whether an entailment relation holds between two texts. Our method makes use of machine learning techniques using a limited data set, no external knowledge bases (e.g. WordNet), and no handcrafted inference rules. We achieve an accuracy of 74.5% for text pairs in the Information Extraction and Question Answering task, 63.6% for the RTE-2 test data, and 66.9% for the RET-3 test data."}
{"_id":"ce98064c220c66a038e95f1a19049c7b9704761a","title":"Understanding what concerns consumers: a semantic approach to product feature extraction from consumer reviews","text":"The Web has become an excellent source for gathering consumer opinions (more specifically, consumer reviews) about products. Consumer reviews are essential for retailers and product manufacturers to understand the general responses of customers to their products and improve their marketing campaigns or products accordingly. In addition, consumer reviews enable retailers to recognize the specific preferences of each customer, which facilitates effective marketing decisions. As the number of consumer reviews expands, it is essential and desirable to develop an efficient and effective sentiment analysis technique that is capable of extracting product features stated in consumer reviews (i.e., product feature extraction) and determining the sentiments (positive or negative semantic orientations) of consumers for these product features (i.e., opinion orientation identification). Product feature extraction is critical to sentiment analysis, because its effectiveness significantly affects the performance of opinion orientation identification, as well as the ultimate effectiveness of sentiment analysis. Therefore, this study concentrates on product feature extraction from consumer reviews. C.-P. Wei (&) Institute of Service Science, College of Technology Management, National Tsing Hua University, Hsinchu, Taiwan, ROC e-mail: cpwei@mx.nthu.edu.tw Y.-M. Chen Department of Information Management, College of Management, National Sun Yat-Sen University, Kaohsiung, Taiwan, ROC C.-S. Yang Department of Information Management, College of Informatics, Yuan-Ze University, Chung-Li, Taiwan, ROC e-mail: csyang@saturn.yzu.edu.tw C. C. Yang College of Information Science and Technology, Drexel University, Philadelphia, PA, USA e-mail: chris.yang@ischool.drexel.edu 123 Inf Syst E-Bus Manage (2010) 8:149\u2013167 DOI 10.1007\/s10257-009-0113-9"}
{"_id":"2afec14a4d0d90800eaa1176490674080dbb8c0f","title":"A random graph model of kidney exchanges: efficiency, individual-rationality and incentives","text":"In kidney exchanges, hospitals share patient lists and receive transplantations. A kidney-paired donation (KPD) mechanism needs to promote full sharing of information about donor-patient pairs, and identify a Pareto efficient outcome that also satisfies participation constraints of hospitals. We introduce a random graph model of the KPD exchange and then fully characterize the structure of the efficient outcome and the expected number of transplantations that can be performed. Random graph theory allows early experimental results to be explained analytically, and enables the study of participation incentives in a methodological way. We derive a square-root law between the welfare gains from sharing patient-donor pairs in a central pool and the individual sizes of hospitals, illustrating the urgent need for the nationwide expansion of such programs. Finally, we establish through theoretical and computational analysis that enforcing simple individual rationality constraints on the outcome can mitigate the negative impact of strategic behavior by hospitals."}
{"_id":"bb00d4fb500cb918dfa208d9605d7f1dc70d7b64","title":"A continuous-time sigma-delta modulator with 88-dB dynamic range and 1.1-MHz signal bandwidth","text":"This paper presents the design and experimental results of a continuous-time \/spl Sigma\/\/spl Delta\/ modulator for ADSL applications. Multibit nonreturn-to-zero (NRZ) DAC pulse shaping is used to reduce clock jitter sensitivity. The nonzero excess loop delay problem in conventional continuous-time \/spl Sigma\/\/spl Delta\/ modulators is solved by our proposed architecture. A prototype third-order continuous-time \/spl Sigma\/\/spl Delta\/ modulator with 5-bit internal quantization was realized in a 0.5-\/spl mu\/m double-poly triple-metal CMOS technology, with a chip area of 2.4 \/spl times\/ 2.4 mm\/sup 2\/. Experimental results show that the modulator achieves 88-dB dynamic range, 84-dB SNR, and 83-dB SNDR over a 1.1-MHz signal bandwidth with an oversampling ratio of 16, while dissipating 62 mW from a 3.3-V supply."}
{"_id":"1d22ff36a6d349606e5de839e79f2b4db6ca491a","title":"Code coverage for suite evaluation by developers","text":"One of the key challenges of developers testing code is determining a test suite's quality -- its ability to find faults. The most common approach is to use code coverage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by a suite's ability to kill mutants (artificially seeded potential faults). Determining which criteria best predict mutation kills is critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites --- they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that evaluation results differ from those for suite-comparison: statement (not block, branch, or path) coverage predicts mutation kills best."}
{"_id":"055aa6f52c911f4edbda0113d064f83f963a8abe","title":"MiMaze, a 3D Multi-Player Game on the Internet","text":"This paper describes the design, implementation, and evaluation of MiMaze, a distributed 3D multiplayer game on the Internet. The major contribution of this work is to provide the first experimental results for this new type of application. An original fully distributed communication architecture has been designed for MiMaze. It uses multicast communication system based on RTP\/UDP\/IP, and a distributed synchronization mechanisms to guarantee the consistency of the game, regardless of heterogeneous network delay. This paper provides evaluation results on the Mbone. The experimental evaluation shows that the major problem will not be with real-time interaction, but with scalability."}
{"_id":"2fae769324b1116938c6bc65e4ddcd5441c8b206","title":"Scalable Distributed Monte-Carlo Tree Search","text":"Monte-Carlo Tree Search (MCTS) is remarkably successful in two-player games, but parallelizing MCTS has been notoriously difficult to scale well, especially in distributed environments. For a distributed parallel search, transposition-table driven scheduling (TDS) is known to be efficient in several domains. We present a massively parallel MCTS algorithm, that applies the TDS parallelism to the Upper Confidence bound Applied to Trees (UCT) algorithm, which is the most representative MCTS algorithm. To drastically decrease communication overhead, we introduce a reformulation of UCT called Depth-First UCT. The parallel performance of the algorithm is evaluated on clusters using up to 1,200 cores in artificial game-trees. We show that this approach scales well, achieving 740-fold speedups in the best case."}
{"_id":"b2c60061ad32e28eb1e20aff42e062c9160786be","title":"Diverse and Controllable Image Captioning with Part-of-Speech Guidance","text":"Automatically describing an image is an important capability for virtual assistants. Significant progress has been achieved in recent years on this task of image captioning. However, classical prediction techniques based on maximum likelihood trained LSTM nets don\u2019t embrace the inherent ambiguity of image captioning. To address this concern, recent variational auto-encoder and generative adversarial network based methods produce a set of captions by sampling from an abstract latent space. But, this latent space has limited interpretability and therefore, a control mechanism for captioning remains an open problem. This paper proposes a captioning technique conditioned on part-of-speech. Our method provides human interpretable control in form of part-of-speech. Importantly, part-of-speech is a language prior, and conditioning on it provides: (i) more diversity as evaluated by counting n-grams and the novel sentences generated, (ii) achieves high accuracy for the diverse captions on standard captioning metrics."}
{"_id":"baa94fb6b3adbecf69c1f5acd03aedd91dba9571","title":"Credit Card Fraud Detection: A Novel Approach Using Aggregation Strategy and Feedback Mechanism","text":"With the rapid development of electronic commerce, the number of transactions by credit cards are increasing rapidly. As online shopping becomes the most popular transaction mode, cases of transaction fraud are also increasing. In this paper, we propose a novel fraud detection method that composes of four stages. To enrich a cardholder\u2019s behavioral patterns, we first utilize the cardholders\u2019 historical transaction data to divide all cardholders into different groups such that the transaction behaviors of the members in the same group are similar. We thus propose a window-sliding strategy to aggregate the transactions in each group. Next, we extract a collection of specific behavioral patterns for each cardholder based on the aggregated transactions and the cardholder\u2019s historical transactions. Then we train a set of classifiers for each group on the base of all behavioral patterns. Finally, we use the classifier set to detect fraud online and if a new transaction is fraudulent, a feedback mechanism is taken in the detection process in order to solve the problem of concept drift. The results of our experiments show that our approach is better than others."}
{"_id":"b2624c3cb508bf053e620a090332abce904099a1","title":"Dynamic Memory Networks for Visual and Textual Question Answering","text":"Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision."}
{"_id":"abb74644e2bb1d1e8610e9782a6050192c3ceddf","title":"3D human face description: landmarks measures and geometrical features","text":"Morphometric measures and geometrical features are widely used to descript faces. Generally, they are extracted punctually from landmarks, namely anthropometric reference points. The aims are various, such as face recognition, facial expression recognition, face detection, study of changes in facial morphology due to growth, or dysmorphologies. Most of the time, landmarks were extracted with the help of an algorithm or manually located on the faces. Then, measures are computed or geometrical features are extracted to perform the scope of the study. This paper is intended as a survey collecting and explaining all these features, in order to provide a structured user database of the potential parameters and their charateristics. Firstly, facial soft-tissue landmarks are defined and contextualized; then the various morphometric measures are introduced and some results are given; lastly, the most important measures are compared to identify the best one for face recognition applications."}
{"_id":"02f872de0dc3f1d54ba68f9d751b7828f64d189c","title":"KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera","text":"KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. Uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions are shown. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch."}
{"_id":"849eaeeef1e11280bb7812239d34712b30023165","title":"RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments","text":"RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. RGB-D cameras rely on either structured light patterns combined with stereo sensing [6,10] or time-of-flight laser sensing [1] to generate depth estimates that can be associated with RGB pixels. Very soon, small, high-quality RGB-D cameras developed for computer gaming and home entertainment applications will become available at cost below $100. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. The robotics and computer vision communities have developed a variety of techniques for 3D mapping based on laser range scans [8, 11], stereo cameras [7], monocular cameras [3], and unsorted collections of photos [4]. While RGB-D cameras provide the opportunity to build 3D maps of unprecedented richness, they have drawbacks that make their application to 3D mapping difficult: They provide depth only up to a limited distance (typically less than 5m), depth values are much noisier than those provided by laser scanners, and their field of view (\u223c 60\u25e6) is far more constrained than that of specialized cameras or laser scanners typically used for 3D mapping (\u223c 180\u25e6). In our work, we use a camera developed by PrimeSense [10]. The key insights of this investigation are: first, that existing frame matching techniques are not sufficient to provide robust visual odometry with these cameras; second, that a tight integration of depth and color information can yield robust frame matching and loop closure detection; third, that building on best practice techniques in SLAM and computer graphics makes it possible to build and visualize accurate and extremely rich 3D maps with such cameras; and, fourth, that it will be feasible to build complete robot navigation and interaction systems solely based on cheap depth cameras."}
{"_id":"8d7eb0d250feebe1179532392e0dfdb9e2f2e80a","title":"Time-of-Flight sensor calibration for accurate range sensing","text":"Over the past years Time-of-Flight (ToF) sensors have become a considerable alternative to conventional distance sensing techniques like laser scanners or image based stereo-vision. Due to the ability to provide full-range distance information at high frame-rates, ToF sensors achieve a significant impact onto current research areas like online object recognition, collision prevention or scene and object reconstruction. Nevertheless, ToF-cameras like the Photonic Mixer Device (PMD) still exhibit a number of error sources that affect the accuracy of measured distance information. For this reason, major error sources for ToFcameras will be discussed, along with a new calibration approach that combines intrinsic, distance as well as a reflectivity related error calibration in an overall, easy to use system and thus significantly reduces the number of necessary reference images. The main contribution, in this context, is a new intensity-based calibration model that requires less input data compared to other models and thus significantly contributes to the reduction of calibration data. 2010 Elsevier Inc. All rights reserved."}
{"_id":"3e9517494eff0a2375348714d5eb6ecfd9b6cf60","title":"CALIBRATION OF A PMD-CAMERA USING A PLANAR CALIBRATION PATTERN TOGETHER WITH A MULTI-CAMERA SETUP","text":"We discuss the joint calibration of novel 3D range cameras based on the time-of-flight principle with the Photonic Mixing Device (PMD) and standard 2D CCD cameras. Due to the small field-of-view (fov) and low pixel resolution, PMD-cameras are difficult to calibrate with traditional calibration methods. In addition, the 3D range data contains systematic errors that need to be compensated. Therefore, a calibration method is developed that can estimate full intrinsic calibration of the PMD-camera including optical lens distortions and systematic range errors, and is able to calibrate the external orientation together with multiple 2D cameras that are rigidly coupled to the PMD-camera. The calibration approach is based on a planar checkerboard pattern as calibration reference, viewed from multiple angles and distances. By combining the PMD-camera with standard CCD-cameras the internal camera parameters can be estimated more precisely and the limitations of the small fov can be overcome. Furthermore we use the additional cameras to calibrate the systematic depth measurement error of the PMD-camera. We show that the correlation between rotation and translation estimation is significantly reduced with our method."}
{"_id":"c40ee92a78975f110480cb7e089ddfb8af25a337","title":"Learning Linear Threshold Functions in the Presence of Classification Noise","text":"I show that the linear threshold functions are polynomially learnable in the presence of classification noise, i.e., polynomial in <italic>n<\/italic>, 1\/\u03b5, 1\/\u03b4, and 1\/&sgr;, where <italic>n<\/italic> is the number of Boolean attributes, \u03b5 and \u03b4 are the usual accuracy and confidence parameters, and &sgr; indicates the minimum distance of any example from the target hyperplane, which is assumed to be lower than the average distance of the examples from any hyperplane. This result is achieved by modifying the Perceptron algorithm\u2014for each update, a weighted average of a sample of misclassified examples and a correction vector is added to the current weight vector. Similar modifications are shown for the Weighted Majority algorithm. The correction vector is simply   the mean of the normalized examples. In the special case of Boolean threshold functions, the modified Perceptron algorithm performs <italic>O<\/italic> (<italic>n<\/italic><supscrpt>2<\/supscrpt>\u03b5<supscrpt>\u22122<\/supscrpt> ) iterations over <italic>O<\/italic>(<italic>n<\/italic><supscrpt>4<\/supscrpt>\u03b5<supscrpt> \u22122<\/supscrpt>ln(<italic>n<\/italic>\/(\u03b4\u03b5))) examples. This improves on the previous classification-noise result of Angluin and Laird to a much larger concept class with a similar number of examples, but with multiple iterations over the examples."}
{"_id":"09ef59bb37b619ef5850060948674cb495ddebb6","title":"Usability Evaluation of Google Classroom : Basis for the Adaptation of GSuite E-Learning Platform","text":"Electronic learning is a technology learning that plays an important role in modern education and training. Its great contribution lies in the fact that content is available at any place and device from a fixed device to mobile device. Nowadays, education is accessible everywhere with the use of technology. There are several LMS (Learning Management Systems) available. One of the new tool available was released by Google under GSuite. Pangasinan State University is currently subscribed to GSuite for Education, and recently Google introduces Classroom as an eLearning platform for an educational institution. This research aims to evaluate the new product, its functionalities for the purpose of adapting and deployment. The main objective of this paper is to identify the usability and evaluation of the Learning Management System (LMS) Google Classroom, its functionalities, features, and satisfaction level of the students. Based on the result, the respondents agreed that GSuite classroom is recommended. The result of this study will be the proposed e-learning platform for Pangasinan State University, Lingayen Campus initially need in the College of Hospitality Management, Business and Public Administration."}
{"_id":"e1010530b9cfbaba451f37987a5feb3c659cda1f","title":"Learning to Read Chest X-Ray Images from 16000+ Examples Using CNN","text":"Chest radiography (chest X-ray) is a low-cost yet effective and widely used medical imaging procedures. The lacking of qualified radiologist seriously limits the applicability of the technique. We explore the possibility of designing a computer-aided diagnosis for chest X-rays using deep convolutional neural networks. Using a real-world dataset of 16,000 chest X-rays with natural language diagnosis reports, we can train a multi-class classification model from images and preform accurate diagnosis, without any prior domain knowledge."}
{"_id":"b89c61aeaa22c42e35f2e12daa32a04a9c276b07","title":"Automatic detection of discontinuities from 3D point clouds for the stability analysis of jointed rock masses","text":"We describe an strategy for automatic detection of discontinuities in jointed rock masses from a 3D point cloud. The method consists on a sequence of processes for feature extraction and characterization of the discontinuities that use voxelization, robust detection of planes by sequential RANSAC and clustering. The results obtained by applying this methodology to the 3D point cloud are a set of discontinuity families, characterized by their dips and dip directions, and surface wedges. This geometric information is essential for a later stability analysis of the jointed rock mass. The ultimate aim of our investigation is the automatic extraction of discontinuity families for stability analysis of tunnels excavated in jointed rock masses."}
{"_id":"bec6af81b99e3254d49bed9d5c9ebe941cf13c4d","title":"Efficient calibration for rssi-based indoor localization by bayesian experimental design on multi-task classification","text":"RSSI-based indoor localization is getting much attention. Thanks to a number of researchers, the localization accuracy has already reached a sufficient level. However, it is still not easy-to-use technology because of its heavy installation cost. When an indoor localization system is installed, it needs to collect RSSI data for training classifiers. Existing techniques need to collect enough data at each location. This is why the installation cost is very heavy. We propose a technique to gather data efficiently by using machine learning techniques. Our proposed algorithm is based on multi-task learning and Bayesian optimization. This algorithm can remove the need to collect data of all location labels and select location labels to acquire new data efficiently. We verify this algorithm by using a Wi-Fi RSSI dataset collected in a building. The empirical results suggest that the algorithm is superior to an existing algorithm applying single-task learning and Active Class Selection."}
{"_id":"6b24792298d47409cdf23012f593d49e2be0d4f3","title":"Deep Neural Architecture for Multi-Modal Retrieval based on Joint Embedding Space for Text and Images","text":"Recent advances in deep learning and distributed representations of images and text have resulted in the emergence of several neural architectures for cross-modal retrieval tasks, such as searching collections of images in response to textual queries and assigning textual descriptions to images. However, the multi-modal retrieval scenario, when a query can be either a text or an image and the goal is to retrieve both a textual fragment and an image, which should be considered as an atomic unit, has been significantly less studied. In this paper, we propose a gated neural architecture to project image and keyword queries as well as multi-modal retrieval units into the same low-dimensional embedding space and perform semantic matching in this space. The proposed architecture is trained to minimize structured hinge loss and can be applied to both cross- and multi-modal retrieval. Experimental results for six different cross- and multi-modal retrieval tasks obtained on publicly available datasets indicate superior retrieval accuracy of the proposed architecture in comparison to the state-of-art baselines."}
{"_id":"8fbd9a178014f9a9b0d8834315f03aa725d8b1a5","title":"From \u201cclassic\u201d child abuse and neglect to the new era of maltreatment","text":"The evolution of the concept of child abuse leads to consider new types of maltreatment that in the future will certainly be taken into account with a new era of social pediatrics.Pediatric care has been based on the increased awareness of the importance of meeting the psychosocial and developmental needs of children and of the role of families in promoting the health."}
{"_id":"f5eeacec4b4a19e60b6fc187dc08aca12dd13625","title":"Overcoming Security Challenges in Microservice Architectures","text":"The microservice architectural style is an emerging trend in software engineering that allows building highly scalable and flexible systems. However, current state of the art provides only limited insight into the particular security concerns of microservice system. With this paper, we seek to unravel some of the mysteries surrounding microservice security by: providing a taxonomy of microservices security; assessing the security implications of the microservice architecture; and surveying related contemporary solutions, among others Docker Swarm and Netflix security decisions. We offer two important insights. On one hand, microservice security is a multi-faceted problem that requires a layered security solution that is not available out of the box at the moment. On the other hand, if these security challenges are solved, microservice architectures can improve security; their inherent properties of loose coupling, isolation, diversity, and fail fast all contribute to the increased robustness of a system. To address the lack of security guidelines this paper describes the design and implementation of a simple security framework for microservices that can be leveraged by practitioners. Proof-of-concept evaluation results show that the performance overhead of the security mechanisms is around 11%."}
{"_id":"48c010d7ba115ae9bd5535d6932c77238c3f9926","title":"vDesign: a CAVE-based virtual design environment using hand interactions","text":"The cave automatic virtual environment (CAVE) system is one of the most fully immersive systems for virtual reality environments. By providing users with realistic perception and immersive experience, CAVE systems have been widely used in many fields, including military, education, health care, entertainment, design, and others. In this paper, we focus on the design applications in the CAVE. The design applications involve many interactions between the user and the CAVE. However, the conventional interaction tool, the wand, cannot provide fast and convenient interactions. In this paper, we propose vDesign, a CAVE-based virtual design environment using hand interactions. The hand interactions in vDesign are classified into menu navigation and object manipulations. For menu navigation, we define two interactions: activating the main menu and selecting a menu item. For object manipulations, we define three interactions: moving, rotating, and scaling an object. By using the proposed hand interactions, we develop the functions of image segmentation and image composition in vDesign. With the image segmentation function, the designer can select and cut the interested objects from different images. With the image composition function, the designer can manipulate the segmented objects and combine them as a composite image. We implemented the vDesign prototype in CAVE and conducted experiments to evaluate the interaction performance in terms of manipulation time and distortion. The experimental results demonstrated that the proposed hand interactions can provide faster and more accurate interactions compared to the traditional wand interactions."}
{"_id":"3805cd9f0db2a71bd33cb72ad6ca7bd23fe95e35","title":"A support vector approach for cross-modal search of images and texts","text":"Building bilateral semantic associations between images and texts is among the fundamental problems in computer vision. In this paper, we study two complementary cross-modal prediction tasks: (i) predicting text(s) given a query image (\u201cIm2Text\u201d), and (ii) predicting image(s) given a piece of text (\u201cText2Im\u201d). We make no assumption on the specific form of text; i.e., it could be either a set of labels, phrases, or even captions. We pose both these tasks in a retrieval framework. For Im2Text, given a query image, our goal is to retrieve a ranked list of semantically relevant texts from an independent text-corpus (i.e., texts with no corresponding images). Similarly, for Text2Im, given a query text, we aim to retrieve a ranked list of semantically relevant images from a collection of unannotated images (i.e., images without any associated textual meta-data). We propose a novel Structural SVM based unified framework for these two tasks, and show how it can be efficiently trained and tested. Using a variety of loss functions, extensive experiments are conducted on three popular datasets (two medium-scale datasets containing few thousands of samples, and one web-scale dataset containing one million samples). Experiments demonstrate that our framework gives promising results compared to competing baseline cross-modal search techniques, thus confirming its efficacy."}
{"_id":"9f6f09db2e6c1db8fba09844fbb02ac0260635a7","title":"M-Learning: A New Paradigm of Learning Mathematics in Malaysia","text":"M-Learning is a new learning paradigm of the new social structure with mobile and wireless technologies. Smart school is one of the four flagship applications for Multimedia Super Corridor (MSC) under Malaysian government initiative to improve education standard in the country. With the advances of mobile devices technologies, mobile learning could help the government in realizing the initiative. This paper discusses the prospect of implementing mobile learning for primary school students. It indicates significant and challenges and analysis of user perceptions on potential mobile applications through a survey done in primary school context. The authors propose the m-Learning for mathematics by allowing the extension of technology in the traditional classroom in term of learning and teaching."}
{"_id":"b35a21517d0f1e134172581c698b3654b9bde95b","title":"Brand Name Logo Recognition of Fast Food and Healthy Food among Children","text":"The fast food industry has been increasingly criticized for creating brand loyalty in young consumers. Food marketers are well versed in reaching children and youth given the importance of brand loyalty on future food purchasing behavior. In addition, food marketers are increasingly targeting the Hispanic population given their growing spending power. The fast food industry is among the leaders in reaching youth and ethnic minorities through their marketing efforts. The primary objective of this study was to determine if young children recognized fast food restaurant logos at a higher rate than other food brands. Methods Children (n\u00a0=\u00a0155; 53% male; 87% Hispanic) ages 4\u20138\u00a0years were recruited from elementary schools and asked to match 10 logo cards to products depicted on a game board. Parents completed a survey assessing demographic and psychosocial characteristics associated with a healthy lifestyle in the home. Results Older children and children who were overweight were significantly more likely to recognize fast food restaurant logos than other food logos. Moreover, parents\u2019 psychosocial and socio-demographic characteristics were associated with the type of food logo recognized by the children. Conclusions Children\u2019s high recognition of fast food restaurant logos may reflect greater exposure to fast food advertisements. Families\u2019 socio-demographic characteristics play a role in children\u2019s recognition of food logos."}
{"_id":"efa13067ad6afb062db44e20116abf2c89b7e9a9","title":"LSTM Model to Forecast Time Series for EC2 Cloud Price","text":"With the widespread use of spot instances in Amazon EC2, which utilize unused capacity with unfixed price, predicting price is important for users. In this paper, we try to forecast spot instance price by using long short-term memory (LSTM) algorithm to predict time series of the prices. We apply cross validation technique to our data set, and extract some features; this help our model to learn deeply. We make the prediction for 10 regions, and measure the performance using root-mean-square error (RMSE). We apply our data to other statistical models to compare their performance; we found that our model works more efficiently, also the error is decreasing more with cross validation and the result is much faster. Based on our result we try to find the availability zone that less become over on-demand price and less changing price over time, which help users to choose the most stable availability zone."}
{"_id":"789506ae5f631d88298c870607630344c26b1b59","title":"Automatic scraper of celebrity images from heterogeneous websites based on face recognition and sorting for profiling","text":"Now days it has become trend to follow all the celebrities as we consider them as our role models. So instead of searching the images of various celebrities in different websites we can find them in a single website by sorting all the images. Reliable database of images is essential for any image recognition system. Through Internet we find all the required images. These images will serve as samples for automatic recognition system. With these images we do face detection, face recognition, face sorting using various techniques like local binary patterns, haar cascades. We make an overall analysis of the detector. Using opencv we detect and recognize images. Similarity matching is done to check how the images are related to each other. Collection of images is based on user defined templates, which are in web browser environment. With the help of this system one can give their requirement and the image of celebrity is displayed based on that."}
{"_id":"5ee0d8aeb2cb01ef4d8a858d234e72a7400c03ac","title":"Convolution Kernels on Discrete Structures","text":"We introduce a new method of constructing kernels on sets whose elements are discrete structures like strings, trees and graphs. The method can be applied iteratively to build a kernel on a innnite set from kernels involving generators of the set. The family of kernels generated generalizes the family of radial basis kernels. It can also be used to deene kernels in the form of joint Gibbs probability distributions. Kernels can be built from hidden Markov random elds, generalized regular expressions, pair-HMMs, or ANOVA de-compositions. Uses of the method lead to open problems involving the theory of innnitely divisible positive deenite functions. Fundamentals of this theory and the theory of reproducing kernel Hilbert spaces are reviewed and applied in establishing the validity of the method."}
{"_id":"41ab8a3c6088eb0576ba65e114ebd37340c2bae1","title":"What Does This Imply? Examining the Impact of Implicitness on the Perception of Hate Speech","text":""}
{"_id":"07233b4fb2231de9dc902fd694a6c205fa42d66a","title":"An autonomous polymerization motor powered by DNA hybridization.","text":"We present a synthetic molecular motor capable of autonomous nanoscale transport in solution. Inspired by bacterial pathogens such as Rickettsia rickettsii, which locomote by inducing the polymerization of the protein actin at their surfaces to form 'comet tails', the motor operates by polymerizing a double-helical DNA tail2. DNA strands are propelled processively at the living end of the growing polymers, demonstrating autonomous locomotion powered by the free energy of DNA hybridization."}
{"_id":"18914b4a81b917ee3b7e83adf51c68071dc26124","title":"0.18-V input charge pump with forward body biasing in startup circuit using 65nm CMOS","text":"In this paper, a 0.18-V input three-stage charge pump circuit applying forward body bias is proposed. In the developed charge pump, all the MOSFETs are forward body biased by using the inter-stage\/output voltages. By applying the proposed charge pump as the startup in the boost converter, the lower kick-up input voltage of the boost converter can be achieved. To verify the circuit characteristics, four test circuits have been implemented by using 65nm CMOS process. The measured available output current of the proposed charge pump under 0.18-V input voltage can be improved more than 150%. In addition, the boost converter can successfully been boosted from 0.18-V input to the 0.74-V output under 6mA output current. The proposed circuit is suitable for extremely low voltage applications such as harvesting energy sources."}
{"_id":"a407bc2cc5a5d8c25e1cb4c536ab6d92597134a3","title":"A Blockchain-Based Micro Economy Platform for Distributed Infrastructure Initiatives","text":"Distributed Infrastructure Initiatives (DIIs) are communities that collaboratively produce and consume infrastructure. To develop a healthy ecosystem, DIIs require an economic model that balances supply and demand, but there is currently a lack of tooling to support implementing these. In this research, we propose an architecture for a platform that enables DIIs to implement such models, focused around a digital currency based on blockchain technology. The currency is issued according to the amount participants contribute to the initiative, which is quantified based on operational metrics gathered from the infrastructure. Furthermore, the platform enables participants to deploy smart contracts which encode self-enforcing agreements about the infrastructure services they exchange. The architecture has been evaluated through a case study at The Things Network (TTN) a global distributed crowdsourced Internet of Things initiative. The case study revealed that the architecture is effective for the selected case at TTN. In addition, the results motivate future research lines to support scalability (i.e., to deploy the architecture on a larger scale) and security."}
{"_id":"0a82c11218726076711c225666893bc55f09f2d7","title":"Safe robotic grasping: Minimum impact-force grasp selection","text":"This paper addresses the problem of selecting from a choice of possible grasps, so that impact forces will be minimised if a collision occurs while the robot is moving the grasped object along a post-grasp trajectory. Such considerations are important for safety in human-robot interaction, where even a certified \u201chuman-safe\u201d (e.g. compliant) arm may become hazardous once it grasps and begins moving an object, which may have significant mass, sharp edges or other dangers. Additionally, minimising collision forces is critical to preserving longevity of robots which operate in uncertain and hazardous environments, e.g. robots deployed for nuclear decommissioning, where removing a damaged robot from a contaminated zone for repairs may be extremely difficult and costly. Also, unwanted collisions between a robot and critical infrastructure (e.g. pipework) in such high-consequence environments can be disastrous. In this paper we investigate how the safety of the post-grasp motion can be considered during the pre-grasp approach phase, so that the selected grasp is optimal in terms of applying minimum impact forces if a collision occurs during the desired post-grasp manipulation. We build on the methods of augmented robot-object dynamic model and \u201ceffective mass\u201d and propose a method for combining these concepts with modern grasp and trajectory planners, to enable the robot to achieve a grasp which maximises the safety of the post-grasp trajectory, by minimising potential collision forces. We demonstrate the effectiveness of our approach through several experiments with both simulated and real robots."}
{"_id":"551558995669a43113c85b638a89adb5775cc2ce","title":"Quantifying Political Polarity Based on Bipartite Opinion Networks","text":"Political inclinations of individuals (liberal vs. conservative) largely shape their opinions on several issues such as abortion, gun control, nuclear power, etc. These opinions are openly exerted in online forums, news sites, the parliament, and so on. In this paper, we address the problem of quantifying political polarity of individuals and of political issues for classification and ranking. We use signed bipartite networks to represent the opinions of individuals on issues, and formulate the problem as a node classification task. We propose a linear algorithm that exploits network effects to learn both the polarity labels as well as the rankings of people and issues in a completely unsupervised manner. Through extensive experiments we demonstrate that our proposed method provides an effective, fast, and easy-to-implement solution, while outperforming three existing baseline algorithms adapted to signed networks, on real political forum and US Congress datasets. Experiments on a wide variety of synthetic graphs with varying polarity and degree distributions of the nodes further demonstrate the robustness of our approach. Introduction Many individuals use online media to exert their opinions on a variety of topics. Hotly debated topics include liberal vs. conservative policies such as tax cuts and gun control, social issues such as abortion and same-sex marriage, environmental issues such as climate change and nuclear power plants, etc. These openly debated issues in blogs, forums, and news websites shape the nature of public opinion and affect the direction of politics, media, and public policy. In this paper, we address the problem of quantifying political polarity in opinion datasets. Given a set of individuals from two opposing camps (liberal vs. conservative) debating a set of issues or exerting opinions on a set of subjects (e.g. human subjects, political issues, congressional bills), we aim to address two problems: (1) classify which person lies in which camp, and which subjects are favored by each camp; and (2) rank the people and the subjects by the magnitude or extent of their polarity. Here while the classification enables us to determine the two camps, ranking helps us understand the extremity to which a person\/subject is polarized; e.g. same-sex marriage may be highly polarized among the Copyright c \u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. two camps (liberals being strictly in favor, and conservatives strictly being against), while gun control may not belong to a camp fully favoring or opposing it (i.e., is less polarized). Ranking also helps differentiate moderate vs. extreme individuals, as well as unifying vs. polarizing subjects; such as (unifying) bills voted in the same way, e.g. all \u2018yea\u2019 by the majority of congressmen vs. (polarizing) bills that are voted quite oppositely by the two camps. A large body of prior research focuses on sentiment analysis on politically oriented text (Cohen and Ruths 2013; Conover et al. 2011b; 2011a; Pak and Paroubek 2010; Tumasjan et al. 2010). The main goal of these works is to classify political text. In this work, on the other hand, we deal with network data to classify its nodes. Moreover, these methods mostly rely on supervised techniques whereas we focus on un\/semi-supervised classification. Other prior research on polarization have exploited link mining and graph clustering to study the social structure on social media networks (Adamic and Glance 2005; Livne et al. 2011; Conover et al. 2011b; Guerra et al. 2013) where the edges depict the \u2018mention\u2019 or \u2018hyperlink\u2019 relations and not opinions. Moreover, these works do not perform ranking. Different from previous works, our key approach is to exploit network effects to both classify and rank individuals and political subjects by their polarity. The opinion datasets can be effectively represented as signed bipartite networks, where one set of nodes represent individuals, the other set represent subjects, and signed edges between the individuals and subjects depict the +\/\u2212 opinions. As such, we cast the problem as a node classification task on such networks. Our main contributions can be summarized as follows: \u2022 We cast the political polarity classification and ranking problem as a node classification task on edge-signed bipartite opinion networks. \u2022 We propose an algorithm, called signed polarity propagation (SPP), that computes probabilities (i.e., polarity scores) of nodes of belonging to one of two classes (e.g., liberal vs. conservative), and use these scores for classification and ranking. Our method is easy-to-implement and fast\u2014running time grows linearly with network size. \u2022 We show the effectiveness of our algorithm, in terms of both prediction and ranking, on synthetic and real datasets with ground truth from the US Congress and Political Forum. Further, we modify three existing algorithms to handle signed networks, and compare them to SPP. Our experiments reveal the advantages and robustness of our method on diverse settings with various polarity and degree distributions. Related Work Scoring and ranking the nodes of a graph based on the network structure has been studied extensively, with wellknown algorithms like PageRank (Brin and Page 1998), and HITS (Kleinberg 1998). These, however, are applied on graphs where the edges are unsigned and therefore cannot be directly used to compute polarity scores. Computing polarity scores can be cast as a network classification problem, where the task is to assign probabilities (i.e., scores) to nodes of belonging to one of two classes, which is the main approach we take in our work. There exist a large body of work on network-based classification (Getoor et al. 2007; Neville and Jensen 2003; Macskassy and Provost 2003). Semi-supervised algorithms based on network propagation have also been used in classifying political orientation (Lin and Cohen 2008; Zhou, Resnick, and Mei 2011). However, all the existing methods work with unsigned graphs in which the edges do not represent opinions but simply relational connections such as HTML hyperlinks between blog articles or \u2018mention\u2019 relations in Twitter. Signed networks have only recently attracted attention (Leskovec, Huttenlocher, and Kleinberg 2010b). Most existing studies focused on tackling the edge sign prediction problem (Yang et al. 2012; Chiang et al. 2011; Leskovec, Huttenlocher, and Kleinberg 2010a; Symeonidis, Tiakas, and Manolopoulos 2010). Other works include the study of trust\/distrust propagation (Guha et al. 2004; DuBois, Golbeck, and Srinivasan 2011; Huang et al. 2012), and product\/merchant quality estimation from reviews (McGlohon, Glance, and Reiter 2010). These works do not address the problem of classifying and ranking nodes in signed graphs. With respect to studies on political orientation through social media, (Adamic and Glance 2005; Adar et al. 2004) use link mining and graph clustering to analyze political communities in the blogosphere. While these and most clustering algorithms are designed to work with unsigned graphs, there also exist approaches for clustering signed graphs (Traag and Bruggeman 2009; Lo et al. 2013; Zhang et al. 2013). Clustering, however, falls short in scoring the nodes and hence quantifying polarity for ranking. Related, (Livne et al. 2011) utilize graph and text mining techniques to analyze differences between political parties and their online media usage in conveying a cohesive message. Most recently, (Cohen and Ruths 2013) use supervised classification techniques to classify three groups of Twitter users with varying political activity (figures, active, and modest) by their political orientation. Other works that exploit supervised classification using text features include (Conover et al. 2011a; Golbeck and Hansen 2011; Pennacchiotti and Popescu 2011). Similar to (Adamic and Glance 2005), there exist related works that study the social structure for measuring polarity. These works rely on the existence of (unsigned) social links between the users of social media and study the communities induced by polarized debate; an immediate consequence of the homophily principle, which states that people with similar beliefs and opinions tend to establish social ties. (Livne et al. 2011) and (Conover et al. 2011b) both use modularity (Newman 2006) as a measure of segregation between political groups in Twitter. (Guerra et al. 2013) compare modularity of polarized and non-polarized networks and propose two new measures of polarity based on community boundaries. Again, these works do not study the opinion networks with signed edges. As our main task is quantifying polarity, work on sentiment analysis is also related. There exist a long list of works on sentiment and polarity prediction in political text (Tumasjan et al. 2010; Awadallah, Ramanath, and Weikum 2010; Conover et al. 2011b; He et al. 2012), as well as in tweets, blogs, and news articles (Pak and Paroubek 2010; Godbole, Srinivasaiah, and Skiena 2007; Thomas, Pang, and Lee 2006; Balasubramanyan et al. 2012). These differ from our work as they use text-based sentiment analysis, while we focus on the network effects in signed graphs. Proposed Method Problem Overview We consider the problem of polarity prediction and ranking in opinion datasets (e.g. forums, blogs, the congress). Opinion datasets mainly consist of a set of people (e.g. users in a forum, representatives in The House) and a set of subjects (e.g. political issues, political people, congressional bills). Each person often maintains a positive or negative opinion toward a particular subject (e.g. a representative votes \u2018yes\u2019 for a bill, a person \u2018likes\u2019 a political individual). This opinion is often an exposition of the person\u2019s latent political leaning\u2014liberal or conservative. For example, we could think of a person with strong negative opinion toward gay&lesbian rights to be more conservati"}
{"_id":"53176495ce9ea8e7b23278d6f90c13d37eff9312","title":"miRDeep2 accurately identifies known and hundreds of novel microRNA genes in seven animal clades","text":"microRNAs (miRNAs) are a large class of small non-coding RNAs which post-transcriptionally regulate the expression of a large fraction of all animal genes and are important in a wide range of biological processes. Recent advances in high-throughput sequencing allow miRNA detection at unprecedented sensitivity, but the computational task of accurately identifying the miRNAs in the background of sequenced RNAs remains challenging. For this purpose, we have designed miRDeep2, a substantially improved algorithm which identifies canonical and non-canonical miRNAs such as those derived from transposable elements and informs on high-confidence candidates that are detected in multiple independent samples. Analyzing data from seven animal species representing the major animal clades, miRDeep2 identified miRNAs with an accuracy of 98.6-99.9% and reported hundreds of novel miRNAs. To test the accuracy of miRDeep2, we knocked down the miRNA biogenesis pathway in a human cell line and sequenced small RNAs before and after. The vast majority of the >100 novel miRNAs expressed in this cell line were indeed specifically downregulated, validating most miRDeep2 predictions. Last, a new miRNA expression profiling routine, low time and memory usage and user-friendly interactive graphic output can make miRDeep2 useful to a wide range of researchers."}
{"_id":"ba86c86b4b8b7989698babf0280980235d598548","title":"GA3C: GPU-based A3C for Deep Reinforcement Learning","text":"We introduce and analyze the computational aspects of a hybrid CPU\/GPU implementation of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. Our analysis concentrates on the critical aspects to leverage the GPU\u2019s computational power, including the introduction of a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. We also show the potential for the use of larger DNN models on a GPU. Our TensorFlow implementation achieves a significant speed up compared to our CPU-only implementation, and it will be made publicly available to other researchers."}
{"_id":"a858bf95d8cdae54f21967f555b8831db6a149e3","title":"1 Augmented Reality","text":"This paper is an overview of technologies that fall under the term augmented reality. Augmented reality refers to a system in which the physical surroundings of a person are mixed with real-time computer generated information creating an enhanced perception of surrounding environment. Being partly virtual and real, augmented reality applications have quite extreme requirements to be practical to use. It also has very much potential in numerous different application areas. These issues make augmented reality both an interesting and challenging subject from scientific and business perspectives."}
{"_id":"ca9f8fadbd14253e02b07ae48fc7a5bac44f6a94","title":"Arrested development? Reconsidering dual-systems models of brain function in adolescence and disorders","text":"The dual-systems model of a ventral affective system, whose reactivity confers risks and liabilities, and a prefrontal control system, whose regulatory capacities buffer against these vulnerabilities, is an intuitive account that pervades many fields in the cognitive neurosciences--especially in the study of populations that differ from neurotypical adults, such as adolescents or individuals with affective or impulse regulation disorders. However, recent evidence that is inconsistent with dual-systems models illustrates the complexity of developmental and clinical variations in brain function. Building new models to account for this complexity is critical to progress in these fields, and will be facilitated by research that emphasizes network-based approaches and maps relationships between structure and function, as well as brain and behavior, over time."}
{"_id":"64c8217cc46df711f294cdd823d04ff2cc602280","title":"Gradient and Curvature from Photometric Stereo Including Local Confidence Estimation","text":"Photometric stereo is one technique for 3D shape determination that has been implemented in a variety of experimental settings and that has produced consistently good results. The idea is to use intensity values recorded from multiple images obtained from the same viewpoint but under different conditions of illumination. The resulting radiometric constraint makes it possible to obtain local estimates of both surface orientation and surface curvature without requiring global smoothness assumptions and without requiring prior image segmentation. This paper moves photometric stereo one step closer to practical viability by describing an experimental setting in which surface gradient estimation is achieved on full frame video data at near video frame rates (i.e., 15Hz). The implementation uses commercially available hardware. Reflectance is modeled empirically using measurements obtained from a calibration sphere. Estimation of the gradient, (p, q), requires only simple table lookup. Curvature estimation uses, in addition, the reflectance map, R(p, q). The required lookup table and reflectance maps are derived during calibration. Because reflectance is modeled empirically, no prior physical model of the reflectance characteristics of the objects to be analyzed is assumed. At the same time, if a good physical model is available, it can be retrofit to the method for implementation purposes. Photometric stereo is subject to error in the presence of cast shadows and interreflection. No purely local technique can succeed since these phenomena are inherently non-local. Nevertheless, this paper demonstrates that one can exploit the redundancy in three light source photometric stereo to, in most cases, locally detect the presence of cast shadows and interreflection. Detection is facilitated by explicitly including a local confidence estimate in the lookup table used for gradient estimation."}
{"_id":"857176d022369e963d3ff1be2cb9e1ca2f674520","title":"DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning","text":"We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.1"}
{"_id":"f5bbdfe37f5a0c9728c9099d85f0799e67e3d07d","title":"Challenges during the transition to Agile Methodologies : A Holistic Overview","text":""}
{"_id":"7a953aaf29ef67ee094943d4be50d753b3744573","title":"\"GrabCut\": interactive foreground extraction using iterated graph cuts","text":""}
{"_id":"24171780855cb31e5f7ea74908485e2a0142b620","title":"Challenges of design and practical application of LTCC chip antennas","text":"In this paper key challenges related to design, manufacturing and practical application of small LTCC antennas are described and discussed. A summary of the state of the art in LTCC antenna technology is presented and then the focus is put on limitations that have to be taken into account in the course of antenna design and manufacturing. Finally, some aspects related to practical application of LTCC antennas are discussed."}
{"_id":"ba2ceb8f6c9bf49da1b366e4757d202724c3bff4","title":"The radiation properties of electrically small folded spherical helix antennas","text":"The radiation properties of several electrically small, folded spherical helix antennas are presented. The primary variables considered in the design of these antennas are the number of helical turns and the number of helical arms. The principle design objectives are to achieve self resonance, a low quality factor (Q), and a practical radiation resistance for small values of ka. Designs are presented for ka less than 0.5, where the antennas are self resonant, exhibiting an efficiency in excess of 95%, a Q within 1.5 times the fundamental limit, and a radiation resistance near 50 \/spl Omega\/. The relationship between the number of helical turns, the number of helical arms, and achieving self resonance at low frequencies is discussed."}
{"_id":"cfd913e1edd1a15b7456ef6d222c1319e056eddc","title":"Small Spherical Antennas Using Arrays of Electromagnetically Coupled Planar Elements","text":"This letter presents the design, fabrication, and experimental characterization of small spherical antennas fabricated using arrays of non-interconnected planar conductor elements. The antennas are based upon spherical resonator structures with radiation Q-factors approaching $1.5\\times$ the fundamental lower limit. The antennas are formed by coupling these resonators to an impedance-matched coplanar strip transmission line. Direct electrical connection between the feed and the antenna are made only to conductor elements coplanar with the transmission line, simplifying the fabrication process. The incident energy excites a collective resonant mode of the entire sphere (an electric dipole resonance), thereby inducing currents in each of the rings of the structure. The presence of the conductor elements outside of the feed plane is critical towards achieving the excellent bandwidth behavior observed here. The fabricated antennas have a normalized size $ka=0.54$ (where $k$ is the wavenumber and $a$ is the radius of the sphere) and exhibit high radiative efficiencies ($>$ 90%) and bandwidth performance near the fundamental limit for their size."}
{"_id":"ae5d79dc7abd6b649a7d0bb9e108f79e2696e128","title":"The Spherical Coil as an Inductor, Shield, or Antenna","text":"The spherical coil is an idealized form of inductor having, on a spherical surface, a single-layer winding of constant axial pitch. Its magnetic field inside is uniform and outside is that of a small coil or magnetic dipole. Its properties exemplify exactly some of the rules that are approximately applicable to practical inductors. Simple formulas are given for self-inductance, mutual inductance, coupling coefficient, effect of iron core, and radiation power factor in free space or sea water. This coil is the basis for evaluating the shielding effect of a closed conductive (nonmagnetic) metal shell. A special winding is described which enables simple and exact computation of self-resonance (the length of wire being just 1\/2 wavelength in some cases)."}
{"_id":"f23ecb25c3250fc6e2d3401dc2f54ffd6135ae2e","title":"Substrate-Integrated Millimeter-Wave and Terahertz Antenna Technology","text":"Significant advances in the development of millimeter-wave and terahertz (30-10 000 GHz) technologies have been made to cope with the increasing interest in this still not fully explored electromagnetic spectrum. The nature of electromagnetic waves over this frequency range is well suited for the development of high-resolution imaging applications, molecular-sensitive spectroscopic devices, and ultrabroadband wireless communications. In this paper, millimeter-wave and terahertz antenna technologies are overviewed including the conventional and nonconventional planar\/nonplanar antenna structures based on different platforms. As a promising technological platform, substrate-integrated circuits (SICs) attract more and more attention. Various substrate-integrated waveguide (SIW) schemes and other synthesized guide techniques have been widely employed in the design of antennas and arrays. Different types of substrate-integrated antennas and beamforming networks are discussed with respect to theoretical and experimental results in connection with electrical and mechanical performances."}
{"_id":"5964e7db2d1e0ad4bcf239fe71142b81646c75d1","title":"An overlap invariant entropy measure of 3D medical image alignment","text":"This paper is concerned with the development of entropy-based registration criteria for automated 3D multi-modality medical image alignment. In this application where misalignment can be large with respect to the imaged field of view, invariance to overlap statistics is an important consideration. Current entropy measures are reviewed and a normalised measure is proposed which is simply the ratio of the sum of the marginal entropies and the joint entropy. The effect of changing overlap on current entropy measures and this normalised measure are compared using a simple image model and experiments on clinical image data. Results indicate that the normalised entropy measure provides significantly improved behaviour over a range of imaged fields of view. ( 1999 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."}
{"_id":"baf55ddda9949e15c7c6880affffe93b75acad45","title":"Node Representation Learning for Multiple Networks: The Case of Graph Alignment","text":"Recent advances in representation learning produce node embeddings that may be used successfully in many downstream tasks (e.g., link prediction), but do not generally extend beyond a single network. Motivated by the prevalence of multi-network problems, such as graph alignment, similarity, and transfer learning, we introduce an elegant and principled node embedding formulation, Cross-network Matrix Factorization or xNetMF, to effectively and efficiently learn embeddings from similarities based on the nodes\u2019 structural and attribute identities (if available), which are comparable across networks. Leveraging the Nystr\u00f6m method for low-rank approximation, we achieve significant reduction in space and time compared to the na\u00efve approach of directly factorizing the node similarity matrix, with xNetMF running up to 30\u00d7 faster than existing representation learning methods. We study its effectiveness on the task of network alignment, and introduce REGAL (REpresentationbased Graph ALignment), the first method to use node representation learning for this task. Extensive experiments on a variety of challenging real-world networks with and without attributes show that REGAL outperforms existing network alignment methods by 20 to 30% in accuracy on average, uses attributes up to 40\u00d7 faster, and scales to networks with millions of nodes each."}
{"_id":"5af4ca674872b121fdbbdd46c38db4989dc5fb9d","title":"Canine computer interaction: towards designing a touchscreen interface for working dogs","text":"Touchscreens can provide a way for service dogs to relay emergency information about their handlers from a home or office environment. In this paper, we build on work exploring the ability of canines to interact with touchscreen interfaces. We observe new requirements for training and explain best practices found in training techniques. Learning from previous work, we also begin to test new dog interaction techniques such as lift-off selection and sliding gestural motions. Our goal is to understand the affordances needed to make touchscreen interfaces usable for canines and help the future design of touchscreen interfaces for assistance dogs in the home."}
{"_id":"ee8cf34756e95f4f58522a663111b04858a8baaa","title":"Exploring large scale data for multimedia QA: an initial study","text":"With the explosive growth of multimedia contents on the internet, multimedia search has become more and more important. However, users are often bewildered by the vast quantity of information content returned by the search engines. In this scenario, Multimedia Question-Answering (MMQA) emerges as a way to return precise answers by leveraging advanced media content and linguistic analysis as well as domain knowledge. This paper performs an initial study on exploring large scale data for MMQA. First, we construct a web video dataset and discuss its query strategy, statistics, feature description and groundtruth. We then conduct experiments based on the dataset to answer definition event questions using three schemes. We finally conclude the study with discussion for future work."}
{"_id":"e2d634ee9e9abaca804b69af69a40cf00897b2d0","title":"Industrial Robotics","text":""}
{"_id":"353082e23c6677ce6f6af602bbb010e737a99bd0","title":"An experimental study of big spatial data systems","text":"With the rise of location-aware IoT devices, there is an increased desire to process and manage the stationary and moving trajectory data generated by these real-time sensors. There has been a corresponding evolution of distributed database and compute technology to handle the increasing data load. Here we describe challenges in managing this kind of data and survey the technologies that address those challenges. We evaluate two distributed database technologies against a simulated high volume sensor data source and discuss the performance of predominant moving trajectory access patterns."}
{"_id":"62580bd7f32994e4d0f97860f994d0cf237eaf46","title":"Time-Frequency Distributions Based on Compact Support Kernels: Properties and Performance Evaluation","text":"This paper presents two new time-frequency distributions (TFDs) based on kernels with compact support (KCS) namely the separable (CB) (SCB) and the polynomial CB (PCB) TFDs. The implementation of this family of TFDs follows the method developed for the Cheriet-Belouchrani (CB) TFD. The mathematical properties of these three TFDs are analyzed and their performance is compared to the best classical quadratic TFDs using several tests on multicomponent signals with linear and nonlinear frequency modulation (FM) components including the noise effects. Instead of relying solely on visual inspection of the time-frequency domain plots, comparisons include the time slices' plots and the evaluation of the Boashash-Sucic's normalized instantaneous resolution performance measure that permits to provide the optimized TFD using a specific methodology. In all presented examples, the KCS-TFDs show a significant interference rejection, with the component energy concentration around their respective instantaneous frequency laws yielding high resolution measure values."}
{"_id":"c5d5ff11d974574fb79907151dba2f23b5f08063","title":"On the Capacity Comparison Between MIMO-NOMA and MIMO-OMA","text":"Non-orthogonal multiple access (NOMA) has been shown in the literature to have a better performance than OMA in terms of sum channel capacity; however, the capacity superiority of NOMA over OMA has been only proved for single antenna systems, and the proof for the capacity superiority of multiple-input multiple-output NOMA (MIMO-NOMA) over conventional MIMO-OMA has not been available yet. In this paper, we will provide our proof to demonstrate that the MIMO-NOMA is strictly better than MIMO-OMA in terms of sum channel capacity (except for the case where only one user is being communicated to), i.e., for any rate pair achieved by MIMO-OMA, there is a power split for which MIMO-NOMA can achieve rate pairs that are strictly larger. Based on this result, we prove that the MIMO-NOMA can also achieve a larger sum ergodic capacity than MIMO-OMA. Our analytical results are verified by simulations."}
{"_id":"ee0a18889a0007a88e9411880bb1c2e75a7610ea","title":"Analysis of the Impact of Negative Sampling on Link Prediction in Knowledge Graphs","text":"Knowledge graphs are large, useful, but incomplete knowledge repositories. \u008cey encode knowledge through entities and relations which de\u0080ne each other through the connective structure of the graph. \u008cis has inspired methods for the joint embedding of entities and relations in continuous low-dimensional vector spaces, that can be used to induce new edges in the graph, i.e., link prediction in knowledge graphs. Learning these representations relies on contrasting positive instances with negative ones. Knowledge graphs include only positive relation instances, leaving the door open for a variety of methods for selecting negative examples. We present an empirical study on the impact of negative sampling on the learned embeddings, assessed through the task of link prediction. We use state-of-the-art knowledge graph embedding methods \u2013 Rescal , TransE, DistMult and ComplEX \u2013 and evaluate on benchmark datasets \u2013 FB15k and WN18. We compare well known methods for negative sampling and propose two new embedding based sampling methods. We note a marked di\u0082erence in the impact of these sampling methods on the two datasets, with the \u201dtraditional\u201d corrupting positives method leading to best results on WN18, while embedding based methods bene\u0080t FB15k."}
{"_id":"6e653192f5afd42c4f05bcc44afb2c6bd37510ac","title":"BRIDE - A toolchain for framework-independent development of industrial service robot applications","text":"Software integration is still a challenging and time consuming task and therefore a major part of the development of industrial and domestic service robot applications. The presented toolchain BRIDE is able to streamline this process by the separation of user roles and the separation of developer concerns of software components to ensure a frame-work independent implementation. The impact of the BRIDE toolchain in the development process is demonstrated in a case study on the SyncMM mobile manipulation control framework."}
{"_id":"1b90942a7661d956115716f33bd23deb4632266e","title":"The String B-tree: A New Data Structure for String Search in External Memory and Its Applications","text":"We introduce a new text-indexing data structure, the String B-Tree, that can be seen as a link between some traditional external-memory and string-matching data structures. In a short phrase, it is a combination of B-trees and Patricia tries for internal-node indices that is made more effective by adding extra pointers to speed up search and update operations. Consequently, the String B-Tree overcomes the theoretical limitations of inverted files, B-trees, prefix B-trees, suffix arrays, compacted tries and suffix trees. String B-trees have the same worst-case performance as B-trees but they manage unbounded-length strings and perform much more powerful search operations such as the ones supported by suffix trees. String B-trees are also effective in main memory (RAM model) because they improve the online suffix tree search on a dynamic set of strings. They also can be successfully applied to database indexing and software duplication."}
{"_id":"36246ff904be7044ecd536d072b1388ea59aaf43","title":"Risk indicators and psychopathology in traumatised children and adolescents with a history of sexual abuse","text":"Childhood sexual abuse (CSA) is widespread amongst South African (SA) children, yet data on risk factors and psychiatric consequences are limited and mixed. Traumatised children and adolescents referred to our Youth Stress Clinic were interviewed to obtain demographic, sexual abuse, lifetime trauma and psychiatric histories. Data for 94 participants (59 female, 35 male; mean age 14.25 [8.25\u201319] years) exposed to at least one lifetime trauma were analysed. Sexual abuse was reported in 53% of participants (42.56% females, 10.63% males) with 64% of violations committed by perpetrators known to them. Multinomial logistic regression analysis revealed female gender (P\u00a0=\u00a00.002) and single-parent families (P\u00a0=\u00a00.01) to be significant predictors of CSA (62.5%). CSA did not predict exposure to other traumas. Sexually abused children had significantly higher physical and emotional abuse subscale scores and total CTQ scores than non-abused children. Depression (33%, X 2\u00a0=\u00a010.89, P\u00a0=\u00a00.001) and PTSD (63.8%, X 2\u00a0=\u00a04.79, P\u00a0=\u00a00.034) were the most prevalent psychological consequences of trauma and both were significantly associated with CSA. High rates of CSA predicted high rates of PTSD in this traumatised sample. Associations we found appear consistent with international studies of CSA and, should be used to focus future social awareness, prevention and treatment strategies in developing countries."}
{"_id":"7e3b4d3b257eb2b907aafd6d158e7fe468fe9eee","title":"Estimating software based on use case points","text":"It is well documented that software product cost estimates are notoriously inaccurate across the software industry. Creating accurate cost estimates for software product development projects early in the product development lifecycle has always been a challenge for the industry. This article describes how a large multi-team software engineering organization (over 450 engineers) estimates project cost accurately and early in the software development lifecycle using Use Case Points, and the process of evaluating metrics to ensure the accuracy of the model.The engineering teams of Agilis Solutions in partnership with FPT Software, provide our customers with accurate estimates for software product projects early in the product lifecycle. The bases for these estimates are initial definitions of Use Cases, given point factors and modified for technical and environmental factors according to the Use Case Point method defined within the Rational Unified Process. After applying the process across hundreds of sizable (60 man-months average) software projects, we have demonstrated metrics that prove an estimating accuracy of less than 9% deviation from actual to estimated cost on 95% of our projects. Our process and this success factor is documented over a period of five years, and across more than 200 projects."}
{"_id":"1f0465bdee312659d13dbb303ca93df6ac259c40","title":"Fast Change Point Detection on Dynamic Social Networks","text":"A number of real world problems in many domains (e.g. sociology, biology, political science and communication networks) can be modeled as dynamic networks with nodes representing entities of interest and edges representing interactions among the entities at different points in time. A common representation for such models is the snapshot model where a network is defined at logical time-stamps. An important problem under this model is change point detection. In this work we devise an effective and efficient three-step-approach for detecting change points in dynamic networks under the snapshot model. Our algorithm achieves up to 9X speedup over the state-of-the-art while improving quality on both synthetic and real world networks."}
{"_id":"5d52ab2645380ca0732717a914f337f7a1b1c4a5","title":"Thin skin elastodynamics","text":"We present a novel approach for simulating thin hyperelastic skin. Real human skin is only a few millimeters thick. It can stretch and slide over underlying body structures such as muscles, bones, and tendons, revealing rich details of a moving character. Simulating such skin is challenging because it is in close contact with the body and shares its geometry. Despite major advances in simulating elastodynamics of cloth and soft bodies for computer graphics, such methods are difficult to use for simulating thin skin due to the need to deal with non-conforming meshes, collision detection, and contact response. We propose a novel Eulerian representation of skin that avoids all the difficulties of constraining the skin to lie on the body surface by working directly on the surface itself. Skin is modeled as a 2D hyperelastic membrane with arbitrary topology, which makes it easy to cover an entire character or object. Unlike most Eulerian simulations, we do not require a regular grid and can use triangular meshes to model body and skin geometry. The method is easy to implement, and can use low resolution meshes to animate high-resolution details stored in texture-like maps. Skin movement is driven by the animation of body shape prescribed by an artist or by another simulation, and so it can be easily added as a post-processing stage to an existing animation pipeline. We provide several examples simulating human and animal skin, and skin-tight clothes."}
{"_id":"af82f1b9fdee7e6f92fccab2e6f02816965bf937","title":"Boosting Alzheimer Disease Diagnosis Using PET Images","text":"Alzheimer's disease (AD) is one of the most frequent type of dementia. Currently there is no cure for AD and early diagnosis is crucial to the development of treatments that can delay the disease progression. Brain imaging can be a biomarker for Alzheimer's disease. This has been shown in several works with MR Images, but in the case of functional imaging such as PET, further investigation is still needed to determine their ability to diagnose AD, especially at the early stage of Mild Cognitive Impairment (MCI). In this paper we study the use of PET images of the ADNI database for the diagnosis of AD and MCI. We adopt a Boosting classification method, a technique based on a mixture of simple classifiers, which performs feature selection concurrently with the segmentation thus is well suited to high dimensional problems. The Boosting classifier achieved an accuracy of 90.97% in the detection of AD and 79.63% in the detection of MCI."}
{"_id":"0d2edc46f81f9a0b0b62937507ad977b46729f64","title":"The BOBYQA algorithm for bound constrained optimization without derivatives","text":"BOBYQA is an iterative algorithm for finding a minimum of a function F (x), x\u2208R, subject to bounds a\u2264x\u2264b on the variables, F being specified by a \u201cblack box\u201d that returns the value F (x) for any feasible x. Each iteration employs a quadratic approximation Q to F that satisfies Q(y j ) = F (y j ), j = 1, 2, . . . , m, the interpolation points y j being chosen and adjusted automatically, but m is a prescribed constant, the value m= 2n+1 being typical. These conditions leave much freedom in Q, taken up when the model is updated by the highly successful technique of minimizing the Frobenius norm of the change to the second derivative matrix of Q. Thus no first derivatives of F are required explicitly. Most changes to the variables are an approximate solution to a trust region subproblem, using the current quadratic model, with a lower bound on the trust region radius that is reduced cautiously, in order to keep the interpolation points well separated until late in the calculation, which lessens damage from computer rounding errors. Some other changes to the variables are designed to improve the model without reducing F . These techniques are described. Other topics include the starting procedure that is given an initial vector of variables, the value of m and the initial trust region radius. There is also a new device called RESCUE that tries to restore normality if severe loss of accuracy occurs in the matrix calculations of the updating of the model. Numerical results are reported and discussed for two test problems, the numbers of variables being between 10 and 320. Department of Applied Mathematics and Theoretical Physics, Centre for Mathematical Sciences, Wilberforce Road, Cambridge CB3 0WA, England."}
{"_id":"c23136a48527a20d6bfef019337ba4494077f7c5","title":"Stigma and its public health implications","text":""}
{"_id":"8dd33fcab782ef611b6ff28ce2f2a355ee5a7a5c","title":"Convolutional Neural Networks for Automatic State-Time Feature Extraction in Reinforcement Learning Applied to Residential Load Control","text":"Direct load control of a heterogeneous cluster of residential demand flexibility sources is a high-dimensional control problem with partial observability. This paper proposes a novel approach that uses a convolutional neural network (CNN) to extract hidden state-time features to mitigate the curse of partial observability. More specific, a CNN is used as a function approximator to estimate the state-action value function or <inline-formula> <tex-math notation=\"LaTeX\">${Q}$ <\/tex-math><\/inline-formula>-function in the supervised learning step of fitted <inline-formula> <tex-math notation=\"LaTeX\">${Q}$ <\/tex-math><\/inline-formula>-iteration. The approach is evaluated in a qualitative simulation, comprising a cluster of thermostatically controlled loads that only share their air temperature, while their envelope temperature remains hidden. The simulation results show that the presented approach is able to capture the underlying hidden features and able to successfully reduce the electricity cost the cluster."}
{"_id":"1713695284c16e05d88b719c52f073dc9f5a0e06","title":"Forgetting Is Regulated through Rac Activity in Drosophila","text":"Initially acquired memory dissipates rapidly if not consolidated. Such memory decay is thought to result either from the inherently labile nature of newly acquired memories or from interference by subsequently attained information. Here we report that a small G protein Rac-dependent forgetting mechanism contributes to both passive memory decay and interference-induced forgetting in Drosophila. Inhibition of Rac activity leads to slower decay of early memory, extending it from a few hours to more than one day, and to blockade of interference-induced forgetting. Conversely, elevated Rac activity in mushroom body neurons accelerates memory decay. This forgetting mechanism does not affect memory acquisition and is independent of Rutabaga adenylyl cyclase-mediated memory formation mechanisms. Endogenous Rac activation is evoked on different time scales during gradual memory loss in passive decay and during acute memory removal in reversal learning. We suggest that Rac's role in actin cytoskeleton remodeling may contribute to memory erasure."}
{"_id":"5a95410f9f8c842fbd3fdea9b2dabbc60a8547ee","title":"Automatic Labelling of Tabla Signals","text":"Most of the recent developments in the field of music indexing and music information retrieval are focused on western music. In this paper, we present an automatic music transcription system dedicated to Tabla a North Indian percussion instrument. Our approach is based on three main steps: firstly, the audio signal is segmented in adjacent segments where each segment represents a single stroke. Secondly, rhythmic information such as relative durations are calculated using beat detection techniques. Finally, the transcription (recognition of the strokes) is performed by means of a statistical model based on Hidden Markov Model (HMM). The structure of this model is designed in order to represent the time dependencies between successives strokes and to take into account the specificities of the tabla score notation (transcription symbols may be context dependent). Realtime transcription of Tabla soli (or performances) with an error rate of 6.5% is made possible with this transcriber. The transcription system, along with some additional features such as sound synthesis or phrase correction, are integrated in a user-friendly environment called Tablascope."}
{"_id":"1ff3ebd402e29c3af7226ece7f1d716daf1eb4a9","title":"A 64 GHz 2 Gbps transmit\/receive phased-array communication link in SiGe with 300 meter coverage","text":"This paper presents a 64 GHz transmit\/receive communication link between two 32-element SiGe-based phased arrays. The antenna element is a series-fed patch array, which provides directivity in the elevation plane. The transmit array results in an EIRP of 42 dBm, while the receive array provides an electronic gain of 33 dB and a system NF < 8 dB including the T\/R switch and antenna losses. The arrays can be scanned to +\/\u221250\u00b0 in the azimuth using a 5-bit phase shifter on the SiGe chip, while keeping very low sidelobes and a near-ideal pattern. The communication link uses one array on the transmit side and another array on the receive side, together with external mixers and IF amplifiers. A Keysight M8195A arbitrary waveform generator is used to generate the modulated waveforms on the transmit side and a Keysight DSO804A oscilloscope is used to demodulate the received IF signal. The link performance was measured for different scan angles and modulation formats. Data rates of 1 Gbps using 16-QAM and 2 Gbps using QPSK are demonstrated at 300 m. The system also results in > 4 Gbps data rate at 100 meters, and \u223c 500 Mbps data rate at 800 meters."}
{"_id":"2fb03a66f250a2c51eb2eb30344a13a5e4d8a265","title":"Fabrication and measurement of a large, monolithic, PCB-based AESA","text":"This paper discusses a fabrication approach and experimental validation of a very large, planar active electronically scanned array (AESA). The planar AESA architecture employs a monolithic printed circuit board (PCB) with 768 active antenna elements at X-Band. Manufacturing physically large arrays with high element counts is discussed in relation to construction, assembly and yield considerations. Measured active array patterns of the ESA are also presented."}
{"_id":"5ec3ee90bbc5b23e748d82cb1914d1c45d85bdd9","title":"A Millimeter-Wave (40\u201345 GHz) 16-Element Phased-Array Transmitter in 0.18-$\\mu$ m SiGe BiCMOS Technology","text":"This paper demonstrates a 16-element phased-array transmitter in a standard 0.18-mum SiGe BiCMOS technology for Q-band satellite applications. The transmitter array is based on the all-RF architecture with 4-bit RF phase shifters and a corporate-feed network. A 1:2 active divider and two 1:8 passive tee-junction dividers constitute the corporate-feed network, and three-dimensional shielded transmission-lines are used for the passive divider to minimize area. All signals are processed differentially inside the chip except for the input and output interfaces. The phased-array transmitter results in a 12.5 dB of average power gain per channel at 42.5 GHz with a 3-dB gain bandwidth of 39.9-45.6 GHz. The RMS gain variation is < 1.3 dB and the RMS phase variation is < for all 4-bit phase states at 35-50 GHz. The measured input and output return losses are < -10 dB at 36.6-50 GHz, and <-10 dB at 37.6-50 GHz, respectively. The measured peak-to-peak group delay variation is plusmn 20 ps at 40-45 GHz. The output P-1dB is -5plusmn1.5 dBm and the maximum saturated output power is - 2.5plusmn1.5 dBm per channel at 42.5 GHz. The transmitter shows <1.8 dB of RMS gain mismatch and < 7deg of RMS phase mismatch between the 16 different channels over all phase states. A - 30 dB worst-case port-to-port coupling is measured between adjacent channels at 30-50 GHz, and the measured RMS gain and phase disturbances due to the inter-channel coupling are < 0.15 dB and < 1deg, respectively, at 35-50 GHz. All measurements are obtained without any on-chip calibration. The chip consumes 720 mA from a 5 V supply voltage and the chip size is 2.6times3.2 mm2."}
{"_id":"a1b40af260487c00a2031df1ffb850d3bc368cee","title":"A 28GHz Bulk-CMOS dual-polarization phased-array transceiver with 24 channels for 5G user and basestation equipment","text":"Developing next-generation cellular technology (5G) in the mm-wave bands will require low-cost phased-array transceivers [1]. Even with the benefit of beamforming, due to space constraints in the mobile form-factor, increasing TX output power while maintaining acceptable PA PAE, LNA NF, and overall transceiver power consumption is important to maximizing link budget allowable path loss and minimizing handset case temperature. Further, the phased-array transceiver will need to be able to support dual-polarization communication. An IF interface to the analog baseband is desired for low power consumption in the handset or user equipment (UE) active antenna and to enable use of arrays of transceivers for customer premises equipment (CPE) or basestation (BS) antenna arrays with a low-loss IF power-combining\/splitting network implemented on an antenna backplane carrying multiple tiled antenna modules."}
{"_id":"0515ff7de41fd349b4bff34f7fe4e9c12a7fff47","title":"7.2 A 28GHz 32-element phased-array transceiver IC with concurrent dual polarized beams and 1.4 degree beam-steering resolution for 5G communication","text":"Next-generation mobile technology (5G) aims to provide an improved experience through higher data-rates, lower latency, and improved link robustness. Millimeter-wave phased arrays offer a path to support multiple users at high data-rates using high-bandwidth directional links between the base station and mobile devices. To realize this vision, a phased-array-based pico-cell must support a large number of precisely controlled beams, yet be compact and power efficient. These system goals have significant mm-wave radio interface implications, including scalability of the RFIC+antenna-array solution, increase in the number of concurrent beams by supporting dual polarization, precise beam steering, and high output power without sacrificing TX power efficiency. Packaged Si-based phased arrays [1\u20133] with nonconcurrent dual-polarized TX and RX operation [2,3], concurrent dual-polarized RX operation [3] and multi-IC scaling [3,4] have been demonstrated. However, support for concurrent dual-polarized operation in both RX and TX remains unaddressed, and high output power comes at the cost of power consumption, cooling complexity and increased size. The RFIC reported here addresses these challenges. It supports concurrent and independent dual-polarized operation in TX and RX modes, and is compatible with a volume-efficient, scaled, antenna-in-package array. A new TX\/RX switch at the shared antenna interface enables high output power without sacrificing TX efficiency, and a t-line-based phase shifter achieves <1\u00b0 RMS error and <5\u00b0 phase steps for precise beam control."}
{"_id":"b7f442298f16684e4c1e69fd4c5002f02a783d27","title":"A novel activation function for multilayer feed-forward neural networks","text":"Traditional activation functions such as hyperbolic tangent and logistic sigmoid have seen frequent use historically in artificial neural networks. However, nowadays, in practice, they have fallen out of favor, undoubtedly due to the gap in performance observed in recognition and classification tasks when compared to their well-known counterparts such as rectified linear or maxout. In this paper, we introduce a simple, new type of activation function for multilayer feed-forward architectures. Unlike other approaches where new activation functions have been designed by discarding many of the mainstays of traditional activation function design, our proposed function relies on them and therefore shares most of the properties found in traditional activation functions. Nevertheless, our activation function differs from traditional activation functions on two major points: its asymptote and global extremum. Defining a function which enjoys the property of having a global maximum and minimum, turned out to be critical during our design-process since we believe it is one of the main reasons behind the gap observed in performance between traditional activation functions and their recently introduced counterparts. We evaluate the effectiveness of the proposed activation function on four commonly used datasets, namely, MNIST, CIFAR-10, CIFAR-100, and the Pang and Lee\u2019s movie review. Experimental results demonstrate that the proposed function can effectively be applied across various datasets where our accuracy, given the same network topology, is competitive with the state-of-the-art. In particular, the proposed activation function outperforms the state-of-the-art methods on the MNIST dataset."}
{"_id":"19bd1797921ec924b69b2b22999b555a5f2b982f","title":"A Survey on Anomalies Detection Techniques and Measurement Methods","text":"Dynamic research area has been applied and researched on anomaly detection in various domains. And various techniques have been proposed to identify unexpected items or events in datasets which differ from the norm. This review tries to provide a basic and structured overview of the anomaly detection techniques. Also, this review discusses major anomaly detection techniques using statistical based and machine learning based techniques. The outcome of this review may facilitate a better understanding of the different techniques in which research has been done on this topic by comparing the pros and cons of the identified techniques. In addition, this review also discusses on the measurement methods used by other researchers in validating their anomalies detection techniques."}
{"_id":"b3769619cdc490ec60aaa3025cd9402a74235f0c","title":"Comparison of EEG signal features and ensemble learning methods for motor imagery classification","text":"Classifying electroencephalogram (EEG) signal in Brain Computer Interface (BCI) is a useful methods to analysis different organs of human body and it can be used for communicate with the outside world and controlling external device. Accuracy classification of extracted features from EEG signals is a problem which many researcher try to improve it. Although many methods for extracting feature and classifying EEG signal have been proposed and developed, many of them suffer from extracting less accurate data from EEG signals. In this work, four signal feature extraction and three ensemble learning method have been reviewed and performances of classification techniques are compared for motor imagery task."}
{"_id":"c1efd29ddb6cb5cf82151ab25fbfc99e20354d9e","title":"Linking GloVe with word2vec","text":"The Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al. [3] is reported to be an efficient and effective method for learning vector representations of words. State-of-the-art performance is also provided by skip-gram with negative-sampling (SGNS) [2] implemented in the word2vec tool. In this note, we explain the similarities between the training objectives of the two models, and show that the objective of SGNS is similar to the objective of a specialized form of GloVe, though their cost functions are defined differently."}
{"_id":"1475e1dea92f1b9c7d2c8ba3a9ead3b91febc6f1","title":"Expanded Hemodialysis: A New Therapy for a New Class of Membranes.","text":"A wide spectrum of molecules is retained in end-stage kidney disease, normally defined as uremic toxins. These solutes have different molecular weights and radii. Current dialysis membranes and techniques only remove solutes in the range of 50-15,000 Da, with limited or no capability to remove solutes in the middle to high molecular weight range (up to 50,000 Da). Improved removal has been obtained with high cut-off (HCO) membranes, with albumin loss representing a limitation to their practical application. Hemodiafiltration (HDF) at high volumes (>23 L\/session) has produced some results on middle molecules and clinical outcomes, although complex hardware and high blood flows are required. A new class of membrane has been recently developed with a cut off (MWCO) close to the molecular weight of albumin. While presenting negligible albumin loss, these membranes have a very high retention onset (MWRO), allowing high clearances of solutes in a wide spectrum of molecular weights. These membranes originally defined (medium cut off) are probably better classified as high retention onset. The introduction of such membranes in the clinical routine has allowed the development of a new concept therapy called \"expanded hemodialysis\" (HDx). The new therapy is based on a special hollow fiber and dialyzer design. Its simple set-up and application offer the possibility to use it even in patients with suboptimal vascular access or even with an indwelling catheter. The system does not require a particular hardware or unusual nursing skill. The quality of dialysis fluid is, however, mandatory to ensure a safe conduction of the dialysis session. This new therapy is likely to modify the outcome of end-stage kidney disease patients, thanks to the enhanced removal of molecules traditionally retained by current dialysis techniques."}
{"_id":"91c50f239890e64c246130e94e795568a2e38c98","title":"Modular Self-Reconfigurable Robot Systems [Grand Challenges of Robotics]","text":"The field of modular self-reconfigurable robotic systems addresses the design, fabrication, motion planning, and control of autonomous kinematic machines with variable morphology. Modular self-reconfigurable systems have the promise of making significant technological advances to the field of robotics in general. Their promise of high versatility, high value, and high robustness may lead to a radical change in automation. Currently, a number of researchers have been addressing many of the challenges. While some progress has been made, it is clear that many challenges still exist. By illustrating several of the outstanding issues as grand challenges that have been collaboratively written by a large number of researchers in this field, this article has shown several of the key directions for the future of this growing field"}
{"_id":"881ad7b896a6e6715a231afe48bbc276167ecf73","title":"The effects of caffeine on heart rate variability in newborns with apnea of prematurity","text":"Objective:Apnea of prematurity is a common complication in premature newborns and caffeine is a widespread medication used to treat this complication. Caffeine may have adverse effects on the cardiovascular and central nervous system, yet its effects on the autonomic nervous system modulation of heart rate have not been studied in premature newborns, which was the objective of our study.Study Design:We prospectively studied 21 premature newborns who\u00a0were treated with caffeine. We analyzed heart rate variability by power spectral density and by dynamic nonlinear analyses methods.Result:There were no changes in heart rate, blood pressure or the autonomic nervous system tone following administration of caffeine, nor were the nonlinear dynamical properties of the system altered by caffeine.Conclusion:Caffeine does not have detrimental effects on heart rate variablility, heart rate or blood pressure in conventional doses given to premature newborns."}
{"_id":"cbfdddf49d76c9e2577c6a555bbf746bd69effe1","title":"The end of the end of ideology.","text":"The \"end of ideology\" was declared by social scientists in the aftermath of World War II. They argued that (a) ordinary citizens' political attitudes lack the kind of stability, consistency, and constraint that ideology requires; (b) ideological constructs such as liberalism and conservatism lack motivational potency and behavioral significance; (c) there are no major differences in content (or substance) between liberal and conservative points of view; and (d) there are few important differences in psychological processes (or styles) that underlie liberal versus conservative orientations. The end-of-ideologists were so influential that researchers ignored the topic of ideology for many years. However, current political realities, recent data from the American National Election Studies, and results from an emerging psychological paradigm provide strong grounds for returning to the study of ideology. Studies reveal that there are indeed meaningful political and psychological differences that covary with ideological self-placement. Situational variables--including system threat and mortality salience--and dispositional variables--including openness and conscientiousness--affect the degree to which an individual is drawn to liberal versus conservative leaders, parties, and opinions. A psychological analysis is also useful for understanding the political divide between \"red states\" and \"blue states.\""}
{"_id":"94983a1f9ccf11dfdba827c5113b0484a85590ee","title":"DeltaIoT: A Self-Adaptive Internet of Things Exemplar","text":"Internet of Things (IoT) consists of networked tiny embedded computers (motes) that are capable of monitoring and controlling the physical world. Examples range from building security monitoring to smart factories. A central problem of IoT is minimising the energy consumption of the motes, while guaranteeing high packet delivery performance, regardless of uncertainties such as sudden changes in traffic load and communication interference. Traditionally, to deal with uncertainties the network settings are either hand-tuned or over-provisioned, resulting in continuous network maintenance or inefficiencies. Enhancing the IoT network with self-adaptation can automate these tasks. This paper presents DeltaIoT, an exemplar that enables researchers to evaluate and compare new methods, techniques and tools for self-adaptation in IoT. DeltaIoT is the first exemplar for research on self-adaptation that provides both a simulator for offline experimentation and a physical setup that can be accessed remotely for real-world experimentation."}
{"_id":"c3b56745ca11a97daf08c2a97834e8f5c625077d","title":"Boosting energy efficiency with mirrored data block replication policy and energy scheduler","text":"Energy efficiency is one of the major challenges in big datacenters. To facilitate processing of large data sets in a distributed fashion, the MapReduce programming model is employed in these datacenters. Hadoop is an open-source implementation of MapReduce which contains a distributed file system. Hadoop Distributed File System provides a data block replication scheme to preserve reliability and data availability. The distribution of the data block replicas over the nodes is performed randomly by meeting some constraints (e.g., preventing storage of two replicas of a data block on a single node). This study makes use of flexibility in the data block placement policy to increase energy efficiency in datacenters. Furthermore, inspired by Zaharia et al.'s delay scheduling algorithm, a scheduling algorithm is introduced, which takes into account energy efficiency in addition to fairness and data locality properties. Computer simulations of the proposed method suggest its superiority over Hadoop's standard settings."}
{"_id":"dd3331156d892c9fc0c669d50d0302c9ecdeb056","title":"Interactive Indirect Illumination Using Voxel Cone Tracing","text":"Indirect illumination is an important element for realistic image synthesis, but its computation is expensive and highly dependent on the complexity of the scene and of the BRDF of the involved surfaces. While off-line computation and pre-baking can be acceptable for some cases, many applications (games, simulators, etc.) require real-time or interactive approaches to evaluate indirect illumination. We present a novel algorithm to compute indirect lighting in real-time that avoids costly precomputation steps and is not restricted to low-frequency illumination. It is based on a hierarchical voxel octree representation generated and updated on the fly from a regular scene mesh coupled with an approximate voxel cone tracing that allows for a fast estimation of the visibility and incoming energy. Our approach can manage two light bounces for both Lambertian and glossy materials at interactive framerates (25-70FPS). It exhibits an almost scene-independent performance and can handle complex scenes with dynamic content thanks to an interactive octree-voxelization scheme. In addition, we demonstrate that our voxel cone tracing can be used to efficiently estimate Ambient Occlusion."}
{"_id":"ab7ac31bbb31b85d23c697be743bf0786d4883b0","title":"Compact low-voltage power-efficient operational amplifier cells for VLSI","text":"\u201cSome Design Aspects of a Two-Stage Rail-to-Rail CMOS Op Amp' by Sander L. J. Gierkink, Peter J. Holzmann, Remco J. Wiegerink and Roelof F. Wassenaar, Analog Integrated Circuits and Signal Processing, Vol. 21, No. 2, Nov. 1999, pp 143-152. \u201cCompact Low-Voltage Power-Efficient Operational Amplifier Cells for VLSI by Klaas-Jan de Langen and Johan H. Huijsing, IEEE Journal of Solid State Circuits, vol. 33, No. 10, Oct. 1998, pp. 1482\u20131496. \u201cDesign Aspects of a Rail-to-Rail CMOS Op Amp' by Glierkink et al., Mesa Research institute, ECT-97-36, pp. 23-28."}
{"_id":"c368e5f5d6390ecd6b431f3b535c707ea8b21993","title":"Mining program workflow from interleaved traces","text":"Successful software maintenance is becoming increasingly critical due to the increasing dependence of our society and economy on software systems. One key problem of software maintenance is the difficulty in understanding the evolving software systems. Program workflows can help system operators and administrators to understand system behaviors and verify system executions so as to greatly facilitate system maintenance. In this paper, we propose an algorithm to automatically discover program workflows from event traces that record system events during system execution. Different from existing workflow mining algorithms, our approach can construct concurrent workflows from traces of interleaved events. Our workflow mining approach is a three-step coarse-to-fine algorithm. At first, we mine temporal dependencies for each pair of events. Then, based on the mined pair-wise tem-poral dependencies, we construct a basic workflow model by a breadth-first path pruning algorithm. After that, we refine the workflow by verifying it with all training event traces. The re-finement algorithm tries to find out a workflow that can interpret all event traces with minimal state transitions and threads. The results of both simulation data and real program data show that our algorithm is highly effective."}
{"_id":"b8090b7b7efa0d971d3e1174facea60129be09c6","title":"A SiC-based isolated DC\/DC converter for high density data center applications","text":"Data centers are increasing in number and size at astounding rates, while operational cost, thermal management, size, and performance continue to be the driving metrics for the power subsystems in the associated computing equipment. This paper presents a SiC-based phase-shifted full bridge (PSFB) converter designed for 10kW output power and targeted at data center applications. The design approach focused on tuning of the converter efficiency and minimizing the thermal management system, resulting in a high-density converter. A unique thermal management system has also been employed, resulting in both increased power density and better thermal management. In this paper, the implementation of this converter is described in detail, along with empirical results, both electrical and thermal."}
{"_id":"e0b65eb21c041ca01043739af988ee011708ec8b","title":"Classification of Brain MR Images using Texture Feature Extraction","text":"Alzheimer\u2019s disease (AD), is a degenerative disease which leads to memory loss and problems with thinking and behaviour.AD is a type of dementia which accounts for an estimated 60% to 80% of cases. Accurate diagnosis depends on the identification of discriminative features of AD. Recently, different feature extraction methods are used for the classification of AD. In this paper, we proposed a classification framework to select features, which are extracted using Gray-Level Co-occurrence Matrix (GLCM) method to distinguish between the AD and the Normal Control (NC). In order to evaluate the proposed method, we have performed evaluations on the MRI acquiring from the OASIS database. The proposed method yields an average testing accuracy of 75.71% which indicates that the proposed method can differentiate AD and NC satisfactorily."}
{"_id":"d62c281dffed526cf12937c4b320a2b74226ae05","title":"Eavesdropping Attacks on High-Frequency RFID Tokens","text":"RFID systems often use near-field magnetic coupling to implement communication channels. The advertised operational range of these channels is less than 10 cm and therefore several implemented systems assume that the communication channel is location limited and therefore relatively secure. Nevertheless, there have been repeated questions raised about the vulnerability of these near-field systems against eavesdropping and skimming attacks. In this paper I revisit the topic of RFID eavesdropping attacks, surveying previous work and explaining why the feasibility of practical attacks is still a relevant and novel research topic. I present a brief overview of the radio characteristics for popular HF RFID standards and present some practical results for eavesdropping experiments against tokens adhering to the ISO 14443 and ISO 15693 standards. Finally, I discuss how an attacker could construct a low-cost eavesdropping device using easy to obtain parts and reference designs."}
{"_id":"9b0b961fa83fb529b9eaa17953bc83ff6e71c1ab","title":"MS-TWSVM: Mahalanobis distance-based Structural Twin Support Vector Machine","text":"The distribution information of data points in two classes as the structural information is inserted into the classifiers to improve their generalization performance. Recently many algorithms such as S-TWSVM has used this information to construct two nonparallel hyperplanes which each one lies as close as possible to one class and being far away from the other. It is well known that different classes have different data distribution in real world problems, thus the covariance matrices of these classes are not the same. In these situations, the Mahalanobis is often more popular than Euclidean as a measure of distance. In this paper, in addition to apply the idea of S-TWSVM, the classical Euclidean distance is replaced by Mahalanobis distance which leads to simultaneously consider the covariance matrices of the two classes. By this modification, the orientation information in two classes can be better exploited than S-TWSVM. The experiments indicate our proposed algorithm is often superior to other learning algorithms in terms of generalization performance."}
{"_id":"06881a2960a83e6cf075c30888fe69af405a8bbc","title":"- 1-Simultaneous measurement of impulse response and distortion with a swept-sine technique","text":"A novel measurement technique of the transfer function of weakly not-linear, approximately time-invariant systems is presented. The method is implemented with low-cost instrumentation; it is based on an exponentially-swept sine signal. It is applicable to loudspeakers and other audio components, but also to room acoustics measurements. The paper presents theoretical description of the method and experimental verification in comparison with MLS."}
{"_id":"4ddbeb946a4ff4853f2e98c547bb0b39cc6a4480","title":"Metamaterial-Based Antennas: Research and Developments","text":"A brief review of metamaterials and their applications to antenna systems is given. Artificial magnetic conductors and electrically small radiating and scattering systems are emphasized. Single negative, double negative, and zero-index metamaterial systems are discussed as a means to manipulate their size, efficiency, bandwidth, and directivity characteristics. key words: metamaterials, electrically small antennas, complex media, artificial magnetic conductors"}
{"_id":"773c85e88bbf54de684c85c9104750dce79f9688","title":"The high-conductance state of neocortical neurons in vivo","text":"Intracellular recordings in vivo have shown that neocortical neurons are subjected to an intense synaptic bombardment in intact networks and are in a 'high-conductance' state. In vitro studies have shed light on the complex interplay between the active properties of dendrites and how they convey discrete synaptic inputs to the soma. Computational models have attempted to tie these results together and predicted that high-conductance states profoundly alter the integrative properties of cortical neurons, providing them with a number of computational advantages. Here, we summarize results from these different approaches, with the aim of understanding the integrative properties of neocortical neurons in the intact brain."}
{"_id":"af1b1279bb28dc24488cf3050f86257ad1cd02b3","title":"Inferring Destination from Mobility Data","text":"Destination prediction in a moving vehicle has several applications such as alternative route recommendations even in cases where the driver has not entered their destination into the system. In this paper a hierarchical approach to destination prediction is presented. A Discrete Time Markov Chain model is used to make an initial prediction of a general region the vehicle might be travelling to. Following that a more complex Bayesian Inference Model is used to make a fine grained prediction within that destination region. The model is tested on a dataset of 442 taxis operating in Porto, Portugal. Experiments are run on two maps. One is a smaller map concentrating specificially on trips within the Porto city centre and surrounding areas. The second map covers a much larger area going as far as Lisbon. We achieve predictions for Porto with average distance error of less than 0.6 km from early on in the trip and less than 1.6 km dropping to less than 1 km for the wider area."}
{"_id":"15c6a03461ce80ef39587bf86e8b88d4b20457a2","title":"Non-psychotropic plant cannabinoids: new therapeutic opportunities from an ancient herb.","text":"Delta(9)-tetrahydrocannabinol binds cannabinoid (CB(1) and CB(2)) receptors, which are activated by endogenous compounds (endocannabinoids) and are involved in a wide range of physiopathological processes (e.g. modulation of neurotransmitter release, regulation of pain perception, and of cardiovascular, gastrointestinal and liver functions). The well-known psychotropic effects of Delta(9)-tetrahydrocannabinol, which are mediated by activation of brain CB(1) receptors, have greatly limited its clinical use. However, the plant Cannabis contains many cannabinoids with weak or no psychoactivity that, therapeutically, might be more promising than Delta(9)-tetrahydrocannabinol. Here, we provide an overview of the recent pharmacological advances, novel mechanisms of action, and potential therapeutic applications of such non-psychotropic plant-derived cannabinoids. Special emphasis is given to cannabidiol, the possible applications of which have recently emerged in inflammation, diabetes, cancer, affective and neurodegenerative diseases, and to Delta(9)-tetrahydrocannabivarin, a novel CB(1) antagonist which exerts potentially useful actions in the treatment of epilepsy and obesity."}
{"_id":"442fb4e092c0b63721d8bef973c2affbac1f708b","title":"A topology for three-stage Solid State Transformer","text":"Solid State Transformer (SST) is a new type of power transformer based on power electronic converters and high frequency transformers. The SST realizes voltage transformation, galvanic isolation, and power quality improvements in a single device. In the literature, a number of topologies have been introduced for the SST. In this work, employing a modular multilevel converter, a new SST topology is introduced which provides not only high-voltage AC (HVAC), low-voltage AC (LVAC) and low-voltage DC (LVDC) ports, but also high-voltage DC (HVDC) port. Besides, the proposed topology is easily scalable to higher voltage levels."}
{"_id":"7ec432f17555c345988b16cf076bfabaabb0dabf","title":"Continental Airlines Flies High with Real-Time Business Intelligence","text":"Real-time business intelligence (BI) is taking Continental Airlines to new heights. Powered by a real-time data warehouse, the company has dramatically changed all aspects of its business. Continental\u2019s president and COO, Larry Kellner, describes the impact of real-time BI in the following way: \u201cReal-time BI is critical to the accomplishment of our business strategy and has created significant business benefits.\" In fact, Continental has realized more than $500 million in cost savings and revenue generation over the past six years from its BI initiatives, producing an ROI of more than 1,000 percent."}
{"_id":"31e4725e74bf623aeaf86782f52d9f140b2af153","title":"A MINI UNMANNED AERIAL VEHICLE ( UAV ) : SYSTEM OVERVIEW AND IMAGE ACQUISITION","text":"In the last years UAV (Unmanned Aerial Vehicle)-systems became relevant for applications in precision farming and in infrastructure maintenance, like road maintenance and dam surveillance. This paper gives an overview about UAV (Unmanned Aerial Vehicle) systems and their application for photogrammetric recording and documentation of cultural heritage. First the historical development of UAV systems and the definition of UAV-helicopters will be given. The advantages of a photogrammetric system on-board a model helicopter will be briefly discussed and compared to standard aerial and terrestrial photogrammetry. UAVs are mostly low cost systems and flexible and therefore a suitable alternative solution compared to other mobile mapping systems. A mini UAV-system was used for photogrammetric image data acquisition near Palpa in Peru. A settlement from the 13 century AD, which was presumably used as a mine, was flown with a model helicopter. Based on the image data, an accurate 3D-model will be generated in the future. With an orthophoto and a DEM derived from aerial images in a scale of 1:7 000, a flight planning was build up. The determined flying positions were implemented in the flight control system. Thus, the helicopter is able to fly to predefined pathpoints automatically. Tests in Switzerland and the flights in Pinchango Alto showed that using the built-in GPS\/INSand stabilization units of the flight control system, predefined positions could be reached exactly to acquire the images. The predicted strip crossings and flying height were kept accurately in the autonomous flying mode."}
{"_id":"81e0f458a894322baf170fa4d6fa8099bd055c39","title":"Statistical Decision Theory and Bayesian Analysis, 2nd Edition","text":""}
{"_id":"2df6c9cc1c561c8b8ee2cb11a5f52f8fea6537b1","title":"A computer vision-aided motion sensing algorithm for mobile robot's indoor navigation","text":"This paper presents the design and analysis of a computer vision-aided motion sensing algorithm for wheeled mobile robot's indoor navigation. The algorithm is realized using two vision cameras attached on a wheeled mobile robot. The first camera is positioned at front-looking direction while the second camera is positioned at downward-looking direction. An algorithm is developed to process the images acquired from the cameras to yield the mobile robot's positions and orientations. The proposed algorithm is implemented on a wheeled mobile robot for real-world effectiveness testing. Results are compared and shown the accuracy of the proposed algorithm. At the end of the paper, an artificial landmark approach is introduced to improve the navigation efficiency. Future work involved implementing the proposed artificial landmark for indoor navigation applications with minimized accumulated errors."}
{"_id":"13b2c975571893815e02a94e34cd64e1ce9100a2","title":"A Fast Algorithm for Edge-Preserving Variational Multichannel Image Restoration","text":"We generalize the alternating minimization algorithm recently proposed in [32] to efficiently solve a general, edge-preserving, variational model for recovering multichannel images degraded by withinand cross-channel blurs, as well as additive Gaussian noise. This general model allows the use of localized weights and higher-order derivatives in regularization, and includes a multichannel extension of total variation (MTV) regularization as a special case. In the MTV case, we show that the model can be derived from an extended half-quadratic transform of Geman and Yang [14]. For color images with three channels and when applied to the MTV model (either locally weighted or not), the per-iteration computational complexity of this algorithm is dominated by nine fast Fourier transforms. We establish strong convergence results for the algorithm including finite convergence for some variables and fast q-linear convergence for the others. Numerical results on various types of blurs are presented to demonstrate the performance of our algorithm compared to that of the MATLAB deblurring functions. We also present experimental results on regularization models using weighted MTV and higher-order derivatives to demonstrate improvements in image quality provided by these models over the plain MTV model."}
{"_id":"3505447904364877605aabaa450c09568c8db1ec","title":"Smart irrigation using low-cost moisture sensors and XBee-based communication","text":"Deficiency in fresh water resources globally has raised serious alarms in the last decade. Efficient management of water resources play an important role in the agriculture sector. Unfortunately, this is not given prime importance in the third world countries because of adhering to traditional practices. This paper presents a smart system that uses a bespoke, low cost soil moisture sensor to control water supply in water deficient areas. The sensor, which works on the principle of moisture dependent resistance change between two points in the soil, is fabricated using affordable materials and methods. Moisture data acquired from a sensor node is sent through XBEE wireless communication modules to a centralized server that controls water supply. A user-friendly interface is developed to visualize the daily moisture data. The low-cost and wireless nature of the sensing hardware presents the possibility to monitor the moisture levels of large agricultural fields. Moreover, the proposed moisture sensing method has the ability to be incorporated into an automated drip-irrigation scheme and perform automated, precision agriculture in conjunction with de-centralized water control."}
{"_id":"24c5877251ba8b31570256f46247740e42aa59e4","title":"Quantifying social group evolution","text":"The rich set of interactions between individuals in society results in complex community structure, capturing highly connected circles of friends, families or professional cliques in a social network. Thanks to frequent changes in the activity and communication patterns of individuals, the associated social and communication network is subject to constant evolution. Our knowledge of the mechanisms governing the underlying community dynamics is limited, but is essential for a deeper understanding of the development and self-optimization of society as a whole. We have developed an algorithm based on clique percolation that allows us to investigate the time dependence of overlapping communities on a large scale, and thus uncover basic relationships characterizing community evolution. Our focus is on networks capturing the collaboration between scientists and the calls between mobile phone users. We find that large groups persist for longer if they are capable of dynamically altering their membership, suggesting that an ability to change the group composition results in better adaptability. The behaviour of small groups displays the opposite tendency\u2014the condition for stability is that their composition remains unchanged. We also show that knowledge of the time commitment of members to a given community can be used for estimating the community\u2019s lifetime. These findings offer insight into the fundamental differences between the dynamics of small groups and large institutions."}
{"_id":"d91777099ffce5aa58efa427078e3fb5c1551cac","title":"Realistic and Fast Cloud Rendering","text":"Clouds are an important aspect of rendering outdoor scenes. Thi paper describes a cloud system that extends texture splatting on par ticles to model a dozen cloud types (e.g., stratus, cumulus congestus, cumulonimb us), an improvement over earlier systems that modeled only one type of cumulus. W e also achieve fast real-time rendering, even for scenes of dense overcast cove rage, which was a limitation for previous systems. We present a new shading model that uses artist-driven contr ols rather than a programmatic approach to approximate lighting. This is sui table when fine-grained control over the look-and-feel is necessary, and artistic r esources are available. We also introduce a way to simulate cloud formation and dissipa tion using texture"}
{"_id":"59923c9dee2a97e0403460ac9a49021930e9a5d2","title":"No-reference image and video quality assessment: a classification and review of recent approaches","text":"The field of perceptual quality assessment has gone through a wide range of developments and it is still growing. In particular, the area of no-reference (NR) image and video quality assessment has progressed rapidly during the last decade. In this article, we present a classification and review of latest published research work in the area of NR image and video quality assessment. The NR methods of visual quality assessment considered for review are structured into categories and subcategories based on the types of methodologies used for the underlying processing employed for quality estimation. Overall, the classification has been done into three categories, namely, pixel-based methods, bitstream-based methods, and hybrid methods of the aforementioned two categories. We believe that the review presented in this article will be helpful for practitioners as well as for researchers to keep abreast of the recent developments in the area of NR image and video quality assessment. This article can be used for various purposes such as gaining a structured overview of the field and to carry out performance comparisons for the state-of-the-art"}
{"_id":"0d7256ac0119d01acbb2ff6e124c4d60635fae1f","title":"Managing Organizational Knowledge By Diagnosing Intellectual Capital : Framing and Advancing the State of the Field","text":"Copyright \u00a9 2001, Idea Group Publishing. Since organizational knowledge is at the crux of sustainable competitive advantage, the burgeoning field of intellectual capital is an exciting area for both researchers and practitioners. Intellectual capital is conceptualized from numerous disciplines making the field a mosaic of perspectives. Accountants are interested in how to measure it on the balance sheet, information technologists want to codify it on systems, sociologists want to balance power with it, psychologists want to develop minds because of it, human resource managers want to calculate an ROI on it, and training and development officers want to make sure that they can build it. The following article represents a comprehensive literature review from a variety of managerial disciplines. In addition to highlighting the research to date, avenues for future pursuit are also offered."}
{"_id":"21d699e1cba89f8e3b40522530ea86b3253c111e","title":"Intellectual capital ROI : a causal map of human capital antecedents and consequents","text":"This report describes the results of a ground-breaking research study that measured the antecedents and consequents of effective human capital management. The research sample consisted of 76 senior executives from 25 companies in the financial services industry. The results of the study yielded a holistic causal map that integrated constructs from the fields of intellectual capital, knowledge management, human resources, organizational behaviour, information technology and accounting. The integration of both quantitative and qualitative measures in an overall conceptual model yielded several research implications. The resulting structural equation model allows participating organizations and researchers to gauge the effectiveness of an organization\u2019s human capital capabilities. This will allow practitioners and researchers to more efficiently allocate resources with regard to human capital management. The potential outcomes of the study are limitless, since a program of consistent re-evaluation can lead to the establishment of causal relationships between human capital management and economic and business results. Introduction Today\u2019s knowledge-based world consists of universal dynamic change and massive information bombardment. By the year 2010, the codified information base of the world is expected to `\u0300 double every 11 hours\u2019\u2019 (Bontis, 1999, p. 435). Information storage capacities continue to expand enormously. In 1950, IBM\u2019s Rama C tape contained 4.4 megabytes and they were able to store as many as 50 of these tapes together. At that time, 220 megabytes represented the frontiers of information storage. Many of today\u2019s standard desktop computers are being sold with 40 gigabytes of hard disk space. It is sobering to remember that full motion video in uncompressed form requires 1 gigabyte per minute and that the 83 minutes of Snow White digitized in full colour amount to 15 terabytes of space. Unfortunately, the conscious mind is only capable of processing somewhere between 16 and 40 bits of information (ones and zeros) per second. How do we reconcile this information bombardment conundrum when it seems that human beings are the bottle-neck? The current issue and full text archive of this journal is available at http:\/\/www.emeraldinsight.com\/1469-1930.htm The authors would like to acknowledge the following organizations for their financial support: Accenture, Saratoga Institute and the Institute for Intellectual Capital Research. The authors would also like to highlight the contribution of Vanessa Yeh, who administered the data collection phase of this research."}
{"_id":"5a46da9aec9238b5acf5f83b2bb9e453be37367b","title":"Producing sustainable competitive advantage through the effective management of people *","text":"Executive Overview Achieving competitive success through people involves fundamentally altering how we think about the workforce and the employment relationship. It means achieving success by working with people, not by replacing them or limiting the scope of their activities. It entails seeing the workforce as a source of strategic advantage, not just as a cost to be minimized or avoided. Firms that take this different perspective are often able to successfully outmaneuver and outperform their rivals. ........................................................................................................................................................................"}
{"_id":"4ddabe9893c8e2db7d4870b1aefbae4d20d22e43","title":"HOW MUCH DOES INDUSTRY MATTER , REALLY ?","text":"In this paper, we examine the importance of year, industry, corporate-parent, and businessspecific effects on the profitability of U.S. public corporations within specific 4-digit SIC categories. Our results indicate that year, industry, corporate-parent, and business-specific effects account for 2 percent, 19 percent, 4 percent, and 32 percent, respectively, of the aggregate variance in profitability. We also find that the importance of the effects differs substantially across broad economic sectors. Industry effects account for a smaller portion of profit variance in manufacturing but a larger portion in lodging\/entertainment, services, wholesale\/retail trade, and transportation. Across all sectors we find a negative covariance between corporate-parent and industry effects. A detailed analysis suggests that industry, corporate-parent, and business-specific effects are related in complex ways. \uf6d9 1997 by John Wiley & Sons, Ltd."}
{"_id":"97293655d32d8dfa0c3f3aad446b412d2512cecf","title":"From sunlight to phytomass: on the potential efficiency of converting solar radiation to phyto-energy.","text":"The relationship between solar radiation capture and potential plant growth is of theoretical and practical importance. The key processes constraining the transduction of solar radiation into phyto-energy (i.e. free energy in phytomass) were reviewed to estimate potential solar-energy-use efficiency. Specifically, the out-put:input stoichiometries of photosynthesis and photorespiration in C(3) and C(4) systems, mobilization and translocation of photosynthate, and biosynthesis of major plant biochemical constituents were evaluated. The maintenance requirement, an area of important uncertainty, was also considered. For a hypothetical C(3) grain crop with a full canopy at 30\u00b0C and 350 ppm atmospheric [CO(2) ], theoretically potential efficiencies (based on extant plant metabolic reactions and pathways) were estimated at c. 0.041 J J(-1) incident total solar radiation, and c. 0.092 J J(-1) absorbed photosynthetically active radiation (PAR). At 20\u00b0C, the calculated potential efficiencies increased to 0.053 and 0.118 J J(-1) (incident total radiation and absorbed PAR, respectively). Estimates for a hypothetical C(4) cereal were c. 0.051 and c. 0.114 J J(-1), respectively. These values, which cannot be considered as precise, are less than some previous estimates, and the reasons for the differences are considered. Field-based data indicate that exceptional crops may attain a significant fraction of potential efficiency."}
{"_id":"382213e67d451cccb6cd41939b8e33e451a45e26","title":"App Miscategorization Detection: A Case Study on Google Play","text":"An ongoing challenge in the rapidly evolving app market ecosystem is to maintain the integrity of app categories. At the time of registration, app developers have to select, what they believe, is the most appropriate category for their apps. Besides the inherent ambiguity of selecting the right category, the approach leaves open the possibility of misuse and potential gaming by the registrant. Periodically, the app store will refine the list of categories available and potentially reassign the apps. However, it has been observed that the mismatch between the description of the app and the category it belongs to, continues to persist. Although some common mechanisms (e.g., a complaint-driven or manual checking) exist, they limit the response time to detect miscategorized apps and still open the challenge on categorization. We introduce FRAC+: (FR)amework for (A)pp (C)ategorization. FRAC+ has the following salient features: (i) it is based on a data-driven topic model and automatically suggests the categories appropriate for the app store, and (ii) it can detect miscategorizated apps. Extensive experiments attest to the performance of FRAC+. Experiments on Google Play shows that FRAC+\u2019s topics are more aligned with Google\u2019s new categories and 0.35-1.10 percent game apps are detected to be miscategorized."}
{"_id":"8936eb96bc8c67a84beb289787029f69fa28e4c2","title":"Local Edge-Preserving Multiscale Decomposition for High Dynamic Range Image Tone Mapping","text":"A novel filter is proposed for edge-preserving decomposition of an image. It is different from previous filters in its locally adaptive property. The filtered image contains local means everywhere and preserves local salient edges. Comparisons are made between our filtered result and the results of three other methods. A detailed analysis is also made on the behavior of the filter. A multiscale decomposition with this filter is proposed for manipulating a high dynamic range image, which has three detail layers and one base layer. The multiscale decomposition with the filter addresses three assumptions: 1) the base layer preserves local means everywhere; 2) every scale's salient edges are relatively large gradients in a local window; and 3) all of the nonzero gradient information belongs to the detail layer. An effective function is also proposed for compressing the detail layers. The reproduced image gives a good visualization. Experimental results on real images demonstrate that our algorithm is especially effective at preserving or enhancing local details."}
{"_id":"3f481fe3f753b26f1b0b46c4585b9d39617d01aa","title":"Using neural networks and GIS to forecast land use changes : a Land Transformation Model","text":"The Land Transformation Model (LTM), which couples geographic information systems (GIS) with artificial neural networks (ANNs) to forecast land use changes, is presented here. A variety of social, political, and environmental factors contribute to the model\u2019s predictor variables of land use change. This paper presents a version of the LTM parameterized for Michigan\u2019s Grand Traverse Bay Watershed and explores how factors such as roads, highways, residential streets, rivers, Great Lakes coastlines, recreational facilities, inland lakes, agricultural density, and quality of views can influence urbanization patterns in this coastal watershed. ANNs are used to learn the patterns of development in the region and test the predictive capacity of the model, while GIS is used to develop the spatial, predictor drivers and perform spatial analysis on the results. The predictive ability of the model improved at larger scales when assessed using a moving scalable window metric. Finally, the individual contribution of each predictor variable was examined and shown to vary across spatial scales. At the smallest scales, quality views were the strongest predictor variable. We interpreted the multi-scale influences of land use change, illustrating the relative influences of site (e.g. quality of views, residential streets) and situation (e.g. highways and county roads) variables at different scales. # 2002 Elsevier Science Ltd. All rights reserved."}
{"_id":"c13ee933b7ddccb4a1ccc7820c09fda8f32d1fb5","title":"Business & IT Alignment in Theory and Practice","text":"A key success factor for a successful company in a dynamic environment is effective and efficient information technology (IT) supporting business strategies and processes. In recent surveys however it is concluded that in most companies IT is not aligned with business strategy. The alignment between business needs and IT capabilities is therefore still a prominent area of concern. This paper reports the first stages of a research program exploring the differences of business & IT alignment (BIA) in theory and in practice. The paper presents an overview of the development of theory on BIA and reports the issues with aligning IT to business in practice based on a number of focus-group discussions with CIOs and IT managers. In line with the practical approach to BIA that the CIOs and IT managers in the focus-groups took, the last part of the paper builds upon Luftman's BIA maturity model and reports the application of the model to 12 Dutch firms"}
{"_id":"606b2c57cfed7328dedf88556ac657e9e1608311","title":"Blockstack : A New Decentralized Internet","text":"The traditional internet has many central points of failure and trust, like (a) the Domain Name System (DNS) servers, (b) public-key infrastructure, and (c) end-user data stored on centralized data stores. We present the design and implementation of a new internet, called Blockstack, where users don\u2019t need to trust remote servers. We remove any trust points from the middle of the network and use blockchains to secure critical data bindings. Blockstack implements services for identity, discovery, and storage and can survive failures of underlying blockchains. The design of Blockstack is informed by three years of experience from a large blockchain-based production system. Blockstack gives comparable performance to traditional internet services and enables a much-needed security and reliability upgrade to the traditional internet."}
{"_id":"212cdc602cf9222297beaa9cbdb7792441ceb7e4","title":"The impact of modified Hatha yoga on chronic low back pain: a pilot study.","text":"PURPOSE\nThe purpose of this randomized pilot study was to evaluate a possible design for a 6-week modified hatha yoga protocol to study the effects on participants with chronic low back pain.\n\n\nPARTICIPANTS\nTwenty-two participants (M = 4; F = 17), between the ages of 30 and 65, with chronic low back pain (CLBP) were randomized to either an immediate yoga based intervention, or to a control group with no treatment during the observation period but received later yoga training.\n\n\nMETHODS\nA specific CLBP yoga protocol designed and modified for this population by a certified yoga instructor was administered for one hour, twice a week for 6 weeks. Primary functional outcome measures included the forward reach (FR) and sit and reach (SR) tests. All participants completed Oswestry Disability Index (ODI) and Beck Depression Inventory (BDI) questionnaires. Guiding questions were used for qualitative data analysis to ascertain how yoga participants perceived the instructor, group dynamics, and the impact of yoga on their life.\n\n\nANALYSIS\nTo account for drop outs, the data were divided into better or not categories, and analyzed using chi-square to examine differences between the groups. Qualitative data were analyzed through frequency of positive responses.\n\n\nRESULTS\nPotentially important trends in the functional measurement scores showed improved balance and flexibility and decreased disability and depression for the yoga group but this pilot was not powered to reach statistical significance. Significant limitations included a high dropout rate in the control group and large baseline differences in the secondary measures. In addition, analysis of the qualitative data revealed the following frequency of responses (1) group intervention motivated the participants and (2) yoga fostered relaxation and new awareness\/learning.\n\n\nCONCLUSION\nA modified yoga-based intervention may benefit individuals with CLB, but a larger study is necessary to provide definitive evidence. Also, the impact on depression and disability could be considered as important outcomes for further study. Additional functional outcome measures should be explored. This pilot study supports the need for more research investigating the effect of yoga for this population."}
{"_id":"d4f276f2e370d9010f0570dfe61ff9dc57d7b489","title":"An Overview on Evaluating and Predicting Scholarly Article Impact","text":"Scholarly article impact reflects the significance of academic output recognised by academic peers, and it often plays a crucial role in assessing the scientific achievements of researchers, teams, institutions and countries. It is also used for addressing various needs in the academic and scientific arena, such as recruitment decisions, promotions, and funding allocations. This article provides a comprehensive review of recent progresses related to article impact assessment and prediction. The review starts by sharing some insight into the article impact research and outlines current research status. Some core methods and recent progress are presented to outline how article impact metrics and prediction have evolved to consider integrating multiple networks. Key techniques, including statistical analysis, machine learning, data mining and network science, are discussed. In particular, we highlight important applications of each technique in article impact research. Subsequently, we discuss the open issues and challenges of article impact research. At the same time, this review points out some important research directions, including article impact evaluation by considering Conflict of Interest, time and location information, various distributions of scholarly entities, and rising stars."}
{"_id":"effc383d5c56d2830a81c6f75dbee8afc6eca218","title":"\"An Odd Kind of Pleasure\": Differentiating Emotional Challenge in Digital Games","text":"Recent work introduced the notion of emotional challenge as a means to afford more unique and diverse gaming experiences. However, players' experience of emotional challenge has received little empirical attention. It remains unclear whether players enjoy it and what exactly constitutes the challenge thereof. We surveyed 171 players about a challenging or an emotionally challenging experience, and analyzed their responses with regards to what made the experience challenging, their emotional response, and the relation to core player experience constructs. We found that emotional challenge manifested itself in different ways, by confronting players with difficult themes or decisions, as well as having them deal with intense emotions. In contrast to more'conventional' challenge, emotional challenge evoked a wider range of negative emotions and was appreciated significantly more by players. Our findings showcase the appeal of uncomfortable gaming experiences, and extend current conceptualizations of challenge in games."}
{"_id":"8ce651d8b9d4e575881939eec98590161aa51c2d","title":"RFID Systems: A Survey on Security Threats and Proposed Solutions","text":"Low-cost Radio Frequency Identification (RFID) tags affixed to consumer items as smart labels are emerging as one of the most pervasive computing technology in history. This can have huge security implications. The present article surveys the most important technical security challenges of RFID systems. We first provide a brief summary of the most relevant standards related to this technology. Next, we present an overview about the state of the art on RFID security, addressing both the functional aspects and the security risks and threats associated to its use. Finally, we analyze the main security solutions proposed until date."}
{"_id":"317072c8b7213d884f5b2d4d3133368d17c412ab","title":"Broadband Substrate Integrated Waveguide Cavity-Backed Bow-Tie Slot Antenna","text":"A novel design technique for broadband substrate integrated waveguide cavity-backed slot antenna is demonstrated in this letter. Instead of using a conventional narrow rectangular slot, a bow-tie-shaped slot is implemented to get broader bandwidth performance. The modification of the slot shape helps to induce strong loading effect in the cavity and generates two closely spaced hybrid modes that help to get a broadband response. The slot antenna incorporates thin cavity backing (height <;0.03\u03bb0 ) in a single substrate and thus retains low-profile planar configuration while showing unidirectional radiation characteristics with moderate gain. A fabricated prototype is also presented that shows a bandwidth of 1.03 GHz (9.4%), a gain of 3.7 dBi over the bandwidth, 15 dB front-to-back ratio, and cross-polarization level below -18 dB."}
{"_id":"17cea7a5d8caa91815a4e1e4188bc8475d0b21cc","title":"Sparse PCA via Bipartite Matchings","text":"We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. These components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but as we show this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data matrix, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the data; this allows us to obtain polynomial-time approximation guarantees via spectral bounds. We evaluate our algorithm on real data-sets and empirically demonstrate that in many cases it outperforms existing, deflationbased approaches."}
{"_id":"aa51d17fc646651218feb33058a4dba95f8e2ebe","title":"Low-power high-speed level shifter design for block-level dynamic voltage scaling environment","text":"Two novel level shifters that are suitable for block-level dynamic voltage scaling environment (namely, V\/sub DD\/-hopping) are proposed. In order to achieve reduction in power consumption and delay, the first proposed level shifter which is called contention mitigated level shifter (CMLS) uses a contention-reduction technique. The simulation results with 65-nm CMOS model show 24% reduction in power and 50% decrease in delay with 4% area increase compared with the conventional level shifter. The second proposed level shifter which is called bypassing enabled level shifter (BELS) implements a bypass function and it is fabricated using 0.35\/spl mu\/m CMOS technology. The measurement results show that the power and delay of the proposed BELS are reduced by 50% and 65%, respectively with 60% area overhead over the conventional level shifter."}
{"_id":"2c7886a75462716bcdc797ca6ee1ffbe3dffd67a","title":"An accurate algebraic solution for moving source location using TDOA and FDOA measurements","text":"This paper proposes an algebraic solution for the position and velocity of a moving source using the time differences of arrival (TDOAs) and frequency differences of arrival (FDOAs) of a signal received at a number of receivers. The method employs several weighted least-squares minimizations only and does not require initial solution guesses to obtain a location estimate. It does not have the initialization and local convergence problem as in the conventional linear iterative method. The estimated accuracy of the source position and velocity is shown to achieve the Crame\/spl acute\/r-Rao lower bound for Gaussian TDOA and FDOA noise at moderate noise level before the thresholding effect occurs. Simulations are included to examine the algorithm's performance and compare it with the Taylor-series iterative method."}
{"_id":"80d2e35888a5f072aae0c6f367c52f33dc874f8d","title":"Towards dropout training for convolutional neural networks","text":"Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage."}
{"_id":"61655a1ec2163c18279561a8fb40f8df84cc8d3a","title":"Techniques and Programs Used in Bidding and Playing phases of Imperfect Information Game \u2013 an Overview","text":"Bridge is an international imperfect information game played with similar rules all over the world and it is played by millions of players. It is an intelligent game; it increases creativity and knowledge of human mind. Many of the researchers analyses the Bridge bidding and playing phases, and they developed many programs for getting better results. The programs were performing well and it is also matter of time before the computer beats most human bridge players. As such, the researchers mainly focused on the techniques and computer programs which were used in bridge bidding"}
{"_id":"7ae152644e006474e8dca8da61fc6aa11969890e","title":"High-Gain Reconfigurable Sectoral Antenna Using an Active Cylindrical FSS Structure","text":"A novel design of a high-gain reconfigurable sectoral antenna using an active cylindrical frequency selective surface (FSS) structure is presented. The FSS structure consists of metallic discontinuous strips with PIN diodes in their discontinuities, and it is placed cylindrically around an omnidirectional electromagnetically coupled coaxial dipole (ECCD) array. The cylindrical FSS structure is divided into two semi-cylinders. By controlling the state of diodes in each semi-cylinder, a directive radiation pattern is obtained that can be swept in the entire azimuth plane. The effect of the diode-state configuration and the radius of the cylindrical structure are carefully studied to obtain an optimum sectoral radiation pattern. In addition, a solution for increasing the matching bandwidth of the antenna is also proposed. An experimental prototype was fabricated, and the measured results show a beamwidth of 20\u00b0 in elevation and 70\u00b0 in the azimuth plane at 2.1 GHz with a gain of 13 dBi. With these features, the proposed antenna is suitable for base-station applications in wireless communication systems."}
{"_id":"99aa2c7249abf64b20f2fe6a197ac722d5c2f0e2","title":"Methodology for customer relationship management","text":"Customer relationship management (CRM) is a customer-focused business strategy that dynamically integrates sales, marketing and customer care service in order to create and add value for the company and its customers. This change towards a customer-focused strategy is leading to a strong demand for CRM solutions by companies. However, in spite of companies interest in this new management model, many CRM implementations fail. One of the main reasons for this lack of success is that the existing methodologies being used to approach a CRM project are not adequate, since they do not satisfactorily integrate and complement the strategic and technological aspects of CRM. This paper describes a formal methodology for directing the process of developing and implementing a CRM System that considers and integrates various aspects, such as defining a customer strategy, re-engineering customer-oriented business processes, human resources management, the computer system, management of change and continuous improvement. 2005 Elsevier Inc. All rights reserved."}
{"_id":"9210a0bcc4487f7d2430f123db155617546e862e","title":"Music Education and Its Impact on Students with Special Needs","text":"Part of the Curriculum and Instruction Commons, Curriculum and Social Inquiry Commons, Disability and Equity in Education Commons, Educational Methods Commons, Elementary Education and Teaching Commons, Higher Education and Teaching Commons, Junior High, Intermediate, Middle School Education and Teaching Commons, Other Educational Administration and Supervision Commons, Other Teacher Education and Professional Development Commons, Pre-Elementary, Early Childhood, Kindergarten Teacher Education Commons, Scholarship of Teaching and Learning Commons, and the Special Education Administration Commons Survey: Let us know how this paper benefits you."}
{"_id":"7088fad9a6dedcadb9116c50f622c90bc4405e3a","title":"Composite Scale of Morningness: psychometric properties, validity with Munich ChronoType Questionnaire and age\/sex differences in Poland.","text":"The present study aimed at testing psychometric properties of the Composite Scale of Morningness (CSM) and validating it with mid sleep on free days (MSF) derived from the Munich ChronoType Questionnaire (MCTQ) in Poland, along with analyzing age and sex differences in the CSM and MSF. A sample of 952 Polish residents (62.6% females) aged between 13 and 46 was tested. Additionally, a sample of 33 university students were given MCTQ and filled in a sleep diary for 8 days. MSF derived from MCTQ was related to the one from sleep diary (r=.44). The study revealed good reliability of the CSM (\u03b1=.84) and its validity: greater morningness preference was associated with earlier MSF from MCTQ (r=-.52). CSM scores were distributed over its full range, with a mean of 34, and did not differ between sexes, although females were earlier than males by 23minutes in MSF. Regarding age, eveningness estimated with both CSM and MSF was greatest in subjects aged 16-18years, and a shift toward eveningness during puberty and a shift back toward morningness in older age was observed. The Polish version of the CSM consisted of two components of morningness. Cutoff scores were: for evening types (lower 10%) 24 or less, for morning types (upper 10%) 43 or more. The Polish CSM presents good psychometric properties, which are similar to those reported in other language versions, and also presents sex\/age patterns similar to those found previously."}
{"_id":"3e696e53734615c4fd43ebeccc66ab1ee924a66c","title":"Analysis of different PM machines with concentrated windings and flux barriers in stator core","text":"The new stator structure with magnetic flux-barriers in the stator yoke or tooth region represents an efficient method for reducing the sub-harmonics of electric machines with fractional slots, tooth-concentrated windings. In this paper the both flux-barriers techniques are considered during the analysis of different PM machines. The 12-teeth single-layer and double-layer concentrated winding in combination with a 10-poles and 14-poles PM rotor are investigated. For the all machine topologies the new stator design is used to improve their performances and characteristics. The flux-barrier effects on the main machine parameters, such as in the air-gap flux density harmonics, dq-machine parameters, characteristic currents, electromagnetic torque, and so on, are studied carefully. Comparisons performed with the analogous conventional machines (with conventional stator) show that, the new stator design offers significant advantages."}
{"_id":"76bb26087807bf29932a0a8a3b53f7d381be17a9","title":"Feature-Rich Memory-Based Classification for Shallow NLP and Information Extraction","text":"Memory-Based Learning (MBL) is based on the storage of all available training data, and similarity-based reasoning for handling new cases. By interpreting tasks such as POS tagging and shallow parsing as classification tasks, the advantages of MBL (implicit smoothing of sparse data, automatic integration and relevance weighting of information sources, handling exceptional data) contribute to state-of-the-art accuracy. However, Hidden Markov Models (HMM) typically achieve higher accuracy than MBL (and other Machine Learning approaches) for tasks such as POS tagging and chunking. In this paper, we investigate how the advantages of MBL, such as its potential to integrate various sources of information, come to play when we compare our approach to HMMs on two Information Extraction (IE) datasets: the well-known Seminar Announcement data set and a new German Curriculum Vitae data set. 1 Memory-Based Language Processing Memory-Based Learning (MBL) is a supervised classification-based learning method. A vector of feature values (an instance) is associated with a class by a classifier that lazily extrapolates from the most similar set (nearest neighbors) selected from all stored training examples. This is in contrast to eager learning methods like decision tree learning [26], rule induction [9], or Inductive Logic Programming [7], which abstract a generalized structure from the training set beforehand (forgetting the examples themselves), and use that to derive a classification for a new instance. In MBL, a distance metric on the feature space defines what are the nearest neighbors of an instance. Metrics with feature weights based on information-theory or other relevance statistics allow us to use rich representations of instances and their context, and to balance the influences of diverse information sources in computing distance. Natural Language Processing (NLP) tasks typically concern the mapping of an input representation (e.g., a series of words) into an output representation (e.g., the POS tags corresponding to each word in the input). Most NLP tasks can therefore easily be interpreted as sequences of classification 34 Zavrel, Daelemans tasks: e.g., given a word and some representation of its context, decide what tag to assign to each word in its context. By creating a separate classification instance (a \u201cmoving window\u201d approach) for each word and its context, shallow syntactic or semantic structures can be produced for whole sentences or texts. In this paper, we argue that more semantic and complex input-output mappings, such as Information Extraction, can also effectively be modeled by such a Memory-based classification-oriented framework, and that this approach has a number of very interesting advantages over rivalling methods, most notably that each classification decision can be made dependent on a very rich and diverse set of features. The properties of MBL as a lazy, similarity-based learning method seem make a good fit to the properties of typical disambiguation problems in NLP: \u2022 Similar input representations lead to similar output. E.g., words occurring in a similar context in general have the same POS tag. Similarity-based reasoning is the core of MBL. \u2022 Many sub-generalizations and exceptions. By keeping in memory all training instances, exceptions included, an MBL approach can capture generalization from exceptional or low-frequency cases according to [12]. \u2022 Need for integration of diverse types of information. E.g., in Information Extraction, lexical features, spelling features, syntactic as well as phrasal context features, global text structure, and layout features can potentially be very relevant. \u2022 Automatic smoothing in very rich event spaces. Supervised learning of NLP tasks regularly runs into problems of sparse data; not enough training data is available to extract reliable parameters for complex models. MBL incorporates an implicit robust form of smoothing by similarity [33]. In the remainder of this Section, we will show how a memory-, similarity-, and classification-based approach can be applied to shallow syntactic parsing, and can lead to state-of-the-art accuracy. Most of the tasks discussed here can also easily be modeled using Hidden Markov Models (HMM), and often with surprising accuracy. We will discuss the strengths of the HMMs and draw a comparison between the classification-based MBL method and the sequence-optimizing HMM approach (Section 1.2). 1.1 Memory-Based Shallow Parsing Shallow parsing is an important component of most text analysis systems in Text Mining applications such as information extraction, summary generation, and question answering. It includes discovering the main constituents of sentences (NPs, VPs, PPs) and their heads, and determining syntactic relationships like subject, object, adjunct relations between verbs and heads of other constituents. This is an important first step to understanding the who, what, when, and where of sentences in a text. Feature-Rich Memory-Based Classification for Information Extraction 35 In our approach to memory-based shallow parsing, we carve up the syntactic analysis process into a number of classification tasks with input vectors representing a focus item and a dynamically selected surrounding context. These classification tasks can be segmentation tasks (e.g., decide whether a focus word or tag is the start or end of an NP) or disambiguation tasks (e.g., decide whether a chunk is the subject NP, the object NP or neither). Output of some memory-based modules is used as input by other memory-based modules (e.g., a tagger feeds a chunker and the latter feeds a syntactic relation assignment module). Similar ideas about cascading of processing steps have also been explored in other approaches to text analysis: e.g., finite state partial parsing [1,18], statistical decision tree parsing [23], and maximum entropy parsing [30]. The approach briefly described here is explained and evaluated in more detail in [10,11,6] . Chunking The phrase chunking task can be defined as a classification task by generalizing the approach of [28], who proposed to convert NP-chunking to tagging each word with I for a word inside an NP, O for outside an NP, and B for between two NPs). The decision on these so called IOB tags for a word can be made by looking at the Part-of-Speech tag and the identity of the focus word and its local context. For the more general task of chunking other non-recursive phrases, we simply extend the tag set with IOB tags for each type of phrase. To illustrate this encoding with the extended IOB tag set, we can tag the sentence: But\/CC [NP the\/DT dollar\/NN NP] [ADVP later\/RB ADVP] [VP rebounded\/VBD VP] ,\/, [VP finishing\/VBG VP] [ADJP slightly\/RB higher\/R ADJP] [Prep against\/IN Prep] [NP the\/DT yen\/NNS NP] [ADJP although\/IN ADJP] [ADJP slightly\/RB lower\/JJR ADJP] [Prep against\/IN Prep] [NP the\/DT mark\/NN NP] .\/. as: But\/CCO the\/DTI\u2212NP dollar\/NNI\u2212NP later\/RBI\u2212ADV P rebounded\/VBDI\u2212V P ,\/,O finishing\/VBGI\u2212V P slightly\/RBI\u2212ADV P higher\/RBRI\u2212ADV P against\/INI\u2212Prep the\/DTI\u2212NP yen\/NNSI\u2212NP although\/INI\u2212ADJP slightly\/RBB\u2212ADJP lower\/JJRI\u2212ADJP against\/INI\u2212Prep the\/DTI\u2212NP mark\/NNI\u2212NP .\/.O Table 1 (from [6]) shows the accuracy of this memory-based chunking approach when training and testing on Wall Street Journal material. We report Precision, Recall, and F\u03b2=1 scores, a weighted harmonic mean of Recall and Precision (F\u03b2 = (\u03b2+1)\u2217P\u2217R \u03b22\u2217P+R) ). 1 An online demonstration of the Memory-Based Shallow Parser can be found at http:\/\/ilk.kub.nl . 36 Zavrel, Daelemans type precision recall F\u03b2=1 NPchunks 92.5 92.2 92.3 VPchunks 91.9 91.7 91.8 ADJPchunks 68.4 65.0 66.7 ADVPchunks 78.0 77.9 77.9 Prepchunks 95.5 96.7 96.1 PPchunks 91.9 92.2 92.0 ADVFUNCs 78.0 69.5 73.5 Table 1. Results of chunking\u2013labeling experiments. Reproduced from [6]. Grammatical Relation Finding After POS tagging, phrase chunking and labeling, the last step of the shallow parsing consists of resolving the (types of) attachment between labeled phrases. This is done by using a classifier to assign a grammatical relation (GR) between pairs of words in a sentence. In our approach, one of these words is always a verb, since this yields the most important GRs. The other word (focus) is the head of the phrase which is annotated with this grammatical relation in the treebank (e.g., a noun as head of an NP). An instance for such a pair of words is constructed by extracting a set of feature values from the sentence. The instance contains information about the verb and the focus: a feature for the word form and a feature for the POS of both. It also has similar features for the local context of the focus. Experiments on the training data suggest an optimal context width of two words to the left and one to the right. In addition to the lexical and the local context information, superficial information about clause structure was included: the distance from the verb to the focus, counted in words. A negative distance means that the focus is to the left of the verb. Other features contain the number of other verbs between the verb and the focus, and the number of intervening commas. These features were chosen by manual \u201cfeature engineering\u201d. Table 2 shows some of the feature-value instances corresponding to the following sentence (POS tags after the slash, chunks denoted with square and curly brackets, and adverbial functions after the dash): [ADVP Not \/RB surprisingly \/RB ADVP] ,\/, [NP Peter \/NNP Miller \/NNP NP] ,\/, [NP who \/WP NP] [VP organized \/VBD VP] [NP the \/DT conference \/NN NP] {PP-LOC [Prep in \/IN Prep] [NP New \/NNP York \/NNP NP] PP-LOC} ,\/, [VP does \/VBZ not \/RB want \/VB to \/TO come \/VB VP] {PP-DIR [Prep to \/IN Prep] [NP Paris \/NNP NP] PP-DIR} [Prep without \/IN Prep] [VP bringing \/VBG VP] [NP his \/PRP$ wife \/NN NP]. Table 3 shows the results of the experiments. In the first row, only POS tag features are used. Other rows show the results when adding several types of chunk informati"}
{"_id":"b0417e375954ee1503cadafee67fcf0873ff8dbf","title":"A Literature Review on the Design of Spherical Rolling Robots","text":"There are many advantages to the use of spherical robot designs. The current status of the design of spherical rolling robots is reviewed."}
{"_id":"a75eef1178c4f260d5e1f08dce15dcc217015463","title":"A Handwritten Character Recognition System Using Directional Element Feature and Asymmetric Mahalanobis Distance","text":"This paper presents a precise system for handwritten Chinese and Japanese character recognition. Before extracting directional element feature (DEF) from each character image, transformation based on partial inclination detection (TPID) is used to reduce undesired effects of degraded images. In the recognition process, city block distance with deviation (CBDD) and asymmetric Mahalanobis distance (AMD) are proposed for rough classification and fine classification. With this recognition system, the experimental result of the database ETL9B reaches to 99.42%."}
{"_id":"92862e13ceb048d596d05b5c788765649be9d851","title":"Co-FAIS: Cooperative fuzzy artificial immune system for detecting intrusion in wireless sensor networks","text":"Due to the distributed nature of Denial-of-Service attacks, it is tremendously challenging to identify such malicious behavior using traditional intrusion detection systems in Wireless Sensor Networks (WSNs). In the current paper, a bio-inspired method is introduced, namely the cooperative-based fuzzy artificial immune system (Co-FAIS). It is a modular-based defense strategy derived from the danger theory of the human immune system. The agents synchronize and work with one another to calculate the abnormality of sensor behavior in terms of context antigen value (CAV) or attackers and update the fuzzy activation threshold for security response. In such a multi-node circumstance, the sniffer module adapts to the sink node to audit data by analyzing the packet components and sending the log file to the next layer. The fuzzy misuse detector module (FMDM) integrates with a danger detector module to identify the sources of danger signals. The infected sources are transmitted to the fuzzy Q-learning vaccination modules (FQVM) in order for particular, required action to enhance system abilities. The Cooperative Decision Making Modules (Co-DMM) incorporates danger detector module with the fuzzy Q-learning vaccination module to produce optimum defense strategies. To evaluate the performance of the proposed model, the Low Energy Adaptive Clustering Hierarchy (LEACH) was simulated using a network simulator. The model was subsequently compared against other existing soft computing methods, such as fuzzy logic controller (FLC), artificial immune system (AIS), and fuzzy Q-learning (FQL), in terms of detection accuracy, counter-defense, network lifetime and energy consumption, to demonstrate its efficiency and viability. The proposed method improves detection accuracy and successful defense rate performance against attacks compared to conventional empirical methods. & 2014 Elsevier Ltd. All rights reserved."}
{"_id":"c9a9a1e13031c6548c7e52e45ed9b69edc6a4921","title":"Emotion regulation: affective, cognitive, and social consequences.","text":"One of life's great challenges is successfully regulating emotions. Do some emotion regulation strategies have more to recommend them than others? According to Gross's (1998, Review of General Psychology, 2, 271-299) process model of emotion regulation, strategies that act early in the emotion-generative process should have a different profile of consequences than strategies that act later on. This review focuses on two commonly used strategies for down-regulating emotion. The first, reappraisal, comes early in the emotion-generative process. It consists of changing the way a situation is construed so as to decrease its emotional impact. The second, suppression, comes later in the emotion-generative process. It consists of inhibiting the outward signs of inner feelings. Experimental and individual-difference studies find reappraisal is often more effective than suppression. Reappraisal decreases emotion experience and behavioral expression, and has no impact on memory. By contrast, suppression decreases behavioral expression, but fails to decrease emotion experience, and actually impairs memory. Suppression also increases physiological responding for suppressors and their social partners. This review concludes with a consideration of five important directions for future research on emotion regulation processes."}
{"_id":"b7fba13783ff14db9eb869dc3c9e4c69a9455b04","title":"Internet of things capability and alliance: Entrepreneurial orientation, market orientation and product and process innovation","text":"Internet of things capability and alliance: entrepreneurial orientation, market orientation and product and process innovation Xiaoyu Yu Bang Nguyen Yi Chen Article information: To cite this document: Xiaoyu Yu Bang Nguyen Yi Chen , (2016),\"Internet of things capability and alliance: entrepreneurial orientation, market orientation and product and process innovation\", Internet Research, Vol. 26 Iss 2 pp. Permanent link to this document: http:\/\/dx.doi.org\/10.1108\/IntR-10-2014-0265"}
{"_id":"605ed83a6d1f4eaf995e85830f373923b11d6c13","title":"Cover your ACKs: pitfalls of covert channel censorship circumvention","text":"In response to increasingly sophisticated methods of blocking access to censorship circumvention schemes such as Tor, recently proposed systems such as Skypemorph, FreeWave, and CensorSpoofer have used voice and video conferencing protocols as \"cover channels\" to hide proxy connections. We demonstrate that even with perfect emulation of the cover channel, these systems can be vulnerable to attacks that detect or disrupt the covert communications while having no effect on legitimate cover traffic. Our attacks stem from differences in the channel requirements for the cover protocols, which are peer-to-peer and loss tolerant, and the covert traffic, which is client-proxy and loss intolerant. These differences represent significant limitations and suggest that such protocols are a poor choice of cover channel for general censorship circumvention schemes."}
{"_id":"2e6a52d23dba71c974adeabee506aee72df7b3bc","title":"Characterizing debate performance via aggregated twitter sentiment","text":"Television broadcasters are beginning to combine social micro-blogging systems such as Twitter with television to create social video experiences around events. We looked at one such event, the first U.S. presidential debate in 2008, in conjunction with aggregated ratings of message sentiment from Twitter. We begin to develop an analytical methodology and visual representations that could help a journalist or public affairs person better understand the temporal dynamics of sentiment in reaction to the debate video. We demonstrate visuals and metrics that can be used to detect sentiment pulse, anomalies in that pulse, and indications of controversial topics that can be used to inform the design of visual analytic systems for social media events."}
{"_id":"ec4a94637ecd115219869e9df8902cb7282481e0","title":"Semantic Sentiment Analysis of Twitter","text":"Sentiment analysis over Twitter offer organisations a fast and effective way to monitor the publics\u2019 feelings towards their brand, business, directors, etc. A wide range of features and methods for training sentiment classifiers for Twitter datasets have been researched in recent years with varying results. In this paper, we introduce a novel approach of adding semantics as additional features into the training set for sentiment analysis. For each extracted entity (e.g. iPhone) from tweets, we add its semantic concept (e.g. \u201cApple product\u201d) as an additional feature, and measure the correlation of the representative concept with negative\/positive sentiment. We apply this approach to predict sentiment for three different Twitter datasets. Our results show an average increase of F harmonic accuracy score for identifying both negative and positive sentiment of around 6.5% and 4.8% over the baselines of unigrams and part-of-speech features respectively. We also compare against an approach based on sentiment-bearing topic analysis, and find that semantic features produce better Recall and F score when classifying negative sentiment, and better Precision with lower Recall and F score in positive sentiment classification."}
{"_id":"07f33ce4159c1188ad20b864661731e246512239","title":"From bias to opinion: a transfer-learning approach to real-time sentiment analysis","text":"Real-time interaction, which enables live discussions, has become a key feature of most Web applications. In such an environment, the ability to automatically analyze user opinions and sentiments as discussions develop is a powerful resource known as real time sentiment analysis. However, this task comes with several challenges, including the need to deal with highly dynamic textual content that is characterized by changes in vocabulary and its subjective meaning and the lack of labeled data needed to support supervised classifiers. In this paper, we propose a transfer learning strategy to perform real time sentiment analysis. We identify a task - opinion holder bias prediction - which is strongly related to the sentiment analysis task; however, in constrast to sentiment analysis, it builds accurate models since the underlying relational data follows a stationary distribution.\n Instead of learning textual models to predict content polarity (i.e., the traditional sentiment analysis approach), we first measure the bias of social media users toward a topic, by solving a relational learning task over a network of users connected by endorsements (e.g., retweets in Twitter). We then analyze sentiments by transferring user biases to textual features. This approach works because while new terms may arise and old terms may change their meaning, user bias tends to be more consistent over time as a basic property of human behavior. Thus, we adopted user bias as the basis for building accurate classification models. We applied our model to posts collected from Twitter on two topics: the 2010 Brazilian Presidential Elections and the 2010 season of Brazilian Soccer League. Our results show that knowing the bias of only 10% of users generates an F1 accuracy level ranging from 80% to 90% in predicting user sentiment in tweets."}
{"_id":"a22445456a70896f850150001ecf4d0a726be9a9","title":"Hedonic and instrumental motives in anger regulation.","text":"What motivates individuals to regulate their emotions? One answer, which has been highlighted in emotion-regulation research, is that individuals are motivated by short-term hedonic goals (e.g., the motivation to feel pleasure). Another answer, however, is that individuals are motivated by instrumental goals (e.g., the motivation to perform certain behaviors). We suggest that both answers have merit. To demonstrate the role instrumental goals may play in emotion regulation, we pitted short-term hedonic motives and instrumental motives against each other, by testing whether individuals were motivated to experience a potentially useful, albeit unpleasant, emotion. We found that (a) individuals preferred activities that would increase their level of anger (but not their level of excitement) when they were anticipating confrontational, but not nonconfrontational, tasks and that (b) anger improved performance in a confrontational, but not a nonconfrontational, task. These findings support a functional view of emotion regulation, and demonstrate that in certain contexts, individuals may choose to experience emotions that are instrumental, despite short-term hedonic costs."}
{"_id":"8f92b07d5ad2937ebddcc66e1be820805f53e4ec","title":"Building Poker Agent Using Reinforcement Learning with Neural Networks","text":"Poker is a game with incomplete and imperfect information. The ability to estimate opponent and interpret its actions makes a player as a world class player. Finding optimal game strategy is not enough to win poker game. As in real life as in online poker game, the most time of it consists of opponent analysis. This paper illustrates a development of poker agent using reinforcement learning with neural networks. 1 STAGE OF THE RESEARCH Poker is a game with incomplete and imperfect information. The ability to estimate opponent and interpret its actions makes a player as a world class player. Finding optimal game strategy is not enough to win poker game. As in real life as in online poker game, the most time of it consists of opponent analysis. Author is working on development of poker agent that would find optimal game strategy using reinforcement learning (RL) in combination with artificial neural network (ANN) for value function approximation. 2 OUTLINE OF OBJECTIVES This paper illustrates a development of poker agent using reinforcement learning with neural networks. Complete poker agent should have an ability to create optimal game strategy that makes decisions based on information: \uf0a7 Hand strength\/potential estimation; \uf0a7 Table card estimation; \uf0a7 Opponent hand strength prediction; \uf0a7 Opponent classification (tight\/loose passive\/ aggressive); \uf0a7 Current state evaluation using neural network. AI Poker agent should be able to find an optimal strategy by itself (unsupervised learning) using information given above. It also should be able to adapt opponent play style and change its strategy during the game."}
{"_id":"7efa3b18c6d3aa8f20b5b91bb85dd9b76436dd18","title":"MATLAB Based BackPropagation Neural Network for Automatic Speech Recognition","text":"Speech interface to computer is the next big step that the technology needs to take for general users. Automatic speech recognition (ASR) will play an important role in taking technology to the people. There are numerous applications of speech recognition such as direct voice input in aircraft, data entry, speech-to-text processing, voice user interfaces such as voice dialling. ASR system can be divided into two different parts, namely feature extraction and feature recognition. In this paper we present MATLAB based feature recognition using backpropagation neural network for ASR. The objective of this research is to explore how neural networks can be employed to recognize isolated-word speech as an alternative to the traditional methodologies. The general techniques developed here can be further extended to other applications such as sonar target recognition, missile tracking and classification of underwater acoustic signals. Back-propagation neural network algorithm uses input training samples and their respective desired output values to learn to recognize specific patterns, by modifying the activation values of its nodes and weights of the links connecting its nodes. Such a trained network is later used for feature recognition in ASR systems."}
{"_id":"10faedfeb8ab4280b43538a3cfdeabc46fad7d41","title":"Automated Fingerprint Identification and Imaging Systems","text":"More than a century has passed since Alphonse Bertillon first conceived and then industriously practiced the idea of using body measurements for solving crimes [1]. Just as his idea was gaining popularity, it faded into relative obscurity by a far more significant and pract ic l discovery of the uniqueness of the human fingerprints 1. Soon after this discovery, many major law enforcement departments embraced the idea of first \u201cbooking\u201d the fingerprints of criminals, so tha t their records are readily available and later using leftover fingerprint smudges (latent s), the identity of criminals can be determined. These agencies sponsored a rigorous study of fingerprints, developed s cientific 1In 1893, the Home Ministry Office, UK, accepted that no two ind ividuals have the same fingerprints."}
{"_id":"3261659127bebca4d805e8592906b37ece8d0ca3","title":"Automatic Linguistic Annotation of Large Scale L 2 Databases : The EF-Cambridge Open Language Database ( EFCamDat )","text":"\u2217Naturalistic learner productions are an important empirical resource for SLA research. Some pioneering works have produced valuable second language (L2) resources supporting SLA research.1 One common limitation of these resources is the absence of individual longitudinal data for numerous speakers with different backgrounds across the proficiency spectrum, which is vital for understanding the nature of individual variation in longitudinal development.2 A second limitation is the relatively restricted amounts of data annotated with linguistic information (e.g., lexical, morphosyntactic, semantic features, etc.) to support investigation of SLA hypotheses and obtain patterns of development for different linguistic phenomena. Where available, annotations tend to be manually obtained, a situation posing immediate limitations to the quantity of data that could be annotated with reasonable human resources and within reasonable time. Natural Language Processing (NLP) tools can provide automatic annotations for parts-of-speech (POS) and syntactic structure and are indeed increasingly applied to learner language in various contexts. Systems in computer-assisted language learning (CALL) have used a parser and other NLP tools to automatically detect learner errors and provide feedback accordingly.3 Some work aimed at adapting annotations provided by parsing tools to accurately describe learner syntax (Dickinson & Lee, 2009) or evaluated parser performance on learner language and the effect of learner errors on the parser. Krivanek and Meurers (2011) compared two parsing methods, one using a hand-crafted lexicon and one trained on a corpus. They found that the former is more successful in recovering the main grammatical dependency relations whereas the latter is more successful in recovering optional, adjunction relations. Ott and Ziai (2010) evaluated the performance of a dependency parser trained on native German (MaltParser; Nivre et al., 2007) on 106 learner answers to a comprehension task in L2 German. Their study indicates that while some errors can be problematic for the parser (e.g., omission of finite verbs) many others (e.g., wrong word order) can be parsed robustly, resulting in overall high performance scores. In this paper we have two goals. First, we introduce a new English L2 database, the EF Cambridge Open Language Database, henceforth EFCAMDAT. EFCAMDAT was developed by the Department of Theoretical and Applied Linguistics at the University of Cambridge in collaboration with EF Education First, an international educational organization. It contains writings submitted to Englishtown, the"}
{"_id":"378cf0ebeeca44ccd7f04b5e618c6854d0f467ef","title":"A Radius and Ulna TW3 Bone Age Assessment System","text":"An end-to-end system to automate the well-known Tanner-Whitehouse (TW3) clinical procedure to estimate the skeletal age in childhood is proposed. The system comprises the detailed analysis of the two most important bones in TW3: the radius and ulna wrist bones. First, a modified version of an adaptive clustering segmentation algorithm is presented to properly semi-automatically segment the contour of the bones. Second, up to 89 features are defined and extracted from bone contours and gray scale information inside the contour, followed by some well- founded feature selection mathematical criteria, based on the ideas of maximizing the classes' separability. Third, bone age is estimated with the help of a generalized softmax perceptron (GSP) neural network (NN) that, after supervised learning and optimal complexity estimation via the application of the recently developed posterior probability model selection (PPMS) algorithm, is able to accurately predict the different development stages in both radius and ulna from which and with the help of the TW3 methodology, we are able to conveniently score and estimate the bone age of a patient in years, in what can be understood as a multiple- class (multiple stages) pattern recognition approach with posterior probability estimation. Finally, numerical results are presented to evaluate the system performance in predicting the bone stages and the final patient bone age over a private hand image database, with the help of the pediatricians and the radiologists expert diagnoses."}
{"_id":"76e5089c5e93bef05fdc04695c63c7582a9dd9d0","title":"An Approach to Data Privacy in Smart Home using Blockchain Technology","text":"Nowadays, numerous applications of smart home systems provide recommendations for users, including reducing their energy consumption, warnings of defective devices, selecting reliable devices and software, diagnoses, etc [1]. The internet connected, dynamic and heterogeneous nature of the smart home environment creates new security, authentication, and privacy challenges [2]. To solve those challenges, an approach to data privacy in smart home using blockchain technology, which is called smart home based the IoT-Blockchain (SHIB), is proposed in this paper. In order to demonstrate the proposed architecture, an experimental scenario using Ganache, Remix, and web3. js is built among the user, service provider, and smart home to evaluate the performance of the smart contract in the SHIB. Based on the experiment results, the SHIB architecture brings the advantages like data privacy, trust access control, and high extension ability. In addition, the comparison between the proposed architecture and existing models in different parameters such as smart contract, the privacy of data, usage of tokens, updating the policies, and misbehavior judging are performed."}
{"_id":"03827b4aef6809ac90487ef1a9d27048088db413","title":"Semi-supervised Density-Based Clustering","text":"Most of the effort in the semi-supervised clustering literature was devoted to variations of the K-means algorithm. In this paper we show how background knowledge can be used to bias a partitional density-based clustering algorithm. Our work describes how labeled objects can be used to help the algorithm detecting suitable density parameters for the algorithm to extract density-based clusters in specific parts of the feature space. Considering the set of constraints estabilished by the labeled dataset we show that our algorithm, called SSDBSCAN, automatically finds density parameters for each natural cluster in a dataset. Four of the most interesting characteristics of SSDBSCAN are that (1) it only requires a single, robust input parameter, (2) it does not need any user intervention, (3) it automaticaly finds the noise objects according to the density of the natural clusters and (4) it is able to find the natural cluster structure even when the density among clusters vary widely. The algorithm presented in this paper is evaluated with artificial and real-world datasets, demonstrating better results when compared to other unsupervised and semi-supervised density-based approaches."}
{"_id":"46a473b1a8ae8d70bb01ccf914005e411b077d0d","title":"Shake-your-head: revisiting walking-in-place for desktop virtual reality","text":"The Walking-In-Place interaction technique was introduced to navigate infinitely in 3D virtual worlds by walking in place in the real world. The technique has been initially developed for users standing in immersive setups and was built upon sophisticated visual displays and tracking equipments.\n In this paper, we propose to revisit the whole pipeline of the Walking-In-Place technique to match a larger set of configurations and apply it notably to the context of desktop Virtual Reality. With our novel \"Shake-Your-Head\" technique, the user is left with the possibility to sit down, and to use small screens and standard input devices such as a basic webcam for tracking. The locomotion simulation can compute various motions such as turning, jumping and crawling, using as sole input the head movements of the user. We also introduce the use of additional visual feedback based on camera motions to enhance the walking sensations.\n An experiment was conducted to compare our technique with classical input devices used for navigating in desktop VR. Interestingly, the results showed that our technique could even allow faster navigations when sitting, after a short learning. Our technique was also perceived as more fun and increasing presence, and was generally more appreciated for VR navigation."}
{"_id":"1a9bb0a637cbaca8489afd69d6840975a9834a05","title":"Should one compute the Temporal Difference fix point or minimize the Bellman Residual? The unified oblique projection view","text":"We investigate projection methods, for evaluating a linear approximation of the value function of a policy in a Markov Decision Process context. We consider two popular approaches, the one-step Temporal Difference fix-point computation (TD(0)) and the Bellman Residual (BR) minimization. We describe examples, where each method outperforms the other. We highlight a simple relation between the objective function they minimize, and show that while BR enjoys a performance guarantee, TD(0) does not in general. We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008). Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average."}
{"_id":"c62b19bfe035130d618cbf6fbd1b966fea92fa0c","title":"Analysis Enhanced Particle-based Flow Visualization","text":"Particle-based fluid simulation (PFS), such as Smoothed Particle Hydrodynamics (SPH) and Position-based Fluid (PBF), is a mesh-free method that has been widely used in various fields, including astrophysics, mechanical engineering, and biomedical engineering for the study of liquid behaviors under different circumstances. Due to its meshless nature, most analysis techniques that are developed for mesh-based data need to be adapted for the analysis of PFS data. In this work, we study a number of flow analysis techniques and their extension for PFS data analysis, including the FTLE approach, Jacobian analysis, and an attribute accumlation framework. In particular, we apply these analysis techniques to free surface fluids. We demonstrate that these analyses can reveal some interesting underlying flow patterns that would be hard to see otherwise via a number of PFS simulated flows with different parameters and boundary settings. In addition, we point out that an in-situ analysis framework that performs these analyses can potentially be used to guide the adaptive PFS to allocate the computation and storage power to the regions of interest during the simulation."}
{"_id":"83f80b0c517f60eaec01442eb58a6ffb8bd8ab60","title":"Look into Person: Joint Body Parsing & Pose Estimation Network and a New Benchmark","text":"Human parsing and pose estimation have recently received considerable interest due to their substantial application potentials. However, the existing datasets have limited numbers of images and annotations and lack a variety of human appearances and coverage of challenging cases in unconstrained environments. In this paper, we introduce a new benchmark named \u201cLook into Person (LIP)\u201d that provides a significant advancement in terms of scalability, diversity, and difficulty, which are crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels and 16 body joints, which are captured from a broad range of viewpoints, occlusions, and background complexities. Using these rich annotations, we perform detailed analyses of the leading human parsing and pose estimation approaches, thereby obtaining insights into the successes and failures of these methods. To further explore and take advantage of the semantic correlation of these two tasks, we propose a novel joint human parsing and pose estimation network to explore efficient context modeling, which can simultaneously predict parsing and pose with extremely high quality. Furthermore, we simplify the network to solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into the parsing results without resorting to extra supervision. The datasets, code and models are available at http:\/\/www.sysu-hcp.net\/lip\/."}
{"_id":"d7088b8102edc27106a0ad8cee9bc6c4845edfe9","title":"OctoPocus: a dynamic guide for learning gesture-based command sets","text":"We describe OctoPocus, an example of a dynamic guide that combines on-screen feedforward and feedback to help users learn, execute and remember gesture sets. OctoPocus can be applied to a wide range of single-stroke gestures and recognition algorithms and helps users progress smoothly from novice to expert performance. We provide an analysis of the design space and describe the results of two experi-ments that show that OctoPocus is significantly faster and improves learning of arbitrary gestures, compared to con-ventional Help menus. It can also be adapted to a mark-based gesture set, significantly improving input time compared to a two-level, four-item Hierarchical Marking menu."}
{"_id":"b45df87ab743d92a6a508815c5cab6d0a52dd3ce","title":"Controversies surrounding nonspeech oral motor exercises for childhood speech disorders.","text":"The use of nonspeech oral motor exercises (NSOMEs) for influencing children\u2019s speech sound productions is a common therapeutic practice used by speech-language pathologists (SLPs) in the United States, Canada, and the United Kingdom. Reports from these countries have documented that between 71.5% and 85% of practicing clinicians use some type of NSOMEs in therapy to change children\u2019s speech productions. NSOMEs can be defined as any therapy technique that does not require the child to produce a speech sound but is used to influence the development of speaking abilities. The National Center for Evidence-Based Practice in Communication Disorders (NCEP) of the American Speech-Language-Hearing Association (ASHA) developed this definition: \u2018\u2018Oral-motor exercises are activities that involve sensory stimulation to or actions of the lips, jaw, tongue, soft palate, larynx, and respiratory muscles which are intended to influence the physiologic underpinnings of the oropharyngeal mechanism and thus improve its functions; oral-motor exercises may include active muscle exercise, muscle stretching, passive exercise, and sensory stimulation.\u2019\u2019 The term \u2018\u2018oral motor,\u2019\u2019 which relates to movements and placements of the oral musculature, is established in the field of SLP. Although the existence and importance of the oral-motor aspects of speech production is well understood, the use and effectiveness of nonspeech oral-motor activities is disputed because of the lack of theoretical and empirical support. To understand more about the use of NSOMEs for changing disordered productions of speech, a colleague and I conducted a nationwide survey of 537 practicing clinicians from 48 states. We found that SLPs frequently used the exercises of blowing, tongue wagging and pushups, cheek puffing, the alternating movement of pucker-smile, \u2018\u2018big smile,\u2019\u2019 and tongue-to-nose-to-chin. The clinicians believed these NSOMEs would help their clients obtain tongue elevation, protrusion, and lateralization; increase their tongue and lip strength; become more aware of their articulators; stabilize the jaw; control drooling; and increase velopharyngeal and sucking abilities. The respondents to the survey reported using these techniques for children with a wide variety of different speech disorders stemming from a wide variety of etiologies: dysarthria, childhood apraxia of speech (CAS), Down syndrome, late talkers, phonological impairment, functional misarticulations, and hearing impairment. It makes one curious why clinicians would select these nonspeech therapeutic techniques because they lack adequate theoretical underpinnings for their use. Additionally, the research studies that have evaluated treatment efficacy using the NSOME techniques, although admittedly scant and not at the highest levels of scientific rigor, do not show therapeutic effectiveness. Not only are these nonspeech tasks lacking in theoretical and data support for their use, their application to improve speech intelligibility also often defies logical reasoning. So why do clinicians use these techniques? As I have previously pointed out, SLPs have several possible reasons for using NSOMEs. Some of these reasons may be that"}
{"_id":"5383ee00a5c0c3372081fccfeceb812b7854c9ea","title":"Spatial as Deep: Spatial CNN for Traffic Scene Understanding","text":"Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-byslice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7% and 4.6% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53%."}
{"_id":"458d8adb34e1de582b6ce553755fb6493093fecf","title":"Multi-modality web video categorization","text":"This paper reports a first comprehensive study and large-scale test on web video (so-called user generated video or micro video) categorization. Observing that web videos are characterized by a much higher diversity of quality, subject, style, and genres compared with traditional video programs, we focus on studying the effectiveness of different modalities in dealing with such high variation. Specifically, we propose two novel modalities including a semantic modality and a surrounding text modality, as effective complements to most commonly used low-level features. The semantic modality includes three feature representations, i.e., concept histogram, visual word vector model and visual word Latent Semantic Analysis (LSA), while text modality includes the titles, descriptions and tags of web videos. We conduct a set of comprehensive experiments for evaluating the effectiveness of the proposed feature representations over different classifiers such as Support Vector Machine (SVM), Gaussian Mixture Model (GMM) and Manifold Ranking (MR). Our experiments on a large-scale dataset with 11k web videos (nearly 450 hours in total) demonstrate that (1) the proposed multimodal feature representation is effective for web video categorization, and (2) SVM outperforms GMM and MR on nearly all the modalities."}
{"_id":"b422615322420d906c75a40843b3a5f200587f9d","title":"A Fast-Response Pseudo-PWM Buck Converter With PLL-Based Hysteresis Control","text":"Hysteresis voltage-mode control is a simple and fast control scheme for switched-mode power converters. However, it is well-known that the switching frequency of a switched-mode power converter with hysteresis control depends on many factors such as loading current and delay of the controller which vary from time to time. It results in a wide noise spectrum and leads to difficulty in shielding electro-magnetic interference. In this work, a phase-lock loop (PLL) is utilized to control the hysteresis level of the comparator used in the controller, while not interfering with the intrinsic behavior of the hysteresis controller. Some design techniques are used to solve the integration problem and to improve the settling speed of the PLL. Moreover, different control modes are implemented. A buck converter with proposed control scheme is fabricated using a commercial 0.35-\u03bc m CMOS technology. The chip area is 1900 \u03bcm \u00d7 2200 \u03bcm. The switching frequency is locked to 1 MHz, and the measured frequency deviation is within \u00b11%. The measured load transient response between 160 and 360 mA is 5 \u03bc s only."}
{"_id":"6e612d643d44d9bee5ab0532aa25fe3fa66e85d5","title":"Memristor-The Missing Circuit Element LEON 0","text":"A new two-terminal circuit element-called the memrirtorcharacterized by a relationship between the charge q(t) s St% i(7J d7 and the flux-linkage (p(t) = J-\u2018-m vfrj d T is introduced os the fourth boric circuit element. An electromagnetic field interpretation of this relationship in terms of a quasi-static expansion of Maxwell\u2019s equations is presented. Many circuit~theoretic properties of memdstorr are derived. It is shown that this element exhibiis some peculiar behavior different from that exhibited by resistors, inductors, or capacitors. These properties lead to a number of unique applications which cannot be realized with RLC networks alone. I + \u201d -3 nl Although a physical memristor device without internal power supply has not yet been discovered, operational laboratory models have been built with the help of active circuits. Experimental results ore presented to demonstrate the properties and potential applications of memristors. (a) I. 1NTR00~cnoN I + Y -3 T HIS PAPER presents the logical and scientific basis for the existence of a new two-terminal circuit element called the memristor (a contraction for memory (b) resistor) which has every right to be as basic as the three classical circuit elements already in existence, namely, the resistor, inductor, and capacitor. Although the existence of a memristor in the form of a physical device without internal power supply has not yet been discovered, its laboratory realization in the form of active circuits will be presented in Section II.\u2019 Many interesting circuit-theoretic properties possessed by the memristor, the most important of which is perhaps the passivity property which provides the circuit-theoretic basis for its physical realizability, will be derived in Section III. An electromagnetic field interpretation of the memristor characterization will be presented in Section IV with the help of a quasi-static expansion of Maxwell\u2019s equations. Finally, some novel applications of memristors will be presented in Section V. 1 + \u201d -3"}
{"_id":"79f026f743997ab8b5251f6e915a0a427576b142","title":"Millimeter-wave antennas for radio access and backhaul in 5G heterogeneous mobile networks","text":"Millimeter-wave communications are expected to play a key role in future 5G mobile networks to overcome the dramatic traffic growth expected over the next decade. Such systems will severely challenge antenna technologies used at mobile terminal, access point or backhaul\/fronthaul levels. This paper provides an overview of the authors' recent achievements in the design of integrated antennas, antenna arrays and high-directivity quasi-optical antennas for high data-rate 60-GHz communications."}
{"_id":"9302a33f0ca803e68c48ddc76fbc7a8a7517999f","title":"Frequency compensation of high-speed, low-voltage CMOS multistage amplifiers","text":"This paper presents the frequency compensation of high-speed, low-voltage multistage amplifiers. Two frequency compensation techniques, the Nested Miller Compensation with Nulling Resistors (NMCNR) and Reversed Nested Indirect Compensation (RNIC), are discussed and employed on two multistage amplifier architectures. A four-stage pseudo-differential amplifier with CMFF and CMFB is designed in a 1.2 V, 65-nm CMOS process. With NMCNR, it achieves a phase margin (PM) of 59\u00b0 with a DC gain of 75 dB and unity-gain frequency (fug) of 712 MHz. With RNIC, the same four-stage amplifier achieves a phase margin of 84\u00b0, DC gain of 76 dB and fug of 2 GHz. Further, a three-stage single-ended amplifier is designed in a 1.1-V, 40-nm CMOS process. The three-stage OTA with RNIC achieves PM of 81\u00b0, DC gain of 80 dB and fug of 770 MHz. The same OTA achieves PM of 59\u00b0 with NMCNR, while maintaining a DC gain of 75 dB and fug of 262 MHz. Pole-splitting, to achieve increased stability, is illustrated for both compensation schemes. Simulations illustrate that the RNIC scheme achieves much higher PM and fug for lower values of compensation capacitance compared to NMCNR, despite the growing number of low voltage amplifier stages."}
{"_id":"d151823fabbe748b41d90ab0a01b8f9e90822241","title":"Time-Frequency Masking in the Complex Domain for Speech Dereverberation and Denoising","text":"In real-world situations, speech is masked by both background noise and reverberation, which negatively affect perceptual quality and intelligibility. In this paper, we address monaural speech separation in reverberant and noisy environments. We perform dereverberation and denoising using supervised learning with a deep neural network. Specifically, we enhance the magnitude and phase by performing separation with an estimate of the complex ideal ratio mask. We define the complex ideal ratio mask so that direct speech results after the mask is applied to reverberant and noisy speech. Our approach is evaluated using simulated and real room impulse responses, and with background noises. The proposed approach improves objective speech quality and intelligibility significantly. Evaluations and comparisons show that it outperforms related methods in many reverberant and noisy environments."}
{"_id":"9ef98f87a6b1bd7e6c006f056f321f9d291d4c45","title":"Depth Images-based Human Detection , Tracking and Activity Recognition Using Spatiotemporal Features and Modified HMM","text":"Human activity recognition using depth information is an emerging and challenging technology in computer vision due to its considerable attention by many practical applications such as smart home\/office system, personal health care and 3D video games. This paper presents a novel framework of 3D human body detection, tracking and recognition from depth video sequences using spatiotemporal features and modified HMM. To detect human silhouette, raw depth data is examined to extract human silhouette by considering spatial continuity and constraints of human motion information. While, frame differentiation is used to track human movements. Features extraction mechanism consists of spatial depth shape features and temporal joints features are used to improve classification performance. Both of these features are fused together to recognize different activities using the modified hidden Markov model (M-HMM). The proposed approach is evaluated on two challenging depth video datasets. Moreover, our system has significant abilities to handle subject's body parts rotation and body parts missing which provide major contributions in human activity recognition."}
{"_id":"1737177098539e295235678d664fcdb833568b94","title":"Cloud Task Scheduling Based on Load Balancing Ant Colony Optimization","text":"The cloud computing is the development of distributed computing, parallel computing and grid computing, or defined as the commercial implementation of these computer science concepts. One of the fundamental issues in this environment is related to task scheduling. Cloud task scheduling is an NP-hard optimization problem, and many meta-heuristic algorithms have been proposed to solve it. A good task scheduler should adapt its scheduling strategy to the changing environment and the types of tasks. This paper proposes a cloud task scheduling policy based on Load Balancing Ant Colony Optimization (LBACO) algorithm. The main contribution of our work is to balance the entire system load while trying to minimizing the make span of a given tasks set. The new scheduling strategy was simulated using the CloudSim toolkit package. Experiments results showed the proposed LBACO algorithm outperformed FCFS (First Come First Serve) and the basic ACO (Ant Colony Optimization)."}
{"_id":"591bc36fbfa094495c01951026f275292b87b92c","title":"A Particle Swarm Optimization-Based Heuristic for Scheduling Workflow Applications in Cloud Computing Environments","text":"Cloud computing environments facilitate applications by providing virtualized resources that can be provisioned dynamically. However, users are charged on a pay-per-use basis. User applications may incur large data retrieval and execution costs when they are scheduled taking into account only the \u2018execution time\u2019. In addition to optimizing execution time, the cost arising from data transfers between resources as well as execution costs must also be taken into account. In this paper, we present a particle swarm optimization (PSO) based heuristic to schedule applications to cloud resources that takes into account both computation cost and data transmission cost. We experiment with a workflow application by varying its computation and communication costs. We compare the cost savings when using PSO and existing \u2018Best Resource Selection\u2019 (BRS) algorithm. Our results show that PSO can achieve: a) as much as 3 times cost savings as compared to BRS, and b) good distribution of workload onto resources."}
{"_id":"8186763a49f36c32af6144f789ca3a5b03a3c01e","title":"QRSF: QoS-aware resource scheduling framework in cloud computing","text":"Cloud computing harmonizes and delivers the ability of resource sharing over different geographical sites. Cloud resource scheduling is a tedious task due to the problem of finding the best match of resource-workload pair. The efficient management of dynamic nature of resource can be done with the help of cloud workloads. Till cloud workload is deliberated as a central capability, the resources cannot be utilized in an effective way. In literature, very few efficient resource scheduling policies for energy, cost and time constraint cloud workloads are reported. This paper presents an efficient cloud workload management framework in which cloud workloads have been identified, analyzed and clustered through K-means on the basis of weights assigned and their QoS requirements. Further scheduling has been done based on different scheduling policies and their corresponding algorithms. The performance of the proposed algorithms has been evaluated with existing scheduling policies through CloudSim toolkit. The experimental results show that the proposed framework gives better results in terms of energy consumption, execution cost and time of different cloud workloads as compared to existing algorithms."}
{"_id":"bb59dd3db07e4e82b0fc734e37531417d6834f86","title":"Systematic literature reviews in software engineering - A systematic literature review","text":"Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. ! 2008 Elsevier B.V. All rights reserved."}
{"_id":"c665f4a9b2ca7beb4ad4a7221e8830ea38b8d769","title":"Combining Knowledge and Data Driven Insights for Identifying Risk Factors using Electronic Health Records","text":"BACKGROUND\nThe ability to identify the risk factors related to an adverse condition, e.g., heart failures (HF) diagnosis, is very important for improving care quality and reducing cost. Existing approaches for risk factor identification are either knowledge driven (from guidelines or literatures) or data driven (from observational data). No existing method provides a model to effectively combine expert knowledge with data driven insight for risk factor identification.\n\n\nMETHODS\nWe present a systematic approach to enhance known knowledge-based risk factors with additional potential risk factors derived from data. The core of our approach is a sparse regression model with regularization terms that correspond to both knowledge and data driven risk factors.\n\n\nRESULTS\nThe approach is validated using a large dataset containing 4,644 heart failure cases and 45,981 controls. The outpatient electronic health records (EHRs) for these patients include diagnosis, medication, lab results from 2003-2010. We demonstrate that the proposed method can identify complementary risk factors that are not in the existing known factors and can better predict the onset of HF. We quantitatively compare different sets of risk factors in the context of predicting onset of HF using the performance metric, the Area Under the ROC Curve (AUC). The combined risk factors between knowledge and data significantly outperform knowledge-based risk factors alone. Furthermore, those additional risk factors are confirmed to be clinically meaningful by a cardiologist.\n\n\nCONCLUSION\nWe present a systematic framework for combining knowledge and data driven insights for risk factor identification. We demonstrate the power of this framework in the context of predicting onset of HF, where our approach can successfully identify intuitive and predictive risk factors beyond a set of known HF risk factors."}
{"_id":"0c369e983e93e9b2be9066d5e0fec7a60660a40a","title":"On the Resolution Limits of Superimposed Projection","text":"Multi-projector super-resolution is the dual of multi-camera super-resolution. The goal of projector super-resolution is to produce a high resolution frame via superimposition of multiple low resolution subframes. Prior work claims that it is impossible to improve resolution via superimposed projection except in specialized circumstances. Rigorous analysis has been previously restricted to the special case of uniform display sampling, which reduces the problem to a simple shift-invariant deblurring. To understand the true behavior of superimposed projection as an inverse of classical camera super-resolution, one must consider the effects of non-uniform displacements between component subframes. In this paper, we resolve two fundamental theoretical questions concerning resolution enhancement via superimposed projection. First, we show that it is possible to reproduce frequencies that are well beyond the Nyquist limit of any of the component subframes. Second, we show that nonuniform sampling and pixel reconstruction functions impose fundamental limits on achievable resolution."}
{"_id":"7fae42a6d1690e81f6b0c500c8a22330a6c2c75f","title":"Droop control in LV-grids","text":"Remote electrification with island supply systems, the increasing acceptance of the microgrids concept and the penetration of the interconnected grid with DER and RES require the application of inverters and the development of new control algorithms. One promising approach is the implementation of conventional f\/U-droops into the respective inverters, thus down scaling the conventional grid control concept to the low voltage grid. Despite contradicting line parameters, the applicability of this proceeding is outlined and the boundary conditions are derived"}
{"_id":"1724b33cd5d166d9dab26a61687a1e04e52a86c3","title":"Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization","text":"We give novel algorithms for stochastic strongly-convex optimization in the gradient oracle model which return a O( 1 T )-approximate solution after T iterations. The first algorithm is deterministic, and achieves this rate via gradient updates and historical averaging. The second algorithm is randomized, and is based on pure gradient steps with a random step size. This rate of convergence is optimal in the gradient oracle model. This improves upon the previously known best rate of O( log(T ) T ), which was obtained by applying an online stronglyconvex optimization algorithm with regret O(log(T )) to the batch setting. We complement this result by proving that any algorithm has expected regret of \u03a9(log(T )) in the online stochastic strongly-convex optimization setting. This shows that any online-to-batch conversion is inherently suboptimal for stochastic strongly-convex optimization. This is the first formal evidence that online convex optimization is strictly more difficult than batch stochastic convex optimization."}
{"_id":"728d1de052a40aa41ef8813bf128fb2a6db22597","title":"Oxytocin Attenuates Amygdala Reactivity to Fear in Generalized Social Anxiety Disorder","text":"Patients with generalized social anxiety disorder (GSAD) exhibit heightened activation of the amygdala in response to social cues conveying threat (eg, fearful\/angry faces). The neuropeptide oxytocin (OXT) decreases anxiety and stress, facilitates social encounters, and attenuates amygdala reactivity to threatening faces in healthy subjects. The goal of this study was to examine the effects of OXT on fear-related amygdala reactivity in GSAD and matched healthy control (CON) subjects. In a functional magnetic resonance imaging study utilizing a double-blind placebo-controlled within-subjects design, we measured amygdala activation to an emotional face matching task of fearful, angry, and happy faces following acute intranasal administration of OXT (24\u2009IU or 40.32\u2009\u03bcg) and placebo in 18 GSAD and 18 CON subjects. Both the CON and GSAD groups activated bilateral amygdala to all emotional faces during placebo, with the GSAD group exhibiting hyperactivity specifically to fearful faces in bilateral amygdala compared with the CON group. OXT had no effect on amygdala activity to emotional faces in the CON group, but attenuated the heightened amygdala reactivity to fearful faces in the GSAD group, such that the hyperactivity observed during the placebo session was no longer evident following OXT (ie, normalization). These findings suggest that OXT has a specific effect on fear-related amygdala activity, particularly when the amygdala is hyperactive, such as in GSAD, thereby providing a brain-based mechanism of the impact of OXT in modulating the exaggerated processing of social signals of threat in patients with pathological anxiety."}
{"_id":"5fb7bb492fcdae221480cf39185fdad5d877a2f2","title":"Generalized State Coherence Transform for multidimensional localization of multiple sources","text":"In our recent work an effective method for multiple source localization has been proposed under the name of cumulative State Coherence Transform (cSCT). Exploiting the physical meaning of the frequency-domain blind source separation and the sparse time-frequency dominance of the acoustic sources, multiple reliable TDOAs can be estimated with only two microphones, regardless of the permutation problem and of the microphone spacing. In this paper we present a multidimensional generalization of the cSCT which allows one to localize several sources in the P-dimensional space. An important approximation is made in order to perform a disjoint TDOA estimation over each dimension which reduces the localization problem to linear complexity. Furthermore the approach is invariant to the array geometry and to the assumed acoustic propagation model. Experimental results on simulated data show a precise 2-D localization of 7 sources by only using an array of three elements."}
{"_id":"a14fcb383f9ba008da3df52f06a79ec874e5a0a4","title":"An Overview of BioCreative II.5","text":"We present the results of the BioCreative II.5 evaluation in association with the FEBS Letters experiment, where authors created Structured Digital Abstracts to capture information about protein-protein interactions. The BioCreative II.5 challenge evaluated automatic annotations from 15 text mining teams based on a gold standard created by reconciling annotations from curators, authors, and automated systems. The tasks were to rank articles for curation based on curatable protein-protein interactions; to identify the interacting proteins (using UniProt identifiers) in the positive articles (61); and to identify interacting protein pairs. There were 595 full-text articles in the evaluation test set, including those both with and without curatable protein interactions. The principal evaluation metrics were the interpolated area under the precision\/recall curve (AUC iP\/R), and (balanced) F-measure. For article classification, the best AUC iP\/R was 0.70; for interacting proteins, the best system achieved good macroaveraged recall (0.73) and interpolated area under the precision\/recall curve (0.58), after filtering incorrect species and mapping homonymous orthologs; for interacting protein pairs, the top (filtered, mapped) recall was 0.42 and AUC iP\/R was 0.29. Ensemble systems improved performance for the interacting protein task."}
{"_id":"f6edfb817cc8a9c8fb5b5a622bb0245522b99b25","title":"Requirements, Traceability and DSLs in Eclipse with the Requirements Interchange Format (ReqIF)","text":"Requirements engineering (RE) is a crucial aspect in systems development and is the area of ongoing research and process improvement. However, unlike in modelling, there has been no established standard that activities could converge on. In recent years, the emerging Requirements Interchange Format (RIF\/ReqIF) gained more and more visibility in industry, and research projects start to investigate these standards. To avoid redundant efforts in implementing the standard, the VERDE and Deploy projects cooperate to provide a stable common basis for RIF\/ReqIF that could be leveraged by other research projects too. In this paper, we present an Eclipse-based extensible implementation of a RIF\/ReqIF-based requirements editing platform. In addition, we are concerned with two related aspects of RE that take advantage of the common platform. First, how can the quality of requirements be improved by replacing or complementing natural language requirements with formal approaches such as domain specific languages or models. Second, how can we establish robust traceability that links requirements and model constructs and other artefacts of the development process. We present two approaches to traceability and two approaches to modelling. We believe that our research represents a significant contribution to the existing tooling landscape, as it is the first clean-room implementation of the RIF\/ReqIF standard. We believe that it will help reduce gaps in often heterogeneous tool chains and inspire new conceptual work and new tools."}
{"_id":"ea31dd9f633b2071906cdc3f147ded8d1963ef11","title":"FRPAs and High-Gain Directional Antennas","text":"Many types of GNSS antennas have been developed in recent years to make them suitable for different applications. Since their designs and performance requirements vary depending on their application, they have been grouped into six different categories in this book: 1. The design and performance of each type of antenna will be described here and throughout the rest of the book. This chapter discusses the design of the first two varieties: FRPAs and high-gain directional antennas. Figure 2.1 shows a representative sample of these antennas. FRPAs are the most popular and widely used of all GNSS antennas. There are many different types of these antennas; hence they will need two chapters\u2014this chapter and Chapter 3\u2014to fully cover all the important types. This chapter will discuss the following FRPA designs: microstrip patch antennas, which are the most ubiquitous of all GNSS antennas, and quadrifilar helix antennas (QHAs), which are also popular for handheld receivers and crossed \" drooping \" bow-type dipoles, which provide relatively wider bandwidths and good circular polarization. Spiral antennas such as the Archimedean spiral antenna will be discussed in Chapter 3 under multiband antennas since they are ultrawideband and can cover the entire GNSS band from 1.1 to 1.6 GHz. The conical spiral antenna is also a high-gain di"}
{"_id":"fdd02106537a89069b6993f4120abc54a0f7d1a5","title":"$V_{T}$ Compensation Circuit for AM OLED Displays Composed of Two TFTs and One Capacitor","text":"In this paper, we propose the threshold-voltage compensation pixel circuit that is composed of two thin-film transistors (TFTs) and one capacitor (2T1C). It not only compensates the deviation of the threshold voltage of the driver TFT but also actualizes the large aperture ratio for organic light-emitting diode (OLED) devices as well as the traditional 2T1C circuit. We show the result of SPICE simulation for the pixel circuit; it indicates that the circuit can allocate the relatively large aperture ratio for OLED devices"}
{"_id":"9ca61f6ede25100da67b5388c941bf69929a7528","title":"AGDISTIS - Graph-Based Disambiguation of Named Entities Using Linked Data","text":"Over the last decades, several billion Web pages have been made available on the Web. The ongoing transition from the current Web of unstructured data to the Web of Data yet requires scalable and accurate approaches for the extraction of structured data in RDF (Resource Description Framework) from these websites. One of the key steps towards extracting RDF from text is the disambiguation of named entities. While several approaches aim to tackle this problem, they still achieve poor accuracy. We address this drawback by presenting AGDISTIS, a novel knowledge-base-agnostic approach for named entity disambiguation. Our approach combines the Hypertext-Induced Topic Search (HITS) algorithm with label expansion strategies and string similarity measures. Based on this combination, AGDISTIS can efficiently detect the correct URIs for a given set of named entities within an input text. We evaluate our approach on eight different datasets against state-of-theart named entity disambiguation frameworks. Our results indicate that we outperform the state-of-the-art approach by up to 29% F-measure."}
{"_id":"3334a80676fefc486575bd2ddf1b281a640742f1","title":"Web Spam Taxonomy","text":"Web spamming refers to actions intended to mislead search engines into ranking some pages higher than they deserve. Recently, the amount of web spam has increased dramatically, leading to a degradation of search results. This paper presents a comprehensive taxonomy of current spamming techniques, which we believe can help in developing appropriate countermeasures."}
{"_id":"9d5f8b16c8efdcea3cf2d2ffe6970218283d6a2d","title":"Eye-tracking of men's preferences for waist-to-hip ratio and breast size of women.","text":"Studies of human physical traits and mate preferences often use questionnaires asking participants to rate the attractiveness of images. Female waist-to-hip ratio (WHR), breast size, and facial appearance have all been implicated in assessments by men of female attractiveness. However, very little is known about how men make fine-grained visual assessments of such images. We used eye-tracking techniques to measure the numbers of visual fixations, dwell times, and initial fixations made by men who viewed front-posed photographs of the same woman, computer-morphed so as to differ in her WHR (0.7 or 0.9) and breast size (small, medium, or large). Men also rated these images for attractiveness. Results showed that the initial visual fixation (occurring within 200 ms from the start of each 5 s test) involved either the breasts or the waist. Both these body areas received more first fixations than the face or the lower body (pubic area and legs). Men looked more often and for longer at the breasts, irrespective of the WHR of the images. However, men rated images with an hourglass shape and a slim waist (0.7 WHR) as most attractive, irrespective of breast size. These results provide quantitative data on eye movements that occur during male judgments of the attractiveness of female images, and indicate that assessments of the female hourglass figure probably occur very rapidly."}
{"_id":"af4dabbe0b886ea18a863ddc78ef16c9f15ea89d","title":"Static analysis of cable-driven manipulators with non-negligible cable mass","text":"This paper addresses the static analysis of cable-driven robotic manipulators with non-negligible cable mass. An approach to computing the static displacement of a homogeneous elastic cable is presented. The resulting cable-displacement expression is used to solve the inverse kinematics of general cable-driven robotic manipulators. In addition, the sag-induced stiffness of the cables is derived. Finally, two sample robotic manipulators with dimensions and system parameters similar to a large scale cable-driven manipulator currently under development are analyzed. The results show that cable sag can have a significant effect on both the inverse kinematics and stiffness of such manipulators."}
{"_id":"663b44145ffa2661f5d4a109df334beae3373952","title":"Criminal multilation of the human body in Sweden--a thirty-year medico-legal and forensic psychiatric study.","text":"During the 30-year period 1961-1990, a total of 22 deaths with criminal multilation\/dismemberment of the human body were registered in Sweden. The multilations occurred in time clusters, mostly during the summer and winter periods, and increased during the three decades, with incidence rates of 0.05, 0.1, and 0.125 per million inhabitants and year, respectively. Multilation was noted 6.6 times more often in large urban areas than in the rest of Sweden. Defensive mutilation, in order to get rid of the corpse or make its identity more difficult, was noted in ten instances, aggressive mutilation following outrageous overkilling in four, offensive mutilation (lust murder) in seven, and necromanic multilation in one instance. In the last-mentioned case the cause of death was natural, while all deaths in the first three groups were homicidal, or homicide was strongly suspected. All perpetrators were males, in six instances assisted by other persons. In more than half of the cases the perpetrator's occupation was associated with application of anatomical knowledge, e.g., butcher, physician, veterinary assistant, or hunter. The perpetrators of the defensive and aggressive mutilations were mostly disorganized, i.e., alcoholics or drug users with previous psychiatric contacts and criminal histories, while the lust murderers were mostly organized, with a history of violent crimes (including the \"serial killing\" type), drug abuse and mental disorders with anxiety and schizophrenia, in that order to a diminishing degree. There were differences in mode of mutilation, depending on whether the mutilation was carried out by a layman, a butcher, or a physician. In only one case was the perpetrator convicted for the mutilation act itself; in the remaining instances the manslaughter, as a more serious crime, assimilated the mutilation. When the mutilation made it impossible to establish the cause of death, the perpetrators, despite strong circumstantial evidence indicating murder, were acquitted."}
{"_id":"ff7e2fc52f65aa45dc667b7c6abae70a3eb1fbe0","title":"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric","text":"While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations."}
{"_id":"15c61fdb1eb2f2f1e78ac11638342b66da71c858","title":"Leaving so soon?: understanding and predicting web search abandonment rationales","text":"Users of search engines often abandon their searches. Despite the high frequency of Web search abandonment and its importance to Web search engines, little is known about why searchers abandon beyond that it can be for good or bad reasons. In this paper, we ex-tend previous work by studying search abandonment using both a retrospective survey and an in-situ method that captures aban-donment rationales at abandonment time. We show that although satisfaction is a common motivator for abandonment, one-in-five abandonment instances does not relate to satisfaction. We also studied the automatic prediction of the underlying reason for ob-served abandonment. We used features of the query and the results, interaction with the result page (e.g., cursor movements, scrolling, clicks), and the full search session. We show that our classifiers can learn to accurately predict the reasons for observed search abandonment. Such accurate predictions help search providers estimate user satisfaction for queries without clicks, affording a more complete understanding of search engine performance."}
{"_id":"0d1c553a6716f6a634a8e826ca8dd3bbe16b00c0","title":"A state-of-the-art 3D sensor for robot navigation","text":"This paper relates first experiences using a state-of-the-art, time-of-flight sensor that is able to deliver 3D images. The properties and capabilities of the sensor make it a potential powerful tool for applications within mobile robotics especially for real-time tasks, as the sensor features a frame rate of up to 30 frames per second. Its capabilities in terms of basic obstacle avoidance and local path-planning are evaluated and compared to the performance of a standard laser scanner."}
{"_id":"ec846fbc1d8046afe67d630ca172fa0ea42fdb9a","title":"IEC 61850 impact on substation design","text":"IEC 61850 communication networks and systems for utility automation is a standard for communications that creates an environment that will allow dramatic changes in the design and operation of substations. The paper describes the different components of such systems and the distribution of functions between the devices included in it. Multiple Merging Units communicating over a 100 Mb Ethernet with multiple devices and\/or a central computer that receives and processes current and voltage samples with a rate of 256 samples\/cycle allow the implementation of all typical substation protection, automation, control, monitoring and recording functions in a very efficient way. The concepts also allow to change the design of layout of the high voltage switchgear."}
{"_id":"537afb5047cd33b9f9a3aa2dcc47c6de7f1618b4","title":"OPC Unified Architecture: A Service-oriented Architecture for Smart Grids","text":"In this paper, the OPC UA is introduced as a key technology for realizing a variety of Smart Grid use cases enabling relevant tasks of automation and control. OPC UA is the successor of the established Classic OPC specification and state of the art regarding information exchange in the industrial automation branch. One of its key improvements over the Classic OPC is that the area of application is no longer restricted to industrial automation but OPC UA can be applied almost in every domain facing challenges in automated control. This improvement stems from a more generic and object-oriented approach. For the adoption of OPC UA to Smart Grids, two of the most important data models -- the Common Information Model (CIM) and the IEC 61850 -- have been identified to be integrated into OPC UA communication. In this contribution, basic OPC UA features and functionalities (information modeling, communication services, and information security) are introduced and discussed in the context of Smart Grids."}
{"_id":"828f95aff57eb9bd2cea168364827cb0384caf9e","title":"The social psychology of stigma.","text":"This chapter addresses the psychological effects of social stigma. Stigma directly affects the stigmatized via mechanisms of discrimination, expectancy confirmation, and automatic stereotype activation, and indirectly via threats to personal and social identity. We review and organize recent theory and empirical research within an identity threat model of stigma. This model posits that situational cues, collective representations of one's stigma status, and personal beliefs and motives shape appraisals of the significance of stigma-relevant situations for well-being. Identity threat results when stigma-relevant stressors are appraised as potentially harmful to one's social identity and as exceeding one's coping resources. Identity threat creates involuntary stress responses and motivates attempts at threat reduction through coping strategies. Stress responses and coping efforts affect important outcomes such as self-esteem, academic achievement, and health. Identity threat perspectives help to explain the tremendous variability across people, groups, and situations in responses to stigma."}
{"_id":"f732290a4a2a77ba5070ec3383df679bfc13f713","title":"Gait support for complete spinal cord injury patient by synchronized leg-swing with HAL","text":"Biped walking improves the circulation of blood as well as bone density of the lower limbs, thereby enhancing the quality of life (QOL). It is significant not only to healthy people but also to physically challenged persons such as complete spinal cord injury (SCI) patients. The purpose of this paper is to propose an estimation algorithm that infers the intention related to the forward leg-swing in order to support the gait for complete SCI patients wearing an exoskeleton system called a Hybrid Assistive Limb (HAL), and to verify the effectiveness of the proposed algorithm through a clinical trial. The proposed algorithm infers the patient's intention in synchronization with the deviation of the center of the ground reaction force (CoGRF) that is observed immediately before a person starts walking. The patient conveys this intention by inducing the deviation of the CoGRF, using crutches or handrails with both of his\/her arms. In the clinical trial, we confirmed that the algorithm inferred the patient's intention to swing the leg forward, and achieved a smooth gait in synchronization with it. As a result, the gait speed and cadence of the SCI patient with HAL during the 10-meter walking test increased to 6.67 [m\/min] and 20 [steps\/min], respectively after several trials."}
{"_id":"e4e73bad851bff528bb84750f293966b9a7113d4","title":"A Web Scraping Methodology for Bypassing Twitter API Restrictions","text":"Retrieving information from social networks is the first and primordial step many data analysis fields such as Natural Language Processing, Sentiment Analysis and Machine Learning. Important data science tasks relay on historical data gathering for further predictive results. Most of the recent works use Twitter API, a public platform for collecting public streams of information, which allows querying chronological tweets for no more than three weeks old. In this paper, we present a new methodology for collecting historical tweets within any date range using web scraping techniques bypassing for Twitter API restrictions."}
{"_id":"ceb6bf1f066ed67a3bd1175b0e9ea60615fcc7a5","title":"Learning to Collaborate for Question Answering and Asking","text":"Question answering (QA) and question generation (QG) are closely related tasks that could improve each other; however, the connection of these two tasks is not well explored in literature. In this paper, we give a systematic study that seeks to leverage the connection to improve both QA and QG. We present a training algorithm that generalizes both Generative Adversarial Network (GAN) and Generative Domain-Adaptive Nets (GDAN) under the question answering scenario. The two key ideas are improving the QG model with QA through incorporating additional QA-specific signal as the loss function, and improving the QA model with QG through adding artificially generated training instances. We conduct experiments on both document based and knowledge based question answering tasks. We have two main findings. Firstly, the performance of a QG model (e.g in terms of BLEU score) could be easily improved by a QA model via policy gradient. Secondly, directly applying GAN that regards all the generated questions as negative instances could not improve the accuracy of the QA model. Learning when to regard generated questions as positive instances could bring performance boost."}
{"_id":"db8196da3890be9b95307dc67833938a98eedde8","title":"1 Pavement distress detection methods : a review 2","text":"The road pavement condition affects safety and comfort, traffic and travel times, vehicles 8 operating cost and emission levels. In order to optimize the road pavement management and 9 guarantee satisfactory mobility conditions for all the road users, the Pavement Management 10 System (PMS) is an effective tool for the road manager. An effective PMS requires the availability of 11 pavement distress data, the possibility of data maintenance and updating, in order to evaluate the 12 best maintenance program. In the last decade, many researches have been focused on pavement 13 distress detection, using a huge variety of technological solutions for both data collection and 14 information extraction and qualification. This paper presents a literature review of data collection 15 systems and processing approach aimed at the pavement condition evaluation. Both commercial 16 solutions and research approaches have been included. The main goal is to draw a framework of 17 the actual existing solutions, considering them from a different point of view in order to identify 18 the most suitable for further research and technical improvement, also considering the automated 19 and semi-automated emerging technologies. An important attempt is to evaluate the aptness of the 20 data collection and extraction to the type of distress, considering the distress detection, 21 classification and quantification phases of the procedure. 22"}
{"_id":"059eb34d95c73e37dca8e35b0ac5a2fb0142f3ee","title":"The Stratosphere platform for big data analytics","text":"We present Stratosphere, an open-source software stack for parallel data analysis. Stratosphere brings together a unique set of features that allow the expressive, easy, and efficient programming of analytical applications at very large scale. Stratosphere\u2019s features include \u201cin situ\u201d data processing, a declarative query language, treatment of user-defined functions as first-class citizens, automatic program parallelization and optimization, support for iterative programs, and a scalable and efficient execution engine. Stratosphere covers a variety of \u201cBig Data\u201d use cases, such as data warehousing, information extraction and integration, data cleansing, graph analysis, and statistical analysis applications. In this paper, we present the overall system architecture design decisions, introduce Stratosphere through example queries, and then dive into the internal workings of the system\u2019s components that relate to extensibility, programming model, optimization, and query execution. We experimentally compare Stratosphere against popular open-source alternatives, and we conclude with a research outlook for the next years."}
{"_id":"f9b291ca6b44937956086cec37189e79d44c88ea","title":"Modeling urban dynamics through GIS-based cellular automata","text":"In urban systems modeling, there are many elaborate dynamic models based on intricate decision processes whose simulation must be based on customized software if their space\u00b1time properties are to be explored e\u0080ectively. In this paper we present a class of urban models whose dynamics are based on theories of development associated with cellular automata (CA), whose data is \u00aene-grained, and whose simulation requires software which can handle an enormous array of spatial and temporal model outputs. We \u00aerst introduce the generic problem of modeling within GIS, noting relevant CA models before outlining a generalized model based on Xie's (1996, A general model for cellular urban dynamics. Geographical Analysis, 28, 350\u00b1373) ``dynamic urban evolutionary modeling'' (DUEM) approach. We present ways in which land uses are structured through their life cycles, and ways in which existing urban activities spawn locations for new activities. We de\u00aene various decision rules that embed distance and direction, density thresholds, and transition or mutation probabilities into the model's dynamics, and we then outline the software designed to generate e\u0080ective urban simulations consistent with GIS data inputs, outputs and related functionality. Finally, we present a range of hypothetical urban simulations that illustrate the diversity of model types that can be handled within the framework as a prelude to more realistic applications which will be reported in later papers. # 1999 Published by Elsevier Science Ltd. All"}
{"_id":"3969e582e68e418a2b79c604cd35d5d81de9b35d","title":"Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology","text":"Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions for these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited high convergent, discriminant, and factorial validity. Perceived usefulness was significantly correlated with both selfreported current usage (r=.63, Study 1) and self-predicted future usage (r =.85, Study 2). Perceived ease of use was also significantly correlated with current usage (r= .45, Study 1) and future usage (r =.59, Study 2). In both studies, usefulness had a significantly greater correlation with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecedent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance."}
{"_id":"b418dfe5bbc09656adf2b4738276d45dd787ab8c","title":"Challenges for the Self-Safety in Autonomous Vehicles","text":"The combination of multiple functions having different and complementary capabilities enables the emergence of Autonomous Vehicles. Their deployment is limited by the level of complexity they represent together with the challenges encountered in real environments with strong safety concerns. Thus a major concern prior to massive deployment is on how to ensure the safety of autonomous vehicles despite likely internal (e.g. malfunctions) and external (e.g., aggressive behaviors) disturbances they might undergo. This paper presents the challenges that undergoes the design and development of autonomous vehicles with respect to their functional architecture and adaptive behaviors from a safety perspective. For the purpose of the rationales, we define needs and requirements that lead to the formulation of an architectural framework. Our approach is based on paradigms and technologies from non-automotive domains to address non-functional system properties like safety, reliability and security. The notion of micro-services is also introduced for the self-safety of autonomous vehicles. These are part of the proposed framework that should facilitate the analysis, design, development and validation for the adequate composition and orchestration of services aimed to warrant the required non-functional properties, such as safety. In the present paper, we introduce the structural and behavioral adaptations of the framework to offer a holistic and scalable vision of the safety over the system."}
{"_id":"37fd2fc9ae5baebe2f7ddb5456bc68f993d7bd66","title":"Error-Related EEG Potentials Generated During Simulated Brain\u2013Computer Interaction","text":"Brain-computer interfaces (BCIs) are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the electroencephalogram (EEG) recorded right after the occurrence of an error. Several studies show the presence of ErrP in typical choice reaction tasks. However, in the context of a BCI, the central question is: ldquoAre ErrP also elicited when the error is made by the interface during the recognition of the subject's intent?rdquo We have thus explored whether ErrP also follow a feedback indicating incorrect responses of the simulated BCI interface. Five healthy volunteer subjects participated in a new human-robot interaction experiment, which seem to confirm the previously reported presence of a new kind of ErrP. However, in order to exploit these ErrP, we need to detect them in each single trial using a short window following the feedback associated to the response of the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 83.5% and 79.2%, respectively, using a classifier built with data recorded up to three months earlier."}
{"_id":"714ed82f4f3d38677f823cafb1f037b10d32cc3d","title":"ERP components on reaction errors and their functional significance: a tutorial","text":"Some years ago we described a negative (Ne) and a later positive (Pe) deflection in the event-related brain potentials (ERPs) of incorrect choice reactions [Falkenstein, M., Hohnsbein, J., Hoormann, J., Blanke, L., 1990. In: Brunia, C.H.M., Gaillard, A.W.K., Kok, A. (Eds.), Psychophysiological Brain Research. Tilburg Univesity Press, Tilburg, pp. 192-195. Falkenstein, M., Hohnsbein, J., Hoormann, J., 1991. Electroencephalography and Clinical Neurophysiology, 78, 447-455]. Originally we assumed the Ne to represent a correlate of error detection in the sense of a mismatch signal when representations of the actual response and the required response are compared. This hypothesis was supported by the results of a variety of experiments from our own laboratory and that of Coles [Gehring, W. J., Goss, B., Coles, M.G.H., Meyer, D.E., Donchin, E., 1993. Psychological Science 4, 385-390. Bernstein, P.S., Scheffers, M.K., Coles, M.G.H., 1995. Journal of Experimental Psychology: Human Perception and Performance 21, 1312-1322. Scheffers, M.K., Coles, M. G.H., Bernstein, P., Gehring, W.J., Donchin, E., 1996. Psychophysiology 33, 42-54]. However, new data from our laboratory and that of Vidal et al. [Vidal, F., Hasbroucq, T., Bonnet, M., 1999. Biological Psychology, 2000] revealed a small negativity similar to the Ne also after correct responses. Since the above mentioned comparison process is also required after correct responses it is conceivable that the Ne reflects this comparison process itself rather than its outcome. As to the Pe, our results suggest that this is a further error-specific component, which is independent of the Ne, and hence associated with a later aspect of error processing or post-error processing. Our new results with different age groups argue against the hypotheses that the Pe reflects conscious error processing or the post-error adjustment of response strategies. Further research is necessary to specify the functional significance of the Pe."}
{"_id":"f2c4082faeff5d63b0144ef371c8964621ee33bf","title":"Clinical Applications of Brain-Computer Interfaces: Current State and Future Prospects","text":"Braincomputer interfaces (BCIs) allow their users to communicate or control external devices using brain signals rather than the brain's normal output pathways of peripheral nerves and muscles. Motivated by the hope of restoring independence to severely disabled individuals and by interest in further extending human control of external systems, researchers from many fields are engaged in this challenging new work. BCI research and development has grown explosively over the past two decades. Efforts have begun recently to provide laboratory-validated BCI systems to severely disabled individuals for real-world applications. In this paper, we discuss the current status and future prospects of BCI technology and its clinical applications. We will define BCI, review the BCI-relevant signals from the human brain, and describe the functional components of BCIs. We will also review current clinical applications of BCI technology and identify potential users and potential applications. Lastly, we will discuss current limitations of BCI technology, impediments to its widespread clinical use, and expectations for the future."}
{"_id":"13a8c7c30ca67cb88b4c5b78d3109d1c5a0f6025","title":"Anterior cingulate cortex, error detection, and the online monitoring of performance.","text":"An unresolved question in neuroscience and psychology is how the brain monitors performance to regulate behavior. It has been proposed that the anterior cingulate cortex (ACC), on the medial surface of the frontal lobe, contributes to performance monitoring by detecting errors. In this study, event-related functional magnetic resonance imaging was used to examine ACC function. Results confirm that this region shows activity during erroneous responses. However, activity was also observed in the same region during correct responses under conditions of increased response competition. This suggests that the ACC detects conditions under which errors are likely to occur rather than errors themselves."}
{"_id":"9b1b350dc58def7b7d7b147b779aa0b534b5b335","title":"QR code location based reverse car-searching route recommendation model","text":"To deal with the reverse car-searching issue in large buildings and parking lots, a Quick Response Code (QR code) based reverse car-searching route recommendation model is designed. By scanning the deployed QR codes, a Smartphone can pinpoint the host location and parking location efficiently. Based on the submitted location information, the central control system can finally return the recommended routes, which facilitates a host to reach the parking location effectively. In our model, the reverse car-searching route is divided into two parts: choosing the optimal exports (elevator) and computing the shortest walking distance route. Based on the optimal export selection algorithm and regional shortest path algorithm, our model can choose the prior exports (elevator) effectively, and then recommend the optimal walking route in the buildings and parking lots. The simulation shows that this low-cost system can effectively solve the reverse car-searching problem in large buildings and parking lots, save the driver's car-searching time and improve the utilization rate of parking facilities."}
{"_id":"e5ae82aa115f19e84a428e8377fac46aa0b3148e","title":"Ant supervised by PSO and 2-Opt algorithm, AS-PSO-2Opt, applied to Traveling Salesman Problem","text":"AS-PSO-2Opt is a new enhancement of the AS-PSO method. In the classical AS-PSO, the Ant heuristic is used to optimize the tour length of a Traveling Salesman Problem, TSP, and PSO is applied to optimize three parameters of ACO, (\u03b1, \u03b2, \u03c1). The AS-PSO-2Opt consider a post processing resuming path redundancy, helping to improve local solutions and to decrease the probability of falling in local minimum. Applied to TSP, the method allowed retrieving a valuable path solution and a set of fitted parameters for ACO. The performance of the AS-PSO-2Opt is tested on nine different TSP test benches. Experimental results based on a statistical analysis showed that the new proposal performs better than key state of art methods using Genetic algorithm, Neural Network and ACO algorithm. The AS-PSO-2Opt performs better than close related methods such as PSO-ACO-3Opt [9] and ACO with ABC [19] for various test benches."}
{"_id":"9d5de7d9c54f3c815350bcd00847ea25e34341cc","title":"Using association rules for ontology extraction from a Quran corpus","text":"Ontology can be seen as a formal representation of knowledge. They have been investigated in many arti ficial intelligence studies including semantic web, softwa re engineering, and information retrieval. The aim of ontology is t o develop knowledge representations that can be shared and re used. This research project is concerned with the use of assoc iati n rules to extract the Quran ontology. The manual acquisition of ontologies from Quran verses can be very costly; therefore, we ne d an intelligent system for Quran ontology construction using patternbased schemes and associations rules to discover Qu ran concepts and semantics relations from Quran verses. Our system i s based on the combination of statistics and linguistics methods t o extract concepts and conceptual relations from Quran. In particular, linguistic pattern-based approach is exploited to extract spec ific concepts from the Quran, while the conceptual relations are found based on association rules technique. The Quran ontology wil l offer a new and powerful representation of Quran knowledge, and the association rules will help to represent the relations between all classes of connected concepts in the Quran ontology. Keywords\u2014 Ontology; Concepts Extraction ; Quran; Association Rules; Text Mining."}
{"_id":"1d9bd24e65345258259ee24332141e371c6e4868","title":"Learning Image Descriptors with Boosting","text":"We propose a novel and general framework to learn compact but highly discriminative floating-point and binary local feature descriptors. By leveraging the boosting-trick we first show how to efficiently train a compact floating-point descriptor that is very robust to illumination and viewpoint changes. We then present the main contribution of this paper\u2014a binary extension of the framework that demonstrates the real advantage of our approach and allows us to compress the descriptor even further. Each bit of the resulting binary descriptor, which we call BinBoost, is computed with a boosted binary hash function, and we show how to efficiently optimize the hash functions so that they are complementary, which is key to compactness and robustness. As we do not put any constraints on the weak learner configuration underlying each hash function, our general framework allows us to optimize the sampling patterns of recently proposed hand-crafted descriptors and significantly improve their performance. Moreover, our boosting scheme can easily adapt to new applications and generalize to other types of image data, such as faces, while providing state-of-the-art results at a fraction of the matching time and memory footprint."}
{"_id":"27464a00725c941d7ec910e4a7b5240fc2d299d7","title":"SMART VOTING SYSTEM WITH FACE RECOGNITION","text":"Manual voting system has been deployed for many yea rs in our country. However in many parts of our cou ntry people cannot attend the voting because of several r asons. In order to solve these problems there is a need of online election voting system in addition to manual voting system. The long-term goal of this project is to g reatly reduce the cost and complexity of running elections and increase th e accuracy of results by removing the direct involv ement of humans in gathering and counting of votes. The voters will be a le to give their votes at any field areas by usi ng the system if they prefer online voting In this system, the user votes using android applic ation which can be downloaded over internet. The us er has to login before giving his vote. Authentication will b e done using SMS confirmation and face recognition. Users can view parties, enter votes, can verify vote via SMS or email and can view result after result is declared."}
{"_id":"7f1a80fd27f0e09fdb9982ec345745f9959892f5","title":"The Nature, Importance, and Difficulty of Machine Ethics","text":"The question of whether machine ethics exists or might exist in the future is difficult to answer if we can't agree on what counts as machine ethics. Some might argue that machine ethics obviously exists because humans are machines and humans have ethics. Others could argue that machine ethics obviously doesn't exist because ethics is simply emotional expression and machines can't have emotions. A wide range of positions on machine ethics are possible, and a discussion of the issue could rapidly propel us into deep and unsettled philosophical issues. Perhaps, understandably, few in the scientific arena pursue the issue of machine ethics. As we expand computers' decision-making roles in practical matters, such as computers driving cars, ethical considerations are inevitable. Computer scientists and engineers must examine the possibilities for machine ethics because, knowingly or not, they've already engaged in some form of it. Before we can discuss possible implementations of machine ethics, however, we need to be clear about what we're asserting or denying"}
{"_id":"a12cd3d9ae5530a90302a6e4af477e6e24fa0f95","title":"Integrating Conflicting Data: The Role of Source Dependence","text":"Many data management applications, such as setting up Web portals, managing enterprise data, managing community data, and sharing scientific data, require integrating data from multiple sources. Each of these sources provides a set of values and different sourc es can often provide conflicting values. To present quality data to users, it is critical that data integration systems can resolve conflicts and discover true values. Typically, we expect a true value to be provided by more sources than any particular false one, so we can take the value provided by the majority of the sources as the truth. Unfortunately, a false value can be spread through copying and that makes truth discovery extremely tricky. In this paper, we consider how to find true values from conflicting information when there are a large number of sources, among which some may copy from others. We present a novel approach that considers pendencebetween data sources in truth discovery. Intuitively, if two data sources provide a large number of common values and many of these values are rarely provided by other sources ( e.g., particular false values), it is very likely that one copies from the other. We apply Bayesian analysis to decide dependence between sources and design an algorithm that iteratively detects dependence and discovers truth from conflicting information. We also extend our model by considering accuracyof data sources and similarity between values. Our experiments on synthetic data as well as real-world data show that our algorithm can significantly improve accuracy of truth discovery and is scalable when there are a large number of data sources."}
{"_id":"394a3253849bacf24a8e1d98376540b8c9055b6c","title":"Teenager \u2019 s Addiction to Smart Phones and Its Integrated Therapy Method","text":"The fundamental cause of teenagers\u2019 addiction to smart phone is due to stress arising from family and school. Smartphone can cause many problems including musculoskeletal symptom which can develop due to staying in one position for a long time. Furthermore, starring at a small screen for an extended time can cause decreased eye sight, along with amblyopia. Smart phone addiction causes depression and anxiety which will lead to severe mental problems. This will lead to problems associated to one\u2019s social life and human network. Fundamental treatment of cognitive and emotional elements needed to become harmonized as a whole in order to increase the effectiveness of the treatment results. The therapy has been verified to cure the behavioral and emotional aspect related to depression, sense of control, and assimilation in school. From these integrated treatment approaches, this report compared the treatment\u2019s upsides and downsides and also, assessed the possibility the integrated treatment process. We can presume that this report provided the opportunity and support for the creation of a fundamental treatment method that can help teenagers live a healthy life by overcoming addiction to smart phones."}
{"_id":"2645a136bb1f0af81a526f04a1c9eb2b28dccb1b","title":"Communication-efficient algorithms for statistical optimization","text":"We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as O(N-1 + (N\/m)-2). Whenever m \u2264 \u221aN, this guarantee matches the best possible rate achievable by a centralized algorithm with access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap. Requiring only a single round of communication, it has mean-squared error that decays as O(N-1 + (N\/m)-3), and so is more robust to the amount of parallelization."}
{"_id":"29b756ebab9a00e65dcc8e0cc77544dbec5d4531","title":"Reinforcement Learning of Coordination in Cooperative Multi-Agent Systems","text":"We report on an investigation of reinforcement learning techniques for the learning of coordination in cooperative multi-agent systems. Specifically, we focus on a novel action selection strategy for Q-learning (Watkins 1989). The new technique is applicable to scenarios where mutual observation of actions is not possible.To date, reinforcement learning approaches for such <i>independent <\/i> agents did not guarantee convergence to the optimal joint action in scenarios with high miscoordination costs. We improve on previous results (Claus & Boutilier 1998) by demonstrating empirically that our extension causes the agents to converge almost always to the optimal joint action even in these difficult cases."}
{"_id":"641549e1867d66515002ccfe218149af6e38ddcc","title":"Characterization of log periodic planar dipole array antenna","text":"This paper proposes a analysis approach for characterizing the broadband Log Periodic Planar Dipole Array (LPPDA) antenna useful for C-band applications. A simplest geometry of the edge fed Log Periodic Planar Dipole Array is designed and developed. Parameters like length, width of the elements, and spacing between elements are the varied using defined scale factor (\u03c4) and a spacing factor (\u03c3), keeping others constant and their impact was noted down. A parametric analysis is performed. using CST software. A transmission line equivalent circuit of the proposed LPPDA is constructed. Log periodicity behaviour of the proposed antenna is checked by plotting input impedance with respect to logarithm of frequency. An antenna is fabricated and measured in laboratory."}
{"_id":"35b79fa3cc3dad9defdb8cf5aeab5da57413530b","title":"Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus","text":"Bigrams and trigrams are commonly used in statistical natural language processing; this paper will describe techniques for working with much longer n-grams. Suffix arrays (Manber and Myers 1990) were first introduced to compute the frequency and location of a substring (n-gram) in a sequence (corpus) of length N. To compute frequencies over all N(N+1)\/2 substrings in a corpus, the substrings are grouped into a manageable number of equivalence classes. In this way, a prohibitive computation over substrings is reduced to a manageable computation over classes. This paper presents both the algorithms and the code that were used to compute term frequency (tf) and document frequency (df) for all n-grams in two large corpora, an English corpus of 50 million words of Wall Street Journal and a Japanese corpus of 216 million characters of Mainichi Shimbun. The second half of the paper uses these frequencies to find interesting substrings. Lexicographers have been interested in n-grams with high mutual information (MI) where the joint term frequency is higher than what would be expected by chance, assuming that the parts of the n-gram combine independently. Residual inverse document frequency (RIDF) compares document frequency to another model of chance where terms with a particular term frequency are distributed randomly throughout the collection. MI tends to pick out phrases with noncompositional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (which tend to exhibit nonrandom distributions over documents). The combination of both MI and RIDF is better than either by itself in a Japanese word extraction task."}
{"_id":"519ca7401f2346d8e31916b17b0466f5f77b7ad6","title":"QDFS: A quality-aware distributed file storage service based on HDFS","text":"On the basis of the Hadoop distributed file system (HDFS), this paper presents the design and implementation of QDFS, a distributed file storage service system that employs a backup policy based on recovery volumes and a quality-aware data distribution strategy. The experimental results show that QDFS increases the storage utilization ratio and improves the storage performance by being aware of the quality of service of the DataNodes. The improvements of QDFS compared with HDFS make the Hadoop distributed computing model more suitable to unstable wide area network environments."}
{"_id":"289daa2781140eb67e4e7edd548213569c5235c7","title":"Effects of psychological and social factors on organic disease: a critical assessment of research on coronary heart disease.","text":"An extensive research literature in the behavioral sciences and medicine suggests that psychological and social factors may play a direct role in organic coronary artery disease (CAD) pathology. However, many in the medical and scientific community regard this evidence with skepticism. This chapter critically examines research on the impact of psychological and psychosocial factors on the development and outcome of coronary heart disease, with particular emphasis on studies employing verifiable outcomes of CAD morbidity or mortality. Five key variables identified as possible psychosocial risk factors for CAD are addressed: acute and chronic stress, hostility, depression, social support, and socioeconomic status. Evidence regarding the efficacy of psychosocial interventions is also presented. It is suggested that, taken as a whole, evidence for a psychological and social impact on CAD morbidity and mortality is convincing. However, continued progress in this area requires multidisciplinary research integrating expertise in cardiology and the behavioral sciences, and more effective efforts to communicate research findings to a biomedical audience."}
{"_id":"0f2069063f35553917aca882cddfb6cd6e361c3f","title":"Learning Theory and Instructional Design","text":"Designing effective instruction goes beyond systematically executing various steps within an instructional design model. Among a host of considerations, effective instructional design should take into consideration the theoretical bases in which it is grounded. This is not to say that learning theory offers instructional designers answers to design problems but instead, offers clarity, direction and focus throughout the instructional design process. Merrill (2001, p. 294) explains that a \u201ctheoretical tool, in and of itself, is not an instructional design theory but defines instructional components that can be used to define instructional prescriptions more precisely.\u201d Likewise, Merriam and Caffarella (1999, p. 250) make the point that \u201c[learning] theories do not give us solutions, but they do direct our attention to those variables that are crucial in finding solutions.\u201d Thus, understanding theoretical frameworks and properly incorporating them within the scope of instructional design is important for designers to effectively prepare and present instruction as well as for organizational entities to more precisely and efficiently address training-appropriate issues."}
{"_id":"4da2d4a57d41e53eb3ea9e841b7c007820c6d8ff","title":"Oil content, tocopherol composition and fatty acid patterns of the seeds of 51 Cannabis sativa L. genotypes","text":"The oil content, the tocopherol composition, the plastochromanol-8 (P-8) content and the fatty acid composition (19 fatty acids) of the seed of 51 hemp (Cannabis sativa L.) genotypes were studied in the 2000 and 2001 seasons. The oil content of the hemp seed ranged from 26.25% (w\/w) to 37.50%. Analysis of variance revealed significant effects of genotype, year and of the interaction (genotype \u00d7 year) on the oil content. The oil contents of the 51 genotypes in 2000 and 2001 were correlated (r = 0.37**) and averaged 33.19 \u00b1 1.45% in 2000 and 31.21 \u00b1 0.96% in 2001. The \u03b3-tocopherol, \u03b1-tocopherol, \u03b4-tocopherol, P-8- and \u03b2-tocopherol contents of the 51 genotypes averaged 21.68 \u00b1 3.19, 1.82 \u00b1 0.49, 1.20 \u00b1 0.40, 0.18 \u00b1 0.07 and 0.16 \u00b1 0.04 mg 100g\u22121 of seeds, respectively (2000 and 2001 data pooled). Hierarchical clustering of the fatty acid data did not group the hemp genotypes according to their geographic origin. The \u03b3-linolenic acid yield of hemp (3\u201330 kg ha\u22121) was similar to the \u03b3-linolenic acid yield of plant species that are currently used as sources of \u03b3-linolenic acid (borage (19\u201330 kg ha\u22121), evening primrose (7\u201330 kg ha\u22121)). The linoleic acid yield of hemp (129\u2013326 kg ha\u22121) was similar to flax (102\u2013250 kg ha\u22121), but less than in sunflower (868\u20131320 kg ha\u22121). Significant positive correlations were detected between some fatty acids and some tocopherols. Even though the average content of P-8 in hemp seeds was only 1\/120th of the average \u03b3-tocopherol content, P-8 content was more closely correlated with the unsaturated fatty acid content than \u03b3-tocopherol or any other tocopherol fraction. The average broad-sense heritabilities of the oil content, the antioxidants (tocopherols and P-8) and the fatty acids were 0.53, 0.14 and 0.23, respectively. The genotypes Fibrimon 56, P57, Juso 31, GB29, Beniko, P60, FxT, F\u00e9lina 34, Ramo and GB18 were capable of producing the largest amounts of high quality hemp oil."}
{"_id":"b452a829d69fb1e265cf9277ff669bbc2fa8859b","title":"Learning to Remember Translation History with a Continuous Cache","text":"Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost."}
{"_id":"3074acc23b66b9c2a53321a0b2dc4b7f9673890d","title":"A fast method of fog and haze removal","text":"Fog and haze degrade the quality of preview and captured image by reducing the contrast and saturation. As a result the visibility of scene or object degrades. The objective of the present work is to enhance the visibility, saturation, contrast and reduce the noise in a foggy image. We propose a method that uses single frame for enhancing foggy images using multilevel transmission map. The method is fast and free from noise or artifacts that generally arise in such enhancement techniques. A comparison with existing methods shows that the proposed method performs better in terms of both processing time and quality. The proposed method works in real time for VGA resolution. The proposed work also presents a scheme to remove fog, rain and snow in real-time."}
{"_id":"9e538e389d7805d297f66be47cfae47796ef9123","title":"Snapshots in a flash with ioSnap","text":"Snapshots are a common and heavily relied upon feature in storage systems. The high performance of flash-based storage systems brings new, more stringent, requirements for this classic capability. We present ioSnap, a flash optimized snapshot system. Through careful design exploiting common snapshot usage patterns and flash oriented optimizations, including leveraging native characteristics of Flash Translation Layers, ioSnap delivers low-overhead snapshots with minimal disruption to foreground traffic. Through our evaluation, we show that ioSnap incurs negligible performance overhead during normal operation, and that common-case operations such as snapshot creation and deletion incur little cost. We also demonstrate techniques to mitigate the performance impact on foreground I\/O during intensive snapshot operations such as activation. Overall, ioSnap represents a case study of how to integrate snapshots into a modern, well-engineered flash-based storage system."}
{"_id":"1125e6304da5bb15746807aad75d5208267070b2","title":"Analysis of back-end flash in a 1.5b\/stage pipelined ADC","text":"An analysis of the impact of last stage flash in a conventional pipeline ADC is performed in this paper. The performance of a pipeline ADC can be altered significantly by calibrating the comparators in the back-end flash. Also, realizing that the input to the back-end flash (in a pipeline ADC) is not uniformly distributed, this paper proposes alternative back-end flash references to improve the overall performance of the ADC. An analysis of the performance of the pipeline with large offsets in the MDAC stages is also presented in this paper."}
{"_id":"674221c1fbd95e2505ca0921e16d00bb81c81c0e","title":"A Preliminary Taxonomy of Crowdsourcing","text":"Many firms are now asking how they can benefit from the new form of outsourcing labelled \u201ccrowdsourcing\u201d. Like many other forms of outsourcing, crowdsourcing is now being \u201ctalked up\u201d by a somewhat credulous trade press. However, the term crowdsourcing has been used to describe several related, but different phenomena, and what might be successful with one form of crowdsourcing may not be with another. In this paper the notion of crowdsourcing is decomposed to create a taxonomy that expands our understanding of what is meant by the term. This taxonomy focuses on the different capability levels of crowdsourcing suppliers; different motivations; and different allocation of benefits. The management implications of these distinctions are then considered in light of what we know about other forms of outsourcing."}
{"_id":"7862f646d640cbf9f88e5ba94a7d642e2a552ec9","title":"Being John Malkovich","text":"Given a photo of person A, we seek a photo of person B with similar pose and expression. Solving this problem enables a form of puppetry, in which one person appears to control the face of another. When deployed on a webcam-equipped computer, our approach enables a user to control another person\u2019s face in real-time. This image-retrievalinspired approach employs a fully-automated pipeline of face analysis techniques, and is extremely general\u2014we can puppet anyone directly from their photo collection or videos in which they appear. We show several examples using images and videos of celebrities from the Internet."}
{"_id":"1e08547a5eb97c4d1f29b14de0703e40f8e4fd67","title":"Common-Duty-Ratio Control of Input-Parallel Output-Parallel (IPOP) Connected DC\u2013DC Converter Modules With Automatic Sharing of Currents","text":"Input-parallel output-parallel (IPOP) connected converter systems allow the use of low-power converter modules for high-power applications, with interleaving control scheme resulting in smaller filter size, better dynamic performance, and higher power density. In this paper, a new IPOP converter system is proposed, which consists of multiple dual-active half-bridge (DAHB) dc-dc converter modules. Moreover, by applying a common-duty-ratio control scheme, without a dedicated current-sharing controller, the automatic sharing of input currents or load currents is achieved in the IPOP converter even in the presence of substantial differences of 10% in various module parameters. The current-sharing performance of the proposed control method is analyzed using both a small-signal model and a steady-state dc model of the IPOP system. It is concluded that the equal sharing of currents among modules can be achieved by reducing the mismatches in various module parameters, which is achievable in practice. The current-sharing performance of the IPOP converter is also verified by Saber simulation and a 400-W experimental prototype consisting of two DAHB modules. The common-duty-ratio control method can be extended to any IPOP system that consists of three or more converter modules, including traditional dual-active bridge dc-dc converters, which have a characteristic of current source."}
{"_id":"9fb837932bbcc0c7fd33a3692c68426ed1f36682","title":"User Based Aggregation for Biterm Topic Model","text":"Biterm Topic Model (BTM) is designed to model the generative process of the word co-occurrence patterns in short texts such as tweets. However, two aspects of BTM may restrict its performance: 1) user individualities are ignored to obtain the corpus level words co-occurrence patterns; and 2) the strong assumptions that two co-occurring words will be assigned the same topic label could not distinguish background words from topical words. In this paper, we propose Twitter-BTM model to address those issues by considering user level personalization in BTM. Firstly, we use user based biterms aggregation to learn user specific topic distribution. Secondly, each user\u2019s preference between background words and topical words is estimated by incorporating a background topic. Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several stateof-the-art baselines."}
{"_id":"0651b333c2669227b0cc42de403268a4546ece70","title":"A Critical Review of Recurrent Neural Networks for Sequence Learning","text":"Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a selfcontained explication of the state of the art together with a historical perspective and references to primary research."}
{"_id":"d96f734b973cc533e79c7c53969038dd5e71a3c0","title":"Applying QR code in augmented reality applications","text":"In this paper we present an augmented reality (AR) application based on the QR Code. The system can extract the information embedded in a QR Code and show the information in an extended 3D form with the QR Code being the traditional AR marker. Traditional AR systems often use a particularly designed pattern (the marker) to recover the 3D scene structure and identify the object to be displayed on the scene. In these systems, the marker is used only for tracking and identification. They do not convey any other information. Consequently, the applications of these systems are limited and often a registration procedure is required for a new marker.\n QR Code has the advantage of large information capacity and is similar to an AR marker in appearance. Thus, more interesting and useful applications can be developed by combining the QR Code with the traditional AR system. In this paper, we combine these two techniques to develop a product demo system. A QR Code is pasted on the package of a product and then a 3D virtual object is displayed on the QR Code. This system allows the customer to visualize the product via a more direct and interactive way. Our system demonstrates the success of using QR Code as the AR marker to a particular application and we believe it can bring more convenient to our life in the future."}
{"_id":"56b228ad5d1efba154eec2e63f41011f563d6469","title":"A Passive RFID Information Grid for Location and Proximity Sensing for the Blind User","text":"We describe a navigation and location determination system for the blind using an RFID tag grid. Each RFID tag is programmed upon installation with spatial coordinates and information describing the surroundings. This allows for a self-describing, localized information system with no dependency on a centralized database or wireless infrastructure for communications. The system could be integrated into building code requirements as part of ADA (Americans with Disabilities Act) at a cost of less than $1 per square foot. With an established RFID grid infrastructure blind children and adults will gain the independence and freedom to explore and participate in activities without external assistance. An established RFID grid infrastructure will also enable advances in robotics which will benefit from knowing precise location. In this paper, we present an RFID based information grid system with a reader integrated into the user\u2019s shoe, which is connected to the user PDA or cell phone via a Bluetooth. An emphasis is placed on the architecture and design to allow for a truly integrated pervasive environment."}
{"_id":"b1397c9085361f308bd70793fc2427a4416973d7","title":"FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance","text":"This paper describes a probabilistic approach to the problem of recognizing places based on their appearance. The system we present is not limited to localization, but can determine that a new observation comes from a previously unseen place, and so augment its map. Effectively this is a SLAM system in the space of appearance. Our probabilistic approach allows us to explicitly account for perceptual aliasing in the environment\u2014identical but indistinctive observations receive a low probability of having come from the same place. We achieve this by learning a generative model of place appearance. By partitioning the learning problem into two parts, new place models can be learned online from only a single observation of a place. The algorithm complexity is linear in the number of places in the map, and is particularly suitable for online loop closure detection in mo-"}
{"_id":"e20aae4ce14009f689b55ebaf9dac541b88fb18d","title":"Multi-modal Semantic Place Classification","text":"The ability to represent knowledge about space and its position therein is crucial for a mobile robot. To this end, topological and semantic descriptions are gaining popularity for augmenting purely metric space representations. In this paper we present a multi-modal place classification system that allows a mobile robot to identify places and recognize semantic categories in an indoor environment. The system effectively utilizes information from different robotic sensors by fusing multiple visual cues and laser range data. This is achieved using a high-level cue integration scheme based on a Support Vector Machine (SVM) that learns how to optimally combine and weight each cue. Our multi-modal place classification approach can be used to obtain a real-time semantic space labeling system which integrates information over time and space. We perform an extensive experimental evaluation of the method for two different platforms and environments, on a realistic off-line database and in a live experiment on an autonomous robot. The results clearly demonstrate the effecThe International Journal of Robotics Research Vol. 00, No. 00, Xxxxxxxx 2009, pp. 000\u2013000 DOI: 10.1177\/0278364909356483 c The Author(s), 2009. Reprints and permissions: http:\/\/www.sagepub.co.uk\/journalsPermissions.nav Figures 1\u201315, 17, 18 appear in color online: http:\/\/ijr.sagepub.com tiveness of our cue integration scheme and its value for robust place classification under varying conditions. KEY WORDS\u2014recognition, sensor fusion, localization, multi-modal place classification, sensor and cue integration, semantic annotation of space"}
{"_id":"16ccb8d307d3f33ebb395b32db23279b409f1228","title":"RADAR: An In-Building RF-Based User Location and Tracking System","text":"The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF) based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It combines empirical measurements with signal propagation modeling to determine user location and thereby enable locationaware services and applications. We present experimental results that demonstrate the ability of RADAR to estimate user location with a high degree of accuracy."}
{"_id":"e741b329e83eaac82e74572cc106e148be5164ed","title":"A Survey on Automatically Mining Facets for Web Queries","text":"Received Jan 4, 2017 Revised Jun 2, 2017 Accepted Jun 26, 2017 In this paper, a detailed survey on different facet mining techniques, their advantages and disadvantages is carried out. Facets are any word or phrase which summarize an important aspect about the web query. Researchers proposed different efficient techniques which improves the user\u2019s web query search experiences magnificently. Users are happy when they find the relevant information to their query in the top results. The objectives of their research are: (1) To present automated solution to derive the query facets by analyzing the text query; (2) To create taxonomy of query refinement strategies for efficient results; and (3) To personalize search according to user interest. Keyword:"}
{"_id":"59d7d8415dacd300eb4d98b0da3cb32d27503b36","title":"Visualizing Topic Models","text":"Managing large collections of documents is an important problem for many areas of science, industry, and culture. Probabilistic topic modeling offers a promising solution. Topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents. This discovered structure summarizes and organizes the documents. However, topic models are high-level statistical tools\u2014a user must scrutinize numerical distributions to understand and explore their results. In this paper, we present a method for visualizing topic models. Our method creates a navigator of the documents, allowing users to explore the hidden structure that a topic model discovers. These browsing interfaces reveal meaningful patterns in a collection, helping end-users explore and understand its contents in new ways. We provide open source software of our method. Understanding and navigating large collections of documents has become an important activity in many spheres. However, many document collections are not coherently organized and organizing them by hand is impractical. We need automated ways to discover and visualize the structure of a collection in order to more easily explore its contents. Probabilistic topic modeling is a set of machine learning tools that may provide a solution (Blei and Lafferty 2009). Topic modeling algorithms discover a hidden thematic structure in a collection of documents; they find salient themes and represent each document as a combination of themes. However, topic models are high-level statistical tools. A user must scrutinize numerical distributions to understand and explore their results; the raw output of the model is not enough to create an easily explored corpus. We propose a method for using a fitted topic model to organize, summarize, visualize, and interact with a corpus. With our method, users can explore the corpus, moving between high level discovered summaries (the \u201ctopics\u201d) and the documents themselves, as Figure 1 illustrates. Our design is centered around the idea that the model both summarizes and organizes the collection. Our method translates these representations into a visual system for exploring a collection, but visualizing this structure is not enough. The discovered structure induces relationships\u2014between topics and articles, and between articles and articles\u2014which lead to interactions in the visualization. Copyright c \u00a9 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Thus, we have three main goals in designing the visualization: summarize the corpus for the user; reveal the relationships between the content and summaries; and, reveal the relationships across content. We aim to present these in a ways that are accessible and useful to a spectrum of users, not just machine learning experts."}
{"_id":"7a45104e2bf816ad7294a43ed37a638e789db8c3","title":"Qualitative Research & Evaluation Methods: Integrating Theory and Practice","text":"In what case do you like reading so much? What about the type of the qualitative research evaluation methods integrating theory and practice book? The needs to read? Well, everybody has their own reason why should read some books. Mostly, it will relate to their necessity to get knowledge from the book and want to read just to get entertainment. Novels, story book, and other entertaining books become so popular this day. Besides, the scientific books will also be the best reason to choose, especially for the students, teachers, doctors, businessman, and other professions who are fond of reading."}
{"_id":"c625c5960c0fedc3304092aadcaabc714b806bb9","title":"Capturing the Temporal Domain in Echonest Features for Improved Classification Effectiveness","text":"This paper proposes Temporal Echonest Features to harness the information available from the beat-aligned vector sequences of the features provided by The Echo Nest. Rather than aggregating them via simple averaging approaches, the statistics of temporal variations are analyzed and used to represent the audio content. We evaluate the performance on four traditional music genre classification test collections and compare them to state of the art audio descriptors. Experiments reveal, that the exploitation of temporal variability from beat-aligned vector sequences and combinations of different descriptors leads to an improvement of classification accuracy. Comparing the results of Temporal Echonest Features to those of approved conventional audio descriptors used as benchmarks, these approaches perform well, often significantly outperforming their predecessors, and can be effectively used for large scale music genre classification."}
{"_id":"4a127a28fe78fa7c238d4474c8a5c574ade37625","title":"Learning Algorithms from Data","text":"Statistical machine learning is concerned with learning models that describe observations. We train our models from data on tasks like machine translation or object recognition because we cannot explicitly write down programs to solve such problems. A statistical model is only useful when it generalizes to unseen data. Solomonoff114 has proved that one should choose the model that agrees with the observed data, while preferring the model that can be compressed the most, because such a choice guarantees the best possible generalization. The size of the best possible compression of the model is called the Kolmogorov complexity of the model. We define an algorithm as a function with small Kolmogorov complexity. This Ph.D. thesis outlines the problem of learning algorithms from data and shows several partial solutions to it. Our data model is mainly neural networks as they have proven to be successful in various domains like object recognition67,109,122, language modelling90, speech recognition48,39 and others. First, we examine empirical trainability limits for classical neural networks. Then, we extend them by providing interfaces, which provide a way to read memory, access the input, and postpone predictions. The model learns how to use them with reinforcement learning techniques like REINFORCE and Q-learning. Next, we ex-"}
{"_id":"1127dee1f3f64bf02ba679bde052799b53c643da","title":"Generative Topic Embedding: a Continuous Representation of Documents","text":"Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document."}
{"_id":"e51c63cab0806b05fe3acbadced3097033be323f","title":"A study of array antenna with phase compensated technique for 60 GHz communication","text":"This paper presents phase compensated techniques for 60 GHz communication. A phase compensation method for antenna embedded in a mobile device and a planar lens for base station antenna are introduced. For the Line-of Sight (LoS) scenario, end-fire antenna arrays are adapted for mobile device. 8\u00d71 end-fire patch antenna arrays are designed in Low temperature co-fired ceramic (LTCC) substrate. Simulated max gain of the antenna arrays is 13.03 dBi. When the antenna module is mounted, scattering problem is occurred and its max gain is severely degraded to 6.4 dBi. By using phase compensated technique, max gain of the antenna arrays are enhanced to 12.5 dBi. The beam steered gain of the arrays is also simulated and good coverage can be achieved. A planar lens antenna is also studied for high gain of the base station antenna."}
{"_id":"faf4d5076c513c18c3d502f29e1cc811184f3ebb","title":"Comparison and Analysis of Single-Phase Transformerless Grid-Connected PV Inverters","text":"Leakage current minimization is one of the most important considerations in transformerless photovoltaic (PV) inverters. In the past, various transformerless PV inverter topologies have been introduced, with leakage current minimized by the means of galvanic isolation and common-mode voltage (CMV) clamping. The galvanic isolation can be achieved via dc-decoupling or ac-decoupling, for isolation on the dc- or ac-side of the inverter, respectively. It has been shown that the latter provides lower losses due to the reduced switch count in conduction path. Nevertheless, leakage current cannot be simply eliminated by galvanic isolation and modulation techniques, due to the presence of switches' junction capacitances and resonant circuit effects. Hence, CMV clamping is used in some topologies to completely eliminate the leakage current. In this paper, several recently proposed transformerless PV inverters with different galvanic isolation methods and CMV clamping technique are analyzed and compared. A simple modified H-bridge zero-voltage state rectifier is also proposed, to combine the benefits of the low-loss ac-decoupling method and the complete leakage current elimination of the CMV clamping method. The performances of different topologies, in terms of CMV, leakage current, total harmonic distortion, losses and efficiencies are compared. The analyses are done theoretically and via simulation studies, and further validated with experimental results. This paper is helpful for the researchers to choose the appropriate topology for transformerless PV applications and to provide the design principles in terms of common-mode behavior and efficiency."}
{"_id":"0a03b67644a6411ab7ec73551aa27060b8e4ab1d","title":"A Comparative Look into Public IXP Datasets","text":"Internet eXchange Points (IXPs) are core components of the Internet infrastructure where Internet Service Providers (ISPs) meet and exchange traffic. During the last few years, the number and size of IXPs have increased rapidly, driving the flattening and shortening of Internet paths. However, understanding the present status of the IXP ecosystem and its potential role in shaping the future Internet requires rigorous data about IXPs, their presence, status, participants, etc. In this work, we do the first cross-comparison of three well-known publicly available IXP databases, namely of PeeringDB, Euro-IX, and PCH. A key challenge we address is linking IXP identifiers across databases maintained by different organizations. We find different AS-centric versus IXP-centric views provided by the databases as a result of their data collection approaches. In addition, we highlight differences and similarities w.r.t. IXP participants, geographical coverage, and co-location facilities. As a side-product of our linkage heuristics, we make publicly available the union of the three databases, which includes 40.2% more IXPs and 66.3% more IXP participants than the commonly-used PeeringDB. We also publish our analysis code to foster reproducibility of our experiments and shed preliminary insights into the accuracy of the union dataset."}
{"_id":"12596562eedf00cad846f13afd2cc8f4b3dd5b4a","title":"OWLIM: A family of scalable semantic repositories","text":"An explosion in the use of RDF for representing information about resources has driven the requirements for Webscale server systems that can store and process huge quantities of data, and furthermore provide powerful data access and mining functionalities. This paper describes OWLIM, a family of semantic repositories that provide storage, inference and novel data-access features delivered in a scalable, resilient, industrial-strength platform. Ontotext AD, 135 Tsarigradsko Chaussee, Sofia 1784, Bulgaria"}
{"_id":"46acc1dd8ac263a081d21cb95dccb5007a1294d4","title":"Has the athlete trained enough to return to play safely? The acute:chronic workload ratio permits clinicians to quantify a player's risk of subsequent injury.","text":"The return to sport from injury is a difficult multifactorial decision, and risk of reinjury is an important component. Most protocols for ascertaining the return to play status involve assessment of the healing status of the original injury and functional tests which have little proven predictive ability. Little attention has been paid to ascertaining whether an athlete has completed sufficient training to be prepared for competition. Recently, we have completed a series of studies in cricket, rugby league and Australian rules football that have shown that when an athlete's training and playing load for a given week (acute load) spikes above what they have been doing on average over the past 4 weeks (chronic load), they are more likely to be injured. This spike in the acute:chronic workload ratio may be from an unusual week or an ebbing of the athlete's training load over a period of time as in recuperation from injury. Our findings demonstrate a strong predictive (R(2)=0.53) polynomial relationship between acute:chronic workload ratio and injury likelihood. In the elite team setting, it is possible to quantify the loads we are expecting athletes to endure when returning to sport, so assessment of the acute:chronic workload ratio should be included in the return to play decision-making process."}
{"_id":"4b131a9c1ef6a3ea6c410110a15dd673a16ed3f8","title":"Automatic Evaluation of Text Coherence: Models and Representations","text":"This paper investigates the automatic evaluation of text coherence for machine-generated texts. We introduce a fully-automatic, linguistically rich model of local coherence that correlates with human judgments. Our modeling approach relies on shallow text properties and is relatively inexpensive. We present experimental results that assess the predictive power of various discourse representations proposed in the linguistic literature. Our results demonstrate that certain models capture complementary aspects of coherence and thus can be combined to improve performance."}
{"_id":"55840cff7f84c970207b65f084e33cb0992fe45b","title":"Support vector machine approach for protein subcellular localization prediction","text":"MOTIVATION\nSubcellular localization is a key functional characteristic of proteins. A fully automatic and reliable prediction system for protein subcellular localization is needed, especially for the analysis of large-scale genome sequences.\n\n\nRESULTS\nIn this paper, Support Vector Machine has been introduced to predict the subcellular localization of proteins from their amino acid compositions. The total prediction accuracies reach 91.4% for three subcellular locations in prokaryotic organisms and 79.4% for four locations in eukaryotic organisms. Predictions by our approach are robust to errors in the protein N-terminal sequences. This new approach provides superior prediction performance compared with existing algorithms based on amino acid composition and can be a complementary method to other existing methods based on sorting signals.\n\n\nAVAILABILITY\nA web server implementing the prediction method is available at http:\/\/www.bioinfo.tsinghua.edu.cn\/SubLoc\/.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary material is available at http:\/\/www.bioinfo.tsinghua.edu.cn\/SubLoc\/."}
{"_id":"988058ab8dfcb27e9566c6bcef398a4407b1ea04","title":"Toward open manufacturing: A cross-enterprises knowledge and services exchange framework based on blockchain and edge computing","text":""}
{"_id":"be5784888299cb4ada53a25f96f29161f16e7eda","title":"Android malware detection method based on naive Bayes and permission correlation algorithm","text":"In order to detect Android malware more effectively, an Android malware detection model was proposed based on improved naive Bayes classification. Firstly, considering the unknown permission that may be malicious in detection samples, and in order to improve the Android detection rate, the algorithm of malware detection is proposed based on improved naive Bayes. Considering the limited training samples, limited permissions, and the new malicious permissions in the test samples, we used the impact of the new malware permissions and training permissions as the weight. The weighted naive Bayesian algorithm improves the Android malware detection efficiency. Secondly, taking into account the detection model, we proposed a detection model of permissions and information theory based on the improved naive Bayes algorithm. We analyzed the correlation of the permission. By calculating the Pearson correlation coefficient, we determined the value of Pearson correlation coefficient r, and delete the permissions whose value r is less than the threshold $$\\rho $$ \u03c1 and get the new permission set. So, we got the improved detection model by clustering based on information theory. Finally, we detected the 1725 Android malware and 945 non malicious application of multiple data sets in the same simulation environment. The detection rate of the improved the naive Bayes algorithm is 86.54%, and the detection rate of the non-malicious application is increased to 97.59%. Based on the improved naive Bayes algorithm, the false detection rate of the improved detection model is reduced by 8.25%."}
{"_id":"71550eeeb178cde0a6b00e3adb8d4eee482612ad","title":"Heroin epidemics, treatment and ODE modelling.","text":"The UN [United Nations Office on Drugs and Crime (UNODC): World Drug Report, 2005, vol. 1: Analysis. UNODC, 2005.], EU [European Monitoring Centre for Drugs and Drug Addiction (EMCDDA): Annual Report, 2005.http:\/\/annualreport.emcdda.eu.int\/en\/home-en.html.] and WHO [World Health Organisation (WHO): Biregional Strategy for Harm Reduction, 2005-2009. HIV and Injecting Drug Use. WHO, 2005.] have consistently highlighted in recent years the ongoing and persistent nature of opiate and particularly heroin use on a global scale. While this is a global phenomenon, authors have emphasised the significant impact such an epidemic has on individual lives and on society. National prevalence studies have indicated the scale of the problem, but the drug-using career, typically consisting of initiation, habitual use, a treatment-relapse cycle and eventual recovery, is not well understood. This paper presents one of the first ODE models of opiate addiction, based on the principles of mathematical epidemiology. The aim of this model is to identify parameters of interest for further study, with a view to informing and assisting policy-makers in targeting prevention and treatment resources for maximum effectiveness. An epidemic threshold value, R(0), is proposed for the drug-using career. Sensitivity analysis is performed on R(0) and it is then used to examine the stability of the system. A condition under which a backward bifurcation may exist is found, as are conditions that permit the existence of one or more endemic equilibria. A key result arising from this model is that prevention is indeed better than cure."}
{"_id":"098d8570fe03b8267f1d12db7608668400005896","title":"Data resource profile: the World Health Organization Study on global AGEing and adult health (SAGE).","text":"Population ageing is rapidly becoming a global issue and will have a major impact on health policies and programmes. The World Health Organization's Study on global AGEing and adult health (SAGE) aims to address the gap in reliable data and scientific knowledge on ageing and health in low- and middle-income countries. SAGE is a longitudinal study with nationally representative samples of persons aged 50+ years in China, Ghana, India, Mexico, Russia and South Africa, with a smaller sample of adults aged 18-49 years in each country for comparisons. Instruments are compatible with other large high-income country longitudinal ageing studies. Wave 1 was conducted during 2007-2010 and included a total of 34 124 respondents aged 50+ and 8340 aged 18-49. In four countries, a subsample consisting of 8160 respondents participated in Wave 1 and the 2002\/04 World Health Survey (referred to as SAGE Wave 0). Wave 2 data collection will start in 2012\/13, following up all Wave 1 respondents. Wave 3 is planned for 2014\/15. SAGE is committed to the public release of study instruments, protocols and meta- and micro-data: access is provided upon completion of a Users Agreement available through WHO's SAGE website (www.who.int\/healthinfo\/systems\/sage) and WHO's archive using the National Data Archive application (http:\/\/apps.who.int\/healthinfo\/systems\/surveydata)."}
{"_id":"b63f9eeb1210e9519660589ff0e7a5fec9b39b71","title":"Gain Improvement of Rectangular Dielectric Resonator Antenna by Engraving Grooves on Its Side Walls","text":"A new technique for increasing the boresight gain of a rectangular dielectric resonator antenna (DRA) operating at its fundamental radiating <inline-formula><tex-math notation=\"LaTeX\">$TE_{111}^y$<\/tex-math><\/inline-formula> mode is introduced. The idea is to increase the radiations from the side walls of the DRA compared to that of its top wall by engraving grooves on the side walls. A model based on the array theory is developed to explain the high-gain nature of the antenna. The measured results demonstrate that the proposed antenna achieves an impedance bandwidth of 21% over a band of 3.24\u20134 GHz, with a maximum gain of 9.6 dB. This is significantly higher with respect to available data in the literature."}
{"_id":"9dbab944fe238fe0e985b5811397312b2b975035","title":"Comparative Analysis of 6 Bit Thermometer-to-Binary Decoders for Flash Analog-to-Digital Converter","text":"In the design of high speed Flash ADC selection of Thermometer to Binary decoder plays an important role. This paper describes different decoder topologies suitable for Flash ADCs. Comparative analysis between them is presented in terms of hardware required, propagation delay & power consumption. Result shows that fat tree & Mux based topologies are suitable for high speed conversions, but Mux based topology is effective in terms of power consumption. Presence of bubble error reduces output correction capability. In this paper advance scheme is proposed for correction of bubble error in Mux based decoder."}
{"_id":"6d871436d1f04810561b62e9fc7fa3e9ae471d47","title":"E-Readers and Visual Fatigue","text":"The mass digitization of books is changing the way information is created, disseminated and displayed. Electronic book readers (e-readers) generally refer to two main display technologies: the electronic ink (E-ink) and the liquid crystal display (LCD). Both technologies have advantages and disadvantages, but the question whether one or the other triggers less visual fatigue is still open. The aim of the present research was to study the effects of the display technology on visual fatigue. To this end, participants performed a longitudinal study in which two last generation e-readers (LCD, E-ink) and paper book were tested in three different prolonged reading sessions separated by--on average--ten days. Results from both objective (Blinks per second) and subjective (Visual Fatigue Scale) measures suggested that reading on the LCD (Kindle Fire HD) triggers higher visual fatigue with respect to both the E-ink (Kindle Paperwhite) and the paper book. The absence of differences between E-ink and paper suggests that, concerning visual fatigue, the E-ink is indeed very similar to the paper."}
{"_id":"81af5a72ba19c79cfe5c02797d01d3b5df998fa8","title":"Three-Dimensional Skin Deformation as Force Substitution: Wearable Device Design and Performance During Haptic Exploration of Virtual Environments","text":"Virtual reality systems would benefit from a compelling force sensory substitute when workspace or stability limitations prevent the use of kinesthetic force feedback systems. We present a wearable fingertip haptic device with the ability to make and break contact in addition to rendering both shear and normal skin deformation to the fingerpad. A delta mechanism with novel bias spring and tether actuator relocation method enables the use of high-end motors and encoders, allowing precise device control: 10 Hz bandwidth and 0.255 mm RMS tracking error were achieved during testing. In the first of two experiments, participants determined the orientation of a stiff region in a surrounding compliant virtual surface with an average angular error of 7.6 degree, similar to that found in previous studies using traditional force feedback. In the second experiment, we evaluated participants\u2019 ability to interpret differences in friction. The Just Noticeable Difference (JND) of surface friction coefficient discrimination using our skin deformation device was 0.20, corresponding with a reference friction coefficient of 0.5. While higher than that found using kinesthetic feedback, this demonstrates that users can perceive differences in surface friction without world-grounded kinesthetic forces. These experiments show that three DoF skin deformation enables both stiffness and friction discrimination capability in the absence of kinesthetic force feedback."}
{"_id":"3dbfabbb1fcd2798990e91387cbb3e2977315231","title":"The possession game? A comparative analysis of ball retention and team success in European and international football, 2007-2010.","text":"Possession is thought of as central to success in modern football, but questions remain about its impact on positive team outcomes (Bate, 1988; Hughes & Franks, 2005; Pollard & Reep, 1997; Stanhope, 2001). Recent studies (e.g. Bloomfield, Polman, & O'Donoghue, 2005; Carling, Williams, & Reilly, 2005; James, Mellallieu, & Holley, 2002; Jones, James, & Mellalieu, 2004; Lago, 2009; Lago & Martin, 2007; Lago-Pe\u00f1as & Dellal, 2010; Lago-Pe\u00f1as, Lago-Ballesteros, Dellal, & G\u00f3mez, 2010; Taylor, Mellalieu, & James, 2005; Tucker, Mellalieu, James, & Taylor, 2005) that have examined these questions have often been constrained by an exclusive focus on English or Spanish domestic play. Using data from five European leagues, UEFA and FIFA tournaments, the study found that while possession time and passing predicted aggregated team success in domestic league play, both variables were poor predictors at the individual match level once team quality and home advantage were accounted for. In league play, the effect of greater possession was consistently negative; in the Champions League, it had virtually no impact. In national team tournaments, possession failed to reach significance when offensive factors were accounted for. Much of the success behind the 'possession game' was thus a function of elite teams confined in geographic and competitive space. That ball hegemony was not consistently tied to success suggests that a nuanced approach to possession is needed to account for variant strategic environments (e.g. James et al., 2002) and compels match analysts to re-examine the metric's overall value."}
{"_id":"50bcbc85552291ce4a53f5ff835a905673461cff","title":"Mapping interference resolution across task domains: A shared control process in left inferior frontal gyrus","text":"Work in functional neuroimaging has mapped interference resolution processing onto left inferior frontal regions for both verbal working memory and a variety of semantic processing tasks. The proximity of the identified regions from these different tasks suggests the existence of a common, domain-general interference resolution mechanism. The current research specifically tests this idea in a within-subject design using fMRI to assess the activation associated with variable selection requirements in a semantic retrieval task (verb generation) and a verbal working memory task with a trial-specific proactive interference manipulation (recent-probes). High interference trials on both tasks were associated with activity in the midventrolateral region of the left inferior frontal gyrus, and the regions activated in each task strongly overlapped. The results indicate that an elemental component of executive control associated with interference resolution during retrieval from working memory and from semantic memory can be mapped to a common portion of the left inferior frontal gyrus."}
{"_id":"2c361ef5db3231d34656dd86d9b288397f0b929e","title":"No free lunch theorems for optimization","text":"A framework is developed to explore the connection between e ective optimization algorithms and the problems they are solving A number of no free lunch NFL theorems are presented that establish that for any algorithm any elevated performance over one class of problems is exactly paid for in performance over another class These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem Applications of the NFL theorems to information theoretic aspects of optimization and benchmark measures of performance are also presented Other issues addressed are time varying optimization problems and a priori head to head minimax distinctions between optimization algorithms distinctions that can obtain despite the NFL theorems enforcing of a type of uniformity over all algorithms"}
{"_id":"d6647910bfdddeba029448221e33b508b25735a4","title":"Coordinated Scheduling of Residential Distributed Energy Resources to Optimize Smart Home Energy Services","text":"We describe algorithmic enhancements to a decision-support tool that residential consumers can utilize to optimize their acquisition of electrical energy services. The decision-support tool optimizes energy services provision by enabling end users to first assign values to desired energy services, and then scheduling their available distributed energy resources (DER) to maximize net benefits. We chose particle swarm optimization (PSO) to solve the corresponding optimization problem because of its straightforward implementation and demonstrated ability to generate near-optimal schedules within manageable computation times. We improve the basic formulation of cooperative PSO by introducing stochastic repulsion among the particles. The improved DER schedules are then used to investigate the potential consumer value added by coordinated DER scheduling. This is computed by comparing the end-user costs obtained with the enhanced algorithm simultaneously scheduling all DER, against the costs when each DER schedule is solved separately. This comparison enables the end users to determine whether their mix of energy service needs, available DER and electricity tariff arrangements might warrant solving the more complex coordinated scheduling problem, or instead, decomposing the problem into multiple simpler optimizations."}
{"_id":"2d8d5c7d02a7a54b99f1dc2499593a9289888831","title":"Detection and Localization of Curbs and Stairways Using Stereo Vision","text":"We present algorithms to detect and precisely localize curbs and stairways for autonomous navigation. These algorithms combine brightness information (in the form of edgels) with 3-D data from a commercial stereo system. The overall system (including stereo computation) runs at about 4 Hz on a 1 GHz laptop. We show experimental results and discuss advantages and shortcomings of our approach."}
{"_id":"63623c63ffddd08d266d884680d3493e8b7705f1","title":"Stereo Processing by Semiglobal Matching and Mutual Information","text":"This paper describes the semiglobal matching (SGM) stereo method. It uses a pixelwise, mutual information (Ml)-based matching cost for compensating radiometric differences of input images. Pixelwise matching is supported by a smoothness constraint that is usually expressed as a global cost function. SGM performs a fast approximation by pathwise optimizations from all directions. The discussion also addresses occlusion detection, subpixel refinement, and multibaseline matching. Additionally, postprocessing steps for removing outliers, recovering from specific problems of structured environments, and the interpolation of gaps are presented. Finally, strategies for processing almost arbitrarily large images and fusion of disparity images using orthographic projection are proposed. A comparison on standard stereo images shows that SGM is among the currently top-ranked algorithms and is best, if subpixel accuracy is considered. The complexity is linear to the number of pixels and disparity range, which results in a runtime of just 1-2 seconds on typical test images. An in depth evaluation of the Ml-based matching cost demonstrates a tolerance against a wide range of radiometric transformations. Finally, examples of reconstructions from huge aerial frame and pushbroom images demonstrate that the presented ideas are working well on practical problems."}
{"_id":"734e4341a2507c9b6039869aeb4c138bb86ade00","title":"Adaptive neighborhood selection for real-time surface normal estimation from organized point cloud data using integral images","text":"In this paper we present two real-time methods for estimating surface normals from organized point cloud data. The proposed algorithms use integral images to perform highly efficient border- and depth-dependent smoothing and covariance estimation. We show that this approach makes it possible to obtain robust surface normals from large point clouds at high frame rates and therefore, can be used in real-time computer vision algorithms that make use of Kinect-like data."}
{"_id":"154898f34460e95aef932bec5615bbd995824cad","title":"A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms","text":"Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms."}
{"_id":"706a1740113018e8f5f9ded79a2e5a43ca8407d2","title":"A Compact 90-degree Twist using Novel Ridged Waveguide for Integrated Waveguide Subsystems","text":"A compact waveguide twist using a single step of novel ridged structure for integrated waveguide subsystems is proposed. The cross-section of the ridged waveguide has the shape of two squares partially overlapped. The advantages of this configuration are the facilitation of manufacturing and the reduction of overall dimensions. In this paper, the broadband operation principle is described by using equivalent circuit, moreover the its design using HFSS and the experimental results are presented. In the result, the return loss in excess of 30dB could be obtained over 22 percent of frequency band without additional waveguide steps. The twist length is 0.22 times the wavelength of the standard rectangular waveguides"}
{"_id":"005b965c545ef1d67c462756b77d8ef7d812e944","title":"An active volumetric model for 3D reconstruction","text":"In this paper, we present an active volumetric model (AVM) for 3D reconstruction from multiple calibrated images of a scene. The AVM is a physically motivated 3D deformable model which shrinks actively under the influence of multiple simulated forces towards the real scene by throwing away some of its voxels. It provides a computational framework to integrate several constraints in an intelligible way. In the current work, we use three forces derived respectively from the smooth constraint, the compulsory silhouette constraint, and the color consistency constraint. Based on the composition of the 3 forces, our algorithm can significantly restrain holes and floating voxels, which plague voxel coloring algorithms, and produce precise and smooth models. We test our algorithm by experiments based on both synthetic and real data."}
{"_id":"8bd8b5ee7a38ae0a148d8afa1827f41aba5323cf","title":"Effective Fashion Retrieval Based on Semantic Compositional Networks","text":"Typical approaches for fashion retrieval rank clothing images according to the similarity to a user-provided query image. Similarity is usually assessed by encoding images in terms of visual elements such as color, shape and texture. In this work, we proceed differently and consider that the semantics of an outfit is mainly comprised of environmental and cultural concepts such as occasion, style and season. Thus, instead of retrieving outfits using strict visual elements, we find semantically similar outfits that fall into similar clothing styles and are adequate for the same occasions and seasons. We propose a compositional approach for fashion retrieval by arguing that the semantics of an outfit can be recognised by their constituents (i.e., clothing items and accessories). Specifically, we present a semantic compositional network (Comp-Net) in which clothing items are detected from the image and the probability of each item is used to compose a vector representation for the outfit. Comp-Net employs a normalization layer so that weights are updated by taking into consideration the previously known co-occurrence patterns between clothing items. Further, Comp-Net minimizes a cost-sensitive loss function as errors have different costs depending on the clothing item that is misclassified. This results in a space in which semantically related outfits are placed next to each other, enabling to find semantically similar outfits that may not be visually similar. We designed an evaluation setup that takes into account the association between different styles, occasions and seasons, and show that our compositional approach significantly outperforms a variety of recently proposed baselines."}
{"_id":"70e0660ff33b75b751f8635bb39f5b3299335fb7","title":"Using humans as sensors: An estimation-theoretic perspective","text":"The explosive growth in social network content suggests that the largest \"sensor network\" yet might be human. Extending the participatory sensing model, this paper explores the prospect of utilizing social networks as sensor networks, which gives rise to an interesting reliable sensing problem. In this problem, individuals are represented by sensors (data sources) who occasionally make observations about the physical world. These observations may be true or false, and hence are viewed as binary claims. The reliable sensing problem is to determine the correctness of reported observations. From a networked sensing standpoint, what makes this sensing problem formulation different is that, in the case of human participants, not only is the reliability of sources usually unknown but also the original data provenance may be uncertain. Individuals may report observations made by others as their own. The contribution of this paper lies in developing a model that considers the impact of such information sharing on the analytical foundations of reliable sensing, and embed it into a tool called Apollo that uses Twitter as a \"sensor network\" for observing events in the physical world. Evaluation, using Twitter-based case-studies, shows good correspondence between observations deemed correct by Apollo and ground truth."}
{"_id":"32b456688bf7c9cb8dced7348b981dc24d992f05","title":"Enhancing perceptual and attentional skills requires common demands between the action video games and transfer tasks","text":"Despite increasing evidence that shows action video game play improves perceptual and cognitive skills, the mechanisms of transfer are not well-understood. In line with previous work, we suggest that transfer is dependent upon common demands between the game and transfer task. In the current study, participants played one of four action games with varying speed, visual, and attentional demands for 20 h. We examined whether training enhanced performance for attentional blink, selective attention, attending to multiple items, visual search and auditory detection. Non-gamers who played the game (Modern Combat) with the highest demands showed transfer to tasks of attentional blink and attending to multiple items. The game (MGS Touch) with fewer attentional demands also decreased attentional blink, but to a lesser degree. Other games failed to show transfer, despite having many action game characteristics but at a reduced intensity. The results support the common demands hypothesis."}
{"_id":"f8c3bc760a29b3c3a78d1b051601af7d7150cd26","title":"Psychosocial functioning and depression: distinguishing among antecedents, concomitants, and consequences.","text":"In this article we attempt to distinguish empirically between psycho-social variables that are concomitants of depression, and variables that may serve as antecedents or sequelae of this disorder. We review studies that investigated the relationship between depression and any of six psychosocial variables after controlling for the effects of concurrent depression. The six variables examined are attributional style, dysfunctional attitudes, personality, social support, marital distress, and coping style. The review suggests that whereas there is little evidence in adults of a cognitive vulnerability to clinical depression, disturbances in interpersonal functioning may be antecedents or sequelae of this disorder. Specifically, marital distress and low social integration appear to be involved in the etiology of depression, and introversion and interpersonal dependency are identified as enduring abnormalities in the functioning of remitted depressives. We attempt to integrate what is known about the relationships among these latter variables, suggest ways in which they may influence the development of depression, and outline specific issues to be addressed in future research."}
{"_id":"ad79a5d7b3faa86da38b459625e6727bc0c80f2a","title":"Formal Verification With Frama-C: A Case Study in the Space Software Domain","text":"With the increasing importance of software in the aerospace field, as evidenced by its growing size and complexity, a rigorous and reliable software verification and validation process must be applied to ensure conformance with the strict requirements of this software. Although important, traditional validation activities such as testing and simulation can only provide a partial verification of behavior in critical real-time software systems, and thus, formal verification is an alternative to complement these activities. Two useful formal software verification approaches are deductive verification and abstract interpretation, which analyze programs statically to identify defects. This paper explores abstract interpretation and deductive verification by employing Frama-C's value analysis and Jessie plug-ins to verify embedded aerospace control software. The results indicate that both approaches can be employed in a software verification process to make software more reliable."}
{"_id":"9435022a8d60669468cc58331d09ba0a1c593b7e","title":"Validation of heart rate extraction using video imaging on a built-in camera system of a smartphone","text":"As a smartphone is becoming very popular and its performance is being improved fast, a smartphone shows its potential as a low-cost physiological measurement solution which is accurate and can be used beyond the clinical environment. Because cardiac pulse leads the subtle color change of a skin, a pulsatile signal which can be described as photoplethysmographic (PPG) signal can be measured through recording facial video using a digital camera. In this paper, we explore the potential that the reliable heart rate can be measured remotely by the facial video recorded using smartphone camera. First, using the front facing-camera of a smartphone, facial video was recorded. We detected facial region on the image of each frame using face detection, and yielded the raw trace signal from the green channel of the image. To extract more accurate cardiac pulse signal, we applied independent component analysis (ICA) to the raw trace signal. The heart rate was extracted using frequency analysis of the raw trace signal and the analyzed signal from ICA. The accuracy of the estimated heart rate was evaluated by comparing with the heart rate from reference electrocardiogram (ECG) signal. Finally, we developed FaceBEAT, an iPhone application for remote heart rate measurement, based on this study."}
{"_id":"59cc3b7cd89a74c26e359ad7063ef4ad940e1124","title":"The minds of gods: A comparative study of supernatural agency","text":"The present work is the first study to systematically compare the minds of gods by examining some of the intuitive processes that guide how people reason about them. By examining the Christian god and the spirit-masters of the Tyva Republic, it first confirms that the consensus view of the Christian god's mind is one of omniscience with acute concern for interpersonal social behavior (i.e., moral behaviors) and that Tyvan spirit-masters are not as readily attributed with knowledge or concern of moral information. Then, it reports evidence of a moralization bias of gods' minds; American Christians who believe that God is omniscient rate God as more knowledgeable of moral behaviors than nonmoral information. Additionally, Tyvans who do not readily report pro- or antisocial behavior among the things that spirit-masters care about will nevertheless rate spirit-masters' knowledge and concern of moral information higher than nonmoral information. However, this knowledge is distributed spatially; the farther away from spirits' place of governance a moral behavior takes place, the less they know and care about it. Finally, the wider the breadth of knowledge Tyvans attribute to spirit-masters, the more they attribute moral concern for behaviors that transpire beyond their jurisdiction. These results further demonstrate that there is a significant gulf between expressed beliefs and intuitive religious cognition and provides evidence for a moralization bias of gods' minds."}
{"_id":"c129ee744165be49aaf2a6914a623870c474b7bb","title":"An Overview on XML Semantic Disambiguation from Unstructured Text to Semi-Structured Data : Background , Applications , and Ongoing Challenges","text":"Since the last two decades, XML has gained momentum as the standard for Web information management and complex data representation. Also, collaboratively built semi-structured information resources, such as Wikipedia, have become prevalent on the Web and can be inherently encoded in XML. Yet most methods for processing XML and semi-structured information handle mainly the syntactic properties of the data, while ignoring the semantics involved. To devise more intelligent applications, one needs to augment syntactic features with machine-readable semantic meaning. This can be achieved through the computational identification of the meaning of data in context, also known as (a.k.a.) automated semantic analysis and disambiguation, which is nowadays one of the main challenges at the core of the Semantic Web. This survey paper provides a concise and comprehensive review of the methods related to XML-based semi-structured semantic analysis and disambiguation. It is made of four logical parts. First, we briefly cover traditional word sense disambiguation methods for processing flat textual data. Second, we describe and categorize disambiguation techniques developed and extended to handle semi-structured and XML data. Third, we describe current and potential application scenarios that can benefit from XML semantic analysis, including: data clustering and semantic-aware indexing, data integration and selective dissemination, semantic-aware and temporal querying, Web and Mobile Services matching and composition, blog and social semantic network analysis, and ontology learning. Fourth, we describe and discuss ongoing challenges and future directions, including: the quantification of semantic ambiguity, expanding XML disambiguation context, combining structure and content, using collaborative\/social information sources, integrating explicit and implicit semantic analysis, emphasizing user involvement, and reducing computational complexity."}
{"_id":"98ebbb7ff06d45bc6f8bed1ed24351f510b74a22","title":"Thermal characterisation of a copper-clip-bonded IGBT module with double-sided cooling","text":"In pursuing higher power density power electronic equipment, the copper clip bonding concept is one of the double-sided cooling approaches under investigation for power semiconductors. This paper presents a comprehensive thermal assessment of the copper clip concept by undertaking comparative studies on one bespoke copper-clip-bonded IGBT power module and a conventional wire-bonded counterpart. A series of single-sided cooling tests and double-sided cooling tests have been undertaken on these two modules. Reductions are reported in the junction to case thermal resistance of up to 23% for one individual die and around 18% for the parallel operation of two dies in the single-sided cooling tests. Whilst in the double-sided cooling experiments, an additional average 18% thermal improvement is achieved due to the addition of a top fan-cooled heatsink mounted onto the copper clip. The results show clearly that the copper clip technology could provide significant benefits in the heat removal for power semiconductors and enable more power dense equipment."}
{"_id":"f2ae900a87eee49169f9dc983e14f5979c5b260e","title":"Distributed Detection of Single-Stage Multipoint Cyber Attacks in a Water Treatment Plant","text":"A distributed detection method is proposed to detect single stage multi-point (SSMP) attacks on a Cyber Physical System (CPS). Such attacks aim at compromising two or more sensors or actuators at any one stage of a CPS and could totally compromise a controller and prevent it from detecting the attack. However, as demonstrated in this work, using the flow properties of water from one stage to the other, a neighboring controller was found effective in detecting such attacks. The method is based on physical invariants derived for each stage of the CPS from its design. The attack detection effectiveness of the method was evaluated experimentally against an operational water treatment testbed containing 42 sensors and actuators. Results from the experiments point to high effectiveness of the method in detecting a variety of SSMP attacks but also point to its limitations. Distributing the attack detection code among various controllers adds to the scalability of the proposed method."}
{"_id":"5d5385eadf98c38a1b943b1b07bc677d1910d69d","title":"Feature Extraction With Multiscale Covariance Maps for Hyperspectral Image Classification","text":"The classification of hyperspectral images (HSIs) using convolutional neural networks (CNNs) has recently drawn significant attention. However, it is important to address the potential overfitting problems that CNN-based methods suffer when dealing with HSIs. Unlike common natural images, HSIs are essentially three-order tensors which contain two spatial dimensions and one spectral dimension. As a result, exploiting both spatial and spectral information is very important for HSI classification. This paper proposes a new hand-crafted feature extraction method, based on multiscale covariance maps (MCMs), that is specifically aimed at improving the classification of HSIs using CNNs. The proposed method has the following distinctive advantages. First, with the use of covariance maps, the spatial and spectral information of the HSI can be jointly exploited. Each entry in the covariance map stands for the covariance between two different spectral bands within a local spatial window, which can absorb and integrate the two kinds of information (spatial and spectral) in a natural way. Second, by means of our multiscale strategy, each sample can be enhanced with spatial information from different scales, increasing the information conveyed by training samples significantly. To verify the effectiveness of our proposed method, we conduct comprehensive experiments on three widely used hyperspectral data sets, using a classical 2-D CNN (2DCNN) model. Our experimental results demonstrate that the proposed method can indeed increase the robustness of the CNN model. Moreover, the proposed MCMs+2DCNN method exhibits better classification performance than other CNN-based classification strategies and several standard techniques for spectral-spatial classification of HSIs."}
{"_id":"5b742b999160442216f0936aab36cdd3b2e4c6b2","title":"Interactions Between Philosophy and Artificial Intelligence: The Role of Intuition and Non-Logical Reasoning in Intelligence","text":"This paper echoes, from a philosophical standpoint, the claim of McCarthy and Hayes that Philosophy and Artificial Intelligence have important relations. Philosophical problems about the use of \u2019intuition\u2019 in reasoning are related, via a concept of analogical representation, to problems in the simulation of perception, problem-solving and the generation of useful sets of possibilities in considering how to act. The requirements for intelligent decision-making proposed by McCarthy and Hayes are criticised as too narrow, and more general requirements are suggested instead."}
{"_id":"345947186f190649c582204776071ac9a62e8d67","title":"Low-resource routing attacks against tor","text":"Tor has become one of the most popular overlay networks for anonymizing TCP traffic. Its popularity is due in part to its perceived strong anonymity properties and its relatively low latency service. Low latency is achieved through Tor\u00e2 s ability to balance the traffic load by optimizing Tor router selection to probabilistically favor routers with highbandwidth capabilities.\n We investigate how Tor\u00e2 s routing optimizations impact its ability to provide strong anonymity. Through experiments conducted on PlanetLab, we show the extent to which routing performance optimizations have left the system vulnerable to end-to-end traffic analysis attacks from non-global adversaries with minimal resources. Further, we demonstrate that entry guards, added to mitigate path disruption attacks, are themselves vulnerable to attack. Finally, we explore solutions to improve Tor\u00e2 s current routing algorithms and propose alternative routing strategies that prevent some of the routing attacks used in our experiments."}
{"_id":"e2a389d1eff87ded82f528368ab64f2571fe563f","title":"Beyond Linguistic Equivalence. An Empirical Study of Translation Evaluation in a Translation Learner Corpus","text":"The realisation that fully automatic translation in many settings is still far from producing output that is equal or superior to human translation has lead to an intense interest in translation evaluation in the MT community. However, research in this field, by now, has not only largely ignored the tremendous amount of relevant knowledge available in a closely related discipline, namely translation studies, but also failed to provide a deeper understanding of the nature of \"translation errors\" and \"translation quality\". This paper presents an empirical take on the latter concept, translation quality, by comparing human and automatic evaluations of learner translations in the KOPTE corpus. We will show that translation studies provide sophisticated concepts for translation quality estimation and error annotation. Moreover, by applying well-established MT evaluation scores, namely BLEU and Meteor, to KOPTE learner translations that were graded by a human expert, we hope to shed light on properties (and potential shortcomings) of these scores. 1 Translation quality assessment In recent years, researchers in the field of MT evaluation have proposed a large variety of methods for assessing the quality of automatically produced translations. Approaches range from fully automatic quality scoring to efforts aimed at the development of \"human\" evaluation scores that try to exploit the (often tacit) linguistic knowledge of human evaluators. The criteria according to which quality is estimated often include adequacy, the degree of meaning preservation, and fluency, target language correctness (Callison-Burch et al., 2007). The goals of both \"human\" evaluation and fully automatic quality scoring are manifold and cover system optimisation as well as benchmarking and comparison. In translation studies, the scientific (and prescientific) discussion on how to assess the quality of human translations has been going on for centuries. In recent years, the development of appropriate concepts and tools has become even more vital to the discipline due to the pressing needs of the language industry. However, different from the belief, typical to MT, that the \"goodness\" of a translation can be scored on the basis of linguistic criteria alone, the notion of \"translation quality\", in translation studies, has assumed a multi-faceted shape, distancing itself from a simple strive for equivalence and embracing concepts such as functional, stylistic and pragmatic appropriateness as well as textual coherence. In this section, we provide an overview over approaches to translation quality assessment developed in MT and translation studies to specify how \"quality\" is being defined in both fields and which methods and features are used. Due to the amount of available literature, this overview is necessarily incomplete, but still insightful with respect to differences and commonalities between MT and human translation evaluation. 1.1 Automatic MT quality scores MT output is usually evaluated by automatic language-independent metrics which can be applied to any language produced by an MT system. The use of automatic metrics for MT evaluation is legitimate, since MT systems deal with large amounts of data, on which manual evaluation would be very time-consuming and expensive. Automatic metrics typically compute the closeness (adequacy) of a \"hypothesis\" to a \"reference\" translation and differ from each other by how this closeness is measured. The most popular MT eval-"}
{"_id":"89411367a191c96bc778fa6ba2bfd5eeea4638b6","title":"High-Efficient Patch Antenna Array for E-Band Gigabyte Point-to-Point Wireless Services","text":"This letter presents a low-cost, high-gain, and high-efficiency 4 \u00d7 4 circular patch array antenna for gigabyte point-to-point wireless services at E-band (81-86 GHz). The antenna structure consists of two layers. The feed network is placed at the bottom layer, while the circular patches are on the top layer. To increase the efficiency of the antenna array, substrate integrated waveguide (SIW) is used to feed the circular patches through longitudinal slots etched on the top metallic surface. Low-cost printed circuit board (PCB) process is used to fabricate the antenna prototype. Simulated and measured bandwidths of the antenna array are 7.2%, which covers the desired frequency range of 81-86 GHz. Measured gain of the 4 \u00d7 4 antenna array is 18.5 dBi, which is almost constant within the antenna bandwidth. Measured radiation efficiency of 90.3% is obtained."}
{"_id":"f5349e656cff0be73e7f8a9fa39c41637e523a7d","title":"Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding","text":"There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and learn to transfer manipulation strategy across different objects by embedding point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. In order to collect a large number of manipulation demonstrations for different objects, we develop a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects and appliances with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot with our model can even prepare a cup of a latte with appliances it has never seen before."}
{"_id":"a6c41fd3888e884c0e20663c67bccb9efb802da4","title":"Anatomic reconstruction of chronic symptomatic anterolateral proximal tibiofibular joint instability","text":"Symptomatic chronic proximal tibiofibular joint subluxation is a pathology which is difficult to diagnose and treat. Surgical treatment has not been well defined. A report of two patients successfully treated with an anatomic reconstruction of the posterior aspect of the proximal tibiofibular joint is presented."}
{"_id":"ba32db1ab445e123d3be8502f81d65b41a688fad","title":"Big data processing framework of learning weather information and road traffic collision using distributed CEP from CCTV video: Cognitive image processing","text":"Nowadays, the many traffic information are extracted from CCTV. The road occupancy, vehicle speed, accident detection, traffic collision and weather information are calculated in CCTV. These are big data comes from varying sources, such as, social sites or mobile phone GPS signals and so on. The Hadoop and HBase can store and analyze these real-time big data in a distributed processing framework. This framework can be designed as flexible and scalable framework using distributed CEP that process massive real-time traffic data and ESB that integrates other services. In this paper, we propose a new architecture for distributed processing that enables big data processing on the road traffic data with specially weather information and its related information analysis. We tested the proposed framework on road traffic data gathering 41 CCTV from Icheon to Gangneung freeway section in Korea. And the forecasting method is introduced at intermediate local area between CCTV."}
{"_id":"7deb82243f6f070e58ed4c40a68d24c2aa07c45f","title":"The Technology Behind Personal Digital Assistants: An overview of the system architecture and key components","text":"We have long envisioned that one day computers will understand natural language and anticipate what we need, when and where we need it, and proactively complete tasks on our behalf. As computers get smaller and more pervasive, how humans interact with them is becoming a crucial issue. Despite numerous attempts over the past 30 years to make language understanding (LU) an effective and robust natural user interface for computer interaction, success has been limited and scoped to applications that were not particularly central to everyday use. However, speech recognition and machine learning have continued to be refined, and structured data served by applications and content providers has emerged. These advances, along with increased computational power, have broadened the application of natural LU to a wide spectrum of everyday tasks that are central to a user's productivity. We believe that as computers become smaller and more ubiquitous [e.g., wearables and Internet of Things (IoT)], and the number of applications increases, both system-initiated and user-initiated task completion across various applications and web services will become indispensable for personal life management and work productivity. In this article, we give an overview of personal digital assistants (PDAs); describe the system architecture, key components, and technology behind them; and discuss their future potential to fully redefine human?computer interaction."}
{"_id":"625c0f0ab987f72262c1dbc2d0b6483c81ec0da8","title":"A multilevel automatic thresholding method based on a genetic algorithm for a fast image segmentation","text":"In this paper, a multilevel thresholding method which allows the determination of the appropriate number of thresholds as well as the adequate threshold values is proposed. This method combines a genetic algorithm with a wavelet transform. First, the length of the original histogram is reduced by using the wavelet transform. Based on this lower resolution version of the histogram, the number of thresholds and the threshold values are determined by using a genetic algorithm. The thresholds are then projected onto the original space. In this step, a refinement procedure may be added to detect accurate threshold values. Experiments and comparative results with multilevel thresholding methods over a synthetic histogram and real images show the efficiency of the proposed method. 2007 Elsevier Inc. All rights reserved."}
{"_id":"6f20f796bcdc63bdba431e734ec31f02e66f97cc","title":"Three tensions in participatory design for inclusion","text":"One ideal of Participatory Design (PD) is active involvement by all stakeholders as co-designers. However, when PD is applied to real projects, certain compromises are unavoidable, no matter what stakeholders are involved. With this paper we want to shed light on some of the challenges in implementing \"true\" PD in the case of designing with children, in particular children with severe disabilities. We do this work to better understand challenges in an ongoing project, RHYME, and by doing so we hope to provide insight and inspiration for others."}
{"_id":"36298c797f8f6e55c8b685b9768e30758704a81b","title":"The Distributed Simulation of Multi-Agent Systems","text":"Agent-based systems are increasingly being applied in a wide range of areas including telecommunications, business process modelling, computer games, control of mobile robots and military simulations. Such systems are typically extremely complex and it is often useful to be able to simulate an agent-based system to learn more about its behaviour or investigate the implications of alternative architectures. In this paper, we discuss the application of distributed discrete-event simulation techniques to the simulation of multi-agent systems. We identify the efficient distribution of the agents\u2019 environment as a key problem in the simulation of agent-based systems, and present an approach to the decomposition of the environment which facilitates load balancing. Keywords\u2014 agents, distributed simulation, interest management, load balancing."}
{"_id":"e63a1fafb12fb7cef9daa4535fee0e9136f0c47d","title":"Flexible Tactile Sensors Using Screen-Printed P(VDF-TrFE) and MWCNT\/PDMS Composites","text":"This paper presents and compares two different types of screen-printed flexible and conformable pressure sensors arrays. In both variants, the flexible pressure sensors are in the form of segmental arrays of parallel plate structure-sandwiching the piezoelectric polymer polyvinylidene fluoride trifluoroethylene [P(VDF-TrFE)] between two printed metal layers of silver (Ag) in one case and the piezoresistive [multiwall carbon nanotube (MWCNT) mixed with poly(dimethylsiloxane (PDMS)] layer in the other. Each sensor module consists of 4 \u00d7 4 sensors array with 1-mm \u00d7 1-mm sensitive area of each sensor. The screen-printed piezoelectric sensors array exploits the change in polarization level of P(VDF-TrFE) to detect dynamic tactile parameter such as contact force. Similarly, the piezoresistive sensors array exploits the change in resistance of the bulk printed layer of MWCNT\/PDMS composite. The two variants are compared on the basis of fabrication by printing on plastic substrate, ease of processing and handling of the materials, compatibility of the dissimilar materials in multilayers structure, adhesion, and finally according to the response to the normal compressive forces. The foldable pressure sensors arrays are completely realized using screen-printing technology and are targeted toward realizing low-cost electronic skin."}
{"_id":"2fd321ca02b89c93cc57f14bf4b2b0912dbd3893","title":"BodyAvatar: creating freeform 3D avatars using first-person body gestures","text":"BodyAvatar is a Kinect-based interactive system that allows users without professional skills to create freeform 3D avatars using body gestures. Unlike existing gesture-based 3D modeling tools, BodyAvatar centers around a first-person \"you're the avatar\" metaphor, where the user treats their own body as a physical proxy of the virtual avatar. Based on an intuitive body-centric mapping, the user performs gestures to their own body as if wanting to modify it, which in turn results in corresponding modifications to the avatar. BodyAvatar provides an intuitive, immersive, and playful creation experience for the user. We present a formative study that leads to the design of BodyAvatar, the system's interactions and underlying algorithms, and results from initial user trials."}
{"_id":"14df973438a9ba4634bb41740072b9e4704ba47b","title":"A New Concave Hull Algorithm and Concaveness Measure for n-dimensional Datasets","text":"Convex and concave hulls are useful concepts for a wide variety of application areas, such as pattern recognition, image processing, statistics, and classification tasks. Concave hull performs better than convex hull, but it is difficult to formulate and few algorithms are suggested. Especially, an n-dimensional concave hull is more difficult than a 2or 3dimensional one. In this paper, we propose a new concave hull algorithm for n-dimensional datasets. It is simple but creative. We show its application to dataset analysis. We also suggest a concaveness measure and a graph that captures geometric shape of an n-dimensional dataset. Proposed concave hull algorithm and concaveness measure\/graph are implemented using java, and are posted to http:\/\/user.dankook.ac.kr\/ ~bitl\/dkuCH."}
{"_id":"0c253bb9aee9aa1ae7909700eda845bd3124197f","title":"Neural Program Meta-Induction","text":"Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input\/output (I\/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a k-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language [17]. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I\/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance."}
{"_id":"19bdbf4925551a8e10579dc1ea6004c0ff9e2081","title":"Neural Programmer-Interpreters","text":"We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms."}
{"_id":"61d226578cf4ca7d434c498891aaf1d4086a2986","title":"Making Neural Programming Architectures Generalize via Recursion","text":"Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion."}
{"_id":"b5a369ebcb2e9a8169b71791d77e7a3ad992870f","title":"Synthesizing Programs for Images using Reinforced Adversarial Learning","text":"Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator\u2019s output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, OMNIGLOT, CELEBA) and synthetic 3D datasets. A video of the agent can be found at https:\/\/youtu.be\/iSyvwAwa7vk."}
{"_id":"a7894eb5c465b8b7e7c1b486a5a5f8f20184149a","title":"On the Cache Behavior of SPLASH-2 Benchmarks on ARM and ALPHA Processors in Gem5 Full System Simulator","text":"Today cache size and hierarchy level of caches play an important role in improving computer performance. By using full system simulations of gem5, the variation in memory bandwidth, system bus throughput, L1 and L2 cache size misses are measured by running SPLASH-2 Benchmarks on ARM and ALPHA Processors. In this work we calculate cache misses, memory bandwidth and system bus throughput by running SPLASH2 benchmarks on gem5 Full System Mode. Our results show that L1 cache misses decrease as L1 cache size is varied from 16KB to 64KB. L1 cache misses are independent of L2 cache size after the program data resides in L2 cache. The memory bandwidth and system bus throughput decreases as L1 and L2 cache size increases."}
{"_id":"0e8080e768d58d5acd4ad7a4d83bccb61ec867c2","title":"Multi-connectivity in 5G mmWave cellular networks","text":"The millimeter wave (mmWave) frequencies offer the potential of orders of magnitude increases in capacity for next-generation cellular wireless systems. However, links in mmWave networks are highly susceptible to blocking and may suffer from rapid variations in quality. Connectivity to multiple cells - both in the mmWave and in the traditional lower frequencies - is thus considered essential for robust connectivity. However, one of the challenges in supporting multi-connectivity in the mmWave space is the requirement for the network to track the direction of each link in addition to its power and timing. With highly directional beams and fast varying channels, this directional tracking may be the main bottleneck in realizing robust mmWave networks. To address this challenge, this paper proposes a novel measurement system based on (i) the UE transmitting sounding signals in directions that sweep the angular space, (ii) the mmWave cells measuring the instantaneous received signal strength along with its variance to better capture the dynamics and, consequently, the reliability of a channel\/direction and, finally, (iii) a centralized controller making handover and scheduling decisions based on the mmWave cell reports and transmitting the decisions either via a mmWave cell or conventional microwave cell (when control signaling paths are not available). We argue that the proposed scheme enables efficient and highly adaptive cell selection in the presence of the channel variability expected at mmWave frequencies."}
{"_id":"f5e9eddfa9e26ac66b29606353d166dfc73223f5","title":"Value creation process in the fast fashion industry . Towards a networking approach < Value co-creation and the changing role of suppliers and customers >","text":"Purpose \u2013 Quick response, short product life cycles, customer-centric businesses, agile supply chains and reduction of lead times are considered to be the core business strategies in the sector of fast fashion. The work is an attempt to identify the most revealing management and organizational tools that support the final value creation and service delivery processes in the international industry of fast fashion Design\/Methodology\/approach \u2013 In order to fulfill such a purpose, the paper detects the sector through the recent developments from Service-Dominant (S-D) Logic and Network Theory scientific proposals Findings \u2013 Value is a co-created performance, requiring several actors participation. In the fast fashion, such actors are represented by textile businesses, providers, retailers, stores, customers. They all interact within a supply chain becoming a value chain in which stakeholders interact with each other contributing to the final value generation. At the light of this research, it seems to be restrictive to identify in the lead time reduction the success factor of the fast fashion businesses, but it should be pursued in politics of integration, interaction, co-creation and sharing throughout the chain. Fast fashion is an example of totally integrated global chain working and performing as a whole business in which every single component co-creates the final mutual benefit Practical implications \u2013 Textile organizations taking part in the network represented by fast fashion are called to re-organize their activities in function of a highly developed integration level and so to move towards a networking approach, fostering the mutual exchange of resources in order to co-create a common value Originality\/value \u2013 The paper adds value to the study of the fast fashion industry, valorizing the importance of the co-creating perspective within the supply chain management"}
{"_id":"00c06dbe4d54181424f3a948d6a6e8f3dc918015","title":"Efficient Road Detection and Tracking for Unmanned Aerial Vehicle","text":"An unmanned aerial vehicle (UAV) has many applications in a variety of fields. Detection and tracking of a specific road in UAV videos play an important role in automatic UAV navigation, traffic monitoring, and ground-vehicle tracking, and also is very helpful for constructing road networks for modeling and simulation. In this paper, an efficient road detection and tracking framework in UAV videos is proposed. In particular, a graph-cut-based detection approach is given to accurately extract a specified road region during the initialization stage and in the middle of tracking process, and a fast homography-based road-tracking scheme is developed to automatically track road areas. The high efficiency of our framework is attributed to two aspects: the road detection is performed only when it is necessary and most work in locating the road is rapidly done via very fast homography-based tracking. Experiments are conducted on UAV videos of real road scenes we captured and downloaded from the Internet. The promising results indicate the effectiveness of our proposed framework, with the precision of 98.4% and processing 34 frames per second for 1046 \u00d7 595 videos on average."}
{"_id":"3c2724d49dec10f1bff401fbd66a8e1e6c7b1030","title":"Inferring new indications for approved drugs via random walk on drug-disease heterogenous networks","text":"Since traditional drug research and development is often time-consuming and high-risk, there is an increasing interest in establishing new medical indications for approved drugs, referred to as drug repositioning, which provides a relatively low-cost and high-efficiency approach for drug discovery. With the explosive growth of large-scale biochemical and phenotypic data, drug repositioning holds great potential for precision medicine in the post-genomic era. It is urgent to develop rational and systematic approaches to predict new indications for approved drugs on a large scale. In this paper, we propose the two-pass random walks with restart on a heterogenous network, TP-NRWRH for short, to predict new indications for approved drugs. Rather than random walk on bipartite network, we integrated the drug-drug similarity network, disease-disease similarity network and known drug-disease association network into one heterogenous network, on which the two-pass random walks with restart is implemented. We have conducted performance evaluation on two datasets of drug-disease associations, and the results show that our method has higher performance than six existing methods. A case study on the Alzheimer\u2019s disease showed that nine of top 10 predicted drugs have been approved or investigational for neurodegenerative diseases. The experimental results show that our method achieves state-of-the-art performance in predicting new indications for approved drugs. We proposed a two-pass random walk with restart on the drug-disease heterogeneous network, referred to as TP-NRWRH, to predict new indications for approved drugs. Performance evaluation on two independent datasets showed that TP-NRWRH achieved higher performance than six existing methods on 10-fold cross validations. The case study on the Alzheimer\u2019s disease showed that nine of top 10 predicted drugs have been approved or are investigational for neurodegenerative diseases. The results show that our method achieves state-of-the-art performance in predicting new indications for approved drugs."}
{"_id":"0694beaf14f558a75f2dc32f64d151e505dbee17","title":"Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning","text":"Zero-Shot Learning (ZSL) is generally achieved via aligning the semantic relationships between the visual features and the corresponding class semantic descriptions. However, using the global features to represent fine-grained images may lead to sub-optimal results since they neglect the discriminative differences of local regions. Besides, different regions contain distinct discriminative information. The important regions should contribute more to the prediction. To this end, we propose a novel stacked semantics-guided attention (SGA) model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions. Feeding both the integrated visual features and the class semantic features into a multi-class classification architecture, the proposed framework can be trained end-to-end. Extensive experimental results on CUB and NABird datasets show that the proposed approach has a consistent improvement on both fine-grained zero-shot classification and retrieval tasks."}
{"_id":"0c7223f863f0736e541ab8324190ade539f00308","title":"Monitoring of gait performance using dynamic time warping on IMU-sensor data","text":"In this paper, a novel method for monitoring the changes in gait joint angle trajectories recorded using the low-cost and wearable Inertial Measurement Units (IMU) is presented. The introduced method is based on Dynamic Time Warping (DTW), an algorithm commonly used for evaluating the similarity of two time series which may vary in time and speed. DTW is employed as the measure of distance between two gait trajectories taken in different time instances, which could be used as an intuitive and effective measure for the evaluation of gait performances. The experimental results presented in the paper demonstrate that the proposed method is applicable for clinically relevant applications and is consequently adaptable to patients with diseases characterized with gait disorders and to different walking scenarios. The proposed method was firstly validated by applying the DTW-based measure on gait trajectories of five healthy subjects recorded while simulating different levels of walking disabilities. Then proposed measure was applied to estimate the distance between the \u201chealthy\u201d gait trajectories and gait trajectories of three patients with Parkinson's disease (PD) while performing single-task and dual-task overground walking. Also, the proposed measure was demonstrated as an effective measure for monitoring the changes in gait patterns of a PD patient before and after medication-based treatment. This result indicates potential use of proposed method for effective pharmacological management of PD."}
{"_id":"f1699de0ba1789457ef5cfe949c6c7a5d92edf75","title":"Evolved Machines Shed Light on Robustness and Resilience","text":"In biomimetic engineering, we may take inspiration from the products of biological evolution: we may instantiate biologically realistic neural architectures and algorithms in robots, or we may construct robots with morphologies that are found in nature. Alternatively, we may take inspiration from the process of evolution: we may evolve populations of robots in simulation and then manufacture physical versions of the most interesting or more capable robots that evolve. If we follow this latter approach and evolve both the neural and morphological subsystems of machines, we can perform controlled experiments that provide unique insight into how bodies and brains can work together to produce adaptive behavior, regardless of whether such bodies and brains are instantiated in a biological or technological substrate. In this paper, we review selected projects that use such methods to investigate the synergies and tradeoffs between neural architecture, morphology, action, and adaptive behavior."}
{"_id":"6caa85183b231c2dc86eb65faa53a7e747c9fd16","title":"An Introduction to Locally Linear Embedding","text":"Many problemsin informationprocessinginvolvesomeform of dimensionality reduction.Herewe describelocally linearembedding(LLE), anunsupervisedlearningalgorithmthat computeslow dimensional,neighborhood preservingembeddingsof high dimensionaldata.LLE attemptsto discover nonlinearstructurein high dimensionaldataby exploiting thelocal symmetries of linear reconstructions.Notably, LLE mapsits inputs into a single global coordinatesystemof lower dimensionality, and its optimizations\u2014 thoughcapableof generatinghighly nonlinearembeddings\u2014donot involve localminima.We illustratethemethodon imagesof lips usedin audiovisual speechsynthesis."}
{"_id":"25e46de665dfc07ffb304a0311f311a2979638b3","title":"RECENT DEVELOPMENTS IN PAPER CURRENCY RECOGNITION SYSTEM","text":"Currency denomination recognition is one the active research topics at present. And this wide interest is particularly due to the various potential applications it has. Monetary transaction is an integral part of our day to day activities. However, blind people particularly suffer in monetary transactions. They are not able to effectively distinguish between various denominations and are often deceived by other people. Also, a reliable currency recognition system could be used in any sector wherever monetary transaction is of concern. Thus, there is an ardent need to design a system that is helpful in recognition of paper currency notes correctly. Currency denomination detection is a vast area of research and significant progress had been achieved over the years. This paper presents an extensive survey of research on various developments in recent years in identification of currency denomination. A number of techniques applied by various researchers are discussed briefly in order to assess the state of art."}
{"_id":"6b2e0cdd23a6ab3b98ea7134d85f0adffaed0ae2","title":"The effect of substrate noise on VCO performance","text":"This study characterizes the effect of substrate noise on a standard component of the RF front end: the voltage controlled oscillator (VCO), as well as evaluating the effect of VCO bias current and guard rings on noise performance. Frequency effects of substrate noise are also examined through the study of VCOs at three different center frequencies: 900 MHz, 2.4 GHz, and 5.2 GHz. Substrate noise is a serious problem that continues to plague mixed-signal designs. Components of the RF frontend are particularly sensitive to substrate noise as the effectiveness of standard isolation techniques degrades at higher frequencies. This study has shown that the phase noise of a VCO is adversely affected by substrate noise. In the extreme, the VCO can lock to the substrate noise. Guard rings can effectively attenuate substrate noise at lower frequencies. For example, at 900 MHz, as much as 25 dB of isolation is observed. At 5.2 GHz, the isolation reduces to 10 dB. Furthermore, the use of guard rings can improve the response of the VCO to injection locking."}
{"_id":"f6daad3b3d228e9d4324c3af4c3b2404b3d7fc68","title":"Delay-Constrained Hybrid Computation Offloading With Cloud and Fog Computing","text":"To satisfy the delay constraint, the computation tasks can be offloaded to some computing servers, referred to as offloading destinations. Different to most of existing works which usually consider only a single type of offloading destinations, in this paper, we study the hybrid computation offloading problem considering diverse computation and communication capabilities of two types of offloading destinations, i.e., cloud computing servers and fog computing servers. The aim is to minimize the total energy consumption for both communication and computation while completing the computation tasks within a given delay constraint. It is quite challenging because the delay cannot be easily formulated as an explicit expression but depends on the embedded communication-computation scheduling problem for the computation offloading to different destinations. To solve the computation offloading problem, we first define a new concept named computation energy efficiency and divide the problem into four subproblems according to the computation energy efficiency of different types of computation offloading and the maximum tolerable delay. For each subproblem, we give a closed-form computation offloading solution with the analysis of communication-computation scheduling under the delay constraint. The numerical results show that the proposed hybrid computation offloading solution achieves lower energy consumption than the conventional single-type computation offloading under the delay constraint."}
{"_id":"b6bbb228300c72f141a2f05702ddc7f8ab4a8297","title":"Information Security: End User Behavior and Corporate Culture","text":"Information is the life blood of all modern organizations yet the news media continue to report stories of critical information loss. The purpose of information security is to protect valuable assets, such as information, hardware, software and people. The majority of information security specialists believe that promoting good end user behavior and constraining bad end user behavior is an important component of an effective Information Security Management System (ISMS). Implementing effective information security involves understanding security-related risk, then developing and implementing appropriate controls. In general the better employees are at applying the controls the more secure the organization will be, because even the best designed technical controls and procedures will be of limited value if the staff involved do not understand why they have been implemented and what they are accomplishing. Achieving the required level of understanding usually requires more than an annual awareness training initiative and represents a major challenge for most organizations. In fact, for many organizations it will involve a cultural change to ensure the integration of information security concepts into the organizational culture."}
{"_id":"a13dc9739e4637599359d792fd60d511ab8a016e","title":"Robust visual inertial odometry using a direct EKF-based approach","text":"In this paper, we present a monocular visual-inertial odometry algorithm which, by directly using pixel intensity errors of image patches, achieves accurate tracking performance while exhibiting a very high level of robustness. After detection, the tracking of the multilevel patch features is closely coupled to the underlying extended Kalman filter (EKF) by directly using the intensity errors as innovation term during the update step. We follow a purely robocentric approach where the location of 3D landmarks are always estimated with respect to the current camera pose. Furthermore, we decompose landmark positions into a bearing vector and a distance parametrization whereby we employ a minimal representation of differences on a corresponding \u03c3-Algebra in order to achieve better consistency and to improve the computational performance. Due to the robocentric, inverse-distance landmark parametrization, the framework does not require any initialization procedure, leading to a truly power-up-and-go state estimation system. The presented approach is successfully evaluated in a set of highly dynamic hand-held experiments as well as directly employed in the control loop of a multirotor unmanned aerial vehicle (UAV)."}
{"_id":"0bd6442092bc4a9e0e77cd2f302f2db1a242e250","title":"IoT-based continuous glucose monitoring system: A feasibility study","text":"Health monitoring systems based on Internet-of-things (IoT) have been recently introduced to improve the quality of health care services. However, the number of advanced IoT-based continuous glucose monitoring systems is small and the existing systems have several limitations. In this paper we study feasibility of invasive and continuous glucose monitoring (CGM) system utilizing IoT based approach. We designed an IoT-based system architecture from a sensor device to a back-end system for presenting real-time glucose, body temperature and contextual data (i.e. environmental temperature) in graphical and human-readable forms to end-users such as patients and doctors. In addition, nRF communication protocol is customized for suiting to the glucose monitoring system and achieving a high level of energy efficiency. Furthermore, we investigate energy consumption of the sensor device and design energy harvesting units for the device. Finally, the work provides many advanced services at a gateway level such as a push notification service for notifying patient and doctors in case of abnormal situations (i.e. too low or too high glucose level). The results show that our system is able to achieve continuous glucose monitoring remotely in real-time. In addition, the results reveal that a high level of energy efficiency can be achieved by applying the customized nRF component, the power management unit and the energy harvesting unit altogether in the sensor device. c \u00a9 2017 The Authors. Published by E sevier B.V. i ilit f t f re ce r ra hairs."}
{"_id":"d30dbdde4f25d3702348bd471d02b33806d3637a","title":"Predicting Categorical Emotions by Jointly Learning Primary and Secondary Emotions through Multitask Learning","text":"Detection of human emotions is an essential part of affect-aware human-computer interaction (HCI). In daily conversations, the preferred way of describing affects is by using categorical emotion labels (e.g., sad, anger, surprise). In categorical emotion classification, multiple descriptors (with different degrees of relevance) can be assigned to a sample. Perceptual evaluations have relied on primary and secondary emotions to capture the ambiguous nature of spontaneous recordings. Primary emotion is the most relevant category felt by the evaluator. Secondary emotions capture other emotional cues also conveyed in the stimulus. In most cases, the labels collected from the secondary emotions are discarded, since assigning a single class label to a sample is preferred from an application perspective. In this work, we take advantage of both types of annotations to improve the performance of emotion classification. We collect the labels from all the annotations available for a sample and generate primary and secondary emotion labels. A classifier is then trained using multitask learning with both primary and secondary emotions. We experimentally show that considering secondary emotion labels during the learning process leads to relative improvements of 7.9% in F1-score for an 8-class emotion classification task."}
{"_id":"92bfd82eceb67112d1db1c7378144007dc3aa247","title":"A New Concept Varistor With Epoxy\/Microvaristor Composite","text":"A new concept composite varistor is proposed in this paper. This composite varistor has the chains of microvaristors, which are formed by applying an electric field during the curing process and work as current paths. This composite varistor shows superior nonlinear voltage-current characteristics despite small microvaristor content and has a good response against a steep voltage surge of several tens of nanoseconds in front time. In comparison with ceramic varistors where shapes are usually formed at their sintering temperatures of around 1000\u00b0C, the composite varistor has an advantage in flexible shape because its shape is formed by curing the base polymer at a low temperature."}
{"_id":"f7d997a640f2b804676cadb8030d8b2c7bd79d85","title":"On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation","text":"Model selection strategies for machine learning algorithm s typically involve the numerical optimisation of an appropriate model selection criterion, ofte n based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can b e broken down into bias and variance components. While unbiasedness is oft en cited as a beneficial quality of a model selection criterion, we demonstrate that a low varian ce is at least as important, as a nonnegligible variance introduces the potential for over-fitt ing in model selection as well as in training the model. While this observation is in hindsight perhaps rat her obvious, the degradation in performance due to over-fitting the model selection criterion can b e surprisingly large, an observation that appears to have received little attention in the machine lea rning literature to date. In this paper, we show that the effects of this form of over-fitting are often of c mparable magnitude to differences in performance between learning algorithms, and thus canno t be ignored in empirical evaluation. Furthermore, we show that some common performance evaluati on practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and sub sequent selection bias in performance evaluation, which we hope will be incorporated into best pra ctice. While this study concentrates on cross-validation based model selection, the findings are quit general and apply to any model selection practice involving the optimisation of a model se lection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evid nce and optimisation of performance bounds."}
{"_id":"4fb938a0205995244dbff84f72b533b9b3fecd0d","title":"Design of a 1.8V-input 6.5V-output digital level shifter for trimming application","text":"A 1.8V input, 6.5V output step-up digital level shifter is proposed. It is based on contention mitigated digital level shifter (CMLS) but it uses single supply only instead of two. It uses a complete, stand-alone, and unregulated voltage charge pump circuit that generates a 6.5V output voltage from a 1.8V input. Charge pump output is then used as high-side supply (VDDH) of the step-up level shifter, while the low-side supply (VDDl) is externally-supplied 1.8V. Compared to the conventional level shifter where two separate supply voltages (VDDl and VDDH) are required, the proposed design uses a single supply to shifted-up a digital signals. This technique simplifies the user-end power supply requirement of the system, thus the proposed level shifter design is applicable in many low-power and power-constraint applications such as in mobile and portable devices application. The proposed design is implemented using 0.35-\u03bcm TSMC CMOS technology. The schematic and the nestlist have been designed and extracted using LTSPICE tool; while the simulation has been carried out using the HSPICE circuit simulator."}
{"_id":"49d0f7ae8ccd2b859ca1cb481aa57c5e84b17718","title":"Lazy Theta*: Any-Angle Path Planning and Path Length Analysis in 3D","text":"Grids with blocked and unblocked cells are often used to represent continuous 2D and 3D environments in robotics and video games. The shortest paths formed by the edges of 8neighbor 2D grids can be up to \u2248 8% longer than the shortest paths in the continuous environment. Theta* typically finds much shorter paths than that by propagating information along graph edges (to achieve short runtimes) without constraining paths to be formed by graph edges (to find short \u201cany-angle\u201d paths). We show in this paper that the shortest paths formed by the edges of 26-neighbor 3D grids can be \u2248 13% longer than the shortest paths in the continuous environment, which highlights the need for smart path planning algorithms in 3D. Theta* can be applied to 3D grids in a straight-forward manner, but it performs a line-of-sight check for each unexpanded visible neighbor of each expanded vertex and thus it performs many more line-of-sight checks per expanded vertex on a 26-neighbor 3D grid than on an 8-neighbor 2D grid. We therefore introduce Lazy Theta*, a variant of Theta* which uses lazy evaluation to perform only one line-of-sight check per expanded vertex (but with slightly more expanded vertices). We show experimentally that Lazy Theta* finds paths faster than Theta* on 26-neighbor 3D grids, with one order of magnitude fewer line-of-sight checks and without an increase in path length."}
{"_id":"1067ef2c4d8c73bb710add5c7bfe35dd74bcb98a","title":"Mechanisms of facial emotion recognition in autism spectrum disorders: Insights from eye tracking and electroencephalography","text":"While behavioural difficulties in facial emotion recognition (FER) have been observed in individuals with Autism Spectrum Disorder (ASD), behavioural studies alone are not suited to elucidate the specific nature of FER challenges in ASD. Eye tracking (ET) and electroencephalography (EEG) provide insights in to the attentional and neurological correlates of performance, and may therefore provide insight in to the mechanisms underpinning FER in ASD. Given that these processes develop over the course of the developmental trajectory, there is a need to synthesise findings in regard to the developmental stages to determine how the maturation of these systems may impact FER in ASD. We conducted a systematic review of fifty-four studies investigating ET or EEG meeting inclusion criteria. Findings indicate divergence of visual processing pathways in individuals with ASD. Altered function of the social brain in ASD impacts the processing of facial emotion across the developmental trajectory, resulting in observable differences in ET and EEG outcomes."}
{"_id":"22b2bcdbabc47757505cceaeefe8d93be2d2d1ae","title":"Adaptive Deep Brain Stimulation In Advanced Parkinson Disease","text":"OBJECTIVE\nBrain-computer interfaces (BCIs) could potentially be used to interact with pathological brain signals to intervene and ameliorate their effects in disease states. Here, we provide proof-of-principle of this approach by using a BCI to interpret pathological brain activity in patients with advanced Parkinson disease (PD) and to use this feedback to control when therapeutic deep brain stimulation (DBS) is delivered. Our goal was to demonstrate that by personalizing and optimizing stimulation in real time, we could improve on both the efficacy and efficiency of conventional continuous DBS.\n\n\nMETHODS\nWe tested BCI-controlled adaptive DBS (aDBS) of the subthalamic nucleus in 8 PD patients. Feedback was provided by processing of the local field potentials recorded directly from the stimulation electrodes. The results were compared to no stimulation, conventional continuous stimulation (cDBS), and random intermittent stimulation. Both unblinded and blinded clinical assessments of motor effect were performed using the Unified Parkinson's Disease Rating Scale.\n\n\nRESULTS\nMotor scores improved by 66% (unblinded) and 50% (blinded) during aDBS, which were 29% (p = 0.03) and 27% (p = 0.005) better than cDBS, respectively. These improvements were achieved with a 56% reduction in stimulation time compared to cDBS, and a corresponding reduction in energy requirements (p < 0.001). aDBS was also more effective than no stimulation and random intermittent stimulation.\n\n\nINTERPRETATION\nBCI-controlled DBS is tractable and can be more efficient and efficacious than conventional continuous neuromodulation for PD."}
{"_id":"7628d73c73b8ae20c6cf866ce0865a1fb64612a3","title":"Image Texture Feature Extraction Using GLCM Approach","text":"Feature Extraction is a method of capturing visual content of images for indexing & retrieval. Primitive or low level image features can be either general features, such as extraction of color, texture and shape or domain specific features. This paper presents an application of gray level co-occurrence matrix (GLCM) to extract second order statistical texture features for motion estimation of images. The Four features namely, Angular Second Moment, Correlation, Inverse Difference Moment, and Entropy are computed using Xilinx FPGA. The results show that these texture features have high discrimination accuracy, requires less computation time and hence efficiently used for real time Pattern recognition applications."}
{"_id":"a1521e7108979598baecde9c5ef28ed0ca78cd7e","title":"Plant Leaf Recognition using Shape Based Features and Neural Network Classifiers","text":"This paper proposes an automated system for recognizing plant species based on leaf images. Plant leaf images corresponding to three plant types, are analyzed using two different shape modeling techniques, the first based on the Moments-Invariant (M-I) model and the second on the CentroidRadii (C-R) model. For the M-I model the first four normalized central moments have been considered and studied in various combinations viz. individually, in joint 2-D and 3-D feature spaces for producing optimum results. For the C-R model an edge detector has been used to identify the boundary of the leaf shape and 36 radii at 10 degree angular separation have been used to build the feature vector. To further improve the accuracy, a hybrid set of features involving both the M-I and C-R models has been generated and explored to find whether the combination feature vector can lead to better performance. Neural networks are used as classifiers for discrimination. The data set consists of 180 images divided into three classes with 60 images each. Accuracies ranging from 90%-100% are obtained which are comparable to the best figures reported in extant literature. Keywords-plant recognition; moment invariants; centroid-radii model; neural network; computer vision."}
{"_id":"a2e0aabcf0447c5960d5e624855cc8347bef723c","title":"Images Features Extraction of Tobacco Leaves","text":"Images features extraction is very important for the grading process of flue-cured tobacco leaves. In this paper, a machine vision techniques base system is proposed for the automatic inspection of flue-cured tobacco leaves. Machine vision techniques are used in this system to solve problems of features extraction and analysis of tobacco leaves, which include features of color, size, shape andsurface texture. The experimental results show that this system is a viable way for the features extraction of tobacco leaves, and can be used for the automatic classification of tobacco leaves."}
{"_id":"ddf1f27694928a729aefa6ffd6faabfd8ebf2842","title":"PLANT LEAF RECOGNITION","text":"This paper proposes an automated system for recognizing plant species based on leaf images. Plant leaf images corresponding to three plant types, are analyzed using three different shape modelling techniques, the first two based on the Moments-Invariant (M-I) model and the Centroid-Radii (C-R) model and the third based on a proposed technique of Binary-Superposition (B-S). For the M-I model the first four central normalized moments have been considered. For the C-R model an edge detector has been used to identify the boundary of the leaf shape and 36 radii at 10 degree angular separation have been used to build the shape vector. The proposed approach consists of comparing binary versions of the leaf images through superposition and using the sum of non-zero pixel values of the resultant as the feature vector. The data set for experimentations consists of 180 images divided into training and testing sets and comparison between them is done using Manhattan, Euclidean and intersection norms. Accuracies obtained using the proposed technique is seen to be an improvement over the M-I and C-R based techniques, and comparable to the best figures reported in extant literature."}
{"_id":"13754d470d8b5bf060ff42aa5a93b743f8be74a3","title":"COLOUR AND SHAPE ANALYSIS TECHNIQUES FOR WEED DETECTION IN CEREAL FIELDS","text":"Information on weed distribution within the field is necessary to implement spatially variable herbicide application. This paper deals with the development of near-ground image capture and processing techniques in order to detect broad leaf weeds in cereal crops, under actual field conditions. The proposed methods use both colour and shape analysis techniques for discriminating crop, weeds and soil. The performance of algorithms was assessed by comparing the results with a human classification, providing a good success rate. The study shows the potential of using image processing techniques to generate weed maps."}
{"_id":"fe3e91e40a950c6b6601b8f0a641884774d949ae","title":"Distributional Reinforcement Learning With Quantile Regression","text":"In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51."}
{"_id":"399227dca4dda780db053a7e05138c0851d86110","title":"A 28-GHz 32-Element TRX Phased-Array IC With Concurrent Dual-Polarized Operation and Orthogonal Phase and Gain Control for 5G Communications","text":"This paper presents the first reported 28-GHz phased-array IC for 5G communications. Implemented in 130-nm SiGe BiCMOS, the IC includes 32 TRX elements and features concurrent independent beams in two polarizations in either TX or RX operation. Circuit techniques to enable precise beam steering, orthogonal phase and amplitude control at each front end, and independent tapering and beam steering at the array level are presented. A TX\/RX switch design is introduced which minimizes TX path loss resulting in 13.5 dBm\/16 dBm Op1dB\/Psat per front end with >20% peak power added efficiency of the power amplifier (including switch and off-mode LNA) while maintaining a 6 dB noise figure in the low noise amplifier (including switch and off-mode PA). Comprehensive on-wafer measurement results for the IC across multiple samples and temperature variation are presented. A package with four ICs and 64 dual-polarized antennas provides eight 16-element or two 64-element concurrent beams with 1.4\u00b0\/step beam steering (<0.6\u00b0 rms error) across a \u00b150\u00b0 steering range without requiring calibration. A maximum saturated effective isotropic radiated power of 54 dBm is measured in the broadside direction for each polarization. Tapering control without requiring calibration achieves up to 20-dB sidelobe rejection without affecting the main lobe direction."}
{"_id":"aa8a8890421402a27067bc47c1be7a49ac772ccf","title":"A study on brushless PM slotless motor with toroidal winding","text":"This paper presents a study on brushless permanent magnet (PM) slotless motor with toroidal winding (TWSL). Herein, slotted motor with conventional winding (i.e., CWS motor), which has already been applied to the industry, is compared to the TWSL motor. A finite-element analysis is used to comprehend the operation basis of the TWSL motor, including the linkage flux, back electromotive force, and torque. The proposed TWSL motor can generate the torque level as much as the CWS motor does. The proposed TWSL motor is manufactured and experimented upon to validate the finite-element analysis result."}
{"_id":"0d9c20b221b12828182713b3b2f0f2e900a0afac","title":"Efficient Adaptive-Support Association Rule Mining for Recommender Systems","text":"Collaborative recommender systems allow personalization for e-commerce by exploiting similarities and dissimilarities among customers' preferences. We investigate the use of association rule mining as an underlying technology for collaborative recommender systems. Association rules have been used with success in other domains. However, most currently existing association rule mining algorithms were designed with market basket analysis in mind. Such algorithms are inefficient for collaborative recommendation because they mine many rules that are not relevant to a given user. Also, it is necessary to specify the minimum support of the mined rules in advance, often leading to either too many or too few rules; this negatively impacts the performance of the overall system. We describe a collaborative recommendation technique based on a new algorithm specifically designed to mine association rules for this purpose. Our algorithm does not require the minimum support to be specified in advance. Rather, a target range is given for the number of rules, and the algorithm adjusts the minimum support for each user in order to obtain a ruleset whose size is in the desired range. Rules are mined for a specific target user, reducing the time required for the mining process. We employ associations between users as well as associations between items in making recommendations. Experimental evaluation of a system based on our algorithm reveals performance that is significantly better than that of traditional correlation-based approaches."}
{"_id":"2fcca028e02af8e21abf82ce3ddabab6b1e259ae","title":"Denial of Service in Sensor Networks","text":"S ensor networks hold the promise of facilitating large-scale, real-time data processing in complex environments. Their foreseeable applications will help protect and monitor critical military, environmental, safety-critical , or domestic infrastructures and resources. In these and other vital or security-sensitive deployments, keeping the network available for its intended use is essential. The stakes are high: Denial of service attacks against such networks may permit real-world damage to the health and safety of people. Without proper security mechanisms, networks will be confined to limited, controlled environments , negating much of the promise they hold. The limited ability of individual sensor nodes to thwart failure or attack makes ensuring network availability more difficult. To identify denial of service vulnerabilities, we analyze two effective sensor network protocols that did not initially consider security. These examples demonstrate that consideration of security at design time is the best way to ensure successful network deployment. Advances in miniaturization combined with an insatiable appetite for previously unrealizable information gathering have lead to the development of new kinds of networks. In many areas, static infrastruc-tures are giving way to dynamic ad-hoc networks. One manifestation of these trends is the development of highly application-dependent sensor networks. Developers build sensor networks to collect and analyze low-level data from an environment of interest. Accomplishing the network's goal often depends on local cooperation, aggregation, or data processing because individual nodes have limited capabilities. Physically small, nodes have tiny or irreplaceable power reserves, communicate wirelessly, and may not possess unique identifiers. Further, they must form ad hoc relationships in a dense network with little or no preexisting infrastructure. Protocols and algorithms operating in the network must support large-scale distribution, often with only localized interactions among nodes. The network must continue operating even after significant node failure, and it must meet real-time requirements. In addition to the limitations imposed by application-dependent deadlines, because it reflects a changing environment, the data the network gathers may intrinsically be valid for only a short time. Sensor networks may be deployed in a host of different environments, and they often figure into military scenarios. These networks may gather intelligence in battlefield conditions, track enemy troop movements, monitor a secured zone for activity , or measure damage and casualties. An airplane or artillery 1 could deploy these networks to otherwise unreachable regions. Although military applications may be the easiest to imagine, much broader opportunities await. Sensor networks could form \u2026"}
{"_id":"234f46400747a34918882c5d7f5872e03d344bea","title":"Modeling consumer loan default prediction using ensemble neural networks","text":"In this paper, a loan default prediction model is constricted using three different training algorithms, to train a supervised two-layer feed-forward network to produce the prediction model. But first, two attribute filtering functions were used, resulting in two data sets with reduced attributes and the original data-set. Back propagation based learning algorithms was used for training the network. The neural networks are trained using real world credit application cases from a German bank datasets which has 1000 cases; each case with 24 numerical attributes; upon, which the decision is based. The aim of this paper was to compare between the resulting models produced from using different training algorithms, scaled conjugate gradient backpropagation, Levenberg-Marquardt algorithm, One-step secant backpropagation (SCG, LM and OSS) and an ensemble of SCG, LM and OSS. Empirical results indicate that training algorithms improve the design of a loan default prediction model and ensemble model works better than the individual models."}
{"_id":"4524e91bb59167c6babc54c7b0c7656beda342fe","title":"IAP Guidelines on Rickettsial Diseases in Children.","text":"OBJECTIVE\nTo formulate practice guidelines on rickettsial diseases in children for pediatricians across India.\n\n\nJUSTIFICATION\nRickettsial diseases are increasingly being reported from various parts of India. Due to low index of suspicion, nonspecific clinical features in early course of disease, and absence of easily available, sensitive and specific diagnostic tests, these infections are difficult to diagnose. With timely diagnosis, therapy is easy, affordable and often successful. On the other hand, in endemic areas, where healthcare workers have high index of suspicion for these infections, there is rampant and irrational use of doxycycline as a therapeutic trial in patients of undifferentiated fevers. Thus, there is a need to formulate practice guidelines regarding rickettsial diseases in children in Indian context.\n\n\nPROCESS\nA committee was formed for preparing guidelines on rickettsial diseases in children in June 2016. A meeting of consultative committee was held in IAP office, Mumbai and scientific content was discussed. Methodology and results were scrutinized by all members and consensus was reached. Textbook references and published guidelines were also used in few instances to make recommendations. Various Indian and international publications pertinent to present study were collated and guidelines were approved by all committee members. Future updates in these guidelines will be dictated by new scientific data in the field of rickettsial diseases in children.\n\n\nRECOMMENDATIONS\nIndian tick typhus and scrub typhus are commonly seen rickettsial diseases in India. It is recommended that practicing pediatricians should be well conversant with compatible clinical scenario, suggestive epidemiological features, differential diagnoses and suggestive laboratory features to make diagnosis and avoid over diagnosis of these infections, as suggested in these guidelines. Doxycycline is the drug of choice and treatment should begin promptly without waiting for confirmatory laboratory results."}
{"_id":"40e0588779c473cf56a09d2b5bb0af00a8cdb8f0","title":"Traffic prediction in a bike-sharing system","text":"Bike-sharing systems are widely deployed in many major cities, providing a convenient transportation mode for citizens' commutes. As the rents\/returns of bikes at different stations in different periods are unbalanced, the bikes in a system need to be rebalanced frequently. Real-time monitoring cannot tackle this problem well as it takes too much time to reallocate the bikes after an imbalance has occurred. In this paper, we propose a hierarchical prediction model to predict the number of bikes that will be rent from\/returned to each station cluster in a future period so that reallocation can be executed in advance. We first propose a bipartite clustering algorithm to cluster bike stations into groups, formulating a two-level hierarchy of stations. The total number of bikes that will be rent in a city is predicted by a Gradient Boosting Regression Tree (GBRT). Then a multi-similarity-based inference model is proposed to predict the rent proportion across clusters and the inter-cluster transition, based on which the number of bikes rent from\/ returned to each cluster can be easily inferred. We evaluate our model on two bike-sharing systems in New York City (NYC) and Washington D.C. (D.C.) respectively, confirming our model's advantage beyond baseline approaches (0.03 reduction of error rate), especially for anomalous periods (0.18\/0.23 reduction of error rate)."}
{"_id":"7860877056470f53583a74b28e00360e10802336","title":"A Review of Methodological Approaches for the Design and Optimization of Wind Farms","text":"This article presents a review of the state of the art of the Wind Farm Design and Optimization (WFDO) problem. The WFDO problem refers to a set of advanced planning actions needed to extremize the performance of wind farms, which may be composed of a few individual Wind Turbines (WTs) up to thousands of WTs. The WFDO problem has been investigated in different scenarios, with substantial differences in main objectives, modelling assumptions, constraints, and numerical solution methods. The aim of this paper is: (1) to present an exhaustive survey of the literature covering the full span of the subject, an analysis of the state-of-the-art models describing the performance of wind farms as well as its extensions, and the numerical approaches used to solve the problem; (2) to provide an overview of the available knowledge and recent progress in the application of such strategies to real onshore and offshore wind farms; and (3) to propose a comprehensive agenda for future research. OPEN ACCESS Energies 2014, 7 6931"}
{"_id":"f17c78d07dbe14e9d250f57156500e63ee558ee7","title":"MORTALITY FROM CORONARY HEART DISEASE IN SUBJECTS WITH TYPE 2 DIABETES AND IN NONDIABETIC SUBJECTS WITH AND WITHOUT PRIOR MYOCARDIAL INFARCTION","text":"s of articles published since 1993. Single articles and past issues of the Journal can also be ordered for a fee through the Internet (http:\/\/www.nejm.org\/customer\/)."}
{"_id":"631cc57858eb1a94522e0090c6640f6f39ab7e18","title":"Blockchain as a Service for IoT","text":"A blockchain is a distributed and decentralized ledger that contains connected blocks of transactions. Unlike other ledger approaches, blockchain guarantees tamper proof storage of approved transactions. Due to its distributed and decentralized organization, blockchain is beeing used within IoT e.g. to manage device configuration, store sensor data and enable micro-payments. This paper presents the idea of using blockchain as a service for IoT and evaluates the performance of a cloud and edge hosted blockchain implementation."}
{"_id":"dd44809400f0953b270d9df978aa989adfb459b3","title":"AI-Assisted Game Debugging with Cicero","text":"We present Cicero, a mixed-initiative application for prototyping two-dimensional sprite-based games across different genres such as shooters, puzzles, and action games. Cicero provides a host of features which can offer assistance in different stages of the game development process. Noteworthy features include AI agents for gameplay simulation, a game mechanics recommender system, a playtrace aggregator, heatmap-based game analysis, a sequential replay mechanism, and a query system that allows searching for particular interaction patterns. In order to evaluate the efficacy and usefulness of the different features of Cicero, we conducted a user study in which we compared how users perform in game debugging tasks with different kinds of assistance."}
{"_id":"c4bf850225bdb46b09a3c992d1bc87daf9e66248","title":"On the use of windows for harmonic analysis with the discrete Fourier transform","text":"This paper makes available a concise review of data windows and their affect on the detection of harmonic signals in the presence of broad-band noise, and in the presence of nearby strong harmonic interference. We also call attention to a number of common errors in the application of windows when used with the fast Fourier transform. This paper includes a comprehensive catalog of data windows along with their significant performance parameters from which the different windows can be compared. Finally, an example demonstrates the use and value of windows to resolve closely spaced harmonic signals characterized by large differences in amplitude."}
{"_id":"1854f1ff2a1387aff7ed7b93de5d47ffc4cef05e","title":"Film Critics : Influencers or Predictors ?","text":"Critics and their reviews pervade many industries and are particularly important in the entertainment industry. Few marketing scholars, however, have considered the relationship between the market performance of entertainment services and the role of critics. The authors do so here. They show empirically that critical reviews correlate with late and cumulative box office receipts but do not have a significant correlation with early box office receipts. Although still far from any definitive conclusion, this finding suggests that critics, at least from an aggregate-level perspective, appear to act more as leading indicators than as opinion leaders."}
{"_id":"58def848174596119326b4ee981111d5ed538e9d","title":"Knowledge distillation across ensembles of multilingual models for low-resource languages","text":"This paper investigates the effectiveness of knowledge distillation in the context of multilingual models. We show that with knowledge distillation, Long Short-Term Memory(LSTM) models can be used to train standard feed-forward Deep Neural Network (DNN) models for a variety of low-resource languages. We then examine how the agreement between the teacher's best labels and the original labels affects the student model's performance. Next, we show that knowledge distillation can be easily applied to semi-supervised learning to improve model performance. We also propose a promising data selection method to filter un-transcribed data. Then we focus on knowledge transfer among DNN models with multilingual features derived from CNN+DNN, LSTM, VGG, CTC and attention models. We show that a student model equipped with better input features not only learns better from the teacher's labels, but also outperforms the teacher. Further experiments suggest that by learning from each other, the original ensemble of various models is able to evolve into a new ensemble with even better combined performance."}
{"_id":"0862940a255d980d46ef041ab20f153276f96214","title":"3D Object Representations for Fine-Grained Categorization","text":"While 3D object representations are being revived in the context of multi-view object class detection and scene understanding, they have not yet attained wide-spread use in fine-grained categorization. State-of-the-art approaches achieve remarkable performance when training data is plentiful, but they are typically tied to flat, 2D representations that model objects as a collection of unconnected views, limiting their ability to generalize across viewpoints. In this paper, we therefore lift two state-of-the-art 2D object representations to 3D, on the level of both local feature appearance and location. In extensive experiments on existing and newly proposed datasets, we show our 3D object representations outperform their state-of-the-art 2D counterparts for fine-grained categorization and demonstrate their efficacy for estimating 3D geometry from images via ultra-wide baseline matching and 3D reconstruction."}
{"_id":"ed4eff77499cc5ab1ffe24a3a2dc35642a518704","title":"Multiagent-Based Distributed-Energy-Resource Management for Intelligent Microgrids","text":"Microgrid is a combination of distributed generators, storage systems, and controllable loads connected to low-voltage network that can operate either in grid-connected or in island mode. High penetration of power at distribution level creates such multiple microgrids. This paper proposes a two-level architecture for distributed-energy-resource management for multiple microgrids using multiagent systems. In order to match the buyers and sellers in the energy market, symmetrical assignment problem based on nai\u0301ve auction algorithm is used. The developed mechanism allows the pool members such as generation agents, load agents, auction agents, grid agents, and storage agents to participate in market. Three different scenarios are identified based on the supply-demand mismatch among the participating microgrids. At the end of this paper, two case studies are presented with two and four interconnected microgrids participating in the market. Simulation results clearly indicate that the agent-based management is effective in resource management among multiple microgrids economically and profitably."}
{"_id":"8fd21d3f865dc07953dc9836ecd1ac3dbf8193c6","title":"Prediction of human emergency behavior and their mobility following large-scale disaster","text":"The frequency and intensity of natural disasters has significantly increased over the past decades and this trend is predicted to continue. Facing these possible and unexpected disasters, accurately predicting human emergency behavior and their mobility will become the critical issue for planning effective humanitarian relief, disaster management, and long-term societal reconstruction. In this paper, we build up a large human mobility database (GPS records of 1.6 million users over one year) and several different datasets to capture and analyze human emergency behavior and their mobility following the Great East Japan Earthquake and Fukushima nuclear accident. Based on our empirical analysis through these data, we find that human behavior and their mobility following large-scale disaster sometimes correlate with their mobility patterns during normal times, and are also highly impacted by their social relationship, intensity of disaster, damage level, government appointed shelters, news reporting, large population flow and etc. On the basis of these findings, we develop a model of human behavior that takes into account these factors for accurately predicting human emergency behavior and their mobility following large-scale disaster. The experimental results and validations demonstrate the efficiency of our behavior model, and suggest that human behavior and their movements during disasters may be significantly more predictable than previously thought."}
{"_id":"9cbbb518b2d607cd3a8081523550d89623490cd6","title":"The beauty of capturing faces: Rating the quality of digital portraits","text":"Digital portrait photographs are everywhere, and while the number of face pictures keeps growing, not much work has been done to on automatic portrait beauty assessment. In this paper, we design a specific framework to automatically evaluate the beauty of digital portraits. To this end, we procure a large dataset of face images annotated not only with aesthetic scores but also with information about the traits of the subject portrayed. We design a set of visual features based on portrait photography literature, and extensively analyze their relation with portrait beauty, exposing interesting findings about what makes a portrait beautiful. We find that the beauty of a portrait is linked to its artistic value, and independent from age, race and gender of the subject. We also show that a classifier trained with our features to separate beautiful portraits from non-beautiful portraits outperforms generic aesthetic classifiers."}
{"_id":"b7b58efc195e2c6e9c3c83eb52b25fabece7e37b","title":"Universal construction based on 3D printing electric motors: Steps towards self-replicating robots to transform space exploration","text":"Through a recent confluence of technological capacities, self-replicating robots have become a potentially nearterm rather than speculative technology. In a practical sense, self-replicating robots can never become obsolete \u2014 the first self- replicating robots will spawn all future generations of robots, subject to deliberate upgrading and\/or evolutionary change. Furthermore, this technology promises to revolutionise space exploration by bypassing the apparently-insurmountable problem of high launch costs. We present recent efforts in 3D printing the key robotic components required for any such self- replicating machine."}
{"_id":"dbe1ba5051222f0c5dafd8d07b6cfa164f002084","title":"Millimeter-Wave MultiBeam Aperture-Coupled Magnetoelectric Dipole Array With Planar Substrate Integrated Beamforming Network for 5G Applications","text":"A modified topology of the 2-D multibeam antenna array fed by a passive beamforming network (BFN) is proposed by introducing two sets of vertical interconnections into the conventional array configuration. Different from the traditional design, the new array structure can be integrated into multilayered planar substrates conveniently, which has advantages of low loss characteristics, ease of realization, and low fabrication cost for millimeter-wave (mmWave) applications. A  $4\\times4$  multibeam antenna array that can generate 16 beams is then designed, fabricated, and measured in order to demonstrate the correctness of the proposed topology. As the crucial components of the design, two kinds of aperture-coupled magnetoelectric dipole antenna elements with multilayered feed structures and three types of vertical interconnections consisting of the vertical substrate integrated waveguides are investigated, respectively. Wide bandwidth of 16.4%, stable radiation beams, and gain up to 14.7 dBi are achieved by the fabricated prototype. The proposed array configuration provides a new mean to implement the relatively large size 2-D multibeam antenna arrays with planar passive BFNs, which would be attractive for future mmWave wireless systems used for 5G communications and other applications."}
{"_id":"5b6a17327082f2147a58ec63720f25b138c67201","title":"A framework for improved video text detection and recognition","text":"Text displayed in a video is an essential part for the high-level semantic information of the video content. Therefore, video text can be used as a valuable source for automated video indexing in digital video libraries. In this paper, we propose a workflow for video text detection and recognition. In the text detection stage, we have developed a fast localization-verification scheme, in which an edge-based multi-scale text detector first identifies potential text candidates with high recall rate. Then, detected candidate text lines are refined by using an image entropy-based filter. Finally, Stroke Width Transform (SWT)- and Support Vector Machine (SVM)-based verification procedures are applied to eliminate the false alarms. For text recognition, we have developed a novel skeleton-based binarization method in order to separate text from complex backgrounds to make it processible for standard OCR (Optical Character Recognition) software. Operability and accuracy of proposed text detection and binarization methods have been evaluated by using publicly available test data sets."}
{"_id":"f4eec79ae070554fae0a10dfd4dcd4bdd5b087a1","title":"Road-marking Analysis for Autonomous Vehicle Guidance","text":"Driving an automobile autonomously on rural roads requires knowledge about the geometry of the road. Furthermore, knowledge about the meaning of each lane of the road is needed in order to decide which lane should be taken and if the vehicle can do a lane change. This paper addresses the problem of extracting additional information about lanes. The information is extracted from the types of road-markings. The type of lane border markings is estimated in order to find out if a lane change is allowed. Arrows, which are painted on the road, are extracted and classified in order to determine the meaning of a lane such as a turn off lane."}
{"_id":"40a70ac617507b9b8fcbd4562b435985557c926c","title":"Clustering with Lower Bound on Similarity","text":"We propose a new method, called SimClus, for clustering with lower bound on similarity. Instead of accepting k the number of clusters to find, the alternative similarity-based approach imposes a lower bound on the similarity between an object and its corresponding cluster representative (with one representative per cluster). SimClus achieves a O(log n) approximation bound on the number of clusters, whereas for the best previous algorithm the bound can be as poor as O(n). Experiments on real and synthetic datasets show that our algorithm produces more than 40% fewer representative objects, yet offers the same or better clustering quality. We also propose a dynamic variant of the algorithm, which can be effectively used in an on-line setting."}
{"_id":"84421b054a3fc358e606275745223f49e24316b3","title":"The long history of gaming in military training","text":"There is a long history of the dual-use of games in both the military and the entertainment applications. This has taken the form of sand tables, miniatures, board games, and computer games. The current tension between entertainment and military applications of games is just the return of similar concerns that surrounded the gaming tools and technologies of previous generations. Dynamic representations of the physical world are interesting and useful tools in a number of fields, to include the military, city planning, architecture, education, and entertainment. Computer games are tools that allow all of these audiences to accomplish similar goals."}
{"_id":"e2896ae342b284b129dba9f12d046f3bbf313567","title":"A multidisciplinary approach towards computational thinking for science majors","text":"This paper describes the development and initial evaluation of a new course ``Introduction to Computational Thinking'' taken by science majors to fulfill a college computing requirement. The course was developed by computer science faculty in collaboration with science faculty and it focuses on the role of computing and computational principles in scientific inquiry. It uses Python and Python libraries to teach computational thinking via basic programming concepts, data management concepts, simulation, and visualization. Problems with a computational aspect are drawn from different scientific disciplines and are complemented with lectures from faculty in those areas. Our initial evaluation indicates that the problem-driven approach focused on scientific discovery and computational principles increases the student's interest in computing."}
{"_id":"daa63f57c3fbe994c4356f8d986a22e696e776d2","title":"Efficient Global Optimization of Expensive Black-Box Functions","text":"In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome."}
{"_id":"c4a5421c8ac7277fe8d63bd296ba5742408adbbf","title":"Simulation of bulk current injection test using integrated circuit immunity macro model and electromagnetic analysis","text":"This paper provides a technique to predict bulk current injection (BCI) test results. In order to define the threshold of failure, the direct power injection based integrated circuit (IC) immunity macro model for conducted immunity is adopted. Injected radio frequency disturbance that reaches to an integrated circuit is calculated by using electromagnetic analysis with a high accuracy injection probe model. 3D model of equipment under test can provide the terminal voltage of IC which reference is the ground terminal of IC, not BCI test setups reference ground plane. The proposed method is applied to BCI tests and the simulated results have good correlation with experimental results."}
{"_id":"c5fe87291747c39f56bf0cbe4499cca77ae91351","title":"Learning an Intrinsic Image Decomposer Using Synthesized RGB-D Dataset","text":"Intrinsic image decomposition refers to recover the albedo and shading from images, which is an ill-posed problem in signal processing. As realistic labeled data are severely lacking, it is difficult to apply learning methods in this issue. In this letter, we propose using a synthesized dataset to facilitate the solving of this problem. A physically based renderer is used to generate color images and their underlying ground-truth albedo and shading from three-dimensional models. Additionally, we render a Kinect-like noisy depth map for each instance. We utilize this synthetic dataset to train a deep neural network for intrinsic image decomposition and further fine-tune it for real-world images. Our model supports both RGB and RGB-D as input, and it employs both high-level and low-level features to avoid blurry outputs. Experimental results verify the effectiveness of our model on realistic images."}
{"_id":"68f78dcd8644b99a8df8826e97df1589758bd1a8","title":"A new ultra-wideband, ultra-short monocycle pulse generator with reduced ringing","text":"We introduce a new ultra-wideband (UWB), ultra-short, step recovery diode monocycle pulse generator. This pulse generator uses a simple RC high-pass filter as a differentiator to generate the monocycle pulse directly. The pulse-shaping network employs a resistive circuit to achieve UWB matching and substantial removal of the pulse ringing, and rectifying and switching diodes to further suppress the ringing. An ultra-short monocycle pulse of 300-ps pulse duration, -17-dB ringing level, and good symmetry has been demonstrated. Good agreement between the measured and calculated results was achieved."}
{"_id":"873a493e553e4278040ff0b552eb1ae59f008c02","title":"A Distributed Test System Architecture for Open-source IoT Software","text":"In this paper, we discuss challenges that are specific to testing of open IoT software systems. The analysis reveals gaps compared to wireless sensor networks as well as embedded software. We propose a testing framework which (a) supports continuous integration techniques, (b) allows for the integration of project contributors to volunteer hardware and software resources to the test system, and (c) can function as a permanent distributed plugtest for network interoperability testing. The focus of this paper lies in open-source IoT development but many aspects are also applicable to closed-source projects."}
{"_id":"0e1ebbf27b303b8ccf62176e6e9370261963e2c0","title":"Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter","text":"There is a widespread intuitive sense that different kinds of information spread differently on-line, but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment. Here we study this issue on Twitter, analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users. We find significant variation in the ways that widely-used hashtags on different topics spread.\n Our results show that this variation is not attributable simply to differences in \"stickiness,\" the probability of adoption based on one or more exposures, but also to a quantity that could be viewed as a kind of \"persistence\" - the relative extent to which repeated exposures to a hashtag continue to have significant marginal effects. We find that hashtags on politically controversial topics are particularly persistent, with repeated exposures continuing to have unusually large marginal effects on adoption; this provides, to our knowledge, the first large-scale validation of the \"complex contagion\" principle from sociology, which posits that repeated exposures to an idea are particularly crucial when the idea is in some way controversial or contentious. Among other findings, we discover that hashtags representing the natural analogues of Twitter idioms and neologisms are particularly non-persistent, with the effect of multiple exposures decaying rapidly relative to the first exposure.\n We also study the subgraph structure of the initial adopters for different widely-adopted hashtags, again finding structural differences across topics. We develop simulation-based and generative models to analyze how the adoption dynamics interact with the network structure of the early adopters on which a hashtag spreads."}
{"_id":"3145fc2e5cbdf877ef07f7408dcaee5e44ba6d4f","title":"Meme-tracking and the dynamics of the news cycle","text":"Tracking new topics, ideas, and \"memes\" across the Web has been an issue of considerable interest. Recent work has developed methods for tracking topic shifts over long time scales, as well as abrupt spikes in the appearance of particular named entities. However, these approaches are less well suited to the identification of content that spreads widely and then fades over time scales on the order of days - the time scale at which we perceive news and events.\n We develop a framework for tracking short, distinctive phrases that travel relatively intact through on-line text; developing scalable algorithms for clustering textual variants of such phrases, we identify a broad class of memes that exhibit wide spread and rich variation on a daily basis. As our principal domain of study, we show how such a meme-tracking approach can provide a coherent representation of the news cycle - the daily rhythms in the news media that have long been the subject of qualitative interpretation but have never been captured accurately enough to permit actual quantitative analysis. We tracked 1.6 million mainstream media sites and blogs over a period of three months with the total of 90 million articles and we find a set of novel and persistent temporal patterns in the news cycle. In particular, we observe a typical lag of 2.5 hours between the peaks of attention to a phrase in the news media and in blogs respectively, with divergent behavior around the overall peak and a \"heartbeat\"-like pattern in the handoff between news and blogs. We also develop and analyze a mathematical model for the kinds of temporal variation that the system exhibits."}
{"_id":"09031aa6d6743bebebc695955cd77c032cd9192f","title":"Group formation in large social networks: membership, growth, and evolution","text":"The processes by which communities come together, attract new members, and develop over time is a central research issue in the social sciences - political movements, professional organizations, and religious denominations all provide fundamental examples of such communities. In the digital domain, on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as MySpace and LiveJournal. However, the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved: what are the structural features that influence whether individuals will join communities, which communities will grow rapidly, and how do the overlaps among pairs of communities change over time.Here we address these questions using two large sources of data: friendship links and community membership on LiveJournal, and co-authorship and conference publications in DBLP. Both of these datasets provide explicit user-defined communities, where conferences serve as proxies for communities in DBLP. We study how the evolution of these communities relates to properties such as the structure of the underlying social networks. We find that the propensity of individuals to join communities, and of communities to grow rapidly, depends in subtle ways on the underlying network structure. For example, the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community, but also crucially by how those friends are connected to one another. We use decision-tree techniques to identify the most significant structural determinants of these properties. We also develop a novel methodology for measuring movement of individuals between communities, and show how such movements are closely aligned with changes in the topics of interest within the communities."}
{"_id":"3f52e3e498b81356682e7746910a14d4c3851a77","title":"INFORMATION AND THE CHANGE IN THE PARADIGM IN ECONOMICS , PART 1 by","text":"The research for which George Akerlof, Mike Spence, and I are being recognized is part of a larger research program which, today, embraces hundred, perhaps thousands, of researchers around the world. In this lecture, I want to set the particular work which was sited within this broader agenda, and that agenda within the broader perspective of the history of economic thought. I hope to show that Information Economics represents a fundamental change in the prevailing paradigm within economics. Problems of information are central to understanding not only market economics but also political economy, and in the last section of this lecture, I explore some of the implications of information imperfections for political processes."}
{"_id":"1a7c35dc1e98fa2bf32cd0632a1be240f8c45831","title":"Approximate logic synthesis for error tolerant applications","text":"Error tolerance formally captures the notion that -- for a wide variety of applications including audio, video, graphics, and wireless communications -- a defective chip that produces erroneous values at its outputs may be acceptable, provided the errors are of certain types and their severities are within application-specified thresholds. All previous research on error tolerance has focused on identifying such defective but acceptable chips during post-fabrication testing to improve yield. In this paper, we explore a completely new approach to exploit error tolerance based on the following observation: If certain deviations from the nominal output values are acceptable, then we can exploit this flexibility during circuit design to reduce circuit area and delay as well as to increase yield. The specific metric of error tolerance we focus on is error rate, i.e., how often the circuit produces erroneous outputs. We propose a new logic synthesis approach for the new problem of identifying how to exploit a given error rate threshold to maximally reduce the area of the synthesized circuit. Experiment results show that for an error rate threshold within 1%, our approach provides 9.43% literal reductions on average for all the benchmarks that we target."}
{"_id":"c66689fafa0ce5d6d85ac8b361068de31c623516","title":"Generic and Scalable Framework for Automated Time-series Anomaly Detection","text":"This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of person's data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-series. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection."}
{"_id":"31eb33e47570ca3ccddfb53407115a29f70b2b9f","title":"Kinematics and design of a portable and wearable exoskeleton for hand rehabilitation","text":"We present the kinematic design and actuation mechanics of a wearable exoskeleton for hand rehabilitation of post-stroke. Our design method is focused on achieving maximum safety, comfort and reliability in the interaction, and allowing different users to wear the device with no manual regulations. In particular, we propose a kinematic and actuation solution for the index finger flexion\/extension, which leaves full movement freedom on the abduction-adduction plane. This paper presents a detailed kineto-static analysis of the system and a first prototype of the device."}
{"_id":"9d86a9e66856ee6b83b8c62b329d8fa204cf6616","title":"The Growing Relationship Between China and Sub-Saharan Africa : Macroeconomic , Trade , Investment , and Aid Links","text":"China\u2019s economic ascendance over the past two decades has generated ripple effects in the world economy. Its search for natural resources to satisfy the demands of industrialization has led it to Sub-Saharan Africa. Trade between China and Africa in 2006 totaled more than $50 billion, with Chinese companies importing oil from Angola and Sudan, timber from Central Africa, and copper from Zambia. Demand from China has contributed to an upward swing in prices, particularly for oil and metals from Africa, and has given a boost to real GDP in Sub-Saharan Africa. Chinese aid and investment in infrastructure are bringing desperately needed capital to the continent. At the same time, however, strong Chinese demand for oil is contributing to an increase in the import bill for many oil-importing SubSaharan African countries, and its exports of low-cost textiles, while benefiting African consumers, is threatening to displace local production. China poses a challenge to good governance and macroeconomic management in Africa because of the potential Dutch disease implications of commodity booms. China presents both an opportunity for Africa to reduce its marginalization from the global economy and a challenge for it to effectively harness the influx of resources to promote poverty-reducing economic development at home. JEL codes: F01, F35, F41, N55, N57, Q33, Q43"}
{"_id":"552528ae817834765e491d8f783c785c07ca7ccb","title":"A Guide to Differential Privacy Theory in Social Network Analysis","text":"Privacy of social network data is a growing concern which threatens to limit access to this valuable data source. Analysis of the graph structure of social networks can provide valuable information for revenue generation and social science research, but unfortunately, ensuring this analysis does not violate individual privacy is difficult. Simply anonymizing graphs or even releasing only aggregate results of analysis may not provide sufficient protection. Differential privacy is an alternative privacy model, popular in data-mining over tabular data, which uses noise to obscure individuals' contributions to aggregate results and offers a very strong mathematical guarantee that individuals' presence in the data-set is hidden. Analyses that were previously vulnerable to identification of individuals and extraction of private data may be safely released under differential-privacy guarantees. We review two existing standards for adapting differential privacy to network data and analyse the feasibility of several common social-network analysis techniques under these standards. Additionally, we propose out-link privacy, a novel standard for differential privacy over network data, and introduce two powerful out-link private algorithms for common network analysis techniques that were infeasible to privatize under previous differential privacy standards."}
{"_id":"01a9da9886fe353d77b6d6bc6854619ffe148438","title":"A ripple voltage sensing MPPT circuit for ultra-low power microsystems","text":"We propose a maximum power point tracking (MPPT) circuit for micro-scale sensor systems that measures ripple voltages in a switched capacitor energy harvester. Compared to conventional current mirror type MPPT circuits, this design incurs no voltage drop and does not require high bandwidth amplifiers. Using correlated double sampling, high accuracy is achieved with a power overhead of 5%, even at low harvested currents of 1.4uA based on measured results in 180nm CMOS."}
{"_id":"1f979f28a267522126acd8569ec2e3b964a7f656","title":"Student See Versus Student Do: A Comparative Study of Two Online Tutorials","text":"This study examines the impact on student performance after interactive and non-interactive tutorials using a 2\u00d72 treatment-control design. In an undergraduate management course, a control group watched a video tutorial while the treatment group received the same content using a dynamic tutorial. Both groups received the same quiz questions. Using effect size to determine magnitude of change, it was found that those in the treatment condition performed better than those in the control condition. Students were able to take the quiz up to two times. When examining for change in performance from attempt one to attempt two, the treatment group showed a greater magnitude of change. Students who consistently performed lowest on the quizzes outperformed all students in learning gains."}
{"_id":"8c047f39221328e3298c5370dd20bcec3e4a8d13","title":"A programmable and virtualized network & IT infrastructure for the internet of things: How can NFV & SDN help for facing the upcoming challenges","text":"The Internet of Things (IoT) revolution has major impacts on the network & Information Technology (IT) infrastructure. As IT in the past decade, network virtualization is simultaneously on its way with for instance Network Functions Virtualization (NFV) and Software-Defined Networking (SDN). NFV and SDN are approaches enhancing the infrastructure agility thus facilitating the design, delivery and operation of network services in a dynamic and scalable manner. IoT will push the infrastructure to its limit with numerous and diverse requirements to fulfill, we therefore believe that the agility brought by the combination of NFV and SDN is essential to face the IoT revolution. In this article, we first highlight some IoT challenges that the network & IT infrastructure will face. The NFV and SDN benefits are presented from a network operator point of view. Following a description of the IoT ecosystem and a recall of some of the IoT stakeholders expectations, a new multi-layered IoT architecture involving SDN and NFV and based upon network & IT resources is put forward. Finally, the article illustrates how the proposed architecture is able to cope with the identified IoT challenges."}
{"_id":"881fffd2d555344fa4171b295cb27df57e06ca5f","title":"Cable-suspended planar parallel robots with redundant cables: controllers with positive cable tensions","text":"Cable-suspended robots are structurally similar to parallel actuated robots but with the fundamental difference that cables can only pull the end-effector but not push it. From a scientific point of view, this feature makes feedback control of cable-suspended robots lot more challenging than their counterpart parallel-actuated robots. In the case with redundant cables, feedback control laws can be designed to make all tensions positive while attaining desired control performance. This paper describes these approaches and their effectiveness is demostrated through simulations of a three degree-offreedom cable suspended robots with four, five, and six cables."}
{"_id":"c017a2bc601513a1ec2be2d1155e0384e78b6380","title":"Abnormal Event Detection in Videos Using Hybrid Spatio-Temporal Autoencoder","text":"The LSTM Encoder-Decoder framework is used to learn representation of video sequences and applied for detect abnormal event in complex environment. However, it generally fails to account for the global context of the learned representation with a fixed dimension representation and the learned representation is crucial for decoder phase. Based on the LSTM Encoder-Decoder and the Convolutional Autoencoder, we explore a hybrid autoencoder architecture, which not only extracts better spatio-temporal context, but also improves the extrapolate capability of the corresponding decoder with the shortcut connection. The experimental results demonstrate that our approach outperforms lots of state-of-the-art methods on benchmark datasets."}
{"_id":"8884ef78bba4ceea3272ed2d25cafde8fc981c62","title":"Hierarchical classification of Web content","text":"This paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content. The hierarchical structure is initially used to train different second-level classifiers. In the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level. In the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories. Scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level.\nWe use support vector machine (SVM) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification. We found small advantages in accuracy for hierarchical models over flat models. For the hierarchical approach, we found the same accuracy using a sequential Boolean decision rule and a multiplicative decision rule. Since the sequential approach is much more efficient, requiring only 14%-16% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures."}
{"_id":"92b09523943dcba6de8dcd709f4a510675a107df","title":"A Case for Hardware Protection of Guest VMs from Compromised Hypervisors in Cloud Computing","text":"Cloud computing, enabled by virtualization technologies, is becoming a mainstream computing model. Many companies are starting to utilize the infrastructure-as-a-service (IaaS) cloud computing model, leasing guest virtual machines (VMs) from the infrastructure providers for economic reasons: to reduce their operating costs and to increase the flexibility of their own infrastructures. Yet, many companies may be hesitant to move to cloud computing due to security concerns. An integral part of any virtualization technology is the all-powerful hyper visor. A hyper visor is a system management software layer which can access all resources of the platform. Much research has been done on using hyper visors to monitor guest VMs for malicious code and on hardening hyper visors to make them more secure. There is, however, another threat which has not been addressed by researchers -- that of compromised or malicious hyper visors that can extract sensitive or confidential data from guest VMs. Consequently, we propose that a new research direction needs to be undertaken to tackle this threat. We further propose that new hardware mechanisms in the multi core microprocessors are a viable way of providing protections for the guest VMs from the hyper visor, while still allowing the hyper visor to flexibly manage the resources of the physical platform."}
{"_id":"f52f5517f6643739fc939f27d544987d8d681921","title":"Design and Analysis of a 3-Arm Spiral Antenna","text":"A novel 3-arm spiral antenna structure is presented in this paper. This antenna similar to traditional two-arm or four-arm spiral antennas exhibits wideband radiation characteristic and circular polarization. Advantages offered by the new design are two fold. Unlike the traditional spiral antennas the three-arm spiral can be fed by an unbalanced transmission line, such as a coaxial line or coplanar waveguide, and therefore an external balun is not needed at the feed point. Also by proper choice of arms' dimensions the antenna can be directly matched to any practical transmission line characteristic impedance and therefore external matching networks are not required. This is accomplished by feeding the antenna at the outer radius by a coplanar waveguide (CPW) transmission line and tapering it towards the center. The antenna can also be fed from the center using a coaxial or CPW line perpendicular to the plane of the spiral antenna. A full-wave numerical simulation tool is used to optimize the geometry of the proposed 3-arm spiral to achieve a compact size, wide bandwidth operation, and low axial ratio. The antenna is also designed over a ground plane to achieve a unidirectional radiation and center loading is examined that improves the axial ratio. Simulated results like return loss, radiation pattern, gain, and axial ratio are compared with those obtained from measurements and good agreements are shown. Because of its unique feed structure and compact size, application of the proposed 3-arm spiral antenna for wideband array applications is demonstrated"}
{"_id":"2b97d99ac4c00fe972aa114a9c453a457e894418","title":"Network game traffic modelling","text":"A significant share of today's Internet traffic is generated by network gaming. This kind of traffic is interesting in regard to it's market potential as well as to it's real time requirements on the network. For the consideration of game traffic in network dimensioning, traffic models are required that allow to generate a characteristic load for analytical or simulative performance evaluation of networks. In this paper we evaluate the fast action multiplayer game ,,Counter Strike\" from a 36 hour LAN party measurement and present traffic models for client and server. The paper concludes with remarks on the use of game traffic models in simulations and on QoS metrics for an adequate evaluation of simulation results."}
{"_id":"bfaf51d76280c2ec78eb4227924fbff8cf15624d","title":"ROBHAZ-rescue: rough-terrain negotiable teleoperated mobile robot for rescue mission","text":"This paper presents design and integration of the ROBHAZ-DT3, which is a newly developed mobile robot system with chained double-track mechanisms. It is designed to carry out military and civilian missions in various hazardous environments. A passive adaptation mechanism equipped between the front and rear body enables the ROBHAZ-DT3 to have good adaptability to uneven terrains including stairs. The passive adaptation mechanism reduces energy consumption when moving on uneven terrain as well as its simplicity in design and remote control, since no actuator is necessary for adaptation. Based on this novel mobile platform, a rescue version of the ROBHAZ-DT3 with appropriate sensors and a semi-autonomous mapping and localization algorithm is developed to participate in the RoboCup2004 US-Open: Urban Search and Rescue Competition. From the various experiments in the realistic rescue arena, we can verify that the ROBHAZ-DT3 is reliable in travelling rugged terrain and the proposed mapping and localization algorithm are effective in the unstructured environment with uneven ground."}
{"_id":"428b80b4a912f08ce84bf729456407eb2c7392db","title":"Detection of buried objects in FLIR imaging using mathematical morphology and SVM","text":"In this paper we describe a method for detecting buried objects of interest using a forward looking infrared camera (FLIR) installed on a moving vehicle. Infrared (IR) detection of buried targets is based on the thermal gradient between the object and the surrounding soil. The processing of FILR images consists in a spot-finding procedure that includes edge detection, opening and closing. Each spot is then described using texture features such as histogram of gradients (HOG) and local binary patterns (LBP) and assigned a target confidence using a support vector machine (SVM) classifier. Next, each spot together with its confidence is projected and summed in the UTM space. To validate our approach, we present results obtained on 6 one mile long runs recorded with a long wave IR (LWIR) camera installed on a moving vehicle."}
{"_id":"ba18b3312b1b811703290c489aac5da193eb2269","title":"Smart home and smart city solutions enabled by 5G, IoT, AAI and CoT services","text":"In a nearby future 5G technologies will connect the world from the largest megacities to the smallest internet of things in an always online fashion. Such a connected hierarchy must combine the smart cities, the smart homes, and the internet of things into one large coherent infrastructure. This paper suggest a four layer model which join and interfaces these elements by deploying technologies such as 5G, internet of things, cloud of things, and distributed artificial intelligence. Many advantages and service possibilities are offered by this new infrastructure such as interconnected internet of things, smart homes with artificial intelligence, and a platform for new combined smart home and smart city services based on big-data."}
{"_id":"9284c14a3fd370fa4af25c37575b84f58083f72d","title":"A Simple Steganography Algorithm Based on Lossless Compression Technique in WSN","text":"Image security has wide applications in data transferring from source to destination system. However, cryptography is a data secrecy technique between sender and receiver, the steganography increases the level of security and acts a protective layer to the hidden information within the source image. In this paper, a compression scheme based image security algorithm for wireless sensor network is proposed to hide a secret color image within the source color image. The main contribution of this paper is to propose a compression scheme which is based on level matrix and integer matrix, and increases the compression level significantly. The performance of the proposed system is evaluated in terms of peak signal to noise ratio (PSNR), mean square error (MSE), number of pixels change rate (NPCR) and unified average changing intensity (UACI). The proposed method achieves 42.65% PSNR, 27.16% MSE, 99.9% NPCR and 30.99% UACI."}
{"_id":"5beaff7d26b72b0ebad27dafa59c36b8c1d94277","title":"Application of Tikhonov Regularization to Super-Resolution Reconstruction of Brain MRI Images","text":"This paper presents an image super-resolution method that enhances spatial resolution of MRI images in the slice-select direction. The algorithm employs Tikhonov regularization, using a standard model of general imaging process and then reformulating the reconstruction as a regularized minimization problem. Our experimental result shows improvements in both signal-to-noise ratio and visual quality."}
{"_id":"fda3b2e98ea0cf73dd3046aa3c2bea3936e89aa4","title":"Chapter 18 Building Intelligent Tutoring Systems : An Overview","text":"This chapter addresses the challenge of building or authoring an Intelligent Tutoring System (ITS), along with the problems that have arisen and been dealt with, and the solutions that have been tested. We begin by clarifying what building an ITS entails, and then position today's systems in the overall historical context of ITS research. The chapter concludes with a series of open questions and an introduction to the other chapters in this part of the book."}
{"_id":"d9ce43229be476f7ae6b372d97b4d371aa65700d","title":"A segmental CRF approach to large vocabulary continuous speech recognition","text":"This paper proposes a segmental conditional random field framework for large vocabulary continuous speech recognition. Fundamental to this approach is the use of acoustic detectors as the basic input, and the automatic construction of a versatile set of segment-level features. The detector streams operate at multiple time scales (frame, phone, multi-phone, syllable or word) and are combined at the word level in the CRF training and decoding processes. A key aspect of our approach is that features are defined at the word level, and are naturally geared to explain long span phenomena such as formant trajectories, duration, and syllable stress patterns. Generalization to unseen words is possible through the use of decomposable consistency features [1], [2], and our framework allows for the joint or separate discriminative training of the acoustic and language models. An initial evaluation of this framework with voice search data from the Bing Mobile (BM) application results in a 2% absolute improvement over an HMM baseline."}
{"_id":"b8e5bc5f31d0d699c3481e5f313e4729a93c5581","title":"Low power in-memory computing based on dual-mode SOT-MRAM","text":"In this paper, we propose a novel Spin Orbit Torque Magnetic Random Access Memory (SOT-MRAM) array design that could simultaneously work as non-volatile memory and implement a reconfigurable in-memory logic (AND, OR) without add-on logic circuits to memory chip as in traditional logic-in-memory designs. The computed logic output could be simply read out like a normal MRAM bit-cell using the shared memory peripheral circuits. Such intrinsic in-memory logic could be used to process data within memory to greatly reduce power-hungry and long distance data communication in conventional Von-Neumann computing systems. We further employ in-memory data encryption using Advanced Encryption Standard (AES) algorithm as a case study to demonstrate the efficiency of the proposed design. The device to architecture co-simulation results show that the proposed design can achieve 70.15% and 80.87% lower energy consumption compared to CMOS-ASIC and CMOL-AES implementations, respectively. It offers almost similar energy consumption as recent DW-AES implementation, but with 60.65% less area overhead."}
{"_id":"c36d770497b898266031d5f628716f23abc1381b","title":"Today the Earwig, Tomorrow Man?","text":"Kirsh, D., Today the earwig, tomorrow man?, Artificial Intelligence 47 (1991) 161-184. A startling amount of intelligent activity can be controlled without reasoning or thought. By tuning the perceptual system to task relevant properties a creature can cope with relatively sophisticated environments without concepts. There is a limit, however, to how far a creature without concepts can go. Rod Brooks, like many ecologically oriented scientists, argues that the vast majority of intelligent behaviour is concept-free. To evaluate this position I consider what special benefits accrue to concept-using creatures. Concepts are either necessary for certain types of perception, learning, and control, or they make those processes computationally simpler. Once a creature has concepts its capacities are vastly multiplied."}
{"_id":"9f675dbc30a6cb282e40f9929bd3defa96189de6","title":"IOT based crop-field monitoring and irrigation automation","text":"Internet Of Things (IoT)is a shared network of objects or things which can interact with each other provided the Internet connection. IoT plays an important role in agriculture industry which can feed 9.6 billion people on the Earth by 2050. Smart Agriculture helps to reduce wastage, effective usage of fertilizer and thereby increase the crop yield. In this work, a system is developed to monitor crop-field using sensors (soil moisture, temperature, humidity, Light) and automate the irrigation system. The data from sensors are sent to Web server database using wireless transmission. In server database the data are encoded in JSON format. The irrigation is automated if the moisture and temperature of the field falls below the brink. In greenhouses light intensity control can also be automated in addition to irrigation. The notifications are sent to farmers' mobile periodically. The farmers' can able to monitor the field conditions from anywhere. This system will be more useful in areas where water is in scarce. This system is 92% more efficient than the conventional approach."}
{"_id":"4f623e3821d14553b3b286e20910db9225fb723f","title":"Audio-Visual Person Recognition in Multimedia Data From the Iarpa Janus Program","text":"Currently, datasets that support audio-visual recognition of people in videos are scarce and limited. In this paper, we introduce an expansion of video data from the IARPA Janus program to support this research area. We refer to the expanded set, which adds labels for voice to the already-existing face labels, as the Janus Multimedia dataset. We first describe the speaker labeling process, which involved a combination of automatic and manual criteria. We then discuss two evaluation settings for this data. In the core condition, the voice and face of the labeled individual are present in every video. In the full condition, no such guarantee is made. The power of audiovisual fusion is then shown using these publicly-available videos and labels, showing significant improvement over only recognizing voice or face alone. In addition to this work, several other possible paths for future research with this dataset are discussed."}
{"_id":"f722e3d2a9f8fc8adfb0498850f19ff63c624436","title":"Bystander Responses to a Violent Incident in an Immersive Virtual Environment","text":"Under what conditions will a bystander intervene to try to stop a violent attack by one person on another? It is generally believed that the greater the size of the crowd of bystanders, the less the chance that any of them will intervene. A complementary model is that social identity is critical as an explanatory variable. For example, when the bystander shares common social identity with the victim the probability of intervention is enhanced, other things being equal. However, it is generally not possible to study such hypotheses experimentally for practical and ethical reasons. Here we show that an experiment that depicts a violent incident at life-size in immersive virtual reality lends support to the social identity explanation. 40 male supporters of Arsenal Football Club in England were recruited for a two-factor between-groups experiment: the victim was either an Arsenal supporter or not (in-group\/out-group), and looked towards the participant for help or not during the confrontation. The response variables were the numbers of verbal and physical interventions by the participant during the violent argument. The number of physical interventions had a significantly greater mean in the in-group condition compared to the out-group. The more that participants perceived that the Victim was looking to them for help the greater the number of interventions in the in-group but not in the out-group. These results are supported by standard statistical analysis of variance, with more detailed findings obtained by a symbolic regression procedure based on genetic programming. Verbal interventions made during their experience, and analysis of post-experiment interview data suggest that in-group members were more prone to confrontational intervention compared to the out-group who were more prone to make statements to try to diffuse the situation."}
{"_id":"163aaf13875d29ee0424d0a248970f6e8233de7b","title":"Point cloud compression based on hierarchical point clustering","text":"In this work we propose an algorithm for compressing the geometry of a 3D point cloud (3D point-based model). The proposed algorithm is based on the hierarchical clustering of the points. Starting from the input model, it performs clustering to the points to generate a coarser approximation, or a coarser level of detail (LOD). Iterating this clustering process, a sequence of LODs are generated, forming an LOD hierarchy. Then, the LOD hierarchy is traversed top down in a width-first order. For each node encountered during the traversal, the corresponding geometric updates associated with its children are encoded, leading to a progressive encoding of the original model. Special efforts are made in the clustering to maintain high quality of the intermediate LODs. As a result, the proposed algorithm achieves both generic topology applicability and good ratedistortion performance at low bitrates, facilitating its applications for low-end bandwidth and\/or platform configurations."}
{"_id":"49a970d478146a43a9b0224ea5d881511c23c110","title":"Urban-Area and Building Detection Using SIFT Keypoints and Graph Theory","text":"Very high resolution satellite images provide valuable information to researchers. Among these, urban-area boundaries and building locations play crucial roles. For a human expert, manually extracting this valuable information is tedious. One possible solution to extract this information is using automated techniques. Unfortunately, the solution is not straightforward if standard image processing and pattern recognition techniques are used. Therefore, to detect the urban area and buildings in satellite images, we propose the use of scale invariant feature transform (SIFT) and graph theoretical tools. SIFT keypoints are powerful in detecting objects under various imaging conditions. However, SIFT is not sufficient for detecting urban areas and buildings alone. Therefore, we formalize the problem in terms of graph theory. In forming the graph, we represent each keypoint as a vertex of the graph. The unary and binary relationships between these vertices (such as spatial distance and intensity values) lead to the edges of the graph. Based on this formalism, we extract the urban area using a novel multiple subgraph matching method. Then, we extract separate buildings in the urban area using a novel graph cut method. We form a diverse and representative test set using panchromatic 1-m-resolution Ikonos imagery. By extensive testings, we report very promising results on automatically detecting urban areas and buildings."}
{"_id":"7d1e9bd95b07a0faec777e715f46ce95906d63d7","title":"Automated Detection of Arbitrarily Shaped Buildings in Complex Environments From Monocular VHR Optical Satellite Imagery","text":"This paper introduces a new approach for the automated detection of buildings from monocular very high resolution (VHR) optical satellite images. First, we investigate the shadow evidence to focus on building regions. To do that, we propose a new fuzzy landscape generation approach to model the directional spatial relationship between buildings and their shadows. Once all landscapes are collected, a pruning process is developed to eliminate the landscapes that may occur due to non-building objects. The final building regions are detected by GrabCut partitioning approach. In this paper, the input requirements of the GrabCut partitioning are automatically extracted from the previously determined shadow and landscape regions, so that the approach gained an efficient fully automated behavior for the detection of buildings. Extensive experiments performed on 20 test sites selected from a set of QuickBird and Geoeye-1 VHR images showed that the proposed approach accurately detects buildings with arbitrary shapes and sizes in complex environments. The tests also revealed that even under challenging environmental and illumination conditions, reasonable building detection performances could be achieved by the proposed approach."}
{"_id":"596cf71c1695d4eb914482ded1ebcf8f1333e0db","title":"Fast and extensible building modeling from airborne LiDAR data","text":"This paper presents an automatic algorithm which reconstructs building models from airborne LiDAR (light detection and ranging) data of urban areas. While our algorithm inherits the typical building reconstruction pipeline, several major distinct features are developed to enhance efficiency and robustness: 1) we design a novel vegetation detection algorithm based on differential geometry properties and unbalanced SVM; 2) after roof patch segmentation, a fast boundary extraction method is introduced to produce topology-correct water tight boundaries; 3) instead of making assumptions on the angles between roof boundary lines, we propose a data-driven algorithm which automatically learns the principal directions of roof boundaries and uses them in footprint production. Furthermore, we show the extendability of our algorithm by supporting non-flat object patterns with the help of only a few user interactions. We demonstrate the efficiency and accuracy of our algorithm by showing experiment results on urban area data of several different data sets."}
{"_id":"8f86f0276a40081841de28535cbf4ba87c1127f2","title":"Detection of small objects from high-resolution panchromatic satellite imagery based on supervised image segmentation","text":"A new concept for the detection of small objects from modular optoelectronic multispectral scanner (MOMS-02) high spatial resolution panchromatic satellite imagery is presented. We combine supervised shape classification with unsupervised image segmentation in an iterative procedure which allows a target-oriented search for specific object shapes."}
{"_id":"85f25c2811b15ba26dd55204089d6360accd173b","title":"Implicit user modeling for personalized search","text":"Information retrieval systems (e.g., web search engines) are critical for overcoming information overload. A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users, resulting in inherently non-optimal retrieval performance. For example, a tourist and a programmer may use the same word \"java\" to search for different information, but the current search systems would return the same results. In this paper, we study how to infer a user's interest from the user's search context and use the inferred implicit user model for personalized search. We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. We develop an intelligent client-side web search agent (UCAIR) that can perform eager implicit feedback, e.g., query expansion based on previous queries and immediate result reranking based on clickthrough information. Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine."}
{"_id":"1763fa2c20d2a7038af51a97fddf37a1864c0fe2","title":"Animal-inspired design and aerodynamic stabilization of a hexapedal millirobot","text":"The VelociRoACH is a 10 cm long, 30 gram hexapedal millirobot capable of running at 2.7 m\/s, making it the fastest legged robot built to date, relative to scale. We present the design by dynamic similarity technique and the locomotion adaptations which have allowed for this highly dynamic performance. In addition, we demonstrate that rotational dynamics become critical for stability as the scale of a robotic system is reduced. We present a new method of experimental dynamic tuning for legged millirobots, aimed at finding stable limit cycles with minimal rotational energy. By implementing an aerodynamic rotational damper, we further reduced the rotational energy in the system, and demonstrated that stable limit cycles with lower rotational energy are more robust to disturbances. This method increased the stability of the system without detracting from forward speed."}
{"_id":"44fb458c8bcf88c42b55bc601fea2f264211ad95","title":"Dynamic tire friction models for vehicle traction control","text":"In this paper we derive a dynamic friction force model for road\/tire interaction for ground vehicles. The model is based on a similar dynamic friction model for contact developed previously for contact-point friction problems, called the LuGre model [4]. We show that the dynamic LuGre friction model is able to accurately capture velocity and road\/surface dependence of the tire friction force."}
{"_id":"d018519d0f844228557b30f8e2b887fae4f3c2a1","title":"Effects of Platform-Switching on Peri-implant Soft and Hard Tissue Outcomes: A Systematic Review and Meta-analysis.","text":"PURPOSE\nThis systematic review and meta-analysis was aimed at evaluating the longitudinal effect of platform switching on implant survival rates as well as on soft and hard tissue outcomes.\n\n\nMATERIALS AND METHODS\nAn electronic search of the databases of the National Center for Biotechnology Information, PubMed, Ovid (MEDLINE), EMBASE, Web of Science, and Cochrane Collaboration Library was conducted in February 2015. Studies published in English with at least 10 human participants and a 12-month post-loading follow-up were included. Random effects meta-analyses of selected studies were applied to compare the primary and secondary outcomes of platform-switched (PS) and regular-platform (RP) implants, as well as the experimental designs and clinical outcomes.\n\n\nRESULTS\nA total of 26 studies involving 1,511 PS implants and 1,123 RP implants were evaluated. Compared to RP implants, PS implants showed a slight increase in vertical marginal bone loss (VMBL) and pocket depth reduction (weighted mean differences were -0.23 mm and -0.20 mm, respectively). The PS implants had a mean VMBL of 0.36 \u00b1 0.15 mm within the first year of service. The meta-regression suggested a trend of decreased bone resorption at sites with thick soft tissues at baseline.\n\n\nCONCLUSION\nThis study suggested that platform switching may have an indirect protective effect on implant hard tissue outcomes."}
{"_id":"cae0fa761fab754739f20044d73922aa614db661","title":"Parallel Database Recovery for Multicore Main-Memory Databases","text":"Main-memory database systems for multicore servers can ach ieve excellent performance when processing massive volume of OL TP workloads. But crash-resilience mechanism, or namely logg ingand-replay, can either degrade the performance of transact ion processing or slow down the process of failure recovery. In this paper, we show that, by exploiting application semantics, it i s possible to achieve speedy failure recovery without introduci ng any costly logging overhead to the execution of online transact ions. We propose TARS, a parallel log-recovery mechanism that is specifically designed for lightweight, coarse-grained commandlogging approach. TARS leverages a combination of static and dynamic analyses to parallelize the log replay: at compile time, T ARS decomposes stored procedures by carefully analyzing depende ncies within and across programs; at recovery time, a re-executio n schedule with a high degree of parallelism is attained through lig tweight one-pass scans over transaction parameter values. As such, recovery latency is remarkably minimized. We evaluated T ARS in a main-memory database prototype running on a 40-core serve r. Compared to the state-of-the-art mechanisms, T ARS yields significantly higher recovery rate without compromising the effici en y of transaction processing."}
{"_id":"3d1446375d8cb9af85e22200b1c9d27cd3890fb7","title":"A convenient nucleic acid test on the basis of the capillary convective PCR for the on-site detection of enterovirus 71.","text":"The recent and continuing epidemic of enterovirus 71 in China has affected millions of children and resulted in thousands of deaths. Timely diagnosis and management is essential for disease control. Current enterovirus 71 molecular tests require resources that are unavailable for on-site testing. We have developed a simple-to-operate nucleic acid test, the convenient and integrated nucleic acid test, for local medical institutions. It uses a convective PCR for rapid amplification, a dipstick for visual detection of PCR products, and a simple commercial kit for nucleic acid extraction. By using a specially designed reagent and reaction tube containing a dipstick, the amplification and detection processes are well integrated and simplified. Moreover, cross contamination that may be caused by an open-tube detection system can be avoided. On the basis of the convenient and integrated nucleic acid test, an enterovirus 71 assay for on-site testing was developed. After evaluating known hand, foot, and mouth disease virus stocks of 17 strains of 11 different serotypes, this assay showed a favorable detection spectrum and no cross-reactivity. Its clinical performance was established by testing 141 clinical samples and comparing the results with a nested RT-PCR method. The assay showed a clinical sensitivity and specificity of 98.5% and 100%, respectively. Our results suggest that this convenient and integrated nucleic acid test enterovirus 71 assay may serve as an on-site diagnosis tool."}
{"_id":"5f4924da9f8fac603c738cac64fb1785ef96c381","title":"An 8.3 nW \u221272 dBm event driven IoE wake up receiver RF front end","text":"This work presents an ultra-low power event driven wake-up receiver (WuRx) fabricated in a RF CMOS 130 nm process. The receiver consists of an off-chip lumped element matching network, an envelope detector, a decision circuit capable of detecting sub-mV baseband signal voltages and a clock source consuming 1.3 nW. This receiver has demonstrated a sensitivity of \u221272 dBm while consuming a total of 8.3 nW from 1 V and 0.65 V sources."}
{"_id":"354db02f22770bf4cb06fd8b45d58836795d7000","title":"Standardization of marketing strategy 107 Standardization of international marketing strategy by firms from a developing country","text":"A major debate in the international marketing literature deals with the globalization of markets and the extent to which a company\u2019s international marketing strategy can be standardized (Buzzell, 1968; Cavusgil et al., 1993; Douglas and Wind, 1987; Hill and Still, 1984; Jain, 1989; Levitt, 1983; Sorenson and Wiechmann, 1975). Significant progress has been made with respect to the extent to which international marketing strategy can be standardized across national borders (Cavusgil et al., 1993; Jain, 1989). However, almost all previous research has been conducted from a US-firm perspective. Even though a few studies dealt with this debate in the context of less developed economies, the issues studied were viewed from the perspective of US firms\u2019 operating activities in those economies (e.g. Grosse and Zinn, 1990; Hill and Still, 1984; Kacker, 1972). A major gap in the literature is whether our current knowledge can be generalized to foreign companies in other nations, especially the developing world. Firms from developing countries assume an increasingly important role in international competition, since many of the fastest growing economies in the world can be found in these nations. In addition, since developing countries are often culturally different from developed countries, they provide a suitable context to assess the generalizability of the existing knowledge in the standardization literature. Consequently, it is of interest to researchers and practitioners of international marketing to know the perspective of firms from developing countries regarding the standardization and adaptation of marketing strategy. Another void in the literature is that previous studies implicitly assume that standardization concepts are unidimensional at either the overall marketing programme level or the 4-P level (e.g. Akaah, 1991; Boddewyn et al., 1986;"}
{"_id":"64f6b71b331c5eafde453fc608c2af5151c0b84a","title":"Numerically Grounded Language Models for Semantic Error Correction","text":"Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements."}
{"_id":"2051c7baeaae8eb86f74ab8f4cbdf6bc2fee059b","title":"Sequential Self-Folding Structures by 3D Printed Digital Shape Memory Polymers","text":"Folding is ubiquitous in nature with examples ranging from the formation of cellular components to winged insects. It finds technological applications including packaging of solar cells and space structures, deployable biomedical devices, and self-assembling robots and airbags. Here we demonstrate sequential self-folding structures realized by thermal activation of spatially-variable patterns that are 3D printed with digital shape memory polymers, which are digital materials with different shape memory behaviors. The time-dependent behavior of each polymer allows the temporal sequencing of activation when the structure is subjected to a uniform temperature. This is demonstrated via a series of 3D printed structures that respond rapidly to a thermal stimulus, and self-fold to specified shapes in controlled shape changing sequences. Measurements of the spatial and temporal nature of self-folding structures are in good agreement with the companion finite element simulations. A simplified reduced-order model is also developed to rapidly and accurately describe the self-folding physics. An important aspect of self-folding is the management of self-collisions, where different portions of the folding structure contact and then block further folding. A metric is developed to predict collisions and is used together with the reduced-order model to design self-folding structures that lock themselves into stable desired configurations."}
{"_id":"0617301c077e56c44933e2b790a270f3e590db12","title":"A Hybrid Indexing Method for Approximate String Matching","text":"We present a new indexing method for the approximate string matching problem. The method is based on a suffix array combined with a partitioning of the pattern. We analyze the resulting algorithm and show that the average retrieval time is , for some that depends on the error fraction tolerated and the alphabet size . It is shown that for approximately , where . The space required is four times the text size, which is quite moderate for this problem. We experimentally show that this index can outperform by far all the existing alternatives for indexed approximate searching. These are also the first experiments that compare the different existing schemes."}
{"_id":"6c94ec89603ffafcdba4600bc7506a2df35cc246","title":"Extracting Business Process Models Using Natural Language Processing (NLP) Techniques","text":"This Doctoral Consortium paper discusses how NLP can be applied in the domain of BPM in order to automatically generate business process models from existing documentation within the organization. The main idea is that from the syntactic and grammatical structure of a sentence, the components of a business process model can be derived (i.e. activities, resources, tasks, patterns). The result would be a business process model depicted using BPMN - a dedicated business process modeling technique"}
{"_id":"f004f7af674ac13dc73c16cb019e7e67c7fe83fc","title":"Extraction and Adaptation of Fuzzy Rules for Friction Modeling and Control Compensation","text":"Modeling of friction forces has been a challenging task in mechanical engineering. Parameterized approaches for modeling friction find it difficult to achieve satisfactory performance due to the presence of nonlinearity and uncertainties in dynamical systems. This paper aims to develop adaptive fuzzy friction models by the use of data-mining techniques and system theory. Our main technical contributions are twofold: extraction of fuzzy rules and formulation of a static fuzzy friction model and adaptation of the fuzzy friction model by the use of the Lyapunov stability theory, which is associated with a control compensation of a typical motion dynamics. The proposed framework in this paper shows a successful application of adaptive data-mining techniques in engineering. A single-degree-of-freedom mechanical system is employed as an experimental model in simulation studies. Results demonstrate that our proposed fuzzy friction model has promise in the design of uncertain mechanical control systems."}
{"_id":"7e0769e6857e28cadc507c47f2941a2538e69c9d","title":"Offline Evaluation of Response Prediction in Online Advertising Auctions","text":"Click-through rates and conversion rates are two core machine learning problems in online advertising. The evaluation of such systems is often based on traditional supervised learning metrics that ignore how the predictions are used. These predictions are in fact part of bidding systems in online advertising auctions. We present here an empirical evaluation of a metric that is specifically tailored for auctions in online advertising and show that it correlates better than standard metrics with A\/B test results."}
{"_id":"2f9b49160ef60a11e32fae1a102384f77b4c7272","title":"Automatically generating features for learning program analysis heuristics for C-like languages","text":"We present a technique for automatically generating features for data-driven program analyses. Recently data-driven approaches for building a program analysis have been developed, which mine existing codebases and automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches reduce the burden of the analysis designers, but they do not remove it completely; they still leave the nontrivial task of designing so called features to the hands of the designers. Our technique aims at automating this feature design process. The idea is to use programs as features after reducing and abstracting them. Our technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the new programs with respect to the query. Each reduced program serves as a boolean feature for program-query pairs. This feature evaluates to true for a given program-query pair when (as a program) it is included in the program part of the pair. We have implemented our approach for three real-world static analyses. The experimental results show that these analyses with automatically-generated features are cost-effective and consistently perform well on a wide range of programs."}
{"_id":"8f16276617d5036bbebac824e15f585ad0fcf42f","title":"Visualizing Linked Data with JavaScript","text":"Despite the wealth of information contained in the Web of Linked Data, the current limitations and entry barriers of the Semantic Web technologies hinder the users from taking advantage of these information resources. Linked Data visualization can alleviate this problem. In this paper, we adopt a proper Linked Data visualization model, design Linked Data visualization algorithms, and develop a lightweight, easy-to-use prototype tool, LOD Viewer, using the platform independent JavaScript language. LOD Viewer can visualize different sources of RDF data including SPARQL endpoints for Linked Open Data (LOD) sources, and display the data in different graphic illustrations. Our case studies have verified the effectiveness and realizability of the proposed method. The time complexity analysis and experimental test show that the run-time of the proposed algorithms approximately exhibits a linear growth rate as the visualized RDF triples size increases."}
{"_id":"ae60b76436b86e03be63548b6f16b3705edb5ebd","title":"TOPSIS method for multi-attribute group decision-making under single-valued neutrosophic environment","text":"A single-valued neutrosophic set is a special case of neutrosophic set. It has been proposed as a generalization of crisp sets, fuzzy sets, and intuitionistic fuzzy sets in order to deal with incomplete information. In this paper, a new approach for multi-attribute group decision-making problems is proposed by extending the technique for order preference by similarity to ideal solution to single-valued neutrosophic environment. Ratings of alternative with respect to each attribute are considered as single-valued neutrosophic set that reflect the decision makers\u2019 opinion based on the provided information. Neutrosophic set characterized by three independent degrees namely truth-membership degree (T), indeterminacy-membership degree (I), and falsity-membership degree (F) is more capable to catch up incomplete information. Single-valued neutrosophic set-based weighted averaging operator is used to aggregate all the individual decision maker\u2019s opinion into one common opinion for rating the importance of criteria and alternatives. Finally, an illustrative example is provided in order to demonstrate its applicability and effectiveness of the proposed approach."}
{"_id":"887ee1e832c8d2d93a46de053ac579f9b08655b4","title":"Semi-supervised feature learning for improving writer identification","text":"Data augmentation is usually used by supervised learning approaches for offline writer identification, but such approaches require extra training data and potentially lead to overfitting errors. In this study, a semi-supervised feature learning pipeline was proposed to improve the performance of writer identification by training with extra unlabeled data and the original labeled data simultaneously. Specifically, we proposed a weighted label smoothing regularization (WLSR) method for data augmentation, which assigned the weighted uniform label distribution to the extra unlabeled data. The WLSR method could regularize the convolutional neural network (CNN) baseline to allow more discriminative features to be learned to represent the properties of different writing styles. The experimental results on well-known benchmark datasets (ICDAR2013 and CVL) showed that our proposed semisupervised feature learning approach could significantly improve the baseline measurement and perform competitively with existing writer identification approaches. Our findings provide new insights into offline write identification."}
{"_id":"40cfc568a2caa261f2896d09db636053c2d118eb","title":"A Temporal Attentional Model for Rumor Stance Classification","text":"Rumor stance classification is the task of determining the stance towards a rumor in text. This is the first step in effective rumor tracking on social media which is an increasingly important task. In this work, we analyze Twitter users' stance toward a rumorous tweet, in which users could support, deny, query, or comment upon the rumor. We propose a deep attentional CNN-LSTM approach, which takes the sequence of tweets in a thread of conversation as the input. We use neighboring tweets in the timeline as context vectors to capture the temporal dynamism in users' stance evolution. In addition, we use extra features such as friendship, to leverage useful relational features that are readily available in social media. Our model achieves the state-of-the-art results on rumor stance classification on a recent SemEval dataset, improving accuracy and F1 score by 3.6% and 4.2% respectively."}
{"_id":"730f99f6e26167f93876f401b02dc30b04c88862","title":"Modeling WiFi Active Power\/Energy Consumption in Smartphones","text":"We conduct the first detailed measurement study of the properties of a class of WiFi active power\/energy consumption models based on parameters readily available to smartphone app developers. We first consider a number of parameters used by previous models and show their limitations. We then focus on a recent approach modeling the active power consumption as a function of the application layer throughput. Using a large dataset and an 802.11n-equipped smartphone, we build four versions of a previously proposed linear power-throughput model, which allow us to explore the fundamental trade off between accuracy and simplicity. We study the properties of the model in relation to other parameters such as the packet size and\/or the transport layer protocol, and we evaluate its accuracy under a variety of scenarios which have not been considered in previous studies. Our study shows that the model works well in a number of scenarios but its accuracy drops with high throughput values or when tested on different hardware. We further show that a non-linear model can greatly improve the accuracy in these two cases."}
{"_id":"1db7e6222809f0cdf0bbe2e87852295567edf838","title":"Fingerstroke time estimates for touchscreen-based mobile gaming interaction.","text":"The growing popularity of gaming applications and ever-faster mobile carrier networks have called attention to an intriguing issue that is closely related to command input performance. A challenging mirroring game service, which simultaneously provides game service to both PC and mobile phone users, allows them to play games against each other with very different control interfaces. Thus, for efficient mobile game design, it is essential to apply a new predictive model for measuring how potential touch input compares to the PC interfaces. The present study empirically tests the keystroke-level model (KLM) for predicting the time performance of basic interaction controls on the touch-sensitive smartphone interface (i.e., tapping, pointing, dragging, and flicking). A modified KLM, tentatively called the fingerstroke-level model (FLM), is proposed using time estimates on regression models."}
{"_id":"7be1268e8b98b9a706cb782d3e06fc6f69b94ed6","title":"A fast transient LDO based on dual loop FVF with high PSRR","text":"In this work, the design of a dual loop flipped voltage follower (FVF) based low-dropout regulator (LDO) to achieve fast transient response is proposed for all-digital phase locked loop (ADPLL) in the RFID application. With the help of an additional reference generation loop in FVF LDO and the cascaded structure, high power supply rejection ratio (PSRR) is achievable. This design is fabricated in GF 40nm CMOS technology. The FVF LDO core only occupies small area of 0.036 mm2. This area also includes 80pF on chip capacitor. Without large off-chip capacitor, this LDO is suitable for system-on-chip (SoC) requirement. Post layout simulation shows that fast response of 45ns and high PSRR of \u221242dB through up to 10GHz frequency range."}
{"_id":"9e7b9bb459c1fe4e04bcbf24a12c7fef5c2761b2","title":"New compact OMT based on a septum solution","text":"This paper presents a new compact orthomode transducer (OMT) able to extract or combine two orthogonal linear polarizations in a very compact manner with simple manufacturing process. The component looks similar to the well-known septum polarizer but contrary to this one, the new component is able to handle dual linear polarization."}
{"_id":"876ef047c98a640b4153cf6f2314751aa2334d2c","title":"PIXHAWK: A system for autonomous flight using onboard computer vision","text":"We provide a novel hardware and software system for micro air vehicles (MAV) that allows high-speed, low-latency onboard image processing. It uses up to four cameras in parallel on a miniature rotary wing platform. The MAV navigates based on onboard processed computer vision in GPS-denied in- and outdoor environments. It can process in parallel images and inertial measurement information from multiple cameras for multiple purposes (localization, pattern recognition, obstacle avoidance) by distributing the images on a central, low-latency image hub. Furthermore the system can utilize low-bandwith radio links for communication and is designed and optimized to scale to swarm use. Experimental results show successful flight with a range of onboard computer vision algorithms, including localization, obstacle avoidance and pattern recognition."}
{"_id":"34200d9fc5843237c2df7c364afe2c6a4e740a66","title":"Algorithm of a Perspective Transform-Based PDF417 Barcode Recognition","text":"When a PDF417 barcode are recognized, there are major recognition processes such as segmentation, normalization, and decoding. Among them, the segmentation and normalization steps are very important because they have a strong influence on the rate of barcode recognition. There are also previous segmentation and normalization techniques of processing barcode image, but some issues as follows. First, the previous normalization techniques need an additional restoration process and apply an interpolation process. Second, the previous recognition algorithms recognize a barcode image well only when it is placed in the predefined rectangular area. Therefore, we propose a novel segmentation and normalization method in PDF417 with the aims of improving its recognition rate and precision. The segmentation process to detect the barcode area in an image uses the conventional morphology and Hough transformmethods. The normalization process of the bar code region is based on the conventional perspective transformation and warping algorithms. In addition, we perform experiments using both experimental and actual data for evaluating our algorithms. Consequently, our experimental results can be summarized as follows. First, our method showed a stable performance over existing PDF417 barcode detection and recognition. Second, it overcame the limitation problem where the location of an input image should locate in a predefined rectangle area. Finally, it is expected that our result can be used as a restoration tool of printed images such as documents and pictures."}
{"_id":"682640754c867ecc6ae2ccaa5dc68403ee7d2e63","title":"Two-factor authentication for the Bitcoin protocol","text":"We show how to realize two-factor authentication for a Bitcoin wallet. To do so, we explain how to employ an ECDSA adaption of the two-party signature protocol by MacKenzie and Reiter (Int J Inf Secur 2(3\u20134):218\u2013239, 2004. doi: 10.1007\/s10207-004-0041-0 ) in the context of Bitcoin and present a prototypic implementation of a Bitcoin wallet that offers both: two-factor authentication and verification over a separate channel. Since we use a smart phone as the second authentication factor, our solution can be used with hardware already available to most users and the user experience is quite similar to the existing online banking authentication methods."}
{"_id":"84ed097a35ab8e3c2c404a84ed8e22d40b5b9db4","title":"Development of robots for rehabilitation therapy: the Palo Alto VA\/Stanford experience.","text":"For over 25 years, personal assistant robots for severely disabled individuals have been in development. More recently, using robots to deliver rehabilitation therapy has been proposed. This paper summarizes the development and clinical testing of three mechatronic systems for post-stroke therapy conducted at the VA Palo Alto in collaboration with Stanford University. We describe the philosophy and experiences that guided their evolution. Unique to the Palo Alto approach is provision for bimanual, mirror-image, patient-controlled therapeutic exercise. Proof-of-concept was established with a 2-degree-of-freedom (DOF) elbow\/forearm manipulator. Tests of a second-generation therapy robot producing planar forearm movements in 19 hemiplegic and control subjects confirmed the validity and reliability of interaction forces during mechanically assisted upper-limb movements. Clinical trials comparing 3-D robot-assisted therapy to traditional therapy in 21 chronic stroke subjects showed significant improvement in the Fugl-Meyer (FM) measure of motor recovery in the robot group, which exceeded improvements in the control group."}
{"_id":"926bd9147e95101d2ebb8b563696113a57b784ff","title":"Design of a multi-DOF cable-driven mechanism of a miniature serial manipulator for robot-assisted minimally invasive surgery","text":"While multi-fingered robotic hands have been developed for decades, none has been used for surgical operations. \u03bcAngelo is an anthropomorphic master-slave system for teleoperated robot-assisted surgery. As part of this system, this paper focuses on its slave instrument, a miniature three-digit hand. The design of the mechanism of such a manipulator poses a challenge due to the required miniaturization and the many active degrees of freedom. As the instrument has a human-centered design, its relation to the human hand is discussed. Two ways of routing its cable-driven mechanism are investigated and the method of deriving the input-output functions that drive the mechanism is presented."}
{"_id":"7794aba6aec9b1297bf31c76d6a58daf19c27f14","title":"RFID in robot-assisted indoor navigation for the visually impaired","text":"We describe how radio frequency identification (RFID) can be used in robot-assisted indoor navigation for the visually impaired. We present a robotic guide for the visually impaired that was deployed and tested both with and without visually unpaired participants in two indoor environments. We describe how we modified the standard potential fields algorithms to achieve navigation at moderate walking speeds and to avoid oscillation in narrow spaces. The experiments illustrate that passive RFID tags deployed in the environment can act as reliable stimuli that trigger local navigation behaviors to achieve global navigation objectives."}
{"_id":"1c2c978faad6f27e05cf9e3fca509132ace8fae4","title":"Beyond PASCAL: A benchmark for 3D object detection in the wild","text":"3D object detection and pose estimation methods have become popular in recent years since they can handle ambiguities in 2D images and also provide a richer description for objects compared to 2D object detectors. However, most of the datasets for 3D recognition are limited to a small amount of images per category or are captured in controlled environments. In this paper, we contribute PASCAL3D+ dataset, which is a novel and challenging dataset for 3D object detection and pose estimation. PASCAL3D+ augments 12 rigid categories of the PASCAL VOC 2012 [4] with 3D annotations. Furthermore, more images are added for each category from ImageNet [3]. PASCAL3D+ images exhibit much more variability compared to the existing 3D datasets, and on average there are more than 3,000 object instances per category. We believe this dataset will provide a rich testbed to study 3D detection and pose estimation and will help to significantly push forward research in this area. We provide the results of variations of DPM [6] on our new dataset for object detection and viewpoint estimation in different scenarios, which can be used as baselines for the community. Our benchmark is available online at http:\/\/cvgl.stanford.edu\/projects\/pascal3d."}
{"_id":"d01ac7040c253b941192dcd7710635d47d837609","title":"AllConcur: Leaderless Concurrent Atomic Broadcast (Extended Version)","text":"Many distributed systems require coordination between the components involved. With the steady growth of such systems, the probability of failures increases, which necessitates scalable fault-tolerant agreement protocols. The most common practical agreement protocol, for such scenarios, is leader-based atomic broadcast. In this work, we propose ALLCONCUR, a distributed system that provides agreement through a leaderless concurrent atomic broadcast algorithm, thus, not suffering from the bottleneck of a central coordinator. In ALLCONCUR, all components exchange messages concurrently through a logical overlay network that employs early termination to minimize the agreement latency. Our implementation of ALLCONCUR supports standard sockets-based TCP as well as high-performance InfiniBand Verbs communications. ALLCONCUR can handle up to 135 million requests per second and achieves 17\u00d7 higher throughput than today\u2019s standard leader-based protocols, such as Libpaxos. Thus, ALLCONCUR is highly competitive with regard to existing solutions and, due to its decentralized approach, enables hitherto unattainable system designs in a variety of fields."}
{"_id":"22be3a5dac2c67a71c78e6bc70f7f4649c636826","title":"Ontology-based decision making on uncontrolled intersections and narrow roads","text":"Many Advanced Driver Assistance Systems (ADAS) have been developed to improve car safety. However, it is still a challenging problem to make autonomous vehicles to drive safely on urban streets such as uncontrolled intersections (without traffic lights) and narrow roads. In this paper, we introduce a decision making system that can assist autonomous vehicles at uncontrolled intersections and narrow roads. We constructed a machine understandable ontology-based Knowledge Base, which contains maps and traffic regulations. The system makes decisions in comply with traffic regulations such as Right-Of-Way rules when it receives a collision warning signal. The decisions are sent to a path planning system to change the route or stop to avoid collisions."}
{"_id":"ea949df68de667b25adfd68caebde203a4d48b01","title":"The 500MHz band low power rectenna for DTV in the Tokyo area","text":"In this paper, the 500MHz band low power rectenna with high sensitivity characteristic is described. The rectenna is consisting of the folded dipole antenna with a 1.6k\u03a9 impedance and the Cockcroft-Walton type rectifier for improvement of the rectification efficiency and the output dc voltage. Furthermore an L-Type LPF is added to reduce the negative influence due to the junction capacitance of the SBD. The developed rectennas (stage number: 1, 2) achieve the rectification efficiency of 48.9 % (stage number: 1) and 43.1 % (stage number: 2) at -15dBm of the input power. In addition to above, the top level efficiency of 8.7 % (stage number:1) at the input power of -30dBm can be achieved. In the Tokyo are, the output dc voltage of 0.22 V can be measured at the 25 km point from the antenna tower."}
{"_id":"4529c536c241e0e1deaf6f8d03c9e340684e9412","title":"Profanity in media associated with attitudes and behavior regarding profanity use and aggression.","text":"OBJECTIVE\nWe hypothesized that exposure to profanity in media would be directly related to beliefs and behavior regarding profanity and indirectly to aggressive behavior.\n\n\nMETHODS\nWe examined these associations among 223 adolescents attending a large Midwestern middle school. Participants completed a number of questionnaires examining their exposure to media, attitudes and behavior regarding profanity, and aggressive behavior.\n\n\nRESULTS\nResults revealed a positive association between exposure to profanity in multiple forms of media and beliefs about profanity, profanity use, and engagement in physical and relational aggression. Specifically, attitudes toward profanity use mediated the relationship between exposure to profanity in media and subsequent behavior involving profanity use and aggression.\n\n\nCONCLUSIONS\nThe main hypothesis was confirmed, and implications for the rating industry and research field are discussed."}
{"_id":"1c32125cc9fb052b881a6dec812b62ed998915d7","title":"Lessons from applying the systematic literature review process within the software engineering domain","text":"A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone. 2006 Elsevier Inc. All rights reserved."}
{"_id":"90089d3ec1c73857e7fde0d8a234d0e147c5a477","title":"SLR-Tool - A Tool for Performing Systematic Literature Reviews","text":"Systematic literature reviews (SLRs) have been gaining a significant amount of attention from Software Engineering researchers since 2004. SLRs are considered to be a new research methodology in Software Engineering, which allow evidence to be gathered with regard to the usefulness or effectiveness of the technology proposed in Software Engineering for the development and maintenance of software products. This is demonstrated by the growing number of publications related to SLRs that have appeared in recent years. While some tools exist that can support some or all of the activities of the SLR processes defined in (Kitchenham & Charters, 2007), these are not free. The objective of this paper is to present the SLR-Tool, which is a free tool and is available on the following website: http:\/\/alarcosj.esi.uclm.es\/SLRTool\/, to be used by researchers from any discipline, and not only Software Engineering. SLR-Tool not only supports the process of performing SLRs proposed in (Kitchenham & Charters, 2007), but also provides additional functionalities such as: refining searches within the documents by applying text mining techniques; defining a classification schema in order to facilitate data synthesis; exporting the results obtained to the format of tables and charts; and exporting the references from the primary studies to the formats used in bibliographic packages such as EndNote, BibTeX or Ris. This tool has, to date, been used by members of the Alarcos Research Group and PhD students, and their perception of it is that it is both highly necessary and useful. Our purpose now is to circulate the use of SLR-Tool throughout the entire research community in order to obtain feedback from other users."}
{"_id":"444b9f2fff2132251a43dc4a4f8bd213e7763634","title":"Evidence-based software engineering","text":"Objective: Our objective is to describe how softwareengineering might benefit from an evidence-basedapproach and to identify the potential difficultiesassociated with the approach.Method: We compared the organisation and technicalinfrastructure supporting evidence-based medicine (EBM)with the situation in software engineering. We consideredthe impact that factors peculiar to software engineering(i.e. the skill factor and the lifecycle factor) would haveon our ability to practice evidence-based softwareengineering (EBSE).Results: EBSE promises a number of benefits byencouraging integration of research results with a view tosupporting the needs of many different stakeholdergroups. However, we do not currently have theinfrastructure needed for widespread adoption of EBSE.The skill factor means software engineering experimentsare vulnerable to subject and experimenter bias. Thelifecycle factor means it is difficult to determine howtechnologies will behave once deployed.Conclusions: Software engineering would benefit fromadopting what it can of the evidence approach providedthat it deals with the specific problems that arise from thenature of software engineering."}
{"_id":"66392ca9b0fc8bb8c8d27312ddd90ca3b418516a","title":"Factors Influencing the Usage of Websites: The Case of a Generic Portal in the Netherlands","text":"In this paper, we empirically investigate an extension of the Technology Acceptance Model (TAM, Davis, 1989) to explain the individual acceptance and usage of websites. Conceptually, we examine perceived ease-of-use, usefulness, enjoyment, and their impact on attitude towards using, intention to use and actual use. The paper also introduces a new construct, \u201cperceived visual attractiveness\u201d of the website and suggest that it influences usefulness, enjoyment, and ease-of-use. For our empirical research we partnered with a Dutch generic portal site with over 300 000 subscribers at the time the research was conducted. The websurvey resulted in sample size of 825 respondents. The results confirmed all of the 12 hypotheses formulated. Three findings are worth mentioning in particular: (1) intention is most dominantly influenced by attitude (\u03b2 = 0.51), (2) ease-of-use, enjoyment, and usefulness contribute equally to attitude towards using (\u03b2 = 0.23, 0.23, and 0.17 respectively) and (3) visual attractiveness contributes remarkably well to both ease-of-use, enjoyment, and usefulness (\u03b2 = 0.41, 0.35, and 0.21). Although this is not the first research to apply TAM to an internet context, we claim three major contributions: (1) a single website as the unit of analysis, (2) the introduction of visual attractiveness, and (3) the use of \u201creal\u201d website visitors rather than student samples. Promising future research lies in the conceptual connection between actual website features and website use, a connection for which the TAM framework provides a meaningful bridge. Factors Influencing the Usage of Websites: The Case of a Generic Portal in the Netherlands"}
{"_id":"3a9c17cc59386433e743d537abcb7b8076827061","title":"A Comparative Study of Data Clustering Algorithms","text":"Data clustering is a process of partitioning data points into meaningful clusters such that a cluster holds similar data and different clusters hold dissimilar data. It is an unsupervised approach to classify data into different patterns. In general, the clustering algorithms can be classified into the following two categories: firstly, hard clustering, where a data object can belong to a single and distinct cluster and secondly, soft clustering, where a data object can belong to different clusters. In this report we have made a comparative study of three major data clustering algorithms highlighting their merits and demerits. These algorithms are: k-means, fuzzy c-means and K-NN clustering algorithm. Choosing an appropriate clustering algorithm for grouping the data takes various factors into account for illustration one is the size of data to be partitioned."}
{"_id":"fa20723e6d7eaa64ba3fd64d0f505adda3ec18ea","title":"A low-Cost, Open-Source, BCI- VR Game Control Development Environment Prototype for Game Based Neurorehabilitation","text":"In this paper we present a low-cost and open-source brain-computer interface (BCI) virtual-reality (VR) Game Control Development Environment prototype, which we demonstrate using real-time signal processing of Electroencephalography (EEG) event-related desynchronization and synchronization changes (ERD\/ERS) within the Precentral Gyrus (Motor Cortex), allowing a user to control a 3D object within a Virtual Reality Environment. This BCI-VR system prototype was functionally tested on multiple participants and demonstrated live before an audience during the 2017 \u2018Hack the Brain\u2019 at the Dublin Science Gallery. The availability of such an open-source, effective, BCI-VR Game Control Development Environment, at a level acceptable for industry experimentation, has the potential to open up this field to a much wider range of researchers and games developers and to assist the investigation of gaming experiences which both incorporate the specific control features available through BCI as a core element of the game play and the potential for its use in neurorehabilitation."}
{"_id":"e145012743605857e89755703d82de1a9dc2df94","title":"Self-esteem and perceived regard: how I see myself affects my relationship satisfaction.","text":"The authors explored the relations among self-esteem, perceived regard, and satisfaction in dating relationships. On the basis of the dependency regulation model (T. DeHart, B. Pelham, & S. Murray, 2004), the authors hypothesized that self-esteem would influence individuals' self-perceptions and views of how their partners perceive them (metaperception). They also hypothesized that perceived regard (self-perception minus metaperception) would predict relationship satisfaction. Regression analyses indicated that for moderate relationship-relevant traits (i.e., caring, loving), high self-esteem was associated with self-enhancement (idealization), whereas low self-esteem was associated with self-deprecation. For low relationship-relevant traits (i.e., quiet, reserved), both low and high self-esteem individuals self-enhanced. Hierarchical regression analyses indicated that self-esteem and perceived regard for moderate relationship-relevant traits predicted satisfaction. The authors discuss the implications of idealization, self-verification, and self-deprecation for the perceivers and their relationships."}
{"_id":"3d277e7f6c1821aa3af961f359925c26d5167601","title":"A systematic survey on automated concurrency bug detection, exposing, avoidance, and fixing techniques","text":"Currently, concurrent programs are becoming increasingly widespread to meet the demands of the rapid development of multi-core hardware. However, it could be quite expensive and challenging to guarantee the correctness and efficiency of concurrent programs. In this paper, we provide a systematic review of the existing research on fighting against concurrency bugs, including automated concurrency bug exposing, detection, avoidance, and fixing. These four categories cover the different aspects of concurrency bug problems and are complementary to each other. For each category, we survey the motivation, key issues, solutions, and the current state of the art. In addition, we summarize the classical benchmarks widely used in previous empirical studies and the contribution of active research groups. Finally, some future research directions on concurrency bugs are recommended. We believe this survey would be useful for concurrency programmers and researchers."}
{"_id":"bb45e05baf716632e885f39be67ead141c9d0a55","title":"[Skin signs in child abuse].","text":"Child abuse is far more prevalent today than is generally recognized. Up to 90% of victims suffer physical abuse that can be observed in signs on the skin. Dermatologists are particularly qualified to identify these signs and distinguish them from other conditions that can mimic abuse. This review covers the signs of child abuse that can be observed on the skin. We discuss clues that can help differentiate between lesions caused by abuse and those that are accidental, and we describe the skin conditions that mimic physical abuse."}
{"_id":"7e02c5b0e83cd6bba4c94c458bdb7079e97c36cd","title":"Entity Resolution: Theory, Practice & Open Challenges","text":"This tutorial brings together perspectives on ER from a variety of fields, including databases, machine learning, natural language processing and information retrieval, to provide, in one setting, a survey of a large body of work. We discuss both the practical aspects and theoretical underpinnings of ER. We describe existing solutions, current challenges, and open research problems."}
{"_id":"cf234668399ff2d7e5e5a54039907b0fa7cf36d3","title":"Survey on 3D Hand Gesture Recognition","text":"Three-dimensional hand gesture recognition has attracted increasing research interests in computer vision, pattern recognition, and human-computer interaction. The emerging depth sensors greatly inspired various hand gesture recognition approaches and applications, which were severely limited in the 2D domain with conventional cameras. This paper presents a survey of some recent works on hand gesture recognition using 3D depth sensors. We first review the commercial depth sensors and public data sets that are widely used in this field. Then, we review the state-of-the-art research for 3D hand gesture recognition in four aspects: 1) 3D hand modeling; 2) static hand gesture recognition; 3) hand trajectory gesture recognition; and 4) continuous hand gesture recognition. While the emphasis is on 3D hand gesture recognition approaches, the related applications and typical systems are also briefly summarized for practitioners."}
{"_id":"be8d148b7f20690c3cc08d516d9ad6d9316311a1","title":"Model-Driven ERP Implementation","text":"Enterprise Resource Planning (ERP) implementations are very complex. To obtain a fair level of understanding of the system, it is then necessary to model the supported business processes. However, the problem is the accuracy of the mapping between this model and the actual technical implementation. A solution is to make use of the OMG\u2019s Model-Driven Architecture (MDA) framework. In fact, this framework lets the developer model his system at a high abstraction level and allows the MDA tool to generate the implementation details. This paper presents our results in applying the MDA framework to ERP implementation based on a high level model of the business processes. Then, we show how our prototype is structured and implemented in the IBM\/Rational XDE environment"}
{"_id":"5386ad8a18b76c85840385a4d208e239208079cd","title":"A General Divide and Conquer Approach for Process Mining","text":"Operational processes leave trails in the information systems supporting them. Such event data are the starting point for process mining \u2013 an emerging scientific discipline relating modeled and observed behavior. The relevance of process mining is increasing as more and more event data become available. The increasing volume of such data (\u201cBig Data\u201d) provides both opportunities and challenges for process mining. In this paper we focus on two particular types of process mining: process discovery (learning a process model from example behavior recorded in an event log) and conformance checking (diagnosing and quantifying discrepancies between observed behavior and modeled behavior). These tasks become challenging when there are hundreds or even thousands of different activities and millions of cases. Typically, process mining algorithms are linear in the number of cases and exponential in the number of different activities. This paper proposes a very general divide-and-conquer approach that decomposes the event log based on a partitioning of activities. Unlike existing approaches, this paper does not assume a particular process representation (e.g., Petri nets or BPMN) and allows for various decomposition strategies (e.g., SESEor passage-based decomposition). Moreover, the generic divide-andconquer approach reveals the core requirements for decomposing process discovery and conformance checking problems."}
{"_id":"1d323cff44e52dfe7be117b39b7974e61cecdd43","title":"Survey of Consistent Network Updates","text":"Computer networks have become a critical infrastructure. As such, networks should not only meet strict requirements in terms of correctness, availability, and performance, but they should also be very flexible and support fast updates, e.g., due to a change in the security policy, increasing traffic, or failures. In this paper, we present a structured survey of mechanisms and protocols to update computer networks in a fast and consistent manner. In particular, we identify and discuss the different desirable consistency properties that should be provided throughout a network update, the algorithmic techniques which are needed to meet these consistency properties, and the implications on the speed and costs at which updates can be performed. We also explain the relationship between consistent network update problems and classic algorithmic optimization ones. While our survey is mainly motivated by the advent of Software-Defined Networks (SDNs) and their primary need for correct and efficient update techniques, the fundamental underlying problems are not new, and we provide a historical perspective of the subject as well. CCS Concepts: rGeneral and reference \u2192 Surveys and overviews; rNetworks \u2192 Network algorithms; rComputer systems organization\u2192 Distributed architectures;"}
{"_id":"082bcec29d021ef844736565277cfc3a0c9c3c43","title":"Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals","text":"Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem. Although many techniques take advantage of a seed bilingual lexicon, this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model. Even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization. Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate."}
{"_id":"6b66495ecca056c0ca118c1577bb17beca4be8ab","title":"HEVC\/H.265 vs. VP9 state-of-the-art video coding comparison for HD and UHD applications","text":"High Efficiency Video Coding (HEVC) and VP9 are the latest state-of-the-art high-efficiency video coding standards. Both standards aim to decrease the bit-rate while improving the compression efficiency. HEVC\/H.265 video standard was collaboratively developed by ITU-T and ISO\/IEC with up to 50% bit-rate reduction as compared to its predecessor H.264\/AVC. On the other hand, VP9 is an open source video standard developed by Google. While maintaining the same video quality, VP9 has achieved 50% bit-rate reduction as compared to its predecessor VP8. This research paper focuses on the comparison of HEVC and VP9 based on both subjective as well as objective assessments. First, detailed differences in design and specification of the two standards are discussed. Later experiment results are provided, which compare the coding efficiency of both standards on various high-definition 720p, 1090p, and 2160p sample test bench videos. According to our experimental results, it has been empirically proved that HEVC\/H.265 outperforms VP9 in bit-rate savings."}
{"_id":"946390bd206d9b92c1ba930e8faaa62343da4a6e","title":"The Narrative Advantage : Gender and the Language of Crowdfunding","text":"In this study, we set out to examine the role of language in the success of online fundraising\u2014a new form of entrepreneurial project financing. In particular, we evaluate the influence of linguistic content on fundraising outcomes, above and beyond type of product or service offered. Online fundraising settings pose an interesting empirical puzzle: women are systematically more successful than men, an outcome contrary to offline gender inequality. We propose that this outcome is partially explained by linguistic differences between men and women in terms of language they use, and we test this mechanism using data from the online crowdfunding platform Indiegogo. The results support our theory, suggesting a link between micro-level linguistic choices and macro level outcomes: the institution of crowdfunding may reduce gender inequalities in the fundraising arena by benefitting the communication style of women."}
{"_id":"24b904d6d0db56a9e4e659ad977af7b116ab2a09","title":"Low voltage sub-nanosecond pulsed current driver IC for high-resolution LIDAR applications","text":"This paper introduces a new low voltage sub-nanosecond monolithic pulsed current driver for light detection and ranging (LIDAR) applications. Unique architecture based on a controlled current source and Vernier activation sequence, combined with a monolithic implementation that allows operation with low input voltage levels, high-resolution pulse width and sub-nanosecond rise and fall times. An on-chip low voltage pulsed driver sub-nanosecond prototype has been implemented in a TS 0.18-\u03bcm 5V-gated power management process. It incorporates an integrated wide range sesnseFET based current sensor and a rail-to-rail comparator for current regulation. To characterize the avalanche capabilities of the integrated lateral MOSFET power devices required for the driver IC, a separate line of investigation has been carried out. Several lateral diffused MOS (LDMOS) power devices have been custom designed and experimentally evaluated for a life-cycle performance characterization. Post-layout analysis of the power driver IC is in a good agreement with the theoretical predictions. For a 5V input voltage, rise and fall times of the laser pulse light output are on the order of hundreds of picoseconds, with currents up to 5A. To validate the concept of high-resolution pulse width generation and short fall time, a discrete prototype has been constructed and experimentally tested."}
{"_id":"60240fc5d76df560f0ef87ced4528c815119b0df","title":"Semi-Paired Discrete Hashing: Learning Latent Hash Codes for Semi-Paired Cross-View Retrieval","text":"Due to the significant reduction in computational cost and storage, hashing techniques have gained increasing interests in facilitating large-scale cross-view retrieval tasks. Most cross-view hashing methods are developed by assuming that data from different views are well paired, e.g., text-image pairs. In real-world applications, however, this fully-paired multiview setting may not be practical. The more practical yet challenging semi-paired cross-view retrieval problem, where pairwise correspondences are only partially provided, has less been studied. In this paper, we propose an unsupervised hashing method for semi-paired cross-view retrieval, dubbed semi-paired discrete hashing (SPDH). In specific, SPDH explores the underlying structure of the constructed common latent subspace, where both paired and unpaired samples are well aligned. To effectively preserve the similarities of semi-paired data in the latent subspace, we construct the cross-view similarity graph with the help of anchor data pairs. SPDH jointly learns the latent features and hash codes with a factorization-based coding scheme. For the formulated objective function, we devise an efficient alternating optimization algorithm, where the key binary code learning problem is solved in a bit-by-bit manner with each bit generated with a closed-form solution. The proposed method is extensively evaluated on four benchmark datasets with both fully-paired and semi-paired settings and the results demonstrate the superiority of SPDH over several other state-of-the-art methods in term of both accuracy and scalability."}
{"_id":"3b76c51beceb5057b1285bd7d709817cda17adc0","title":"Exploration and apprenticeship learning in reinforcement learning","text":"We consider reinforcement learning in systems with unknown dynamics. Algorithms such as E3 (Kearns and Singh, 2002) learn near-optimal policies by using \"exploration policies\" to drive the system towards poorly modeled states, so as to encourage exploration. But this makes these algorithms impractical for many systems; for example, on an autonomous helicopter, overly aggressive exploration may well result in a crash. In this paper, we consider the apprenticeship learning setting in which a teacher demonstration of the task is available. We show that, given the initial demonstration, no explicit exploration is necessary, and we can attain near-optimal performance (compared to the teacher) simply by repeatedly executing \"exploitation policies\" that try to maximize rewards. In finite-state MDPs, our algorithm scales polynomially in the number of states; in continuous-state linear dynamical systems, it scales polynomially in the dimension of the state. These results are proved using a martingale construction over relative losses."}
{"_id":"4f1e9c218336ce5945078291d56a2133c444e088","title":"Simple, Accurate, and Robust Nonparametric Blind Super-Resolution","text":"This paper proposes a simple, accurate, and robust approach to single image nonparametric blind Super-Resolution (SR). This task is formulated as a functional to be minimized with respect to both an intermediate super-resolved image and a nonparametric blur-kernel. The proposed approach includes a convolution consistency constraint which uses a non-blind learning-based SR result to better guide the estimation process. Another key component is the unnatural bi-l0-l2-norm regularization imposed on the super-resolved, sharp image and the blur-kernel, which is shown to be quite beneficial for estimating the blur-kernel accurately. The numerical optimization is implemented by coupling the splitting augmented Lagrangian and the conjugate gradient (CG). Using the pre-estimated blur-kernel, we finally reconstruct the SR image by a very simple non-blind SR method that uses a natural image prior. The proposed approach is demonstrated to achieve better performance than the recent method by Michaeli and Irani [2] in both terms of the kernel estimation accuracy and image SR quality."}
{"_id":"ed28ce5a2d74e28949f33ae9bb521c4795b53df2","title":"The Role of the Gut Microbiota in Childhood Obesity.","text":"BACKGROUND\nChildhood and adolescent obesity has reached epidemic proportions worldwide. The pathogenesis of obesity is complex and multifactorial, in which genetic and environmental contributions seem important. The gut microbiota is increasingly documented to be involved in the dysmetabolism associated with obesity.\n\n\nMETHODS\nWe conducted a systematic search for literature available before October 2015 in the PubMed and Scopus databases, focusing on the interplay between the gut microbiota, childhood obesity, and metabolism.\n\n\nRESULTS\nThe review discusses the potential role of the bacterial component of the human gut microbiota in childhood and adolescent-onset obesity, with a special focus on the factors involved in the early development of the gut bacterial ecosystem, and how modulation of this microbial community might serve as a basis for new therapeutic strategies in combating childhood obesity. A vast number of variables are influencing the gut microbial ecology (e.g., the host genetics, delivery method, diet, age, environment, and the use of pre-, pro-, and antibiotics); but the exact physiological processes behind these relationships need to be clarified.\n\n\nCONCLUSIONS\nExploring the role of the gut microbiota in the development of childhood obesity may potentially reveal new strategies for obesity prevention and treatment."}
{"_id":"a6ce6adcce4c716f4fd0ead92c2db38b4f04b0b3","title":"Determination of the plan of the A Famosa Fortress , Malaysia","text":"The \u201cA Famosa Fortress\u201d is one of the oldest partially extant European buildings in Malaysia. It was built in 1511 by the Portuguese and went through several architectural developments and changes before being largely destroyed during the British occupation in 1824. With the subsequent overbuilding of the site by Melaka city today, it is impossible to fully reconstruct this fortress in its physical context. In this paper, we focus on determining the fortress layout based on various textual descriptions and old drawings and plans in preparation to building a detailed 3-D digital model. We have identified several important issues arising from the lack of any authoritative documentation. Such plans as exist not only differ in their depiction of the fort, but also use various ancient measurement systems. The paper gives examples of these problems and shows how a verifiable conjectural layout has been constructed. This is then compared against such archaeological evidence as is currently available. We are not aware of any previously published attempt to verify the consistency, similarity and integrity of the documentary data."}
{"_id":"5ed5f368aa5ee237828f058d883eec5d489e650b","title":"Learning effective Gait features using LSTM","text":"Human gait is an important biometric feature for person identification in surveillance videos because it can be collected at a distance without subject cooperation. Most existing gait recognition methods are based on Gait Energy Image (GEI). Although the spatial information in one gait sequence can be well represented by GEI, the temporal information is lost. To solve this problem, we propose a new feature learning method for gait recognition. Not only can the learned feature preserve temporal information in a gait sequence, but it can also be applied to cross-view gait recognition. Heatmaps extracted by a convolutional neutral network (CNN) based pose estimate method are used to describe the gait information in one frame. To model a gait sequence, the LSTM recurrent neural network is naturally adopted. Our LSTM model can be trained with unlabeled data, where the identity of the subject in a gait sequence is unknown. When labeled data are available, our LSTM works as a frame to frame view transformation model (VTM). Experiments on a gait benchmark demonstrate the efficacy of our method."}
{"_id":"70b7de43aaed79f769c0fa53a5a34f195984f5da","title":"Intervention Studies on Forgiveness : A Meta-Analysis","text":"79 A promising area of counseling research that emerged in the 1990s is the scientific investigation of forgiveness interventions. Although the notion of forgiving is ancient (Enright & the Human Development Study Group, 1991), it has not been systematically studied until relatively recently (Enright, Santos, & Al-Mabuk, 1989). Significant to counseling because of its interpersonal nature, forgiveness issues are relevant to the contexts of marriage and dating relationships, parent\u2013child relationships, friendships, professional relationships, and others. In addition, forgiveness is integral to emotional constructs such as anger. As forgiveness therapies (Ferch, 1998; Fitzgibbons, 1986) and the empirical study of these therapies (Freedman & Enright, 1996) begin to unfold, it is important to ask if these interventions can consistently demonstrate salient positive effects on levels of forgiveness and on the mental health of targeted clients. The purpose of this article is to analyze via meta-analysis the existing published interventions on forgiveness. Meta-analysis is a popular vehicle of synthesizing results across multiple studies. Recent successful uses of this method include the study by McCullough (1999), who analyzed five studies that compared the efficacy for depression of standard approaches with counseling with religion-accommo-dative approaches. Furthermore, in order to reach conclusions about the influence of hypnotherapy on treatment for clients with obesity, Allison and Faith (1996) used meta-analysis to examine six studies that compared the efficacy of using cognitive-behavioral therapy (CBT) alone with the use of CBT combined with hypnotherapy. Finally, Morris, Audet, Angelillo, Chalmers, and Mosteller (1992) used meta-analysis to combine the results of 10 studies with contradictory findings to show that the benefits of chlorinating drinking water far outweighed the risks. Although there may be some concern that using forgiveness as an intervention in counseling is in too early a stage of development and that too few studies exist for a proper meta-analysis, the effectiveness of these recent meta-analyses supports this meta-analytic investigation. Certainly any findings must be tempered with due caution. However, this analysis may serve as important guidance for the structure and development of future counseling studies of forgiveness. We first examine the early work in forgiveness interventions by examining the early case studies. From there, we define forgiveness, discuss the models of forgiveness in counseling and the empirically based interventions, and then turn to the meta-analysis. The early clinical case studies suggested that forgiveness might be helpful for people who have experienced deep emotional pain because of unjust treatment. For \u2026"}
{"_id":"54702c9e7fa1aea1ce2802e0d7c26d0db0b48cf4","title":"Supervised Representation Learning for Audio Scene Classification","text":"This paper investigates the use of supervised feature learning approaches for extracting relevant and discriminative features from acoustic scene recordings. Owing to the recent release of open datasets for acoustic scene classification problems, representation learning techniques can now be envisioned for solving the problem of feature extraction. This paper makes a step toward this goal by first introducing a supervised nonnegative matrix factorization SNMF. Our goal through this SNMF is to induce the matrix decomposition to carry out discriminative information in addition to the usual generative ones. We achieve this objective by augmenting the nonnegative matrix factorization optimization problem with a novel loss function related to class labels of each column of the matrix to decompose. While the scale of the datasets available is still small compared to those available in computer vision, we have studied models based on convolutional neural networks. We have analyzed the performances of these models on the DCASE-16 dataset and a corrected version of the LITIS Rouen one. Our experiments show that despite the small-scale setting, supervised feature learning is favorably competitive compared to the current state-of-the-art features. We also point out that for smaller scale dataset, SNMF is indeed slightly less prone to overfitting than convolutional neural networks. While the performances of these learned features are interesting per se, a deeper analysis of their behavior in the acoustic scene problem context raises open and difficult questions that we believe, need to be addressed for further performance breakthroughs."}
{"_id":"471b8e9b2fea73f6cee2b15364611b2d3e5ae471","title":"Pattern recognition and knowledge discovery from road traffic accident data in Ethiopia: Implications for improving road safety","text":"This research tries to view accident data collection and analysis as a system that requires a special view towards understanding the whole and making sense out of it for improved decision making in the effort of reducing the problem of road safety. Under the umbrella of an information architecture research for road safety in developing countries, the objective of this machine learning experimental research is to explore and predict the role of road users on possible injury risks. The research employed Classification and Adaptive Regression Trees (CART) and RandomForest approaches. To identify relevant patterns and illustrate the performance of the techniques for the road safety domain, road accident data collected from Addis Ababa Traffic Office is exposed to many sided analyses. Empirical results showed that the models could classify accidents with promising accuracy."}
{"_id":"9a55434c23b2c9505a4157343251b691ddb7be2f","title":"Normative Social Influence in Persuasive Technology: Intensity versus Effectiveness","text":"Persuasion is a form of social influence that implies intentional but voluntary change of a person\u2019s behaviours, feelings or thoughts about an issue, object or action. Successful persuasion by means of persuasive technology relies on a number of factors and motivators. Recently, the so called social acceptance or rejection motivator has formed the backbone of a new type of persuasion, called Mass Interpersonal Persuasion, or MIP for short. This type of persuasion uses the social influence that is generated on online social networks to persuade people. Though it has been established that normative social influence can be used effectively in persuasive technology, it is unknown if the application of more social pressure also makes it more effective. In order to test the hypothesis that the effectiveness of persuasion increases when the persuasion becomes more intense, a quantitative experiment was conducted on the online social network of Facebook. Although evidence to support the hypothesis was found, it cannot be concluded from this experiment that when utilizing normative social influence in persuasive technology more intense persuasion is also more effective."}
{"_id":"c641facdb4a994e0292b94a3a269b6585473fb0b","title":"The Impact of Information Communication and Technology on Students ' Academic Performance : Evidence from Indonesian EFL Classrooms","text":"The present study examined the impact of Information Communication Technology [ICT] on a group of university students' academic performance in an Indonesian English as a Foreign Language (EFL) classroom. As the platform of the ICT usage in this current study, English learning websites was used as the independent variable. Academic performance (students' score on pre and post test) was used the dependent variable. The participants in the study were 60 students of the Department of Public Health at the State University of Gorontalo, Indonesia, i.e an experimental group of 30 students (n=30) and a control group of 30 students (n=30). They took English courses as one of the compulsory subjects in the university curriculum. This study used a mixed method of a quasi-experimental and a qualitative interview approaches. Based on the result of the quantitative method of data collection, ttests of this study indicated that students in the experiment group performed significantly better than the students in the control group. Test results also showed that there was a significant difference between students' score on the preand post-test. The students' score in the post test and post test in the control group, however, were not significantly different. As interview results showed, participants expressed their positive learning experience with technologies, growing motivation in English learning, and positive feeling and confidence on their language performance."}
{"_id":"c8965cc5c62a245593dbc679aebdf3338bb945fc","title":"Visual odometry for ground vehicle applications","text":"We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual estimates can also be used in conjunction with information from other sources such as GPS, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results obtained with a stereo-head mounted on an autonomous ground vehicle. We give examples of camera trajectories estimated in real-time purely from images over previously unseen distances (600 meters) and periods of time ."}
{"_id":"f6f864260526fb823f5be5f12d111864b70f29ed","title":"Collaborative matrix factorization mechanism for group recommendation in big data-based library systems","text":"Purpose \u2013Academic groups are designed specifically for researchers. A group recommendation procedure is essential to support scholars\u2019 research-based social activities. However, group recommendation methods are rarely applied in online libraries and they often suffer from scalability problem in big data context. The purpose of this paper is to facilitate academic group activities in big data-based library systems by recommending satisfying articles for academic groups. Design\/methodology\/approach \u2013 The authors propose a collaborative matrix factorization (CoMF) mechanism and implement paralleled CoMF under Hadoop framework. Its rationale is collaboratively decomposing researcher-article interaction matrix and group-article interaction matrix. Furthermore, three extended models of CoMF are proposed. Findings \u2013 Empirical studies on CiteULike data set demonstrate that CoMF and three variants outperform baseline algorithms in terms of accuracy and robustness. The scalability evaluation of paralleled CoMF shows its potential value in scholarly big data environment. Research limitations\/implications \u2013 The proposed methods fill the gap of group-article recommendation in online libraries domain. The proposed methods have enriched the group recommendation methods by considering the interaction effects between groups and members. The proposed methods are the first attempt to implement group recommendation methods in big data contexts. Practical implications \u2013 The proposed methods can improve group activity effectiveness and information shareability in academic groups, which are beneficial to membership retention and enhance the service quality of online library systems. Furthermore, the proposed methods are applicable to big data contexts and make library system services more efficient. Social implications \u2013 The proposed methods have potential value to improve scientific collaboration and research innovation. Originality\/value \u2013 The proposed CoMF method is a novel group recommendation method based on the collaboratively decomposition of researcher-article matrix and group-article matrix. The process indirectly reflects the interaction between groups and members, which accords with actual library environments and provides an interpretable recommendation result."}
{"_id":"dfaafdc85710125a0641e7bafeb849e0daf9fd48","title":"Efficient Algorithms for Citation Network Analysis","text":"In the paper very efficient, linear in number of arcs, algorithms for determining Hummon and Doreian\u2019s arc weights SPLC and SPNP in citation network are proposed, and some theoretical properties of these weights are presented. The nonacyclicity problem in citation networks is discussed. An approach to identify on the basis of arc weights an important small subnetwork is proposed and illustrated on the citation networks of SOM (self organizing maps) literature and US patents."}
{"_id":"d8d160d9f6e987aa9e95a2480e97105fdeebca8f","title":"An image encryption scheme based on elliptic curve pseudo random and Advanced Encryption System","text":"Elliptic curve cryptography (ECC) has proven to be an effective cryptography. ECC has its own advantages such as efficient key size compared to other public key infrastructures. This paper exploits the Elliptic curve random generator defined by National Institute of Standards and Technology (NIST) to generate a sequence of arbitrary numbers based on curves. The random generation phase is based on public shared key and a changing point G, which is a generator of a curve to obtain random sequences. Then, Advanced Encryption System is applied to these sequences acquiring arbitrary keys for encrypting image. Using AES alongside well distributed randoms provides a prominent encryption technique. Our experiments show that the proposed method fulfills the basics of cryptography including simpleness and correctness. Moreover, the results of the evaluation prove the effectiveness and security of the proposed method."}
{"_id":"570c2d6678d6e6d9c080cd07c2c38364e5ac8907","title":"Improved direct back EMF detection for sensorless brushless DC (BLDC) motor drives","text":"Improved back EMF detection circuits for low voltage\/low speed and high voltage sensorless BLDC motor drives are presented in this paper. The improvements are based on the direct back EMF sensing method from our previous research work described in reference, which describes a technique for directly extracting phase back EMF information without the need to sense or re-construct the motor neutral. The reference method is not sensitive to switching noise and requires no filtering, achieving much better performance than traditional back EMF sensing scheme. A complementary PWM (synchronous rectification) is proposed to reduce the power dissipation in power devices for low voltage applications. In order to further extend the sensorless BLDC system to lower speed, a pre-conditioning circuit is proposed to amplify the back EMFs at very low speed. As a result, the brushless DC motor can run at lower speed with the improved back EMF sensing scheme. On the other hand, another improved detection circuit is presented for high voltage applications to overcome the delaying problem caused by large sensing resistors. The detailed circuit models are analyzed and experimental results verify the analysis."}
{"_id":"ea33172af0532d7bf9688ad58a86d567e7b84904","title":"How Not to Do a Mindset Intervention: Learning from a Mindset Intervention among Students with Good Grades","text":"The present study examined the effectiveness of a Growth Mindset intervention based on Dweck et al.'s (1995) theory in the Hungarian educational context. A cluster randomized controlled trial classroom experiment was carried out within the framework of a train-the-trainer intervention among 55 Hungarian 10th grade students with high Grade Point Average (GPA). The results suggest that students' IQ and personality mindset beliefs were more incremental in the intervention group than in the control group 3 weeks after the intervention. Furthermore, compared to both the baseline measure and the control group, students' amotivation decreased. However, no intrinsic and extrinsic motivation change was found. Students with low grit scores reported lower amotivation following the intervention. However, in the second follow-up measurement-the end of the semester-all positive changes disappeared; and students' GPA did not change compared to the previous semester. These results show that mindset beliefs are temporarily malleable and in given circumstances, they can change back to their pre-intervention state. The potential explanation is discussed in the light of previous mindset intervention studies and recent findings on wise social psychological interventions."}
{"_id":"46319a2732e38172d17a3a2f0bb218729a76e4ec","title":"Activity Recognition in the Home Using Simple and Ubiquitous Sensors","text":"In this work, a system for recognizing activities in the home setting using a set of small and simple state-change sensors is introduced. The sensors are designed to be \u201ctape on and forget\u201d devices that can be quickly and ubiquitously installed in home environments. The proposed sensing system presents an alternative to sensors that are sometimes perceived as invasive, such as cameras and microphones. Unlike prior work, the system has been deployed in multiple residential environments with non-researcher occupants. Preliminary results on a small dataset show that it is possible to recognize activities of interest to medical professionals such as toileting, bathing, and grooming with detection accuracies ranging from 25% to 89% depending on the evaluation criteria used ."}
{"_id":"74d67d2b4c6fdc9f6498ac546b9685a14a9c02ee","title":"Crowdsourcing of Pollution Data using Smartphones","text":"In this paper we present our research into participatory sensing based solutions for the collection of data on urban pollution and nuisance. In the past 2 years we have been involved in the NoiseTube project which explores a crowdsourcing approach to measuring and mapping urban noise pollution using smartphones. By involving the general public and using off-the-shelf smartphones as noise sensors, we seek to provide a low cost solution for citizens to measure their personal exposure to noise in their everyday environment and participate in the creation of collective noise maps by sharing their geo-localized and annotated measurements with the community. We believe our work represents an interesting example of the novel mobile crowdsourcing applications which are enabled by ubiquitous computing systems. Furthermore we believe the NoiseTube system, and the currently ongoing validation experiments, provide an illustrative context for some of the open challenges faced by creators of ubiquitous crowdsourcing applications and services in general. We will also take the opportunity to present the insights we gained into some of the challenges."}
{"_id":"30492d91c8ac2e476a25bf7ddf445be00b3d2fe3","title":"CrowdProbe: Non-invasive Crowd Monitoring with Wi-Fi Probe","text":"Devices with integrated Wi-Fi chips broadcast beacons for network connection management purposes. Such information can be captured with inexpensive monitors and used to extract user behavior. To understand the behavior of visitors, we deployed our passive monitoring system---CrowdProbe, in a multi-floor museum for six months. We used a Hidden Markov Models (HMM) based trajectory inference algorithm to infer crowd movement using more than 1.7 million opportunistically obtained probe request frames.\n However, as more devices adopt schemes to randomize their MAC addresses in the passive probe session to protect user privacy, it becomes more difficult to track crowd and understand their behavior. In this paper, we try to make use of historical transition probability to reason about the movement of those randomized devices with spatial and temporal constraints. With CrowdProbe, we are able to achieve sufficient accuracy to understand the movement of visitors carrying devices with randomized MAC addresses."}
{"_id":"18ad2478014dd61d38e8197ff7060e9997e7a989","title":"Evolving neural networks to play checkers without relying on expert knowledge","text":"An experiment was conducted where neural networks compete for survival in an evolving population based on their ability to play checkers. More specifically, multilayer feedforward neural networks were used to evaluate alternative board positions and games were played using a minimax search strategy. At each generation, the extant neural networks were paired in competitions and selection was used to eliminate those that performed poorly relative to other networks. Offspring neural networks were created from the survivors using random variation of all weights and bias terms. After a series of 250 generations, the best-evolved neural network was played against human opponents in a series of 90 games on an internet website. The neural network was able to defeat two expert-level players and played to a draw against a master. The final rating of the neural network placed it in the \"Class A\" category using a standard rating system. Of particular importance in the design of the experiment was the fact that no features beyond the piece differential were given to the neural networks as a priori knowledge. The process of evolution was able to extract all of the additional information required to play at this level of competency. It accomplished this based almost solely on the feedback offered in the final aggregated outcome of each game played (i.e., win, lose, or draw). This procedure stands in marked contrast to the typical artifice of explicitly injecting expert knowledge into a game-playing program."}
{"_id":"40fe245fd4ccdb9eb6b61528d7e88f564784caf3","title":"Verifying Anaconda's expert rating by competing against Chinook: experiments in co-evolving a neural checkers player","text":"Since the early days of arti5cial intelligence, there has been interest in having a computer teach itself how to play a game of skill, like checkers, at a level that is competitive with human experts. To be truly noteworthy, such e7orts should minimize the amount of human intervention in the learning process. Recently, co-evolution has been used to evolve a neural network (called Anaconda) that, when coupled with a minimax search, can evaluate checker-boards and play to the level of a human expert, as indicated by its rating of 2045 on an international web site for playing checkers. The neural network uses only the location, type, and number of pieces on the board as input. No other features that would require human expertise are included. Experiments were conducted to verify the neural network\u2019s expert rating by competing it in 10 games against a \u201cnovice-level\u201d version of Chinook, a world-champion checkers program. The neural network had 2 wins, 4 losses, and 4 draws in the 10-game match. Based on an estimated rating of Chinook at the novice level, the results corroborate Anaconda\u2019s expert rating. c \u00a9 2002 Elsevier Science B.V. All rights reserved."}
{"_id":"61b9de524afa0b134052c50b40a4b48ee60bb410","title":"Decentralized Decision Making in the Game of Tic-tac-toe","text":"Traditionally, the game of Tic-tac-toe is a pencil and paper game played by two people who take turn to place their pieces on a 3times3 grid with the objective of being the first player to fill a horizontal, vertical, or diagonal row with their pieces. What if instead of having one person playing against another, one person plays against a team of nine players, each of whom is responsible for one cell in the 3times3 grid? In this new way of playing the game, the team has to coordinate its players, who are acting independently based on their limited information. In this paper, we present a solution that can be extended to the case where two such teams play against each other, and also to other board games. Essentially, the solution uses a decentralized decision making, which at first seems to complicate the solution. However, surprisingly, we show that in this mode, an equivalent level of decision making ability comes from simple components that reduce system complexity"}
{"_id":"82d02f4782bd208b65fd4e3dea06abe95cc48b04","title":"A self-learning evolutionary chess program","text":"A central challenge of artificial intelligence is to create machines that can learn from their own experience and perform at the level of human experts. Using an evolutionary algorithm, a computer program has learned to play chess by playing games against itself. The program learned to evaluate chessboard configurations by using the positions of pieces, material and positional values, and neural networks to assess specific sections of the chessboard. During evolution, the program improved its play by almost 400 rating points. Testing under simulated tournament conditions against Pocket Fritz 2.0 indicated that the evolved program performs above the master level."}
{"_id":"b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e","title":"Machine learning - an artificial intelligence approach","text":"Research in the area of learning structural descriptions from examples is reviewed, giving primary attention to methods of learning characteristic descrip\u00ad tions of single concepts. In particular, we examine methods for finding the maximally-specific conjunctive generalizations (MSC-generalizations) that cover all of the training examples of a given concept. Various important aspects of structural learning in general are examined, and several criteria for evaluating structural learning methods are presented. Briefly, these criteria include (i) ade\u00ad quacy of the representation language, (ii) generalization rules employed, computational efficiency, and (iv) flexibility and extensibility. Selected learning methods developed by Buchanan, et al., Hayes-Roth, Vere, Winston, and the authors are analyzed according to these criteria. Finally, some goals are sug\u00ad gested for future research."}
{"_id":"55688a3f5596ec42636f4bcfd5f11c97c8ae2a8f","title":"Implementing the Chinese Wall Security Model in Workflow Management Systems","text":"The Chinese wall security model (CWSM) was designed to provide access controls that mitigate conflict of interest in commercial organizations, and is especially important for large-scale interenterprise workflow applications. This paper describes how to implement the CWSM in a WfMS. We first demonstrate situations in which the role-based access control model is not sufficient for this, and we then propose a security policy language to solve this problem, also providing support for the intrinsic dynamic access control mechanism defined in the CWSM (i.e., the dynamic binding of subjects and elements in the company data set). This language can also specify several requirements of the dynamic security policy that arise when applying the CWSM in WfMSs. Finally we discuss how to implement a run-time system to implement CWSM policies specified by this language in a WfMS."}
{"_id":"3ff5ff2c5a333add8cac95ed7b8cb422e5f4ebbb","title":"Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer Interface","text":"In motor imagery-based brain computer interfaces (BCI), discriminative patterns can be extracted from the electroencephalogram (EEG) using the common spatial pattern (CSP) algorithm. However, the performance of this spatial filter depends on the operational frequency band of the EEG. Thus, setting a broad frequency range, or manually selecting a subject-specific frequency range, are commonly used with the CSP algorithm. To address this problem, this paper proposes a novel filter bank common spatial pattern (FBCSP) to perform autonomous selection of key temporal-spatial discriminative EEG characteristics. After the EEG measurements have been bandpass-filtered into multiple frequency bands, CSP features are extracted from each of these bands. A feature selection algorithm is then used to automatically select discriminative pairs of frequency bands and corresponding CSP features. A classification algorithm is subsequently used to classify the CSP features. A study is conducted to assess the performance of a selection of feature selection and classification algorithms for use with the FBCSP. Extensive experimental results are presented on a publicly available dataset as well as data collected from healthy subjects and unilaterally paralyzed stroke patients. The results show that FBCSP, using a particular combination feature selection and classification algorithm, yields relatively higher cross-validation accuracies compared to prevailing approaches."}
{"_id":"0f6af4ad085494b94c6ac66d187ccfd91800ba55","title":"Coping with child sexual abuse among college students and post-traumatic stress disorder: the role of continuity of abuse and relationship with the perpetrator.","text":"OBJECTIVE\nThe purpose of this study was to examine the effects of child sexual abuse (CSA) on the use of coping strategies and post-traumatic stress disorder (PTSD) scores in young adults, as well as the role of avoidance and approach coping strategies in those PTSD scores in CSA victims. The role of coping strategies was studied by considering their possible interactive effect with the continuity of abuse and the relationship with the perpetrator; the effect of coping strategies on PTSD was also compared between CSA victim and non-CSA victim participants.\n\n\nMETHOD\nThe sample was comprised of 138 victims of CSA and another 138 participants selected as a comparison group. Data about child sexual abuse were obtained from a questionnaire developed for this purpose. Coping strategies were assessed with the How I Deal with Things Scale (Burt & Katz, 1987), while PTSD scores were assessed with the \"Escala de Gravedad de S\u00edntomas del Trastorno de Estr\u00e9s Postraum\u00e1tico\" (Severity of Symptoms of PTSD Scale; Echebur\u00faa et al., 1997).\n\n\nRESULTS\nParticipants who had been victims of CSA showed significantly higher PTSD scores and lower approach coping strategies scores. However, differences in avoidance coping strategies between groups were not consistent and did not always follow the expected direction. Only the use of avoidance coping strategies was related to PTSD, participants who used these showing higher scores. The effects of avoidance strategies were stronger in continued than in isolated abuse, in intrafamilial than in extrafamilial abuse and in CSA victims than in non-victims.\n\n\nCONCLUSIONS\nThese results confirm the idea of CSA as a high-risk experience that can affect the victim's coping strategies and lead to PTSD to a lesser or greater extent depending on the coping strategy used. Moreover, the role of these strategies varies depending on whether or not the participant is a victim of CSA and on the characteristics of abuse (continuity and relationship with the perpetrator).\n\n\nPRACTICE IMPLICATIONS\nIn terms of intervention, a reduction of avoidance-type strategies appears to have a beneficial effect, especially in the case of intrafamilial and\/or continued CSA victims. The encouragement of \"spontaneous\" approach strategies (devised by the victim herself, without counseling) would probably not lead to more positive outcomes in terms of PTSD symptomatology. However, encouraging CSA survivors to engage in therapy aimed at developing effective approach strategies, as other studies have suggested, may help reduce PTSD symptoms."}
{"_id":"de1fa7d3eb801dd316cd1e40cb029cd5eb309070","title":"An approch for monitoring and smart planning of urban solid waste management using smart-M3 platform","text":"Solid waste management is one of the most important challenges in urban areas throughout the world and it is becoming a critical issue in developing countries where a rapid increase in population has been observed. Waste collection is a complex process that requires the use of large amount of money and an elaborate management of logistics. In this paper an approch to smart waste collection is proposed able to improve and optimize the handling of solid urban waste. Context of smart waste management requires interconnection among heterogeneous devices and data sharing involving a large amount of people. Smart-M3 platform solves these problems offering a high degree of decoupling and scalability.Waste collection is made by real-time monitoring the level of bin's fullness through sensors placed inside the containers. This method enables to exempt from collecting semi-empty bins. Furthermore, incoming data can be provided to decisional algorithms in order to determine the optimal number of waste vehicles or bins to distribute in the territory. The presented solution gives important advantages for both service providers and consumers. The formers could obtain a sensible cost reduction. On the other hand, users may benefit from a higher level of service quality. In order to make users feel closer to their community, they can interact with the system to be aware about the fulness state of the nearest bins. Finally, a mechanism for collecting \u201cgreen points\u201d was introduced for encouraging citizens to recycle."}
{"_id":"0aa86c5d58b77415364622ce56646ba89c30cb63","title":"Evolutionary algorithms for multiobjective optimization: methods and applications","text":"Many real-world problems involve two types of problem difficulty: i) multiple, conflicting objectives and ii) a highly complex search space. On the one hand, instead of a single optimal solution competing goals give rise to a set of compromise solutions, generally denoted as Pareto-optimal. In the absence of preference information, none of the corresponding trade-offs can be said to be better than the others. On the other hand, the search space can be too large and too complex to be solved by exact methods. Thus, efficient optimization strategies are required that are able to deal with both difficulties. Evolutionary algorithms possess several characteristics that are desirable for this kind of problem and make them preferable to classical optimization methods. In fact, various evolutionary approaches to multiobjective optimization have been proposed since 1985, capable of searching for multiple Paretooptimal solutions concurrently in a single simulation run. However, in spite of this variety, there is a lack of extensive comparative studies in the literature. Therefore, it has remained open up to now: \u2022 whether some techniques are in general superior to others, \u2022 which algorithms are suited to which kind of problem, and \u2022 what the specific advantages and drawbacks of certain methods are. The subject of this work is the comparison and the improvement of existing multiobjective evolutionary algorithms and their application to system design problems in computer engineering. In detail, the major contributions are: \u2022 An experimental methodology to compare multiobjective optimizers is developed. In particular, quantitative measures to assess the quality of trade-off fronts are introduced and a set of general test problems is defined, which are i) easy to formulate, ii) represent essential aspects of real-world problems, and iii) test for different types of problem difficulty. \u2022 On the basis of this methodology, an extensive comparison of numerous evolutionary techniques is performed in which further aspects such as the influence of elitism and the population size are also investigated. \u2022 A novel approach to multiobjective optimization, the strength Pareto evolutionary algorithm, is proposed. It combines both established and new techniques in a unique manner. \u2022 Two complex multicriteria applications are addressed using evolutionary algorithms: i) the automatic synthesis of heterogeneous hardware\/systems and ii) the multidimensional exploration of software implementations for digital signal processors. Zusammenfassung Viele praktische Optimierungsprobleme sind durch zwei Eigenschaften charakterisiert: a) mehrere, teilweise im Konflikt stehende Zielfunktionen sind involviert, und b) der Suchraum ist hochgradig komplex. Einerseits f \u0308 uhren widerspr\u00fcchliche Optimierungskriterien dazu, dass es statt eines klar definierten Optimums eine Menge von Kompromissl \u0308 o ungen, allgemein als Pareto-optimal bezeichnet, gibt. Insofern keine Gewichtung der Kriterien vorliegt, m \u0308 ussen die entsprechenden Alternativen als gleichwertig betrachtet werden. Andererseits kann der Suchraum eine bestimmte Gr \u0308 osse und Komplexit \u0308 at \u00fcberschreiten, so dass exakte Optimierungsverfahren nicht mehr anwendbar sind. Erforderlich sind demnach effiziente Suchstrategien, die beiden Aspekten gerecht werden. Evolution\u00e4re Algorithmen sind aufgrund mehrerer Merkmale f \u0308 ur diese Art von Problem besonders geeignet; vor allem im Vergleich zu klassischen Methoden weisen sie gewisse Vorteile auf. Doch obwohl seit 1985 verschiedenste evolution\u00e4re Ans\u00e4tze entwickelt wurden, die mehrere Pareto-optimale L \u0308 osungen in einem einzigen Simulationslauf generieren k \u0308 onnen, mangelt es in der Literatur an umfassenden Vergleichsstudien. Folglich blieb bislang ungekl \u0308 art, \u2022 ob bestimmte Techniken anderen Methoden generell  \u0308 ub rlegen sind, \u2022 welche Algorithmen f \u0308 ur welche Art von Problem geeignet sind und \u2022 wo die spezifischen Vorund Nachteile einzelner Verfahren liegen. Die vorliegende Arbeit hat zum Gegenstand, bestehende evolution \u0308 are Mehrzieloptimierungsverfahren zu vergleichen, zu verbessern und auf Entwurfsprobleme im Bereich der Technischen Informatik anzuwenden. Im Einzelnen werden folgende Themen behandelt: \u2022 Eine Methodik zum experimentellen Vergleich von Mehrzieloptimierungsverfahren wird entwickelt. Unter anderem werden quantitative Qualit \u0308 atsmasse f\u00fcr Mengen von Kompromissl \u0308 osungen eingef \u0308 uhrt und mehrere Testfunktionen definiert, die a) eine einfache Problembeschreibung besitzen, b) wesentliche Merkmale realer Optimierungsprobleme repr \u0308 asentieren und c) erlauben, verschiedene Einflussfaktoren separat zu  \u0308 uberpr\u00fcfen. \u2022 Auf der Basis dieser Methodik wird ein umfangreicher Vergleich diverser evolution\u00e4rer Techniken durchgef \u0308 uhrt, wobei auch weitere Aspekte wie die Auswirkungen von Elitism und der Populationsgr \u0308 osse auf den Optimierungsprozess untersucht werden. \u2022 Ein neues Verfahren, der Strength-Pareto-Evolutionary-Algorithm, wird vorgestellt. Es kombiniert auf spezielle Art und Weise bew \u0308 ahrte und neue Konzepte miteinander. \u2022 Zwei komplexe Mehrzielprobleme werden auf der Basis evolution \u0308 arer Methoden untersucht: a) die automatische Synthese von heterogenen Hardware\/Software-Systemen und b) die mehrdimensionale Exploration von Softwareimplementierungen f \u0308 ur digitale Signalverarbeitungsprozessoren. I would like to thank Prof. Dr. Lothar Thiele for the valuable discussions concerning this research, Prof. Dr. Kalyanmoy Deb for his willingness to be the co-examiner of my thesis,"}
{"_id":"8e1990cbbc636a66ea04db146a65706efea494f6","title":"Wearable Glove-Type Driver Stress Detection Using a Motion Sensor","text":"Increased driver stress is generally recognized as one of the major factors leading to road accidents and loss of life. Even though physiological signals are reported as the most reliable means to measure driver stresses, they often require the use of unique and expensive sensors, which produce dynamic and varying readings within individuals. This paper presents a novel means to predict a driver\u2019s stress level by evaluating the movement pattern of the steering wheel. This is accomplished by using an inertial motion unit sensor, which is placed on a glove worn by the driver. The motion sensor selected for this paper was chosen because for its low cost and the fact that it is least affected by environmental factors as compared with a physiological signal. Experiments were conducted in three different environmental scenarios. The scenarios were classified as \u201curban,\u201d \u201chighway,\u201d and \u201crural,\u201d and they were chosen to simulate contrasting stress conditions experienced by the driver. In this paper, skin conductance and driver self-reports served as a reference stress to predict the driver\u2019s stress level. Galvanic skin response, a well-known stress indicator, was captured along the driver\u2019s palm and the readings were transmitted to a mobile device via low energy Bluetooth for further processing. The results revealed that indirect measurement of steering wheel movement with an inertial motion sensor could obtain accuracies up to an average rate of 94.78%. This demonstrates the opportunity for inclusion of motion sensors in wireless driver assistance systems for ambulatory monitoring of stress levels."}
{"_id":"82e0749092b5b0730a7394ed2a67d23774f88034","title":"The Role of Mental Health Professionals in Gender Reassignment Surgeries: Unjust Discrimination or Responsible Care?","text":"Recent literature has raised an important ethical concern relating to the way in which surgeons approach people with gender dysphoria (GD): it has been suggested that referring transsexual patients to mental assessment can constitute a form of unjust discrimination. The aim of this paper is to examine some of the ethical issues concerning the role of the mental health professional in gender reassignment surgeries (GRS). The role of the mental health professional in GRS is analyzed by presenting the Standards of Care by the World Professional Association of Transgender Health, and discussing the principles of autonomy and non-discrimination. Purposes of psychotherapy are exploring gender identity; addressing the negative impact of GD on mental health; alleviating internalized transphobia; enhancing social and peer support; improving body image; promoting resilience; and assisting the surgeons with the preparation prior to the surgery and the patient\u2019s follow-up. Offering or requesting psychological assistance is in no way a form of negative discrimination or an attack to the patient\u2019s autonomy. Contrarily, it might improve transsexual patients\u2019 care, and thus at the most may represent a form of positive discrimination. To treat people as equal does not mean that they should be treated in the same way, but with the same concern and respect, so that their unique needs and goals can be achieved. Offering or requesting psychological assistance to individuals with GD is a form of responsible care, and not unjust discrimination. This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com\/00266 ."}
{"_id":"e3d6fc079ac78b27cc0cd00e1ea1f67603c5a75b","title":"Semantics and service technologies for the automatic generation of online MCQ tests","text":"Active learning requires that students receive a continuous feedback about their understanding. Multiple-Choice Questions (MCQ) tests have been frequently used to provide students the required feedback and to measure the effectiveness of this learning model. To construct a test is a challenging task, which is time consuming and requires experience. For these reasons, research efforts have been focused on the automatic generation of well-constructed tests. The semantic technologies have played a relevant role in the implementation of these testgeneration systems. Nevertheless, the existing proposals present a set of drawbacks that restrict their applicability to different learning domains and the type of test to be composed. In this paper, we propose a service-oriented and semantic-based system that solves these drawbacks. The system consists of a dynamic strategy of generating candidate distractors (alternatives to the correct answer), a set of heuristics for scoring the distractors' suitability, and a selection of distractors that considers the difficulty level of tests. Besides, the final version of tests is created using the Google Form service, a de-facto standard for elaborating online questionnaires."}
{"_id":"4a4cea4421ff0be7bcc06e92179cd2d5f1102ff8","title":"Solving traveling salesman problems via artificial intelligent search techniques","text":"The traveling salesman problem (TSP) is one of the most intensively studied problems in computational mathematics and combinatorial optimization. It is also considered as the class of the NPcomplete combinatorial optimization problems. By literatures, many algorithms and approaches have been launched to solve such the TSP. However, no current algorithms that can provide the exactly optimal solution of the TSP problem are available. This paper proposes the application of AI search techniques to solve the TSP problems. Three AI search methods, i.e. genetic algorithms (GA), tabu search (TS), and adaptive tabu search (ATS), are conducted. They are tested against ten benchmark real-world TSP problems. As results compared with the exactly optimal solutions, the AI search techniques can provide very satisfactory solutions for all TSP problems. Key-Words: Traveling Salesman Problem, Genetic Algorithm, Tabu Search, Adaptive Tabu Search"}
{"_id":"37f133901021b8f2183158ded92595cb1fa9a19b","title":"Conjunctive representations in learning and memory: principles of cortical and hippocampal function.","text":"The authors present a theoretical framework for understanding the roles of the hippocampus and neocortex in learning and memory. This framework incorporates a theme found in many theories of hippocampal function: that the hippocampus is responsible for developing conjunctive representations binding together stimulus elements into a unitary representation that can later be recalled from partial input cues. This idea is contradicted by the fact that hippocampally lesioned rats can learn nonlinear discrimination problems that require conjunctive representations. The authors' framework accommodates this finding by establishing a principled division of labor, where the cortex is responsible for slow learning that integrates over multiple experiences to extract generalities whereas the hippocampus performs rapid learning of the arbitrary contents of individual experiences. This framework suggests that tasks involving rapid, incidental conjunctive learning are better tests of hippocampal function. The authors implement this framework in a computational neural network model and show that it can account for a wide range of data in animal learning."}
{"_id":"26891f40f3d7850c6aa340c2dc02c9e2c2b8de1d","title":"Estimation of Detection Thresholds for Redirected Walking Techniques","text":"In immersive virtual environments (IVEs), users can control their virtual viewpoint by moving their tracked head and walking through the real world. Usually, movements in the real world are mapped one-to-one to virtual camera motions. With redirection techniques, the virtual camera is manipulated by applying gains to user motion so that the virtual world moves differently than the real world. Thus, users can walk through large-scale IVEs while physically remaining in a reasonably small workspace. In psychophysical experiments with a two-alternative forced-choice task, we have quantified how much humans can unknowingly be redirected on physical paths that are different from the visually perceived paths. We tested 12 subjects in three different experiments: (E1) discrimination between virtual and physical rotations, (E2) discrimination between virtual and physical straightforward movements, and (E3) discrimination of path curvature. In experiment E1, subjects performed rotations with different gains, and then had to choose whether the visually perceived rotation was smaller or greater than the physical rotation. In experiment E2, subjects chose whether the physical walk was shorter or longer than the visually perceived scaled travel distance. In experiment E3, subjects estimate the path curvature when walking a curved path in the real world while the visual display shows a straight path in the virtual world. Our results show that users can be turned physically about 49 percent more or 20 percent less than the perceived virtual rotation, distances can be downscaled by 14 percent and upscaled by 26 percent, and users can be redirected on a circular arc with a radius greater than 22 m while they believe that they are walking straight."}
{"_id":"3a6d9e74fe6e02bc5e2430fe4ad7317a7c00527a","title":"Managing and Mining Graph Data","text":"Will reading habit influence your life? Many say yes. Reading managing and mining graph data is a good habit; you can develop this habit to be such interesting way. Yeah, reading habit will not only make you have any favourite activity. It will be one of guidance of your life. When reading has become a habit, you will not make it as disturbing activities or as boring activity. You can gain many benefits and importances of reading."}
{"_id":"750b9e868e1b68bdfc8141b93c639d289892fe25","title":"Story Cloze Ending Selection Baselines and Data Examination","text":"This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using features based on word embeddings and semantic similarity computation. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best model achieves an accuracy of 72.42, ranking 3rd in the official evaluation."}
{"_id":"bf341864280ae25a35d2db236bfff05b3a3b8e2e","title":"Comparison of a 1450-nm diode laser and a 1320-nm Nd:YAG laser in the treatment of atrophic facial scars: a prospective clinical and histologic study.","text":"BACKGROUND\nAtrophic scar revision techniques, although numerous, have been hampered by inadequate clinical responses and prolonged postoperative recovery periods. Nonablative laser treatment has been shown to effect significant dermal collagen remodeling with minimal posttreatment sequelae. Although many studies have been published regarding the effectiveness of these nonablative lasers on rhytides, there are limited data demonstrating their specific effects on atrophic scars.\n\n\nOBJECTIVE\nTo evaluate and compare the efficacy and safety of long-pulsed 1320-nm Nd:YAG and 1450-nm diode lasers in the treatment of atrophic facial scarring.\n\n\nMETHODS\nA series of 20 patients (skin phototypes I-V) with mild to moderate atrophic facial acne scars randomly received three successive monthly treatments with a long-pulsed 1320-nm Nd:YAG laser on one facial half and a long-pulsed 1450-nm diode laser on the contralateral facial half. Patients were evaluated using digital photography and three-dimensional in vivo microtopography measurements at each treatment visit and at 1, 3, 6, and 12 months postoperatively. Histologic evaluations of cutaneous biopsies obtained before treatment, immediately after the first treatment, and at 1, 3, 6, and 12 months after the third treatment were performed. Clinical assessment scores were determined at each treatment session and follow-up visit. Patient satisfaction surveys were obtained at the end of the study.\n\n\nRESULTS\nMild to moderate clinical improvement was observed after the series of three treatments in the majority of patients studied. Patient satisfaction scores and in vivo microtopography measurements paralleled the photographic and histopathologic changes seen. Side effects of treatment were limited to mild transient erythema, edema, and hyperpigmentation. No scarring or adverse textural changes resulted from the use of either laser system.\n\n\nCONCLUSIONS\nNonablative long-pulsed 1320-nm Nd:YAG and 1450-nm diode lasers each offer clinical improvement for patients with atrophic scarring without significant side effects or complications. The 1450-nm diode laser showed greater clinical scar response at the parameters studied. The use of nonablative laser systems is a good treatment alternative for patients with atrophic scarring who are unable or unwilling to endure the prolonged postoperative recovery process associated with ablative laser skin resurfacing procedures."}
{"_id":"efb802f5cb3926f9bee6c41ea898dc7134297a0e","title":"Best practices for missing data management in counseling psychology.","text":"This article urges counseling psychology researchers to recognize and report how missing data are handled, because consumers of research cannot accurately interpret findings without knowing the amount and pattern of missing data or the strategies that were used to handle those data. Patterns of missing data are reviewed, and some of the common strategies for dealing with them are described. The authors provide an illustration in which data were simulated and evaluate 3 methods of handling missing data: mean substitution, multiple imputation, and full information maximum likelihood. Results suggest that mean substitution is a poor method for handling missing data, whereas both multiple imputation and full information maximum likelihood are recommended alternatives to this approach. The authors suggest that researchers fully consider and report the amount and pattern of missing data and the strategy for handling those data in counseling psychology research and that editors advise researchers of this expectation."}
{"_id":"794d1f355d052efbdcbfd09df93e885e85150fd6","title":"Dynamic Impact of Online Word-of-Mouth and Advertising on Supply Chain Performance","text":"Cooperative (co-op) advertising investments benefit brand goodwill and further improve supply chain performance. Meanwhile, online word-of-mouth (OWOM) can also play an important role in supply chain performance. On the basis of co-op advertising, this paper considers a single supply chain structure led by a manufacturer and examines a fundamental issue concerning the impact of OWOM on supply chain performance. Firstly, by the method of differential game, this paper analyzes the dynamic impact of OWOM and advertising on supply chain performance (i.e., brand goodwill, sales, and profits) under three different supply chain decisions (i.e., only advertising, and manufacturers with and without sharing cost of OWOM with retailers). We compare and analyze the optimal strategies of advertising and OWOM under the above different supply chain decisions. Secondly, the system dynamics model is established to reflect the dynamic impact of OWOM and advertising on supply chain performance. Finally, three supply chain decisions under two scenarios, strong brand and weak brand, are analyzed through the system dynamics simulation. The results show that the input of OWOM can enhance brand goodwill and improve earnings. It further promotes the OWOM reputation and improves the supply chain performance if manufacturers share the cost of OWOM with retailers. Then, in order to eliminate the retailers from word-of-mouth fraud and establish a fair competition mechanism, the third parties (i.e., regulators or e-commerce platforms) should take appropriate punitive measures against retailers. Furthermore, the effect of OWOM on supply chain performance under a strong brand differed from those under a weak brand. Last but not least, if OWOM is improved, there would be more remarkable performance for the weak brand than that for the strong brand in the supply chain."}
{"_id":"3c045560f824473172027c89eaeefa46260afe55","title":"Genetic heritability and shared environmental factors among twin pairs with autism.","text":"CONTEXT\nAutism is considered the most heritable of neurodevelopmental disorders, mainly because of the large difference in concordance rates between monozygotic and dizygotic twins.\n\n\nOBJECTIVE\nTo provide rigorous quantitative estimates of genetic heritability of autism and the effects of shared environment.\n\n\nDESIGN, SETTING, AND PARTICIPANTS\nTwin pairs with at least 1 twin with an autism spectrum disorder (ASD) born between 1987 and 2004 were identified through the California Department of Developmental Services.\n\n\nMAIN OUTCOME MEASURES\nStructured diagnostic assessments (Autism Diagnostic Interview-Revised and Autism Diagnostic Observation Schedule) were completed on 192 twin pairs. Concordance rates were calculated and parametric models were fitted for 2 definitions, 1 narrow (strict autism) and 1 broad (ASD).\n\n\nRESULTS\nFor strict autism, probandwise concordance for male twins was 0.58 for 40 monozygotic pairs (95% confidence interval [CI], 0.42-0.74) and 0.21 for 31 dizygotic pairs (95% CI, 0.09-0.43); for female twins, the concordance was 0.60 for 7 monozygotic pairs (95% CI, 0.28-0.90) and 0.27 for 10 dizygotic pairs (95% CI, 0.09-0.69). For ASD, the probandwise concordance for male twins was 0.77 for 45 monozygotic pairs (95% CI, 0.65-0.86) and 0.31 for 45 dizygotic pairs (95% CI, 0.16-0.46); for female twins, the concordance was 0.50 for 9 monozygotic pairs (95% CI, 0.16-0.84) and 0.36 for 13 dizygotic pairs (95% CI, 0.11-0.60). A large proportion of the variance in liability can be explained by shared environmental factors (55%; 95% CI, 9%-81% for autism and 58%; 95% CI, 30%-80% for ASD) in addition to moderate genetic heritability (37%; 95% CI, 8%-84% for autism and 38%; 95% CI, 14%-67% for ASD).\n\n\nCONCLUSION\nSusceptibility to ASD has moderate genetic heritability and a substantial shared twin environmental component."}
{"_id":"653cc5363cfc8a28c577dc7cd2ddb9c8f24cec48","title":"A least-squares approach to anomaly detection in static and sequential data","text":"We describe a probabilistic, nonparametric method for anomaly detection, based on a squared-loss objective function which has a simple analytical solution. The method emerges from extending recent work in nonparametric leastsquares classification to include a \u201cnone-of-the-above\u201d class which models anomalies in terms of non-anamalous training data. The method shares the flexibility of other kernel-based anomaly detection methods, yet is typically much faster to train and test. It can also be used to distinguish between multiple inlier classes and anomalies. The probabilistic nature of the output makes it straightforward to apply even when test data has structural dependencies; we show how a hidden Markov model framework can be incorporated in order to identify anomalous subsequences in a test sequence. Empirical results on datasets from several domains show the method to have comparable discriminative performance to popular alternatives, but with a clear speed advantage."}
{"_id":"5d78afb70c52401b1d8d4ca269cf69b07124862f","title":"Online Social Network Dependency: Theoretical Development and Testing of Competing Models","text":"The proliferation of new social media technologies has changed the behavioral patterns of online users. This study aims at investigating the structure and dimensionality of the Online Social Network Dependency (OSN Dependency). We tested the competing models built upon the cognitive-behavioral model and the biopsychogical framework of addiction. Our findings suggested that OSN Dependency can be explained by a higher-order factor model with seven first-order factors (i.e. mood alternation, social benefit, negative outcomes, compulsivity, excessive time, withdrawal, and interpersonal control) and two correlated second-order factors (i.e. social components and intrapersonal components). The model provides a good-fit to the data, reflecting logical consistency. Implications of the current investigation for practice and research are provided."}
{"_id":"589f8271d1ab4767822418a570a60c0e70acbb5b","title":"Map API - scalable decentralized map building for robots","text":"Large scale, long-term, distributed mapping is a core challenge to modern field robotics. Using the sensory output of multiple robots and fusing it in an efficient way enables the creation of globally accurate and consistent metric maps. To combine data from multiple agents into a global map, most existing approaches use a central entity that collects and manages the information from all agents. Often, the raw sensor data of one robot needs to be made available to processing algorithms on other agents due to the lack of computational resources on that robot. Unfortunately, network latency and low bandwidth in the field limit the generality of such an approach and make multi-robot map building a tedious task. In this paper, we present a distributed and decentralized back-end for concurrent and consistent robotic mapping. We propose a set of novel approaches that reduce the bandwidth usage and increase the effectiveness of inter-robot communication for distributed mapping. Instead of locking access to the map during operations, we define a version control system which allows concurrent and consistent access to the map data. Updates to the map are then shared asynchronously with agents which previously registered notifications. A technique for data lookup is provided by state-of-the-art algorithms from distributed computing. We validate our approach on real-world datasets and demonstrate the effectiveness of the proposed algorithms."}
{"_id":"4658c87f099cf001ef53c47e5b88b758e4e2b050","title":"AMR-to-text Generation with Synchronous Node Replacement Grammar","text":"EDUCATION PhD 2001 Computer Science, University of California, Berkeley. Directed by Nelson Morgan, Daniel Jurafsky, Charles Fillmore, Jerome Feldman. Dissertation: Statistical Language Understanding Using Frame Semantics MS 1999 Computer Science, University of California, Berkeley. Thesis: Topic-Based Language Models Using EM BA 1995 Double major in Linguistics and Computer Science, University of California, Berkeley."}
{"_id":"e242a542de4ca01fa4c391abe1381093e9e5c9c9","title":"PHD: A Probabilistic Model of Hybrid Deep Collaborative Filtering for Recommender Systems","text":"Collaborative Filtering (CF), a well-known approach in producing recommender systems, has achieved wide use and excellent performance not only in research but also in industry. However, problems related to cold start and data sparsity have caused CF to attract an increasing amount of attention in efforts to solve these problems. Traditional approaches adopt side information to extract effective latent factors but still have some room for growth. Due to the strong characteristic of feature extraction in deep learning, many researchers have employed it with CF to extract effective representations and to enhance its performance in rating prediction. Based on this previous work, we propose a probabilistic model that combines a stacked denoising autoencoder and a convolutional neural network together with auxiliary side information (i.e, both from users and items) to extract users and items\u2019 latent factors, respectively. Extensive experiments for four datasets demonstrate that our proposed model outperforms other traditional approaches and deep learning models making it state of the art."}
{"_id":"0d1d0900cf862f11d3d7812c01d28be27c71a6c7","title":"Compositional dynamic test generation","text":"Dynamic test generation is a form of dynamic program analysis that attempts to compute test inputs to drive a program along a specific program path. Directed Automated Random Testing, or DART for short, blends dynamic test generation with model checking techniques with the goal of systematically executing all feasible program paths of a program while detecting various types of errors using run-time checking tools (like Purify, for instance). Unfortunately, systematically executing all feasible program paths does not scale to large, realistic programs.This paper addresses this major limitation and proposes to perform dynamic test generation compositionally, by adapting known techniques for interprocedural static analysis. Specifically, we introduce a new algorithm, dubbed SMART for Systematic Modular Automated Random Testing, that extends DART by testing functions in isolation, encoding test results as function summaries expressed using input preconditions and output postconditions, and then re-using those summaries when testing higher-level functions. We show that, for a fixed reasoning capability, our compositional approach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). In other words, SMART can perform dynamic test generation compositionally without any reduction in program path coverage. We also show that, given a bound on the maximum number of feasible paths in individual program functions, the number of program executions explored by SMART is linear in that bound, while the number of program executions explored by DART can be exponential in that bound. We present examples of C programs and preliminary experimental results that illustrate and validate empirically these properties."}
{"_id":"6421e02baee72c531cb044760338b314c7161406","title":"Contextual Phrase-Level Polarity Analysis Using Lexical Affect Scoring and Syntactic N-Grams","text":"We present a classifier to predict contextual polarity of subjective phrases in a sentence. Our approach features lexical scoring derived from the Dictionary of Affect in Language (DAL) and extended through WordNet, allowing us to automatically score the vast majority of words in our input avoiding the need for manual labeling. We augment lexical scoring with n-gram analysis to capture the effect of context. We combine DAL scores with syntactic constituents and then extract ngrams of constituents from all sentences. We also use the polarity of all syntactic constituents within the sentence as features. Our results show significant improvement over a majority class baseline as well as a more difficult baseline consisting of lexical n-grams."}
{"_id":"b21aaef087ec31e7c869278f123d222720dab8d9","title":"Industrial Big Data in an Industry 4.0 Environment: Challenges, Schemes, and Applications for Predictive Maintenance","text":"Industry 4.0 can make a factory smart by applying intelligent information processing approaches, communication systems, future-oriented techniques, and more. However, the high complexity, automation, and flexibility of an intelligent factory bring new challenges to reliability and safety. Industrial big data generated by multisource sensors, intercommunication within the system and external-related information, and so on, might provide new solutions for predictive maintenance to improve system reliability. This paper puts forth attributes of industrial big data processing and actively explores industrial big data processing-based predictive maintenance. A novel framework is proposed for structuring multisource heterogeneous information, characterizing structured data with consideration of the spatiotemporal property, and modeling invisible factors, which would make the production process transparent and eventually implement predictive maintenance on facilities and energy saving in the industry 4.0 era. The effectiveness of the proposed scheme was verified by analyzing multisource heterogeneous industrial data for the remaining life prediction of key components of machining equipment."}
{"_id":"9a1a677caf0b50c7d50d737bfe921085a007db99","title":"Fault Classification using Pseudomodal Energies and Neuro-fuzzy modelling","text":"This paper presents a fault classification method which makes use of a Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the vibration signals of cylindrical shells. The calculation of Pseudomodal Energies, for the purposes of condition monitoring, has previously been found to be an accurate method of extracting features from vibration signals. This calculation is therefore used to extract features from vibration signals obtained from a diverse population of cylindrical shells. Some of the cylinders in the population have faults in different substructures. The pseudomodal energies calculated from the vibration signals are then used as inputs to a neuro-fuzzy model. A leave-one-out cross-validation process is used to test the performance of the model. It is found that the neuro-fuzzy model is able to classify faults with an accuracy of 91.62%, which is higher than the previously used multilayer perceptron."}
{"_id":"24d91de65d6ee19975e9cb8e0d7ed7466a403e72","title":"Analysis on credit card fraud detection methods","text":"Due to the rise and rapid growth of E-Commerce, use of credit cards for online purchases has dramatically increased and it caused an explosion in the credit card fraud. As credit card becomes the most popular mode of payment for both online as well as regular purchase, cases of fraud associated with it are also rising. In real life, fraudulent transactions are scattered with genuine transactions and simple pattern matching techniques are not often sufficient to detect those frauds accurately. Implementation of efficient fraud detection systems has thus become imperative for all credit card issuing banks to minimize their losses. Many modern techniques based on Artificial Intelligence, Data mining, Fuzzy logic, Machine learning, Sequence Alignment, Genetic Programming etc., has evolved in detecting various credit card fraudulent transactions. A clear understanding on all these approaches will certainly lead to an efficient credit card fraud detection system. This paper presents a survey of various techniques used in credit card fraud detection mechanisms and evaluates each methodology based on certain design criteria."}
{"_id":"3486d7f6dff3e78813dcf19d47c93e1204c61acc","title":"Automatic Annotation of Semantic Term Types in the Complete ACL Anthology Reference Corpus","text":"In the present paper, we present an automated tagging approach aimed at enhancing a well-known resource, the ACL Anthology Reference Corpus, with semantic class labels for more than 20,000 technical terms that are relevant to the domain of computational linguistics. We use state-of-the-art classification techniques to assign semantic class labels to technical terms extracted from several reference term lists. We also sketch a set of research questions and approaches directed towards the integrated analysis of scientific corpora. To this end, we query the data set resulting from our annotation effort on both the term and the semantic class level level."}
{"_id":"86558ab54bdbfa1c6ec69f464ed7da959db81693","title":"Eddy Current Losses in Transformer Windings and Circuit Wiring","text":"Skin Effect Figure 1 shows the magnetic field (flux lines) in and around a conductor carrying dc or low frequency current I. The field is radially symmetrical, as shown, only if the return current with its associated field is at a great distance. At low frequency, the energy in the magnetic field is trivial compared to the energy loss in the resistance of the wire. Hence the current Introduction As switching power supply operating frequencies increase, eddy current losses and parasitic inductances can greatly impair circuit performance. These high frequency effects are caused by the magnetic field resulting from current flow in transformer windings and circuit wiring. This paper is intended to provide insight into these phenomena so that improved high frequency. performance can be achieved. Among other things, it explains (I) why eddy current losses increase so dramatically with more winding layers, (2) why parallelling thin strips doesn't work, (3) how passive conductors (Faraday shields and C. T .windings) have high losses, and (4) why increasing conductor surface area will actually worsen losses and parasitic inductance if the configuration is not correct."}
{"_id":"767f4e4cca1882e2b6b5ff1e54e3a27366f7c3ea","title":"Collaborating between Local and Global Learning for Distributed Online Multiple Tasks","text":"This paper studies the novel learning scenarios of Distributed Online Multi-tasks (DOM), where the learning individuals with continuously arriving data are distributed separately and meanwhile they need to learn individual models collaboratively. It has three characteristics: distributed learning, online learning and multi-task learning. It is motivated by the emerging applications of wearable devices, which aim to provide intelligent monitoring services, such as health emergency alarming and movement recognition.\n To the best of our knowledge, no previous work has been done for this kind of problems. Thus, in this paper a collaborative learning scheme is proposed for this problem. Specifically, it performs local learning and global learning alternately. First, each client performs online learning using the increasing data locally. Then, DOM switches to global learning on the server side when some condition is triggered by clients. Here, an asynchronous online multi-task learning method is proposed for global learning. In this step, only this client's model, which triggers the global learning, is updated with the support of the difficult local data instances and the other clients' models. The experiments from 4 applications show that the proposed method of global learning can improve local learning significantly. DOM framework is effective, since it can share knowledge among distributed tasks and obtain better models than learning them separately. It is also communication efficient, which only requires the clients send a small portion of raw data to the server."}
{"_id":"6bbab30198ed942748f155d3ca7ea4807ab62714","title":"Detection of obstructive sleep apnea through ECG signal features","text":"Obstructive sleep apnea (OSA) is a common disorder in which individuals stop breathing during their sleep. Most of sleep apnea cases are currently undiagnosed because of expenses and practicality limitations of overnight polysomnography (PSG) at sleep labs, where an expert human observer is needed to work over night. New techniques for sleep apnea classification are being developed by bioengineers for most comfortable and timely detection. In this paper, an automated classification algorithm is presented which processes short duration epochs of the electrocardiogram (ECG) data. The automated classification algorithm is based on support vector machines (SVM) and has been trained and tested on sleep apnea recordings from subjects with and without OSA. The results show that our automated classification system can recognize epochs of sleep disorders with a high degree of accuracy, approximately 96.5%. Moreover, the system we developed can be used as a basis for future development of a tool for OSA screening."}
{"_id":"95f1188fd6d031893ecd823c33fb0908a7cf79dc","title":"A Deep Learning Approach to Contract Element Extraction","text":"We explore how deep learning methods can be used for contract element extraction. We show that a BILSTM operating on word, POS tag, and tokenshape embeddings outperforms the linear sliding-window classifiers of our previous work, without any manually written rules. Further improvements are observed by stacking an additional LSTM on top of the BILSTM, or by adding a CRF layer on top of the BILSTM. The stacked BILSTM-LSTM misclassifies fewer tokens, but the BILSTM-CRF combination performs better when methods are evaluated for their ability to extract entire, possibly multi-token contract elements."}
{"_id":"c7d279de948facf6a9bb3673f8933750f525d19f","title":"A Miniaturized Dual-Band Frequency Selective Surface (FSS) With Closed Loop and Its Complementary Pattern","text":"A single-layer substrate frequency selective surface (FSS) made of miniaturized elements is proposed, with two controllable passbands obtained. Each FSS element consists of a loop wire on the top metal layer and its complementary pattern etched at the bottom one, which provides two transmission poles separated by a transmission zero. An equivalent circuit model is given for predicting the characteristics of the designed FSS, and a good agreement between the simulated and measured transmission coefficients is obtained. Furthermore, the cases of oblique wave incidence and cascading FSSs are also measured and examined."}
{"_id":"9770a7231b095c0ebcd69936ebf5155e76fe05ed","title":"57.5GHz bandwidth 4.8Vpp swing linear modulator driver for 64GBaud m-PAM systems","text":"A novel series-stacked large swing push-pull MOS-HBT driver was implemented in 55nm SiGe BiCMOS technology. The circuit achieves 4.8Vpp differential swing, 57.5GHz band-width and has an output compression point of 12 dBm per side. 4-PAM and 8-PAM eye diagrams were measured at 56 GBaud for a record data rate of 168 Gb\/s. 4-PAM 64GBaud eye diagrams were also demonstrated. The circuit consumes 820\/600 mW with\/without the predriver, for an energy efficiency of 4.88\/3.57 pJ\/b."}
{"_id":"2f95e11768517edfe4015e53f3817fd6922d9e87","title":"Novel Wideband Transition Between Coplanar Waveguide and Microstrip Line","text":"A novel wideband vertical transition for connecting the coplanar waveguide (CPW) to the microstrip line is proposed. This transition can be very useful for millimeter-wave packaging and vertical interconnects. It is multilayered, partly tapered, and consists of only one via interconnect. Two different transitions are designed. The first transition allows connectivity of a CPW with Zc=50 \u03a9 to a microstrip line with Zc=16 \u03a9 with a bandwidth of 10-60 GHz. The second transition has the same characteristic impedance, Zc=50 \u03a9, at the two ports. In this case, the operating frequency is from 40 MHz to 60 GHz. The return losses of both transitions are generally lower than -10 dB over their indicated frequency ranges, while the maximum measured insertion losses are 1.8 and 2.4 dB for the first and second transition, respectively. To extract the S-parameters of the transitions, a new thru-line technique, based on the standard thru-reflect-line two-tier calibration is introduced. Simulation and experimental results, showing good agreement, are presented and discussed."}
{"_id":"0c7ca6a7e9268d17f4677c408406cd039c27af42","title":"Geographic and Genetic Population Differentiation of the Amazonian Chocolate Tree (Theobroma cacao L)","text":"Numerous collecting expeditions of Theobroma cacao L. germplasm have been undertaken in Latin-America. However, most of this germplasm has not contributed to cacao improvement because its relationship to cultivated selections was poorly understood. Germplasm labeling errors have impeded breeding and confounded the interpretation of diversity analyses. To improve the understanding of the origin, classification, and population differentiation within the species, 1241 accessions covering a large geographic sampling were genotyped with 106 microsatellite markers. After discarding mislabeled samples, 10 genetic clusters, as opposed to the two genetic groups traditionally recognized within T. cacao, were found by applying Bayesian statistics. This leads us to propose a new classification of the cacao germplasm that will enhance its management. The results also provide new insights into the diversification of Amazon species in general, with the pattern of differentiation of the populations studied supporting the palaeoarches hypothesis of species diversification. The origin of the traditional cacao cultivars is also enlightened in this study."}
{"_id":"9215560fe16396445a1e84b50d184f88662a60b4","title":"In-Store Gamification: Testing a Location-Based Treasure Hunt App in a Real Retailing Environment","text":"Traditional retailers are facing strong competition from e-commerce. One way to meet this challenge is to follow the marketing movement of focusing on customer experiences. This transformation is based on the notion of engaging customers and one way to drive this engagement is through gamification to support value creation. In this study, we have identified variables affecting intentions to use gamified services and in what ways. For this purpose, we developed an app that generated different levels of gamification by varying the number of game elements. The data from a survey distributed during a field experiment indicates that an increasing level of gamification and technology experience have direct positive associations with intrinsic motivation. Furthermore, intrinsic motivation has a positive direct association with satisfaction, although this is partly mediated by mood. Finally, satisfaction has a positive direct relation with intention to use."}
{"_id":"8c7056a7eb9d2cffc5bcbcc20e7d9b8ea797f5de","title":"A comparison between Chinese and Caucasian head shapes.","text":"Univariate anthropometric data have long documented a difference in head shape proportion between Chinese and Caucasian populations. This difference has made it impossible to create eyewear, helmets and facemasks that fit both groups well. However, it has been unknown to what extend and precisely how the two populations differ from each other in form. In this study, we applied geometric morphometrics to dense surface data to quantify and characterize the shape differences using a large data set from two recent 3D anthropometric surveys, one in North America and Europe, and one in China. The comparison showed the significant variations between head shapes of the two groups and results demonstrated that Chinese heads were rounder than Caucasian counterparts, with a flatter back and forehead. The quantitative measurements and analyses of these shape differences may be applied in many fields, including anthropometrics, product design, cranial surgery and cranial therapy."}
{"_id":"547242ed248a57a726c212a307669aedadaa862e","title":"Evolution of Communication Technologies for Smart Grid applications","text":"The idea of Smart Grid has started to evolve more rapidly with the enhancement in Communication Technologies. Two way communication is a key aspect in realizing Smart Grids and is easily possible with the help of modern day advancements in both wired and wireless communication technologies. This paper discusses some of the major communication technologies which include IEEE specified ZigBee, WiMAX and Wireless LAN (Wi-Fi) technologies, GSM 3G\/4G Cellular, DASH 7 and PLC (Power Line Communications), with special focus on their applications in Smart Grids. The Smart Grid environments and domains such as Home Area Automation, Substation Automation, Automated Metering Infrastructure, Vehicle-to-Grid Communications, etc. are considered as priority areas for developing smarter grids. The advancements, challenges and the opportunities present in these priority areas are discussed in this paper. & 2012 Elsevier Ltd. All rights reserved."}
{"_id":"5c9d22ac7a75967a41788181474a84d2f52af033","title":"Valuating privacy","text":"In several experimental auctions, participants put a dollar value on private information before revealing it to a group. An analysis of results show that a trait's desirability in relation to the group played a key role in the amount people demanded to publicize private information. Because people can easily obtain, aggregate, and disperse personal data electronically, privacy is a central concern in the information age. This concern is clear in relation to financial data and genetic information, both of which can lead to identity abuse and discrimination. However, other relatively harmless information can also be abused, including a person's gender, salary, age, marital status, or shopping preferences. What's unclear is whether it's the fear of such abuse that actually causes people's stated hesitance to reveal their data. Our hypothesis - and the motivation for our study - is that people reveal information when they feel that they're somewhat typical or positively atypical compared to the target group. To test this hypothesis, we conducted experiments that elicit the value people place on their private data. We found, with great significance (more than 95 percent statistical confidence) that a linear relationship exists between an individual's belief about a trait and the value he or she places on it. That is, the less desirable the trait, the greater the price a person demands for releasing the information. Furthermore, we found that small deviations in a socially positive direction are associated with a lower asking price."}
{"_id":"a4b7b9e127f2dabdfd89153ac88a2939fcbca3f2","title":"VMTP: a transport protocol for the next generation of communication systems","text":"The Versatile Message Transaction Protocol (VMTP) is a transport-level protocol designed to support remote procedure call, multicast and real-time communication. The protocol is optimized for efficient page-level network file access in particular.\nIn this paper, we describe the significant aspects of the VMTP design, including the VMTP treatment of sessions, addressing, duplicate suppression, flow control and retransmissions plus its provision for multicast. The VMTP design reflects a change in the use of computer communication as well as a change in the underlying hardware base for the next generation of communication systems. It also challenges certain established notions in the design of protocols."}
{"_id":"cd14bffcea4165b8bda586a79c328267099f70d6","title":"The synchronous data flow programming language LUSTRE","text":"This paper describes the language LUSTRE which is a data flow synchronous language, designed for programming reactive systems-uch as automatic control and monitoring sy s t emsas well as for describing hardware. The data flow aspect of LUSTRE makes it very close to usual description tools in these domains (blockdiagrams, networks of operators, dynamical sample-systems, etc.), and its synchronous interpretation makes it well suited for handling time in programs. Moreover, this synchronous interpretation allows it to be compiled into an efficient sequential program. Finally, the LUSTRE formalism is very similar to temporal logics. This allows the language to be used for both writing programs and expressing program properties, which results in an original program verification methodology."}
{"_id":"542f5fd2387e07e70ddb0dbcb52666aeb1d1efe4","title":"Self-Similarity Driven Color Demosaicking","text":"Demosaicking is the process by which from a matrix of colored pixels measuring only one color component per pixel, red, green, or blue, one can infer a whole color information at each pixel. This inference requires a deep understanding of the interaction between colors, and the involvement of image local geometry. Although quite successful in making such inferences with very small relative error, state-of-the-art demosaicking methods fail when the local geometry cannot be inferred from the neighboring pixels. In such a case, which occurs when thin structures or fine periodic patterns were present in the original, state-of-the-art methods can create disturbing artifacts, known as zipper effect, blur, and color spots. The aim of this paper is to show that these artifacts can be avoided by involving the image self-similarity to infer missing colors. Detailed experiments show that a satisfactory solution can be found, even for the most critical cases. Extensive comparisons with state-of-the-art algorithms will be performed on two different classic image databases."}
{"_id":"262aa1bfe9b9ba413e8feed5fe9f9723c26104b5","title":"Input\/Output Devices and Interaction Techniques","text":"The computing literature often draws a sharp distinction between input and output; computer scientists are used to regarding a screen as a passive output device and a mouse as a pure input device. However, nearly all examples of human-computer interaction require both input and output to do anything useful. For example, what good would a mouse be without the corresponding feedback embodied by the cursor on the screen, as well as the sound and feel of the buttons when they are clicked? The distinction between output devices and input devices becomes even more blurred in the real world. A sheet of paper can be used to both record ideas (input) and display them (output). Clay reacts to the sculptor\u2019s fingers yet also provides feedback through the curvature and texture of its surface. Indeed, the complete and seamless integration of input and output is becoming a common research theme in advanced computer interfaces such as ubiquitous computing (Weiser, 1991) and tangible interaction (Ishii & Ullmer, 1997)."}
{"_id":"de29c51d169cdb5066f6832e9a8878900c43f100","title":"Interactions with big data analytics","text":"Increasingly in the 21st century, our daily lives leave behind a detailed digital record: our shifting thoughts and opinions shared on Twitter, our social relationships, our purchasing habits, our information seeking, our photos and videos\u2014even the movements of our bodies and cars. Naturally, for those interested in human behavior, this bounty of personal data is irresistible. Decision makers of all kinds, from company executives to government agencies to researchers and scientists, would like to base their decisions and actions on this data. In response, a new discipline of big data analytics is forming. Fundamentally, big data analytics is a workflow that distills terabytes of low-value data (e.g., every tweet) down to, in some cases, a single bit of high-value data (Should Company X acquire Company Y? Can we reject the null hypothesis?). The goal is to see the big picture from the minutia of our digital lives. It is no surprise today that big data is useful for HCI researchers and user interface design. As one example, A\/B testing is a standard practice in the usability community to help determine relative differences in user performance using different interfaces. For many years, we have used strict laboratory conditions to evaluate interfaces, but more recently we have seen the ability to implement those tests quickly and on a large population by running controlled Interactions with Big Data Analytics"}
{"_id":"6b68a0c00a1a6d2ffa96402065378b1dae28f0e2","title":"Image Steganography based on a Parameterized Canny Edge Detection Algorithm","text":"Steganography is the science of hiding digital information in such a way that no one can suspect its existence. Unlike cryptography which may arouse suspicions, steganography is a stealthy method that enables data communication in total secrecy. Steganography has many requirements, the foremost one is irrecoverability which refers to how hard it is for someone apart from the original communicating parties to detect and recover the hidden data out of the secret communication. A good strategy to guaranteeirrecoverability is to cover the secret data not usinga trivial method based on a predictable algorithm, but using a specific random pattern based on a mathematical algorithm. This paper proposes an image steganography technique based on theCanny edge detection algorithm. It is designed to hide secret data into a digital image within the pixels that make up the boundaries of objects detected in the image. More specifically, bits of the secret data replace the three LSBs of every color channel of the pixels detected by the Canny edge detection algorithm as part of the edges in the carrier image. Besides, the algorithm is parameterized by three parameters: The size of the Gaussian filter, a low threshold value, and a high threshold value. These parameters can yield to different outputs for the same input image and secret data. As a result, discovering the inner-workings of the algorithm would be considerably ambiguous, misguiding steganalysts from the exact location of the covert data. Experiments showed a simulation tool codenamed GhostBit, meant to cover and uncover secret data using the proposed algorithm. As future work, examining how other image processing techniques such as brightness and contrast adjustment can be taken"}
{"_id":"708f339a2d7a5bb827149448fd3b37385ba9b873","title":"Exploring the factors associated with Web site success in the context of electronic commerce","text":"Web sites are being widely deployed commercially. As the widespread use and dependency on Web technology increases, so does the need to assess factors associated with Web site success. The objective is to explore these factors in the context of electronic commerce (EC). The research framework was derived from information systems and marketing literature. Webmasters from Fortune 1000 companies were used as the target group for a survey. Four factors that are critical to Web site success in EC were identi\u00aeed: (1) information and service quality, (2) system use, (3) playfulness, and (4) system design quality. An analysis of the data provides valuable managerial implications for Web site success in the context of electronic commerce. # 2000 Elsevier Science B.V. All rights reserved."}
{"_id":"df805da2bb2a7e830b615636ee7cd22368a63563","title":"Web Site Usability, Design, and Performance Metrics","text":"Websites provide the key interface for consumer use of the Internet. This research reports on a series of three studies that develop and validate Web site usability, design and performance metrics, including download delay, navigability, site content, interactivity, and responsiveness. The performance metric that was developed includes the subconstructs user satisfaction, the likelihood of return, and the frequency of use. Data was collected in 1997, 1999, and 2000 from corporate Web sites via three methods, namely, a jury, third-party ratings, and a software agent. Significant associations betweenWeb site design elements and Web site performance indicate that the constructs demonstrate good nomological validity. Together, the three studies provide a set of measures with acceptable validity and reliability. The findings also suggest lack of significant common methods biases across the jury-collected data, third-party data, and agent-collected data. Results suggest that Web site success is a first-order construct. Moreover, Web site success is significantly associated with Web site download delay (speed of access and display rate within the Web site), navigation (organization, arrangement, layout, and sequencing), content (amount and variety of product information), interactivity (customization and interactivity), and responsiveness (feedback options and FAQs). (e-Commerce, Web Metrics, or Measurement;Web Site Usability;Design and Performance Constructs; Construct Validity; Nomological Validity)"}
{"_id":"0b990a9c6000b80dc00b69b68f6091844b898215","title":"Marketing in hypermedia computer-mediated environment: Conceptual foundations","text":"This paper addresses the role of marketing in hypermedia computer-mediated environments (CMEs). Our approach considers hypermedia CMEs to be large-scale (i.e. national or global) networked environments, of which the World Wide Web on the Internet is the first and current global implementation. We introduce marketers to this revolutionary new medium, and propose two structural models of consumer behavior in a CME. Then we examine the set of consequent testable research propositions and marketing implications that flow from the models. Marketing in Hypermedia Computer-Mediated Environments: Conceptual Foundations 1) Introduction Firms communicate with their customers through various media. Traditionally, these media follow a passive one-to-many communication model whereby a firm reaches many current and potential customers, segmented or not, through marketing efforts that allow only limited forms of feedback on the part of the customer. For several years now, a revolution has been developing that is dramatically altering this traditional view of advertising and communication media. This revolution is the Internet, the massive global network of interconnected packet-switched computer networks, and as a new marketing medium, has the potential to radically change the way firms do business with their customers. The Internet operationalizes a model of distributed computing that facilitates interactive multimedia many-to-many communication. As such, the Internet supports discussion groups (e.g. USENET news and moderated and unmoderated mailing lists), multi-player games and communications systems (e.g. MUDs, irc, chat, MUSEs), file transfer, electronic mail, and global information access and retrieval systems (e.g. archie, gopher, and the World Wide Web). The business implications of this model \"[where] the engine of democratization sitting on so many desktops is already out of control, is already creating new players in a new game\" (Carroll 1994), will be played out in as yet unknown ways for years to come. This paper is concerned with the marketing implications of commercializing hypermedia computer-mediated environments (CMEs), of which the World Wide Web (Berners-Lee et. al. 1992, 1993) on the Internet is the first and current networked global implementation. While we provide a formal definition subsequently, at this point we informally define a hypermedia CME as a distributed computer network used to access and provide hypermedia content (i.e., multimedia content connected across the network with hypertext links). Though other CMEs are relevant to marketers, including private bulletin board systems (Bunch 1994); public conferencing systems such as the WELL (Figallo 1993; Rheingold 1992, 1993) and ECHO; and commercial online services such as America On-Line, Prodigy, and CompuServe, we restrict our current focus to marketing activities in hypermedia CMEs accessible via the \"Web\" on the Internet. The Internet is an important focus for marketers because consumers and firms are conducting business on the Internet in proportions that dwarf the commercial provider base of the other CMEs combined. There are over 21,700 commercial Internet addressess (Verity and Hof 1994), and an increasing percentage of these commercial addresses are providing Web services. As of December 28, 1994, 1465 firms were listed in Open Market\u2019s (1994) directory of \"Commercial Services on the Net,\" and there were 6370 entries in the \"Business\/Corporations\" directory of the Yahoo Guide to WWW (Filo and Yang 1994). The central thesis driving this research is that hypermedia CMEs, such as but not limited to the World Wide Web on the Internet, require the development and application of new marketing concepts and models. This is because hypermedia CMEs possess unique characteristics, including machine-interactivity, telepresence, hypermedia, and network navigation, which distinguish them from traditional media and some interactive multimedia, on which conventional concepts and models are based. Hoffman & Novak (1995), \"Marketing in Hypermedia CMEs: Conceptual Foundations\" page 1"}
{"_id":"3073eda62f8391db0e695acb69bcb8c68b34c7b4","title":"Data Integration: After the Teenage Years","text":"The field of data integration has expanded significantly over the years, from providing a uniform query and update interface to structured databases within an enterprise to the ability to search, ex- change, and even update, structured or unstructured data that are within or external to the enterprise. This paper describes the evolution in the landscape of data integration since the work on rewriting queries using views in the mid-1990's. In addition, we describe two important challenges for the field going forward. The first challenge is to develop good open-source tools for different components of data integration pipelines. The second challenge is to provide practitioners with viable solutions for the long-standing problem of systematically combining structured and unstructured data."}
{"_id":"8ebbfae66b020d6927784d2e59fdc2ddf3585cd3","title":"A synaptically controlled, associative signal for Hebbian plasticity in hippocampal neurons.","text":"The role of back-propagating dendritic action potentials in the induction of long-term potentiation (LTP) was investigated in CA1 neurons by means of dendritic patch recordings and simultaneous calcium imaging. Pairing of subthreshold excitatory postsynaptic potentials (EPSPs) with back-propagating action potentials resulted in an amplification of dendritic action potentials and evoked calcium influx near the site of synaptic input. This pairing also induced a robust LTP, which was reduced when EPSPs were paired with non-back-propagating action potentials or when stimuli were unpaired. Action potentials thus provide a synaptically controlled, associative signal to the dendrites for Hebbian modifications of synaptic strength."}
{"_id":"8ff35ce40d89b02bff71ed03da24bbd3ce383df3","title":"Screening for trisomy 18 by maternal age, fetal nuchal translucency, free beta-human chorionic gonadotropin and pregnancy-associated plasma protein-A.","text":"OBJECTIVES\nTo derive a model and examine the performance of first-trimester screening for trisomy 18 by maternal age, fetal nuchal translucency (NT) thickness, and maternal serum free beta-human chorionic gonadotropin (beta-hCG) and pregnancy-associated plasma protein-A (PAPP-A).\n\n\nMETHODS\nProspective combined screening for trisomy 21 was performed at 11 + 0 to 13 + 6 weeks in 56 893 singleton pregnancies, including 56 376 cases of euploid fetuses, 395 with trisomy 21 and 122 with trisomy 18. The measured free beta-hCG and PAPP-A were converted into a multiple of the median (MoM) and then into likelihood ratios (LR). Similarly, the measured NT was transformed into LRs using the mixture model of NT distributions. In each case the LRs for NT and the biochemical markers were multiplied by the age and gestation-related risk to derive the risk for trisomy 21 and trisomy 18. Detection rates (DRs) and false-positive rates (FPRs) were calculated by taking the proportions with risks above a given risk threshold.\n\n\nRESULTS\nIn screening with the algorithm for trisomy 21, at a FPR of 3%, the estimated DRs of trisomies 21 and 18 were 89% and 82%, respectively. The use of an algorithm for trisomy 18 identified 93% of affected fetuses at a FPR of 0.2%. When the algorithm for trisomy 21 was used and screen positivity was fixed at a FPR of 3%, and in addition the algorithm for trisomy 18 was used and screen positivity was fixed at a FPR of 0.2%, the overall FPR was 3.1% and the DRs of trisomies 21 and 18 were 90% and 97%, respectively.\n\n\nCONCLUSIONS\nA beneficial side effect of first-trimester combined screening for trisomy 21 is the detection of a high proportion of fetuses with trisomy 18. If an algorithm for trisomy 18 in addition to the one for trisomy 21 is used, more than 95% of trisomy 18 fetuses can be detected with a minor increase of 0.1% in the overall FPR."}
{"_id":"b2acf82e565826149af5ea291261a498c05215eb","title":"Classification and adulteration detection of vegetable oils based on fatty acid profiles.","text":"The detection of adulteration of high priced oils is a particular concern in food quality and safety. Therefore, it is necessary to develop authenticity detection method for protecting the health of customers. In this study, fatty acid profiles of five edible oils were established by gas chromatography coupled with mass spectrometry (GC\/MS) in selected ion monitoring mode. Using mass spectral characteristics of selected ions and equivalent chain length (ECL), 28 fatty acids were identified and employed to classify five kinds of edible oils by using unsupervised (principal component analysis and hierarchical clustering analysis), supervised (random forests) multivariate statistical methods. The results indicated that fatty acid profiles of these edible oils could classify five kinds of edible vegetable oils into five groups and are therefore employed to authenticity assessment. Moreover, adulterated oils were simulated by Monte Carlo method to establish simultaneous adulteration detection model for five kinds of edible oils by random forests. As a result, this model could identify five kinds of edible oils and sensitively detect adulteration of edible oil with other vegetable oils about the level of 10%."}
{"_id":"a4ca2f47bebd8762b34da074e5f638d96583d9de","title":"The active layer morphology of organic solar cells probed with grazing incidence scattering techniques.","text":"Grazing incidence X-ray scattering (GIXS) provides unique insights into the morphology of active materials and thin film layers used in organic photovoltaic devices. With grazing incidence wide angle X-ray scattering (GIWAXS) the molecular arrangement of the material is probed. GIWAXS is sensitive to the crystalline parts and allows for the determination of the crystal structure and the orientation of the crystalline regions with respect to the electrodes. With grazing incidence small angle X-ray scattering (GISAXS) the nano-scale structure inside the films is probed. As GISAXS is sensitive to length scales from nanometers to several hundred nanometers, all relevant length scales of organic solar cells are detectable. After an introduction to GISAXS and GIWAXS, selected examples for application of both techniques to active layer materials are reviewed. The particular focus is on conjugated polymers, such as poly(3-hexylthiophene) (P3HT)."}
{"_id":"1c47e12744c82d7ca658732e1e272b409f464440","title":"Medial temporal lobe activations in fMRI and PET studies of episodic encoding and retrieval.","text":"Early neuroimaging studies often failed to obtain evidence of medial temporal lobe (MTL) activation during episodic encoding or retrieval, but a growing number of studies using functional magnetic resonance imaging (fMRI) and positron emission tomography (PET) have provided such evidence. We review data from fMRI studies that converge on the conclusion that posterior MTL is associated with episodic encoding; too few fMRI studies of retrieval have reported MTL activations to allow firm conclusions about their exact locations. We then turn to a recent meta-analysis of PET studies (Lepage et al., Hippocampus 1998;8:313-322) that appears to contradict the fMRI encoding data. Based on their analysis of the rostrocaudal distribution of activations reported during episodic encoding or retrieval, Lepage et al. (1998) concluded that anterior MTL is strongly associated with episodic encoding, whereas posterior MTL is strongly associated with episodic retrieval. After considering the evidence reviewed by Lepage et al. (1998) along with additional studies, we conclude that PET studies of encoding reveal both anterior and posterior MTL activations. These observations indicate that the contradiction between fMRI and PET studies of encoding was more apparent than real. However, PET studies have reported anterior MTL encoding activations more frequently than have fMRI studies. We consider possible sources of these differences."}
{"_id":"b04bb8fe1ebdaab87e0c3352823644f64467ec47","title":"Dynamic simultaneous fare proration for large-scale network revenue management","text":"Network revenue management is concerned with managing demand for products that require inventory from one or several resources by controlling product availability and\/or prices in order to maximize expected revenues subject to the available resource capacities. One can tackle this problem by decomposing it into resource-level subproblems that can be solved efficiently, e.g. by dynamic programming (DP). We propose a new dynamic fare proration method specifically having large-scale applications in mind. It decomposes the network problem by fare proration and solves the resource-level dynamic programs simultaneously using simple, endogenously obtained dynamic marginal capacity value estimates to update fare prorations over time. An extensive numerical simulation study demonstrates that the method results in tightened upper bounds on the optimal expected revenue, and that the obtained policies are very effective with regard to achieved revenues and required runtime."}
{"_id":"51937487039de8d10e93be941bb1d9a8c7e2de9c","title":"IP covert timing channels: design and detection","text":"A network covert channel is a mechanism that can be used to leak information across a network in violation of a security policy and in a manner that can be difficult to detect. In this paper, we describe our implementation of a covert network timing channel, discuss the subtle issues that arose in its design, and present performance data for the channel. We then use our implementation as the basis for our experiments in its detection. We show that the regularity of a timing channel can be used to differentiate it from other traffic and present two methods of doing so and measures of their efficiency. We also investigate mechanisms that attackers might use to disrupt the regularity of the timing channel, and demonstrate methods of detection that are effective against them."}
{"_id":"041b31bfc95ba05789bf0af91244c6adde584413","title":"Education, Signaling and Mismatch","text":"We assess the importance education as a signal of workers\u0092skills and the e\u00a4ects of poor signaling quality on labor market outcomes. We do so by merging a frictional labor market model with a signaling setup where there is a privately observed idiosyncratic component in the cost of education. Given that highly skilled workers cannot correctly signal their abilities, their wages will be lower and they will not be matched to the \"right\" vacancies, or may be unemployed. Skilled workers will then have lower incentives to move to high productivity markets. Furthermore, fewer vacancies will be created in labor markets where skills matter, and incentives for workers to invest in education will be lower. Overall, an economy where education is a noisier signal generates lower educational attainment, higher unemployment and lower productivity. In addition, we provide evidence suggesting that education plays a poor signaling role in Latin American countries. We then calibrate our model using Peruvian data, and through a quantitative exercise we show that this mechanism could be relevant to explain the relatively bad performance of labor markets in Latin American countries. 1E-mail: larozamena@utdt.edu, hru\u00a4o@utdt.edu. We are grateful to Fernando \u00c1lvarez Parra for his comments and suggestions."}
{"_id":"24b3785a3ffae660a60e9ac4eb104b8adfc5871b","title":"A Fuzzy Logic Controller for Autonomous Wheeled Vehicles","text":"Autonomous vehicles have potential applications in many fields, such as replacing humans in hazardous environments, conducting military missions, and performing routine tasks for industry. Driving ground vehicles is an area where human performance has proven to be reliable. Drivers typically respond quickly to sudden changes in their environment. While other control techniques may be used to control a vehicle, fuzzy logic has certain advantages in this area; one of them is its ability to incorporate human knowledge and experience, via language, into relationships among the given quantities. Fuzzy logic controllers for autonomous vehicles have been successfully applied to address various (and sometimes simultaneous) navigational issues,including: \u2022 reaching a static target (Baturone, et al., 2004, Boada, et al., 2005, El Hajjaji & Bentalba, 2003, Maeda et al., 1991, Chen & Ozguner, 2005), \u2022 tracking moving targets (Ollero, et al., 2001), \u2022 maintaining stable vehicular velocity (Holzmann et al., 1998, Nobe & Wang, 2001), \u2022 following a lane or a wall (Rosa & Garcia-Alegre, 1990, Hessburg & Tomizuka, 1994, Peng & Tomizuka, 1993) and, \u2022 avoiding collision with static and dynamic obstacles (Baturone, et al., 2004, Murphy, 2001, Godjevac, et al., 2001, Seraji, 2005, Lee & Wang, 1994, Wheeler & Shoureshi, 1994, Ye & Wang, 2001). Several researchers combined fuzzy logic controllers with various learning techniques, such as: \u2022 supervised learning method (Godjevac, 2001), \u2022 evolutionary method (Hoffman, 2001, Kim, et al., 2001), \u2022 neural network (Pasquier, et al., 2001, Cang, et al., 2003), \u2022 reinforcement learning (Dai, et al., 2005) and, \u2022 optimization methods (Hong, 1997, Sanchez, et al., 1999)."}
{"_id":"7f1f24f8f003bd3536647690f048e90560c26b79","title":"A Case Study on User Experience (UX) Evaluation of Mobile Augmented Reality Prototypes","text":"Mobile Augmented Reality (MAR) blends the real world with digital objects especially in ubiquitous devices such as smartphones. The MAR applications provide an intelligent interface for users. In this, valuable digital information is advertised in physical spaces. However, the success of these applications is tied directly to the degree of user acceptance. This makes understanding the needs and expectations of the MAR\u2019s potential users of paramount importance for designing and building the proper application. The objective of the paper is to expose an important gap in the development of novel applications in the virtual world. Previous research has shown that it is essential to study and understand the needs and expectations of the potential users of the upcoming application or system. Studying user needs and expectations before offering the developed application ensures a minimum level of acceptance and, of course, success. This paper presents a detailed study comprising of a userexperience (UX) evaluation of different prototypes through the use of three different UX evaluation methods. This kind of evaluation allows new developments to offer systems, which do not fail. The main contributions of this study are that it: 1) solicits expectations when consumers use MAR applications, 2) assesses the UX over different prototypes using three different metrics, 3) provides methodological insights on UX evaluation experiments and, 4) is useful for anyone who wants to develop handheld applications after understanding user expectations and how his experience should progress. The results of the study show that users value concreteness, realizability, personalization, novelty, intuitiveness and the usefulness of presented information. Paying attention to these factors can help develop more acceptable MAR applications and lead to more novel future designs."}
{"_id":"6ccb4bec7491333d6323f4732aef389e9deb6d27","title":"A Computational Analysis of Mahabharata","text":"Indian epics have not been analyzed computationally to the extent that Greek epics have. In this paper, we show how interesting insights can be derived from the ancient epic Mahabharata by applying a variety of analytical techniques based on a combination of natural language processing, sentiment\/emotion analysis and social network analysis methods. One of our key findings is the pattern of significant changes in the overall sentiment of the epic story across its eighteen chapters and the corresponding characterization of the primary protagonists in terms of their sentiments, emotions, centrality and leadership attributes in the epic saga."}
{"_id":"e85b25b092e6d7f5cd372338feb3126706b7c3f0","title":"Looking Beyond the Simple Scenarios: Combining Learners and Optimizers in 3D Temporal Tracking","text":"3D object temporal trackers estimate the 3D rotation and 3D translation of a rigid object by propagating the transformation from one frame to the next. To confront this task, algorithms either learn the transformation between two consecutive frames or optimize an energy function to align the object to the scene. The motivation behind our approach stems from a consideration on the nature of learners and optimizers. Throughout the evaluation of different types of objects and working conditions, we observe their complementary nature \u2014 on one hand, learners are more robust when undergoing challenging scenarios, while optimizers are prone to tracking failures due to the entrapment at local minima; on the other, optimizers can converge to a better accuracy and minimize jitter. Therefore, we propose to bridge the gap between learners and optimizers to attain a robust and accurate RGB-D temporal tracker that runs at approximately 2 ms per frame using one CPU core. Our work is highly suitable for Augmented Reality (AR), Mixed Reality (MR) and Virtual Reality (VR) applications due to its robustness, accuracy, efficiency and low latency. Aiming at stepping beyond the simple scenarios used by current systems, often constrained by having a single object in the absence of clutter, averting to touch the object to prevent close-range partial occlusion or selecting brightly colored objects to easily segment them individually, we demonstrate the capacity to handle challenging cases under clutter, partial occlusion and varying lighting conditions."}
{"_id":"bc1aa3af96192db18426d3ccc950c197311bfa73","title":"A Machine Learning-based Forensic Discriminator of Pornographic and Bikini Images","text":"The increased use of microcomputers and smart-phones has contributed to social progress, but it has also facilitated the exchange of illegal files, such as child pornography photographs, increasing the demand for digital forensic examination in these devices, which can store more than 300,000 images. Based on the large number of files to be analyzed, it is necessary to use capable algorithms to perform this detection, especially in the most challenging scenarios, such as the distinction between pornographic and bikini images. In this work, we present an approach that improves the ``Algorithm for detection of Nudity proposed by Ap-Apid. Our method improved the performance of this algorithm by using machine learning in extracted features from detected skin regions instead of using static rules to classify this kind of images. In addition, we used detected faces as features in order to mitigate false positives in portrait photos. For conducting the training, validation and testing phases, we used the AIIA-PID4 pornographic data set. Furthermore, we also present a statistical analysis by comparing our approach to two algorithms, one based on the \u201cAlgorithm for detection of Nudity\u201d and another proposed by the AIIA-PID4 pornographic data set author. The experimental results showed that we achieved an accuracy of 96.96% and 94.94 in the F1-score metric, increasing the accuracy by 79.19% and 18.21% compared to the referred works, respectively."}
{"_id":"52b6d0c61fe83bf4ee1f74307e9a1aa9e60bf19e","title":"Mobile technology habits:\u00a0patterns of association among device usage, intertemporal preference, impulse control, and reward sensitivity.","text":"Mobile electronic devices are playing an increasingly pervasive role in our daily activities. Yet, there has been very little empirical research investigating how mobile technology habits might relate to individual differences in cognition and affect. The research presented in this paper provides evidence that heavier investment in mobile devices is correlated with a relatively weaker tendency to delay gratification (as measured by a delay discounting task) and a greater inclination toward impulsive behavior (i.e., weaker impulse control, assessed behaviorally and through self-report) but is not related to individual differences in sensitivity to reward. Analyses further demonstrated that individual variation in impulse control mediates the relationship between mobile technology usage and delay of gratification. Although based on correlational results, these findings lend some backing to concerns that increased use of portable electronic devices could have negative impacts on impulse control and the ability to appropriately valuate delayed rewards."}
{"_id":"169476ccd90c41054a78a38192b1138599f4ddc0","title":"Model predictive control for three-level boost converter in photovoltaic systems","text":"In this paper, a three-level Boost (TLB) converter maximum power point tracking (MPPT) control strategy for a two-stage photovoltaic (PV) system based on model predictive control (MPC) is proposed. This method realizes fast and precise control through establishment of a prediction model and minimization of a cost function. In the work, first a three-level predictive model for the boost converter is extracted, then a predictive control algorithm using the Perturb and Observe (P&O) method is proposed, and subsequently voltage balance of the neutral point of the converter is added to the control loop. Based on the proposed method, since the inverter is not obligated to control the DC-link neutral point voltage, its modulation can be used to realize other controls on the AC side to improve grid operation. Results of the control scheme have been verified through Matlab \/ Simulink, and the results have been in agreement with the expectations."}
{"_id":"97902bc7485dc991200848094a69f44c56458b7a","title":"Comparison of Leaf Recognition by Moments and Fourier Descriptors","text":"We test various features for recognition of leaves of wooden species. We compare Fourier descriptors, Zernike moments, Legendre moments and Chebyshev moments. All the features are computed from the leaf boundary only. Experimental evaluation on real data indicates that Fourier descriptors slightly outperform the other tested features."}
{"_id":"abcf3c4232f9d4eb8d0161b32fde2f5624fb97ad","title":"Using Distributed Scrum for Supporting an Online Community-A Qualitative Descriptive Study of Students \u2019 Perceptions","text":"One purpose of higher education is to prepare students for a modern and ever-changing global society characterized by increasing complexity and collaborative environments. Scrum is an agile, widely used framework for project management dealing with the development of complex products. Scrum projects are conducted in small, empowered teams with intense communication, interaction and collaboration between the team members, facilitated by a servant-leader Scrum master. Scrum has been commonly used in professional software development and is also now being adopted in other areas, including education. There have been few studies of the application of Scrum in higher education and very few of them have studied distributed Scrum in an online context. An online learning community has several positive effects for students such as increased learning, engagement, retention and lower risks for isolation and dropouts. Participating in and contributing to a team is dependent on a sense of community, which can be difficult to build up in a distributed environment where members are geographically dispersed and do not have the possibility to meet and communicate face to face. This study examines to what extent and how distributed Scrum can support building an online learning community, from a student perspective. Twenty students, enrolled in an online course in distributed software development, participated in four Scrum projects as members of distributed Scrum teams, each team consisting of five students. Students\u2019 perceptions were investigated by conducting semi-structured interviews. The interview transcripts were analyzed according to Rovai\u2019s four dimensions of a classroom community. The results indicate that students were very satisfied with their distributed Scrum projects and that they experienced a high degree of flexibility during the projects. The Scrum process promoted and initiated communication and interaction among students and they learned how to communicate and collaborate effectively in an online environment. The transparency in Scrum was perceived as a key factor to open communication and effective collaboration and also contributed to increasing their motivation and engagement in the projects. Another interesting outcome of this study was understanding the importance of creating a team with members who are similar regarding competence level, ambition and preferences in working schedule."}
{"_id":"ffb73de64c9c769ae534201768d272c519991d1f","title":"Path-tracking of a tractor-trailer vehicle along rectilinear and circular paths: a Lyapunov-based approach","text":"The problem of asymptotic stabilization for straight and circular forward\/backward motions of a tractor-trailer system is addressed using Lyapunov techniques. Smooth, bounded, nonlinear control laws achieving asymptotic stability along the desired path are designed, and explicit bounds on the region of attraction are provided. The problem of asymptotic controllability with bounded control is also addressed."}
{"_id":"7e80397d3dcb359761d163aaf10bf60c696642d1","title":"Bloom Filter Performance on Graphics Engines","text":"Bloom filters are a probabilistic technique for large-scale set membership tests. They exhibit no false negative test results but are susceptible to false positive results. They are well-suited to both large sets and large numbers of membership tests. We implement the Bloom filters present in an accelerated version of BLAST, a genome biosequence alignment application, on NVIDIA GPUs and develop an analytic performance model that helps potential users of Bloom filters to quantify the inherent tradeoffs between throughput and false positive rates."}
{"_id":"07b27e79099f00a8d50f9e529e6b325ed827ead2","title":"Attribute bagging: improving accuracy of classifier ensembles by using random feature subsets","text":"We present attribute bagging (AB), a technique for improving the accuracy and stability of classi#er ensembles induced using random subsets of features. AB is a wrapper method that can be used with any learning algorithm. It establishes an appropriate attribute subset size and then randomly selects subsets of features, creating projections of the training set on which the ensemble classi#ers are built. The induced classi#ers are then used for voting. This article compares the performance of our AB method with bagging and other algorithms on a hand-pose recognition dataset. It is shown that AB gives consistently better results than bagging, both in accuracy and stability. The performance of ensemble voting in bagging and the AB method as a function of the attribute subset size and the number of voters for both weighted and unweighted voting is tested and discussed. We also demonstrate that ranking the attribute subsets by their classi#cation accuracy and voting using only the best subsets further improves the resulting performance of the ensemble. ? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."}
{"_id":"38676c77a8f5f24ed3e5357b83185064baa4cb6f","title":"Your liking is my curiosity : a social popularity intervention to induce curiosity","text":"Our actions and decisions are regularly influenced by the social environment around us. Can social environment be leveraged to induce curiosity and facilitate subsequent learning? Across two experiments, we show that curiosity is contagious: social environment can influence people\u2019s curiosity about the answers to scientific questions. Our findings show that people are more likely to become curious about the answers to more popular questions, which in turn influences the information they choose to reveal. Given that curiosity has been linked to better learning, these findings have important implications for education."}
{"_id":"8213dbed4db44e113af3ed17d6dad57471a0c048","title":"The Nature of Statistical Learning Theory","text":""}
{"_id":"033eb044ef6a865a53878397633876827b7a8f20","title":"Character-Aware Neural Language Models","text":"We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long shortterm memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-ofthe-art despite having 60% fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level\/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling."}
{"_id":"6e88d09b2adc7a3d9230d324387929ec54a9d886","title":"Data-intensive applications, challenges, techniques and technologies: A survey on Big Data","text":"It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore\u2019s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as dataintensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. 2014 Elsevier Inc. All rights reserved."}
{"_id":"f19983b3e9b7fe8106c0375ebbd9f73a53295a28","title":"Data quality management, data usage experience and acquisition intention of big data analytics","text":"Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics"}
{"_id":"478fbef8568a021c3d91c13128efa19ad719dd88","title":"The 8 requirements of real-time stream processing","text":"Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the \"sea change\" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get \"sensor-tagged\" and report its state or location in real time. This sensorization of the real world will lead to a \"green field\" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged---including off-the-shelf stream processing engines---specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being \"repurposed\" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned."}
{"_id":"6834913a76b686957c0b8c755d1ca6ef3bd76914","title":"Data privacy through optimal k-anonymization","text":"Data de-identification reconciles the demand for release of data for research purposes and the demand for privacy from individuals. This paper proposes and evaluates an optimization algorithm for the powerful de-identification procedure known as k-anonymization. A k-anonymized dataset has the property that each record is indistinguishable from at least k - 1 others. Even simple restrictions of optimized k-anonymity are NP-hard, leading to significant computational challenges. We present a new approach to exploring the space of possible anonymizations that tames the combinatorics of the problem, and develop data-management strategies to reduce reliance on expensive operations such as sorting. Through experiments on real census data, we show the resulting algorithm can find optimal k-anonymizations under two representative cost measures and a wide range of k. We also show that the algorithm can produce good anonymizations in circumstances where the input data or input parameters preclude finding an optimal solution in reasonable time. Finally, we use the algorithm to explore the effects of different coding approaches and problem variations on anonymization quality and performance. To our knowledge, this is the first result demonstrating optimal k-anonymization of a non-trivial dataset under a general model of the problem."}
{"_id":"76c5db9edf820433eae631383f08b4e89e90fffa","title":"Privacy-preserving trajectory data publishing by local suppression","text":"The pervasiveness of location-aware devices has spawned extensive research in trajectory data mining, resulting in many important real-life applications. Yet, the privacy issue in sharing trajectory data among different parties often creates an obstacle for effective data mining. In this paper, we study the challenges of anonymizing trajectory data: high dimensionality, sparseness, and sequentiality. Employing traditional privacy models and anonymization methods often leads to low data utility in the resulting data and ineffective data mining. In addressing these challenges, this is the first paper to introduce local suppression to achieve a tailored privacy model for trajectory data anonymization. The framework allows the adoption of various data utility metrics for different data mining tasks. As an illustration, we aim at preserving both instances of location-time doublets and frequent sequences in a trajectory database, both being the foundation of many trajectory data \u2217Corresponding author Email addresses: ru_che@encs.concordia.ca (Rui Chen), fung@ciise.concordia.ca (Benjamin C. M. Fung), no_moham@encs.concordia.ca (Noman Mohammed), bcdesai@cs.concordia.ca (Bipin C. Desai) Preprint submitted to Information Sciences April 10, 2011 mining tasks. Our experiments on both synthetic and real-life data sets suggest that the framework is effective and efficient to overcome the challenges in trajectory data anonymization. In particular, compared with the previous works in the literature, our proposed local suppression method can significantly improve the data utility in anonymous trajectory data."}
{"_id":"b2aaa63f6f5f0540da1b530853e427574c0848a0","title":"Scholarly use of information: graduate students' information seeking behaviour","text":"Introduction. This study explored graduate students' information behaviour related to their process of inquiry and scholarly activities. Method. In depth, semi-structured interviews were conducted with one hundred graduate students representing all disciplines and departments from Carnegie Mellon University. Analysis. Working in pairs, we coded transcripts of interviews into meaningful categories using ATLAS.ti software. The combined use of quantitative and qualitative analysis aimed to reduce subjectivity. Results. Graduate students often begin with a meeting with professors who provide direction, recommend and provide resources. Other students help to shape graduate students' research activities, and university library personnel provide guidance in finding resources. The Internet plays a major role, although students continue to use print resources. Convenience, lack of sophistication in finding and using resources and course requirements affect their information behaviour. Findings vary across disciplines and between programmes. Conclusion. Libraries can influence students' information behaviour by re-evaluating their instructional programmes and provision of resources and services. They can take a lead by working with academic staff to guide students."}
{"_id":"8a84ea00fc22fb7d4b6d6f0b450e43058cb4113f","title":"Music and language side by side in the brain: a PET study of the generation of melodies and sentences.","text":"Parallel generational tasks for music and language were compared using positron emission tomography. Amateur musicians vocally improvised melodic or linguistic phrases in response to unfamiliar, auditorily presented melodies or phrases. Core areas for generating melodic phrases appeared to be in left Brodmann area (BA) 45, right BA 44, bilateral temporal planum polare, lateral BA 6, and pre-SMA. Core areas for generating sentences seemed to be in bilateral posterior superior and middle temporal cortex (BA 22, 21), left BA 39, bilateral superior frontal (BA 8, 9), left inferior frontal (BA 44, 45), anterior cingulate, and pre-SMA. Direct comparisons of the two tasks revealed activations in nearly identical functional brain areas, including the primary motor cortex, supplementary motor area, Broca's area, anterior insula, primary and secondary auditory cortices, temporal pole, basal ganglia, ventral thalamus, and posterior cerebellum. Most of the differences between melodic and sentential generation were seen in lateralization tendencies, with the language task favouring the left hemisphere. However, many of the activations for each modality were bilateral, and so there was significant overlap. While clarification of this overlapping activity awaits higher-resolution measurements and interventional assessments, plausible accounts for it include component sharing, interleaved representations, and adaptive coding. With these and related findings, we outline a comparative model of shared, parallel, and distinctive features of the neural systems supporting music and language. The model assumes that music and language show parallel combinatoric generativity for complex sound structures (phonology) but distinctly different informational content (semantics)."}
{"_id":"a476d62237bd84719b77190f9008e0745b2d1e27","title":"Scalable Algorithms for Molecular Dynamics Simulations on Commodity Clusters","text":"Although molecular dynamics (MD) simulations of biomolecular systems often run for days to months, many events of great scientific interest and pharmaceutical relevance occur on long time scales that remain beyond reach. We present several new algorithms and implementation techniques that significantly accelerate parallel MD simulations compared with current state-of-the-art codes. These include a novel parallel decomposition method and message-passing techniques that reduce communication requirements, as well as novel communication primitives that further reduce communication time. We have also developed numerical techniques that maintain high accuracy while using single precision computation in order to exploit processor-level vector instructions. These methods are embodied in a newly developed MD code called Desmond that achieves unprecedented simulation throughput and parallel scalability on commodity clusters. Our results suggest that Desmond's parallel performance substantially surpasses that of any previously described code. For example, on a standard benchmark, Desmond's performance on a conventional Opteron cluster with 2K processors slightly exceeded the reported performance of IBM's Blue Gene\/L machine with 32K processors running its Blue Matter MD code."}
{"_id":"5dee13efc6090823d9cb7eaab441b0427575f6f6","title":"Cellular IP: a new approach to Internet host mobility","text":"This paper describes a new approach to Internet host mobility. We argue that by separating local and wide area mobility, the performance of existing mobile host protocols (e.g. Mobile IP) can be significantly improved. We propose Cellular IP, a new lightweight and robust protocol that is optimized to support local mobility but efficiently interworks with Mobile IP to provide wide area mobility support. Cellular IP shows great benefit in comparison to existing host mobility proposals for environments where mobile hosts migrate frequently, which we argue, will be the rule rather than the exception as Internet wireless access becomes ubiquitous. Cellular IP maintains distributed cache for location management and routing purposes. Distributed paging cache coarsely maintains the position of 'idle' mobile hosts in a service area. Cellular IP uses this paging cache to quickly and efficiently pinpoint 'idle' mobile hosts that wish to engage in 'active' communications. This approach is beneficial because it can accommodate a large number of users attached to the network without overloading the location management system. Distributed routing cache maintains the position of active mobile hosts in the service area and dynamically refreshes the routing state in response to the handoff of active mobile hosts. These distributed location management and routing algorithms lend themselves to a simple and low cost implementation of Internet host mobility requiring no new packet formats, encapsulations or address space allocation beyond what is present in IP."}
{"_id":"446545c2a1043e1732b6cbaad01fe7089ad3f36e","title":"Real-time pen-and-ink illustration of landscapes","text":"Non-photorealistic rendering has been proven to be particularly efficient in conveying and transmitting selected visual information. Our paper presents a NPR rendering pipeline that supports pen-and-ink illustration for, but not limited to, complex landscape scenes in real time. This encompasses a simplification framework using clustering which enables new approaches to efficient and coherent rendering of stylized silhouettes, hatching and abstract shading. Silhouette stylization is performed in image-space. This avoids explicit computation of connected lines. Further, coherent hatching of the tree foliage is performed using an approximate view-dependent parameterization computed on-the-fly within the same simplification framework. All NPR algorithms are integrated with photorealistic rendering, allowing seamless transition and combination between a variety of photorealistic and non-photorealistic drawing styles."}
{"_id":"c6fabf4a6208f3024efeb0b5247aa27158bba65f","title":"Planar Array Antennas with Travelling-Wave Excitation in Millimeter-Wave Band","text":"\u3042\u3089\u307e\u3057 \u30df\u30ea\u6ce2\u306e\u5468\u6ce2\u6570\u5e2f\u306b\u304a\u3044\u3066\u6307\u5411\u6027\u8d70\u67fb\u53ef\u80fd\u306a\u5e73\u9762\u30a2\u30f3\u30c6\u30ca\u304c\u671b\u307e\u308c\u3066\u3044\u308b.\u672c\u8ad6\u6587\u3067\u306f,\u30df\u30ea\u6ce2\u306e \u4ee3\u8868\u7684\u306a\u4e8c\u3064\u306e\u5e73\u9762\u30a2\u30f3\u30c6\u30ca\u3067\u3042\u308b\u5c0e\u6ce2\u7ba1\u30a2\u30f3\u30c6\u30ca\u3068\u30de\u30a4\u30af\u30ed\u30b9\u30c8\u30ea\u30c3\u30d7\u30a2\u30f3\u30c6\u30ca\u306b\u3064\u3044\u3066,\u958b\u767a\u3057\u305f\u5e73\u9762\u30a2\u30f3 \u30c6\u30ca\u3092\u7d39\u4ecb\u3059\u308b.\u30da\u30f3\u30b7\u30eb\u30d3\u30fc\u30e0\u304c\u5fc5\u8981\u306a\u6a5f\u68b0\u8d70\u67fb\u578b\u30a2\u30f3\u30c6\u30ca\u306b\u9069\u3057\u305f\u4e8c\u6b21\u5143\u5c0e\u6ce2\u7ba1\u30a2\u30ec\u30fc\u30a2\u30f3\u30c6\u30ca\u3068,\u8907\u6570\u306e \u30b5\u30d6\u30a2\u30ec\u30fc\u306e\u914d\u5217\u304b\u3089\u306a\u308b\u96fb\u5b50\u8d70\u67fb\u65b9\u5f0f\u306e\u30b5\u30d6\u30a2\u30ec\u30fc\u5e73\u9762\u30a2\u30f3\u30c6\u30ca\u7528\u306b,\u5c0e\u6ce2\u7ba1\u30a2\u30ec\u30fc\u30a2\u30f3\u30c6\u30ca\u3068\u30de\u30a4\u30af\u30ed\u30b9\u30c8 \u30ea\u30c3\u30d7\u30b3\u30e0\u30e9\u30a4\u30f3\u30a2\u30f3\u30c6\u30ca\u306b\u3064\u3044\u3066\u958b\u767a\u3057\u305f\u4f8b\u3092\u793a\u3059.\u3053\u308c\u3089\u306b\u3064\u3044\u3066,\u5404\u653e\u5c04\u7d20\u5b50\u3067\u53cd\u5c04\u7279\u6027\u3092\u4f4e\u6e1b\u3059\u308b\u3053\u3068 \u304c,\u7279\u6027\u306e\u5411\u4e0a\u30fb\u8a2d\u8a08\u81ea\u7531\u5ea6\u306e\u62e1\u5927\u306b\u6709\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059.\u307e\u305f,\u3053\u308c\u3089\u306e\u30a2\u30f3\u30c6\u30ca\u306b\u3064\u3044\u3066\u4f4e\u640d\u5931\u306b\u7d66\u96fb\u3059 \u308b\u69cb\u9020\u3092\u793a\u3059\u3068\u540c\u6642\u306b,\u7247\u7aef\u304b\u3089\u7d66\u96fb\u3059\u308b\u3088\u308a\u3082,\u4e2d\u592e\u304b\u3089\u7d66\u96fb\u3059\u308b\u65b9\u304c,\u5468\u6ce2\u6570\u5e2f\u57df\u5e45\u304c\u5e83\u304f\u306a\u308b\u3053\u3068\u3092\u793a\u3059. \u66f4\u306b,\u3053\u308c\u3089\u306e\u30a2\u30f3\u30c6\u30ca\u5b9f\u73fe\u306e\u305f\u3081\u306e\u30ad\u30fc\u3068\u306a\u308b\u9032\u884c\u6ce2\u52b1\u632f\u30a2\u30ec\u30fc\u8a2d\u8a08\u6280\u8853\u306b\u3064\u3044\u3066\u89e3\u8aac\u3059\u308b. \u30ad\u30fc\u30ef\u30fc\u30c9 \u30df\u30ea\u6ce2,\u30a2\u30ec\u30fc\u30a2\u30f3\u30c6\u30ca,\u5e73\u9762\u30a2\u30f3\u30c6\u30ca,\u5c0e\u6ce2\u7ba1\u30a2\u30f3\u30c6\u30ca,\u30de\u30a4\u30af\u30ed\u30b9\u30c8\u30ea\u30c3\u30d7\u30a2\u30f3\u30c6\u30ca"}
{"_id":"58ca5ac14af2765ce1d25c3a82d6f9312437ded0","title":"Multiview Clustering via Adaptively Weighted Procrustes","text":"In this paper, we make a multiview extension of the spectral rotation technique raised in single view spectral clustering research. Since spectral rotation is closely related to the Procrustes Analysis for points matching, we point out that classical Procrustes Average approach can be used for multiview clustering. Besides, we show that direct applying Procrustes Average (PA) in multiview tasks may not be optimal theoretically and empirically, since it does not take the clustering capacity differences of different views into consideration. Other than that, we propose an Adaptively Weighted Procrustes (AWP) approach to overcome the aforementioned deficiency. Our new AWP weights views with their clustering capacities and forms a weighted Procrustes Average problem accordingly. The optimization algorithm to solve the new model is computational complexity analyzed and convergence guaranteed. Experiments on five real-world datasets demonstrate the effectiveness and efficiency of the new models."}
{"_id":"3365109a45c7874049fd858602b66bbe8d75f680","title":"Accelerating Comparative Genomics Workflows in a Distributed Environment with Optimized Data Partitioning","text":"The advent of new sequencing technology has generated massive amounts of biological data at unprecedented rates. High-throughput bioinformatics tools are required to keep pace with this. Here, we implement a workflow-based model for parallelizing the data intensive task of genome alignment and variant calling with BWA and GATK's Haplotype Caller. We explore different approaches of partitioning data and how each affect the run time. We observe granularity-based partitioning for BWA and alignment-based partitioning for Halo type Caller to be the optimal choices for the pipeline. We identify the various challenges encountered while developing such an application and provide an insight into addressing them. We report significant performance improvements, from 12 days to 4 hours, while running the BWA-GATK pipeline using 100 nodes for analyzing high-coverage oak tree data."}
{"_id":"ab8fb76765c698070d62554378bc09e4c7517447","title":"ARTINO: A New High Resolution 3D Imaging Radar System on an Autonomous Airborne Platform","text":"The new radar system ARTINO (Airborne Radar for Three-dimensional Imaging and Nadir Observation), developed at FGAN-FHR, allows to image a direct overflown scene in three dimensions. Integrated in a small, mobile, and dismountable UAV (Unmanned Aerial Vehicle) it will be an ideal tool for various applications. This paper gives an overview about the ARTINO principle, the raw data simulation, the image formation, the technical realisation, and the status of the experimental system. I. THE ARTINO PRINCIPLE ARTINO is a new radar system integrated in a small and dismountable low-wing UAV, which allows to image the direct overflown scene in three dimensions (Figure 1). Fig. 1. Artist impression of an imaging mission using ARTINO. This new system can image the direct overflown scene in three dimensions. General side-looking SAR systems are constraint by shading effects which can hide essential information in the explored scene. The downward-looking concept of ARTINO overcomes this restriction and enables imaging of street canyons and deep terrain in mountainous areas. Moreover, the 3D imaging capability together with the small and mobile platform is an ideal tool of close in time data acquisitions of fast changing terrains, like snow slopes (danger of avalanches) and active volcanoes. This new system could be used for various applications, like DEM (Digital Elevation Model) generation, surveying, city planing, environmental monitoring, disaster relief, surveillance, and reconnaissance. In contrary to similar concepts (e.g. [1]\u2013[3]) the ARTINO principle works with a sparse antenna array distributed along the wings, with the transmitting elements at the tips and the receiving elements in between. Virtual antenna elements are formed by the mean positions of every couple of single transmit and receive elements. Finally, one gets a fully distributed virtual antenna array. The 3D resolution cells are formed by the appliance of the synthetic aperture and a beamforming operation. A detailed description of the ARTINO principle, the used UAV (Figure 2), and the simulation of raw data can be found in [4]. The image formation using the ARTINO principle is extensively discussed in [5]. A detailed description of the technical realization and its status is given in [6]. This paper gives an overview of the concept, the processing, some first simulation results, and the technical realisation. Fig. 2. Photo of the low wing UAV ARTINO. The new radar system will be integrated in the fuselage and the wings. II. MODEL OF THE ARTINO CONCEPT A. Geometrical consideration and signal model ARTINO is supposed to fly at the altitude h along the x-axis with the velocity v. The virtual antenna is composed of Nvirt elements, which are centered at the y-axis and regularly spaced along this axis. The position of the i-th virtual antenna element with i \u2208 [\u2212Nvirt\u22121 2 ; Nvirt\u22121 2 ] is given by \u03b7i = (x, yi, h). The transpose operator is denoted by the superscript . T denotes the pulse-to-pulse time and t the fast time. The antenna position along the x-axis at time T is given by x = v\u00b7T . Figure 3 shows the geometry of the ARTINO principle. The distance d between the virtual antenna elements was determined by simulations in order to optimize the antenna beam (reduction of grating lobes) of the whole array (Figure 4). For the demonstration of the feasibility of this new radar concept, a pulse radar is assumed. To obtain a distinct assignment of each virtual antenna element, it will be necessary for the experimental system that the real antenna elements transmit with a time multiplex from pulse to pulse. In the simulation, all virtual antenna elements transmit simultaneously. A point scatterer P is positioned at \u03be = (\u03bex, \u03bey, \u03bez) with the reflectivity \u03b1(\u03be) (Figure 3). The signal assigned to the 0-7803-9510-7\/06\/$20.00 \u00a9 2006 IEEE 3825 Fig. 3. Geometry for the ARTINO principle \u22123 \u22122 \u22121 0 1 2 3 \u221240 \u221230 \u221220 \u221210 0"}
{"_id":"393f75566fc90724852c4d259159c9ed1438c8dd","title":"An artificial neural network to estimate physical activity energy expenditure and identify physical activity type from an accelerometer.","text":"The aim of this investigation was to develop and test two artificial neural networks (ANN) to apply to physical activity data collected with a commonly used uniaxial accelerometer. The first ANN model estimated physical activity metabolic equivalents (METs), and the second ANN identified activity type. Subjects (n = 24 men and 24 women, mean age = 35 yr) completed a menu of activities that included sedentary, light, moderate, and vigorous intensities, and each activity was performed for 10 min. There were three different activity menus, and 20 participants completed each menu. Oxygen consumption (in ml x kg(-1) x min(-1)) was measured continuously, and the average of minutes 4-9 was used to represent the oxygen cost of each activity. To calculate METs, activity oxygen consumption was divided by 3.5 ml x kg(-1) x min(-1) (1 MET). Accelerometer data were collected second by second using the Actigraph model 7164. For the analysis, we used the distribution of counts (10th, 25th, 50th, 75th, and 90th percentiles of a minute's second-by-second counts) and temporal dynamics of counts (lag, one autocorrelation) as the accelerometer feature inputs to the ANN. To examine model performance, we used the leave-one-out cross-validation technique. The ANN prediction of METs root-mean-squared error was 1.22 METs (confidence interval: 1.14-1.30). For the prediction of activity type, the ANN correctly classified activity type 88.8% of the time (confidence interval: 86.4-91.2%). Activity types were low-level activities, locomotion, vigorous sports, and household activities\/other activities. This novel approach of applying ANNs for processing Actigraph accelerometer data is promising and shows that we can successfully estimate activity METs and identify activity type using ANN analytic procedures."}
{"_id":"10d3f77225eca1d576268ba84ed83f230a5e47c4","title":"Crafting a multi-task CNN for viewpoint estimation","text":"Convolutional Neural Networks (CNNs) were recently shown to provide state-of-theart results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset [28]. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP."}
{"_id":"a8dbadf2c893337cae38de3c898d6181a5c9fe98","title":"Analysis on Credit Card Fraud Detection Methods 1","text":"Due to the theatrical increase of fraud which results in loss of dollars worldwide each year, several modern techniques in detecting fraud are persistently evolved and applied to many business fields. Fraud detection involves monitoring the activities of populations of users in order to estimate, perceive or avoid undesirable behavior. Undesirable behavior is a broad term including delinquency, fraud, intrusion, and account defaulting. This paper presents a survey of current techniques used in credit card fraud detection and telecommunication fraud. The goal of this paper is to provide a comprehensive review of different techniques to detect fraud."}
{"_id":"dda8b34e2532c0f54e0ae9ad3be50085935e6439","title":"Children \u2019 s Eye Movements during Listening : Developmental Evidence for a Constraint-Based Theory of Sentence Processing","text":"Many comprehension studies of grammatical development have focused on the ultimate interpretation that children assign to sentences and phrases, yielding somewhat static snapshots of children's emerging grammatical knowledge. Studies of the dynamic processes underlying children's language comprehension have to date been rare, owing in part to the lack of online sentence processing techniques suitable for use with children. In this chapter, we describe recent work from our research group, which examines the moment-by-moment interpretation decisions of children (age 4 to 6 years) while they listen to spoken sentences. These real-time measures were obtained by recording the children's eye movements as they visually interrogated and manipulated objects in response to spoken instructions. The first of these studies established some striking developmental differences in processing ability, with the youngest children showing an inability to use relevant properties of the referential scene to resolve temporary grammatical ambiguities (Trueswell, Sekerina, Hill, & Logrip, 1999). This finding could be interpreted as support for an early encapsulated syntactic processor that has difficulty using non-syntactic information to revise parsing commitments. However, we will review evidence from a series of follow-up experiments which suggest that this pattern arises from a developing interactive parsing system. Under this account, adult and child sentence comprehension is a \" perceptual guessing game \" in which multiple statistical cues are used to recover detailed linguistic structure. These cues, which include lexical-distribution evidence, verb semantic biases, and referential scene information, come \" online \" (become automated) at different points in the course of development. The developmental timing of these effects is related to their differential reliability and ease of detection in the input."}
{"_id":"41b0d9399573a6ca0d48f9ae06fa14697e80073d","title":"Development and natural history of mood disorders","text":"To expand and accelerate research on mood disorders, the National Institute of Mental Health (NIMH) developed a project to formulate a strategic research plan for mood disorder research. One of the areas selected for review concerns the development and natural history of these disorders. The NIMH convened a multidisciplinary Workgroup of scientists to review the field and the NIMH portfolio and to generate specific recommendations. To encourage a balanced and creative set of proposals, experts were included within and outside this area of research, as well as public stakeholders. The Workgroup identified the need for expanded knowledge of mood disorders in children and adolescents, noting important gaps in understanding the onset, course, and recurrence of early-onset unipolar and bipolar disorder. Recommendations included the need for a multidisciplinary research initiative on the pathogenesis of unipolar depression encompassing genetic and environmental risk and protective factors. Specifically, we encourage the NIMH to convene a panel of experts and advocates to review the findings concerning children at high risk for unipolar depression. Joint analyses of existing data sets should examine specific risk factors to refine models of pathogenesis in preparation for the next era of multidisciplinary research. Other priority areas include the need to assess the long-term impact of successful treatment of juvenile depression and known precursors of depression, in particular, childhood anxiety disorders. Expanded knowledge of pediatric-onset bipolar disorder was identified as a particularly pressing issue because of the severity of the disorder, the controversies surrounding its diagnosis and treatment, and the possibility that widespread use of psychotropic medications in vulnerable children may precipitate the condition. The Workgroup recommends that the NIMH establish a collaborative multisite multidisciplinary Network of Research Programs on Pediatric-Onset Bipolar Disorder to achieve a better understanding of its causes, course, treatment, and prevention. The NIMH should develop a capacity-building plan to ensure the availability of trained investigators in the child and adolescent field. Mood disorders are among the most prevalent, recurrent, and disabling of all illnesses. They are often disorders of early onset. Although the NIMH has made important strides in mood disorders research, more data, beginning with at-risk infants, children, and adolescents, are needed concerning the etiology and developmental course of these disorders. A diverse program of multidisciplinary research is recommended to reduce the burden on children and families affected with these conditions."}
{"_id":"127db6c733f2882754f56835ebc43e58016d8083","title":"Clique Graphs and Overlapping Communities","text":"It is shown how to construct a clique graph in which properties of cliques of a fixed order in a given graph are represented by vertices in a weighted graph. Various definitions and motivations for these weights are given. The detection of communities or clusters is used to illustrate how a clique graph may be exploited. In particular a benchmark network is shown where clique graphs find the overlapping communities accurately while vertex partition methods fail. PACS numbers: 89.75.Hc, 89.75.Fb, 89.75.-k"}
{"_id":"0970c10b5af6c5bbaeedf5d696c3c355aaa97959","title":"Document Image Quality Assessment: A Brief Survey","text":"To maintain, control and enhance the quality of document images and minimize the negative impact of degradations on various analysis and processing systems, it is critical to understand the types and sources of degradations and develop reliable methods for estimating the levels of degradations. This paper provides a brief survey of research on the topic of document image quality assessment. We first present a detailed analysis of the types and sources of document degradations. We then review techniques for document image degradation modeling. Finally, we discuss objective measures and subjective experiments that are used to characterize document image quality."}
{"_id":"13ae3c8afef5a0d6f4c9e684da9fc1fa96caaeb6","title":"Online Anomaly Detection in Crowd Scenes via Structure Analysis","text":"Abnormal behavior detection in crowd scenes is continuously a challenge in the field of computer vision. For tackling this problem, this paper starts from a novel structure modeling of crowd behavior. We first propose an informative structural context descriptor (SCD) for describing the crowd individual, which originally introduces the potential energy function of particle's interforce in solid-state physics to intuitively conduct vision contextual cueing. For computing the crowd SCD variation effectively, we then design a robust multi-object tracker to associate the targets in different frames, which employs the incremental analytical ability of the 3-D discrete cosine transform (DCT). By online spatial-temporal analyzing the SCD variation of the crowd, the abnormality is finally localized. Our contribution mainly lies on three aspects: 1) the new exploration of abnormal detection from structure modeling where the motion difference between individuals is computed by a novel selective histogram of optical flow that makes the proposed method can deal with more kinds of anomalies; 2) the SCD description that can effectively represent the relationship among the individuals; and 3) the 3-D DCT multi-object tracker that can robustly associate the limited number of (instead of all) targets which makes the tracking analysis in high density crowd situation feasible. Experimental results on several publicly available crowd video datasets verify the effectiveness of the proposed method."}
{"_id":"91dfbc19a2bb88b00fd258d36d0f099f5ceb5772","title":"BER analysis of optical eU-OFDM transmission over AWGN","text":"Enhanced unipolar orthogonal frequency-division multiplexing (eU-OFDM) was recently introduced to improve the power-spectral efficiency trade-off of intensity-modulated direct-detection (IM\/DD) optical OFDM transmission schemes. This is accomplished by superimposing unipolar signals over several layers. Bit-error-rate (BER) analysis for each layer however, is eluded by error-propagation from previous layers in the eU-OFDM receiver. This paper presents a general mathematical model for eU-OFDM transmission. From the model, we introduce successive interference cancellation using soft-symbol estimates, which leads to slight performance improvement compared to the original hard-decision counterpart. More importantly, by using soft-symbol estimates we derive analytically tractable expressions that accurately predict the system's performance."}
{"_id":"7022082e8974d54d5bf79c782702f885feb1e47f","title":"A comprehensive study on security attacks on SSL\/TLS protocol","text":"Secure Socket Layer (SSL) protocol was introduced in 1994 and was later renamed as transport layer security (TLS) protocol for securing transport layer. SSL\/TLS protocol is used for securing communication on the network by ensuring data confidentiality, data integrity and authenticity between the communicating party. Authentication of the communicating party and securing transfer of data is done through certificates, key exchange and cipher suites. Security issues were found during evolutionary development of SSL\/TLS protocol. The paper gives a detailed chronological order of attacks of past 22 years on SSL\/TLS protocol."}
{"_id":"e08088b490881afabb9b2298ae1d0702dcf7ba9d","title":"Post-Punching Behavior of Flat Slabs by","text":"Reinforced concrete flat slabs are a common structural system for cast-in-place concrete slabs. Failures in punching shear near the column regions are typically governing at ultimate. In case no punching shear or integrity reinforcement is placed, failures in punching develop normally in a brittle manner with almost no warning signs. Furthermore, the residual strength after punching is, in general, significantly lower than the punching load. Thus, punching of a single column of a flat slab overloads adjacent columns and can potentially lead to their failure on punching, thus triggering the progressive collapse of the structure. Over the past decades, several collapses have been reported due to punching shear failures, resulting in human casualties and extensive damage. Other than placing conventional punching shear reinforcement, the deformation capacity and residual strength after punching can also be enhanced by placing integrity reinforcement to avoid progressive collapses of flat slabs. This paper presents the main results of an extensive experimental campaign performed at the Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL) on the role of integrity reinforcement by means of 20 slabs with dimensions of 1500 x 1500 x 125 mm (\u22485 ft x 5 ft x 5 in.) and various integrity reinforcement layouts. The performance and robustness of the various solutions is investigated to obtain physical explanations and a consistent design model for the load-carrying mechanisms and strength after punching failures. INTRODUCTION Over the past decades, several collapses due to punching shear failure have been reported in Europe and America,"}
{"_id":"9d7af4b0f9a42f914f4d2a3bdafee776407a30d3","title":"User Interface Menu Design Performance and User Preferences: A Review and Ways Forward","text":"This review paper is about menus on web pages and applications and their positioning on the user screen. The paper aims to provide the reader with a succinct summary of the major research in this area along with an easy to read tabulation of the most important studies. Furthermore, the paper concludes with some suggestions for future research regarding how menus and their positioning on the screen could be improved. The two principal suggestions concern trying to use more qualitative methods for investigating the issues and to develop in the future more universally designed menus. Keywords\u2014Menus; navigation of interfaces; universal design;"}
{"_id":"1b7690012a25bb33b429dbd72eca7459b9f50653","title":"PEGASUS: A Policy Search Method for Large MDPs and POMDPs","text":"We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \u201cequivalent\u201d POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng [7], but with \u201csample complexity\u201d bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state\/continuous action problem involving learning to ride a bicycle."}
{"_id":"5e884f51916d37b91c35bae2a45b28d12b7e20d2","title":"Using EM for Reinforcement Learning","text":"We discsus Hinton\u2019s (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximisation procedure of Dempster, Laird & Rubin (1976)."}
{"_id":"cbd6e91afffa0fdd461452a90842241a6d44bf70","title":"Integrating the 3+1 SysML view model with safety engineering","text":"System safety is the property of the system that characterizes its ability to prevent from hazards, which may lead to accidents or losses. Traditionally, system developers are not familiar with system safety analysis processes which are performed by safety engineers. One reason for this is the gap that exists between the traditional development processes, methodologies, notations and tools and the ones used in safety engineering. This gap makes the development of safety aware systems a very complicated task. Several approaches based on UML have been proposed to address this gap. In this paper, an approach to integrate safety engineering with a SysML based development process that is expressed in the form of the V-model, is presented. Preliminary hazard analysis is adopted and applied to a SysML based requirements specification of the mechatronic system that exploits essential use cases. A case study from the railway domain is used to illustrate the proposed approach."}
{"_id":"22dbe3d73538361b07d2f29dda56547afc3f9642","title":"Social Choice for Partial Preferences Using Imputation","text":"Within the field of multiagent systems, the area of computational social choice considers the problems arising when decisions must be made collectively by a group of agents. Usually such systems collect a ranking of the alternatives from each member of the group in turn, and aggregate these individual rankings to arrive at a collective decision. However, when there are many alternatives to consider, individual agents may be unwilling, or unable, to rank all of them, leading to decisions that must be made on the basis of incomplete information. While earlier approaches attempt to work with the provided rankings by making assumptions about the nature of the missing information, this can lead to undesirable outcomes when the assumptions do not hold, and is ill-suited to certain problem domains. In this thesis, we propose a new approach that uses machine learning algorithms (both conventional and purpose-built) to generate plausible completions of each agent\u2019s rankings on the basis of the partial rankings the agent provided (imputations), in a way that reflects the agents\u2019 true preferences. We show that the combination of existing social choice functions with certain classes of imputation algorithms, which forms the core of our proposed solution, is equivalent to a form of social choice. Our system then undergoes an extensive empirical validation under 40 different test conditions, involving more than 50,000 group decision problems generated from real-world electoral data, and is found to outperform existing competitors significantly, leading to better group decisions overall. Detailed empirical findings are also used to characterize the behaviour of the system, and illustrate the circumstances in which it is most advantageous. A general testbed for comparing solutions using real-world and artificial data (Prefmine) is then described, in conjunction with results that justify its design decisions. We move on to propose a new machine learning algorithm intended specifically to learn and impute the preferences of agents, and validate its effectiveness. This Markov-Tree approach is demonstrated to be superior to imputation using conventional machine learning, and has a simple interpretation that characterizes the problems on which it will perform well. Later chapters contain an axiomatic validation of both of our new approaches, as well as techniques for mitigating their manipulability. The thesis concludes with a discussion of the applicability of its contributions, both for multiagent systems and for settings involving human elections. In all, we reveal an interesting connection between machine learning and computational social choice, and introduce a testbed which facilitates future research efforts on computational social choice for partial preferences, by allowing empirical comparisons between competing approaches to be conducted easily, accurately, and quickly. Perhaps most importantly, we offer an important and effective new direction for enabling group decision making when preferences are not completely specified, using imputation methods."}
{"_id":"2c13b73ce6123966a9f8c82b8f26d9e3fbc312b7","title":"The biology of cancer: metabolic reprogramming fuels cell growth and proliferation.","text":"Cell proliferation requires nutrients, energy, and biosynthetic activity to duplicate all macromolecular components during each passage through the cell cycle. It is therefore not surprising that metabolic activities in proliferating cells are fundamentally different from those in nonproliferating cells. This review examines the idea that several core fluxes, including aerobic glycolysis, de novo lipid biosynthesis, and glutamine-dependent anaplerosis, form a stereotyped platform supporting proliferation of diverse cell types. We also consider regulation of these fluxes by cellular mediators of signal transduction and gene expression, including the phosphatidylinositol 3-kinase (PI3K)\/Akt\/mTOR system, hypoxia-inducible factor 1 (HIF-1), and Myc, during physiologic cell proliferation and tumorigenesis."}
{"_id":"e28a0c509ef43f253ee475d30d5b419debefce05","title":"Self-enucleation in a young schizophrenic patient--a case report.","text":"Self-enucleation represents an extreme but fortunately rare form of deliberate self-harm. Case reports of patients who self-enucleate reveal some common features. A case of auto-enucleation in a young schizophrenic patient and a short discussion on deliberate self-harm are presented."}
{"_id":"774e560a2cadcb84f4b1def7b152e5398b062efb","title":"Scalable Modified Kneser-Ney Language Model Estimation","text":"We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM."}
{"_id":"b8737d6ec1b033f6185e1da0f40a14fa44808d3f","title":"Review of control algorithms for robotic ankle systems in lower-limb orthoses, prostheses, and exoskeletons.","text":"This review focuses on control strategies for robotic ankle systems in active and semiactive lower-limb orthoses, prostheses, and exoskeletons. Special attention is paid to algorithms for gait phase identification, adaptation to different walking conditions, and motion intention recognition. The relevant aspects of hardware configuration and hardware-level controllers are discussed as well. Control algorithms proposed for other actuated lower-limb joints (knee and\/or hip), with potential applicability to the development of ankle devices, are also included."}
{"_id":"d6c899b3cfc70d1d31f2d2cdf696ff7567ff0e02","title":"Exoskeleton robot for rehabilitation of elbow and forearm movements","text":"To perform essential daily activities the movement of shoulder, elbow, and wrist play a vital role and therefore proper functioning of upper-limb is very much essential. We therefore have been developing an exoskeleton robot (ExoRob) to rehabilitate and to ease upper limb motion. Toward to make a complete (i.e., 7DOF) upper-arm motion assisted robotic exoskeleton this paper focused on the development of a 2DOF exoskeleton robot to rehabilitate the elbow and forearm movements. The proposed 2DOF ExoRob is supposed to be worn on the lateral side of forearm and provide naturalistic range movements of elbow (flexion\/extension) and forearm (pronation\/supination) motions. This paper also focuses on the modeling and control of the proposed ExoRob. A kinematic model of the ExoRob has been developed based on modified Denavit-Hartenberg notations. Nonlinear sliding mode control technique is employed in dynamic simulation of the proposed ExoRob, where trajectory tracking that corresponds to typical rehab (passive) exercises has been carried out to evaluate the effectiveness of the developed model and controller. Simulated results show that the controller is able to maneuver the ExoRob efficiently to track the desired trajectories, which in this case consisted in passive arm movements. These movements are widely used in rehab therapy and could be performed efficiently with the developed ExoRob and the controller."}
{"_id":"6882dcb241f5aaefe85025bf754f8dd1c1502df1","title":"Robot-aided neurorehabilitation.","text":"Our goal is to apply robotics and automation technology to assist, enhance, quantify, and document neurorehabilitation. This paper reviews a clinical trial involving 20 stroke patients with a prototype robot-aided rehabilitation facility developed at the Massachusetts Institute of Technology, Cambridge, (MIT) and tested at Burke Rehabilitation Hospital, White Plains, NY. It also presents our approach to analyze kinematic data collected in the robot-aided assessment procedure. In particular, we present evidence 1) that robot-aided therapy does not have adverse effects, 2) that patients tolerate the procedure, and 3) that peripheral manipulation of the impaired limb may influence brain recovery. These results are based on standard clinical assessment procedures. We also present one approach using kinematic data in a robot-aided assessment procedure."}
{"_id":"03dd188eb4a34f2723d8e23a5a29ff86894c78f7","title":"An Alternative Way to Analyze Workflow Graphs","text":"At the CAiSE conference in Heidelberg in 1999, Wasim Sadiq and Maria Orlowska presented an algorithm to verify workflow graphs [19]. The algorithm uses a set of reduction rules to detect structural conflicts. This paper shows that the set of reduction rules presented in [19] is not complete and proposes an alternative algorithm. The algorithm translates workflow graphs into so-called WF-nets. WF-nets are a class of Petri nets tailored towards workflow analysis. As a result, Petri-net theory and tools can be used to verify workflow graphs. In particular, our workflow verification tool Woflan [21] can be used to detect design errors. It is shown that the absence of structural conflicts, i.e., deadlocks and lack of synchronization, conforms to soundness of the corresponding WF-net [2]. In contrast to the algorithm presented in [19], the algorithm presented in this paper is complete. Moreover, the complexity of this alternative algorithm is given."}
{"_id":"69e70b8101da9f794c33ee35740344461f262e8e","title":"Smart-Contract Based System Operations for Permissioned Blockchain","text":"Enterprises have paid attention to blockchain (BC), recently permissioned BC characterized with smart-contract, where busi-ness transactions among inter-authorized companies (forming consortium) can automatically be executed based on distributed consensus protocol over user-defined business logics pre-built with program codes. A single BC system will be built across mul-tiple management domains having different operational policies, e.g., datacenter of each organization; this will trigger a problem that its system operations (e.g., backup) will become time-consuming and costly due to the difficulty in unifying and\/or adjusting operational policy, schedule, etc. Toward solving the problem, we propose an operations execution method for BC systems; a primary idea is to define operations as smart-contract so that unified and synchronized cross-organizational operations can be executed effectively by using BC-native features. We de-sign the proposed method as hybrid architecture including in-BC consensus establishment and out-BC event-based instruction execution, in order to be adaptable to the recent heterogeneous BC architecture. Performance evaluation using a prototype with Hyperledger Fabric v1.0 shows that the proposed method can start executing operations within 5 seconds. Furthermore, cost evaluation using model-based estimation shows that the total yearly cost of monthly operations on a 5-organizational BC sys-tem could be reduced by 61 percent compared to a conventional manual method."}
{"_id":"3ee01ec27e4e66e089b72a9989724be611c2ad90","title":"Neural Map: Structured Memory for Deep Reinforcement Learning","text":"A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training."}
{"_id":"106804244aeca715094e12266e3233adca5b78af","title":"A portable powered ankle-foot orthosis for rehabilitation.","text":"Innovative technological advancements in the field of orthotics, such as portable powered orthotic systems, could create new treatment modalities to improve the functional out come of rehabilitation. In this article, we present a novel portable powered ankle-foot orthosis (PPAFO) to provide untethered assistance during gait. The PPAFO provides both plantar flexor and dorsiflexor torque assistance by way of a bidirectional pneumatic rotary actuator. The system uses a portable pneumatic power source (compressed carbon dioxide bottle) and embedded electronics to control the actuation of the foot. We collected pilot experimental data from one impaired and three nondisabled subjects to demonstrate design functionality. The impaired subject had bilateral impairment of the lower legs due to cauda equina syndrome. We found that data from nondisabled walkers demonstrated the PPAFO's capability to provide correctly timed plantar flexor and dorsiflexor assistance during gait. Reduced activation of the tibialis anterior during stance and swing was also seen during assisted nondisabled walking trials. An increase in the vertical ground reaction force during the second half of stance was present during assisted trials for the impaired subject. Data from nondisabled walkers demonstrated functionality, and data from an impaired walker demonstrated the ability to provide functional plantar flexor assistance."}
{"_id":"9a52172fb96a7ad91969a947587cf3db9b8180bf","title":"Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions","text":"Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation, machine translation and word sense disambiguation. We present methods of non-compositionality detection for English noun compounds using the unsupervised learning of a semantic composition function. Compounds which are not well modeled by the learned semantic composition function are considered noncompositional. We explore a range of distributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further. We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex. We show that enforcing sparsity is a useful regularizer in learning complex composition functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance."}
{"_id":"f5d6a1f6581b097e06620063e364f0c538956c3c","title":"Usability Problem Description and the Evaluator Effect in Usability Testing","text":"Previous usability evaluation method (UEM) comparison studies have noted an evaluator effect on problem detection in heuristic evaluation, with evaluators differing in problems found and problem severity judgments. There have been few studies of the evaluator effect in usability testing (UT), task-based testing with end-use rs. UEM comparison studies focus on counting usability problems detected, but we also need to assess the content of usability problem descriptions (UPDs) to more fully mea sure evaluation effectiveness. The goals of this research were to develop UPD gui delines, explore the evaluator effect in UT, and evaluate the usefulness of the guidelines for grading UPD content. Ten guidelines for writing UPDs were developed by consulting usability practitioners through two questionnaires and a card sort. These guidelines are ( briefly): be clear and avoid jargon, describe problem severity, provide backing data, describe problem causes, describe user actions, provide a solution, consider politics and diplomacy, be professional and scientific, describe your methodology, and help the re ader sympathize with the user. A fourth study compared usability reports collect ed from 44 evaluators, both practitioners and graduate students, watching the same 10-minute UT session recording. Three judges measured problem detection for each evaluator a nd graded the reports for following 6 of the UPD guidelines. There was support for existence of an evaluator effect, even when watching prerecorded sessions, with low to moderate individual thoroughness of problem detection across all\/severe problems (22%\/34%), reliability of problem detection (37%\/50%) and reliability of severity judgments (57% for severe ratings). Practiti oners received higher grades averaged across the 6 guidelines than students did, suggesting that the guide lin s may be useful for grading reports. The grades for the guidelines were not cor related with thoroughness, suggesting that the guideline grades complement measures of problem detection. A simulation of evaluators working in groups found a 34% increase in severe problems found by adding a second evaluator. The simulation also found that thoroughness of individual evaluators would have been overestimated if the study had included a small number of evaluators. The final recommendations are to use multipl e evaluators in UT, and to assess both problem detection and description when measuring evaluation effectiveness. iii ACKNOWLEDGEMENTS I would like to thank my advisory committee, Tonya Smith-Jackson, John Burton, Rex Hartson, Brian Kleiner, and Maury Nussbaum. In particular, Tonya Smith -Jackson provided suggestions for experimental design and statistical analysis, and Rex Hartson and Tonya Smith-Jackson provided suggestions for the judging procedure used to analyze the usability problems collected from evaluators. I also owe a special thanks to Laurian Hobby, John Howarth, and Pardha Pyla, each of whom generously donated over 50 hours of their time to read and evaluate hundreds of usability problems. The final study would not have been possible without them. Thanks also to my thesis advisor, Bob Williges, who guided me when I began studying usability methods. This dissertation would not have been possible without the support of my husband, Rob Capra, who shared the journey with me as we both completed our dissertations within a month of each other. I might not have discovered Human Factors or pursued a PhD without him. He is my partner in research, and many important ideas gre w out of discussions of our research at school, at home, and over meals. He is my partner in life, and I love him with all my heart. Many other people contributed to this research. Terence Andre shared the usability movies and reports from studies run by him and his students. Rob Capra assi sted with data coding in the first and fourth studies. Suzanne Aref suggested using factor analysis to cluster the items in the card sort. Joe Dumas shared his experience s analyzing the CUE-4 reports, provided criteria for identifying descriptions that discuss t he same usability problem, and helped recruit practitioners. Rolf Molich gave permiss ion to use the report template and severity rating scales from the CUE studies. The co mparison of usability diagnosis to medical diagnosis was refined through discussions with Rex Hartson and Steve Belz. Thanks to the dozens of usability practitioners and graduate students who volunteered to participate in my studies or provided feedback as pilot participants, especially the practitioners that took extra time above and beyond the study requirem nts to discuss their usability practices and reporting habits. Thanks to my famil and friends for years of encouragement and patience, and fellow students for support and commiseration along the way. Thanks to my former colleagues at what is now AT &T Labs for introducing me to Human Factors and sparking my interest in the field. Thanks to IMDb, Inc. for permission to include screen shots of their website in this document. Information courtesy of: The Internet Movie Database (http:\/\/www.imdb.com\/ ). Used with permission. Portions of this research were conducted while I was supported by the Alexander E. Walter Fellowship from the Grado Department of Industrial and Systems Eng ineering (2001-2004). iv TABLE OF CONTENTS ABSTRACT . ................................................................................................................. ii ACKNOWLEDGEMENTS............................................................................................. iii TABLE OF CONTENTS................................................................................................. iv LIST OF TABLES .........................................................................................................viii LIST OF FIGURES .......................................................................................................... x LIST OF EQUATIONS ..................................................................................................xii CHAPTER"}
{"_id":"ecc7607622c202f5a4e83a9a233930d1ba2f6648","title":"Driver fatigue detection based on saccadic eye movements","text":"The correct determination of driver's level of fatigue has been of vital importance for the safety of driving. There are various methods, such as analyzing facial expression, eyelid activity, and head movements to assess the fatigue level of drivers. This paper describes the design and prototype implementation of a driver fatigue level determination system based on detection of saccadic eye movements. Driver's eye movement speed is used to assess driver's fatigue level. The information about eyes is obtained via infrared led camera device. Movements of pupils were recorded in two driving scenarios with different traffic density. In the first scenario, the traffic density was set to low while the second scenario was based on high density and aggressive traffic. Based on the movements of pupils, the data on saccadic eye movement was analyzed to determine fatigue level of the driver. Acceleration, speed, and size of pupils at both traffic scenarios were compared with data mining techniques, such as segmentation adaptive peak, entropy, and data distribution analyses. Significantly different levels of fatigue were found between the tired and vigorous driver for the different types of scenarios."}
{"_id":"21db1334a75c6e12979d16de2e996c01e95006f5","title":"Emotion, plasticity, context, and regulation: perspectives from affective neuroscience.","text":"The authors present an overview of the neural bases of emotion. They underscore the role of the prefrontal cortex (PFC) and amygdala in 2 broad approach- and withdrawal-related emotion systems. Components and measures of affective style are identified. Emphasis is given to affective chronometry and a role for the PFC in this process is proposed. Plasticity in the central circuitry of emotion is considered, and implications of data showing experience-induced changes in the hippocampus for understanding psychopathology and stress-related symptoms are discussed. Two key forms of affective plasticity are described--context and regulation. A role for the hippocampus in context-dependent normal and dysfunctional emotional responding is proposed. Finally, implications of these data for understanding the impact on neural circuitry of interventions to promote positive affect and on mechanisms that govern health and disease are considered."}
{"_id":"7849a9929595b29caa1503d8acaa9c475b43fae2","title":"On Analyzing User Topic-Specific Platform Preferences Across Multiple Social Media Sites","text":"Topic modeling has traditionally been studied for single text collections and applied to social media data represented in the form of text documents. With the emergence of many social media platforms, users find themselves using different social media for posting content and for social interaction. While many topics may be shared across social media platforms, users typically show preferences of certain social media platform(s) over others for certain topics. Such platform preferences may even be found at the individual level. To model social media topics as well as platform preferences of users, we propose a new topic model known as MultiPlatform-LDA (MultiLDA). Instead of just merging all posts from different social media platforms into a single text collection, MultiLDA keeps one text collection for each social media platform but allowing these platforms to share a common set of topics. MultiLDA further learns the user-specific platform preferences for each topic. We evaluate MultiLDA against TwitterLDA, the state-of-the-art method for social media content modeling, on two aspects: (i) the effectiveness in modeling topics across social media platforms, and (ii) the ability to predict platform choices for each post. We conduct experiments on three real-world datasets from Twitter, Instagram and Tumblr sharing a set of common users. Our experiments results show that the MultiLDA outperforms in both topic modeling and platform choice prediction tasks. We also show empirically that among the three social media platforms, \u201cDaily matters\u201d and \u201cRelationship matters\u201d are dominant topics in Twitter, \u201cSocial gathering\u201d, \u201cOuting\u201d and \u201cFashion\u201d are dominant topics in Instagram, and \u201cMusic\u201d, \u201cEntertainment\u201d and \u201cFashion\u201d are dominant topics in Tumblr."}
{"_id":"1a89144b9d5518e3a5efca27087d8796476fbc61","title":"A3: a coding guideline for HCI+autism research using video annotation","text":"Due to the profile of strengths and weaknesses indicative of autism spectrum disorders (ASD), technology may play a key role in ameliorating communication difficulties with this population. This paper documents coding guidelines established through cross-disciplinary work focused on facilitating communication development in children with ASD using computerized feedback. The guidelines, referred to as A3 (pronounced A-Cubed) or Annotation for ASD Analysis, define and operationalize a set of dependent variables coded via video annotation. Inter-rater reliability data are also presented from a study currently in-progress, as well as related discussion to help guide future work in this area. The design of the A3 methodology is well-suited for the examination and evaluation of the behavior of low-functioning subjects with ASD who interact with technology."}
{"_id":"76ea56d814249d0636670687e335c495476572f8","title":"River flow time series prediction with a range-dependent neural network","text":"Artificial neural networks provide a promising alternative to hydrological time series modelling. However, there are still many fundamental problems requiring further analyses, such as structure identification, parameter estimation, generalization, performance improvement, etc. Based on a proposed clustering algorithm for the training pairs, a new neural network, namely the range-dependent neural network (RDNN) has been developed for better accuracy in hydrological time series prediction. The applicability and potentials of the RDNN in daily streamflow and annual reservoir inflow prediction are examined using data from two watersheds in China. Empirical comparisons of the predictive accuracy, in terms of the model efficiency R and absolute relative errors (ARE), between the RDNN, backpropagation (BP) networks and the threshold auto-regressive (TAR) model are made. The case studies demonstrated that the RDNN network performed significantly better than the BP network, especially for reproducing low-flow events."}
{"_id":"5fc795b50caef136178ca76a889eb51baee0ccea","title":"Short circuit III in high power IGBTs","text":"Short circuit III is the occurrence of a short circuit across the load during the conducting mode of the freewheeling diode. This has the same importance for the application of high power IGBTs as the known short circuit II. Different to short circuit II, the IGBT of the same module is now turned-on starting from very low voltage across its terminals and showing a forward recovery voltage before saturating. After that a dynamic short circuit peak current occurs similar to the one seen in SC II. During the short circuit, a reverse recovery process occurs at the freewheeling diode with a very high voltage slope."}
{"_id":"525caa4f62e14d5579dca7b61604fb8ae6d3a340","title":"Cancer phenotype as the outcome of an evolutionary game between normal and malignant cells","text":"Background:There is variability in the cancer phenotype across individuals: two patients with the same tumour may experience different disease life histories, resulting from genetic variation within the tumour and from the interaction between tumour and host. Until now, phenotypic variability has precluded a clear-cut identification of the fundamental characteristics of a given tumour type.Methods:Using multiple myeloma as an example, we apply the principles of evolutionary game theory to determine the fundamental characteristics that define the phenotypic variability of a tumour.Results:Tumour dynamics is determined by the frequency-dependent fitness of different cell populations, resulting from the benefits and costs accrued by each cell type in the presence of others. Our study shows how the phenotypic variability in multiple myeloma bone disease can be understood through the theoretical approach of a game that allows the identification of key genotypic features in a tumour and provides a natural explanation for phenotypic variability. This analysis also illustrates how complex biochemical signals can be translated into cell fitness that determines disease dynamics.Conclusion:The present paradigm is general and extends well beyond multiple myeloma, and even to non-neoplastic disorders. Furthermore, it provides a new perspective in dealing with cancer eradication. Instead of trying to kill all cancer cells, therapies should aim at reducing the fitness of malignant cells compared with normal cells, allowing natural selection to eradicate the tumour."}
{"_id":"8ef2a5e3dffb0a155a14575c8333b175b61e0675","title":"Machine Learning Applied to Cyber Operations","text":""}
{"_id":"1f000f4346d9fdf0b801dff81fe4873c18fcab4d","title":"Demanding Customers : Consumerist Patients and Quality of Care","text":"Consumerism arises when patients acquire and use medical information from sources other than their physicians. This practice has been hailed as a means of improving quality. This need not be the result. Our theoretical model identifies a channel through which consumerism may reduce quality: consumerist patients place additional demands on their doctors\u2019 time, thus imposing a negative externality on other patients. Relative to a world in which consumerism does not exist, consumerism may harm other consumerists, non-consumerists, or both. Data from a large national survey of physicians confirm the negative effects of consumerism: high levels of consumerist patients are associated with lower perceived quality among physicians."}
{"_id":"7b1cb08db30b4b223ac5601d1ca5baa23c6e9904","title":"Sequential Optimization and Reliability Assessment Method for Efficient Probabilistic Design","text":"Probabilistic design, such as reliability-based design and robust design, offers tool making reliable decisions with the consideration of uncertainty associated with de variables\/parameters and simulation models. Since a probabilistic optimization ofte volves a double-loop procedure for the overall optimization and iterative probabili assessment, the computational demand is extremely high. In this paper, the seq optimization and reliability assessment (SORA) is developed to improve the efficien probabilistic optimization. The SORA method employs a single-loop strategy with a s of cycles of deterministic optimization and reliability assessment. In each cycle, op zation and reliability assessment are decoupled from each other; the reliability asses is only conducted after the deterministic optimization to verify constraint feasibility un uncertainty. The key to the proposed method is to shift the boundaries of violated straints (with low reliability) to the feasible direction based on the reliability informati obtained in the previous cycle. The design is quickly improved from cycle to cycle an computational efficiency is improved significantly. Two engineering applications, reliability-based design for vehicle crashworthiness of side impact and the integr reliability and robust design of a speed reducer, are presented to demonstrate the tiveness of the SORA method. @DOI: 10.1115\/1.1649968 #"}
{"_id":"8c43cf593531013ebb819f3fb9a4c45453a23657","title":"LIPN at SemEval-2017 Task 10: Filtering Candidate Keyphrases from Scientific Publications with Part-of-Speech Tag Sequences to Train a Sequence Labeling Model","text":"This paper describes the system used by the team LIPN in SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications. The team participated in Scenario 1, that includes three subtasks, Identification of keyphrases (Subtask A), Classification of identified keyphrases (Subtask B) and Extraction of relationships between two identified keyphrases (Subtask C). The presented system was mainly focused on the use of part-of-speech tag sequences to filter candidate keyphrases for Subtask A. Subtasks A and B were addressed as a sequence labeling problem using Conditional Random Fields (CRFs) and even though Subtask C was out of the scope of this approach, one rule was included to identify synonyms."}
{"_id":"8d2dd62b1784794e545d44332a5cb66649af0eca","title":"Network densification: the dominant theme for wireless evolution into 5G","text":"This article explores network densification as the key mechanism for wireless evolution over the next decade. Network densification includes densification over space (e.g, dense deployment of small cells) and frequency (utilizing larger portions of radio spectrum in diverse bands). Large-scale cost-effective spatial densification is facilitated by self-organizing networks and intercell interference management. Full benefits of network densification can be realized only if it is complemented by backhaul densification, and advanced receivers capable of interference cancellation."}
{"_id":"4cc1b2abb3be1286389800eadbbf1490a2c66fc1","title":"Ripple-Based Control of Switching Regulators\u2014An Overview","text":"Switching regulators with ripple-based control (i.e., \u00bfripple regulators\u00bf) are conceptually simple, have fast transient responses to both line and load perturbations, and some versions operate with a switching frequency that is proportional to the load current under the discontinuous conduction mode. These characteristics make the ripple regulators well-suited, especially for power management applications in computers and portable electronic devices. Ripple regulators also have some drawbacks, including (in some versions) a poorly defined switching frequency, noise-induced jitter, inadequate dc regulation, and a tendency for fast-scale instability. This paper presents an overview of the various ripple-based control techniques, discusses their merits and limitations, and introduces techniques for reducing the noise sensitivity and the sensitivity to capacitor parameters, improving the frequency stability and the dc regulation, and avoiding fast-scale instability."}
{"_id":"192ed3a93e493c7d7b228ee1bc22d23513cffe35","title":"Maximum likelihood analysis of conflicting observations in social sensing","text":"This article addresses the challenge of truth discovery from noisy social sensing data. The work is motivated by the emergence of social sensing as a data collection paradigm of growing interest, where humans perform sensory data collection tasks. Unlike the case with well-calibrated and well-tested infrastructure sensors, humans are less reliable, and the likelihood that participants' measurements are correct is often unknown a priori. Given a set of human participants of unknown trustworthiness together with their sensory measurements, we pose the question of whether one can use this information alone to determine, in an analytically founded manner, the probability that a given measurement is true. In our previous conference paper, we offered the first maximum likelihood solution to the aforesaid truth discovery problem for corroborating observations only. In contrast, this article extends the conference paper and provides the first maximum likelihood solution to handle the cases where measurements from different participants may be conflicting. The article focuses on binary measurements. The approach is shown to outperform our previous work used for corroborating observations, the state-of-the-art fact-finding baselines, as well as simple heuristics such as majority voting."}
{"_id":"4515d542d825ecc543c270cddfac7821ec340134","title":"GENETIC ALGORITHM APPLIED TO OPTIMIZATION OF THE SHIP HULL FORM WITH RESPECT TO SEAKEEPING PERFORMANCE","text":"Hull form optimization from a hydrodynamic performance point of view is an important aspect in preliminary ship design. This study presents a computational method to estimate the ship seakeeping in regular head waves. In the optimization process, the genetic algorithm (GA) is linked to the computational method to obtain an optimum hull form by taking into account the displacement as a design constraint. New hull forms are obtained from the wellknown S60 hull and the classical Wigley hull taken as initial hulls in the optimization process at two Froude numbers (Fn=0.2 and Fn=0.3). The optimization variables are a combination of ship hull offsets and main dimensions. The objective function of the optimization procedure includes the peak values for vertical absolute motion at the centre of gravity (CG) and the bow point (0.15Lwl) behind the forward perpendicular (FP)."}
{"_id":"f3d84a0b5a4c5de47d7da9ec9514e3f9931bb6f6","title":"User experiences with web-based 3D virtual travel destination marketing portals: the need for visual indication of interactive 3D elements","text":"The tourism sector has found virtual reality technology to be a good way to market travel destinations for consumers. In this paper, we describe two user studies with three web-based 3D virtual travel destination marketing portals. These three portals were developed to support and attract wintertime tourism into the region by offering a possibility to experience in advance, a virtual snowy scenery with different activities, for example downhill skiing. In both user studies with 21 subjects the focus was on user experience with the 3D virtual travel destination marketing portals. In the second study also the visual design aspects within these portals were studied. Our studies indicate that 3D virtual travel destination marketing portals can enhance 2D web pages, if they offer the possibility to explore the location freely and through different kinds of virtual activities. Also our studies support prior findings of the efficiency of glow effect for indicating interactive 3D elements within a 3D virtual environment."}
{"_id":"adeef1a00a7dcee03a9de566e6c53ab134becdd9","title":"Frontomaxillary facial angle in trisomy 21 fetuses at 16-24 weeks of gestation.","text":"OBJECTIVES\nTo establish a normal range for the frontomaxillary facial (FMF) angle by three-dimensional (3D) ultrasound imaging and to examine the FMF angle in trisomy 21 fetuses at 16-24 weeks of gestation.\n\n\nMETHODS\nWe measured the FMF angle using 3D volumes of the fetal profile obtained with the transducer parallel to the long axis of the nose and at 45 degrees to the palate, which had been acquired from 150 normal fetuses and 23 fetuses with trisomy 21.\n\n\nRESULTS\nIn the normal group there was no significant association between the FMF angle and gestational age; the mean FMF angle was 83.9 degrees (range, 76.9-90.2 degrees ) and the 95(th) centile was 88.5 degrees . In 15 (65.2%) of the fetuses with trisomy 21 the FMF angle was greater than 88.5 degrees . Repeatability studies demonstrated that in 95% of cases the difference between two measurements of FMF angle by the same operator and different operators was less than 5 degrees .\n\n\nCONCLUSIONS\nIn the majority of second-trimester fetuses with trisomy 21 the FMF angle is increased."}
{"_id":"0bfa23178121f8fed03e6fc67608da74c8c4d9b1","title":"Multi-Product Utility Maximization for Economic Recommendation","text":"Basic economic relations such as substitutability and complementarity between products are crucial for recommendation tasks, since the utility of one product may depend on whether or not other products are purchased. For example, the utility of a camera lens could be high if the user possesses the right camera (complementarity), while the utility of another camera could be low because the user has already purchased one (substitutability). We propose \\emph{multi-product utility maximization} (MPUM) as a general approach to recommendation driven by economic principles. MPUM integrates the economic theory of consumer choice with personalized recommendation, and focuses on the utility of \\textit{sets} of products for individual users. MPUM considers what the users already have when recommending additional products. We evaluate MPUM against several popular recommendation algorithms on two real-world E-commerce datasets. Results confirm the underlying economic intuition, and show that MPUM significantly outperforms the comparison algorithms under top-K evaluation metrics."}
{"_id":"8523f37425199d72e79467104022277e0175b381","title":"Towards IP over LPWANs technologies: LoRaWAN, DASH7, NB-IoT","text":"In this paper, we discuss a set of solutions that are proposed in order to run IPv6 over IoT and we investigate its applicability over the three Low Power Wide Area Networks (LPWANs) technologies: LoRaWAN, DASH7, NB-IoT. LPWANs are wireless technologies that are used to connect things to the Internet. These technologies are characterized by their large coverage area, long battery operation, low bandwidth, small frame payload size, and the use of asymmetric and non-synchronized communication. Based on this investigation, we highlight the schemes that can be adopted for IP-based LPWANs technologies."}
{"_id":"04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac","title":"Boosting the margin: A new explanation for the effectiveness of voting methods","text":"One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik\u2019s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition."}
{"_id":"48327aaf21902c09a92b90b1122f5bf2de62f56e","title":"A Survey on Ambient-Assisted Living Tools for Older Adults","text":"In recent years, we have witnessed a rapid surge in assisted living technologies due to a rapidly aging society. The aging population, the increasing cost of formal health care, the caregiver burden, and the importance that the individuals place on living independently, all motivate development of innovative-assisted living technologies for safe and independent aging. In this survey, we will summarize the emergence of `ambient-assisted living\u201d (AAL) tools for older adults based on ambient intelligence paradigm. We will summarize the state-of-the-art AAL technologies, tools, and techniques, and we will look at current and future challenges."}
{"_id":"20faa2ef4bb4e84b1d68750cda28d0a45fb16075","title":"Clustering of time series data - a survey","text":"Time series clustering has been shown effective in providing useful information in various domains. There seems to be an increased interest in time series clustering as part of the effort in temporal data mining research. To provide an overview, this paper surveys and summarizes previous works that investigated the clustering of time series data in various application domains. The basics of time series clustering are presented, including general-purpose clustering algorithms commonly used in time series clustering studies, the criteria for evaluating the performance of the clustering results, and the measures to determine the similarity\/dissimilarity between two time series being compared, either in the forms of raw data, extracted features, or some model parameters. The past researchs are organized into three groups depending upon whether they work directly with the raw data either in the time or frequency domain, indirectly with features extracted from the raw data, or indirectly with models built from the raw data. The uniqueness and limitation of previous research are discussed and several possible topics for future research are identified. Moreover, the areas that time series clustering have been applied to are also summarized, including the sources of data used. It is hoped that this review will serve as the steppingstone for those interested in advancing this area of research. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved."}
{"_id":"0abb49fe138e8fb7332c26b148a48d0db39724fc","title":"Stochastic Pooling for Regularization of Deep Convolutional Neural Networks","text":"We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation."}
{"_id":"97862a468d375d6fbd83ed1baf2bd8d74ffefdee","title":"Wireless Sensor Network for Precise Agriculture Monitoring","text":"Precision Agriculture Monitor System (PAMS) is an intelligent system which can monitor the agricultural environments of crops and provides service to farmers. PAMS based on the wireless sensor network (WSN) technique attracts increasing attention in recent years. The purpose of such systems is to improve the outputs of crops by means of managing and monitoring the growth period. This paper presents the design of a WSN for PAMS, shares our real-world experience, and discusses the research and engineering challenges in implementation and deployments."}
{"_id":"f390476f22c5962585e7c2cc7a77de770a34b60a","title":"The Ease of Doing Business Index as a tool for Investment location decisions","text":"The Ease of Doing Business Index (EDBI) uses 41 variables to compare the business environment of different countries. It is widely used by policy makers, researchers and multinational companies. This paper aims to assess EDBI\u2019s consistency and validity in representing the business environment by using factor analysis. It is found that the EDBI presents a limited consistency and descriptive power of a country\u2019s business environment. The consequence of these findings is that multinational firms should handle carefully the EDBI in their investment decisions. GEE The Ease of Doing Business Index as a tool for Investment location decisions \u2013 Jo\u00e3o Zambujal-Oliveira, Ricardo Pinheiro-Alves"}
{"_id":"c87f54d96002e8859d73f1d1d8a2b857c9f80e29","title":"Generation of Pencil Sketch Drawing","text":"This paper proposes a pencil sketch generating system that can create various pencil sketching styles for images, to satisfy different needs of users. The pencil sketch generating process includes process of three parts: line drawing, tone adjustment, and texture rendering. Some operations needed are first proposed, and various pencil sketching styles that generated by the proposed system are illustrated."}
{"_id":"1b068f70592632081e0b4c1864c38c2b36591d20","title":"Embedded 3D printing of strain sensors within highly stretchable elastomers.","text":"A new method, embedded-3D printing (e-3DP), is reported for fabricating strain sensors within highly conformal and extensible elastomeric matrices. e-3DP allows soft sensors to be created in nearly arbitrary planar and 3D motifs in a highly programmable and seamless manner. Several embodiments are demonstrated and sensor performance is characterized."}
{"_id":"66ecbdb79ce324a4e1c6f44353e5aaee0f164ad9","title":"The attention system of the human brain.","text":"The concept of attention as central to human performance extends back to the start of experimental psychology (James 1890), yet even a few years ago, it would not have been possible to outline in even a preliminary form a functional anatomy of the human attentional system. New developments in neuroscience (Hillyard & Picton 1987, Raichle 1983, Wurtz et al 1980) have opened the study of higher cognition to physiological analysis, and have revealed a system of anatomical areas that appear to be basic to the selection of information for focal (conscious) processing. The importance of attention is its unique role in connecting the mental level of description of processes used in cognitive science with the anatomical level common i neuroscience. Sperry (1988, p. 609) describes the central role that mental concepts play in understanding brain function as follows:"}
{"_id":"114ef0ee39bfe835f7df778f36a8ad60571f5449","title":"A multiscale retinex for bridging the gap between color images and the human observation of scenes","text":"Direct observation and recorded color images of the same scenes are often strikingly different because human visual perception computes the conscious representation with vivid color and detail in shadows, and with resistance to spectral shifts in the scene illuminant. A computation for color images that approaches fidelity to scene observation must combine dynamic range compression, color consistency-a computational analog for human vision color constancy-and color and lightness tonal rendition. In this paper, we extend a previously designed single-scale center\/surround retinex to a multiscale version that achieves simultaneous dynamic range compression\/color consistency\/lightness rendition. This extension fails to produce good color rendition for a class of images that contain violations of the gray-world assumption implicit to the theoretical foundation of the retinex. Therefore, we define a method of color restoration that corrects for this deficiency at the cost of a modest dilution in color consistency. Extensive testing of the multiscale retinex with color restoration on several test scenes and over a hundred images did not reveal any pathological behaviour."}
{"_id":"1ef4aac0ebc34e76123f848c256840d89ff728d0","title":"Rapid Synthesis of Massive Face Sets for Improved Face Recognition","text":"Recent work demonstrated that computer graphics techniques can be used to improve face recognition performances by synthesizing multiple new views of faces available in existing face collections. By so doing, more images and more appearance variations are available for training, thereby improving the deep models trained on these images. Similar rendering techniques were also applied at test time to align faces in 3D and reduce appearance variations when comparing faces. These previous results, however, did not consider the computational cost of rendering: At training, rendering millions of face images can be prohibitive; at test time, rendering can quickly become a bottleneck, particularly when multiple images represent a subject. This paper builds on a number of observations which, under certain circumstances, allow rendering new 3D views of faces at a computational cost which is equivalent to simple 2D image warping. We demonstrate this by showing that the run-time of an optimized OpenGL rendering engine is slower than the simple Python implementation we designed for the same purpose. The proposed rendering is used in a face recognition pipeline and tested on the challenging IJB-A and Janus CS2 benchmarks. Our results show that our rendering is not only fast, but improves recognition accuracy."}
{"_id":"336779e60b48443bfd5f45f24191616213cbaf81","title":"The business model concept: theoretical underpinnings and empirical illustrations","text":"Received: 13 December 2001 Revised: 27 March 2002 : 26 July 2002 Accepted:15 October 2002 Abstract The business model concept is becoming increasingly popular within IS, management and strategy literature. It is used within many fields of research, including both traditional strategy theory and in the emergent body of literature on e-business. However, the concept is often used independently from theory, meaning model components and their interrelations are relatively obscure. Nonetheless, we believe that the business model concept is useful in explaining the relation between IS and strategy. This paper offers an outline for a conceptual business model, and proposes that it should include customers and competitors, the offering, activities and organisation, resources and factor market interactions. The causal inter-relations and the longitudinal processes by which business models evolve should also be included. The model criticises yet draws on traditional strategy theory and on the literature that addresses business models directly. The business model is illustrated by an ERP implementation in a European multi-national company. European Journal of Information Systems (2003) 12, 49\u201359. doi:10.1057\/ palgrave.ejis.3000446"}
{"_id":"4623accb0524d3b000866709ec27f1692cc9b15a","title":"Design science in information systems research","text":""}
{"_id":"add1d5602e7aa2bae889ef554e2da7f589117d5b","title":"Balancing customer and network value in business models for mobile services","text":"Designing business models for mobile services is complex. A business model can be seen as a blueprint of four interrelated components: service offering, technical architecture, and organisational and financial arrangements. In this paper the connections among these components are explored by analysing the critical design issues in business models for mobile services, e.g., targeting and branding in the service domain, security and quality of service in the technology domain, network governance in the organisation domain, and revenue sharing in the finance domain. A causal framework is developed linking these critical design issues to expected customer value and expected network value, and hence, to business model viability."}
{"_id":"dd5df1147ae291917515a7956a6bc3d7b845f288","title":"Evaluation in Design-Oriented Research","text":"Design has been recognized for a long time both as art and as science. In the sixties of the previous century design-oriented research began to draw the attention of scientific researchers and methodologists, not only in technical engineering but also in the social sciences. However, a rather limited methodology for design-oriented research has been developed, especially as to the social sciences. In this article we introduce evaluation methodology and research methodology as a systematic input in the process of designing. A designing cycle is formulated with six stages, and for each of these stages operations, guidelines and criteria for evaluation are defined. All this may be used for a considerable improvement of the process and product of designing."}
{"_id":"26f9114b2fdc34696c483e0f29da3b3d89482741","title":"Business Models for Electronic Markets","text":"Introduction Electronic commerce can be defined loosely as \u201cdoing business electronically\u201d (European Commission 1997). Electronic commerce includes electronic trading of physical goods and of intangibles such as information. This encompasses all the trading steps such as online marketing, ordering, payment, and support for delivery. Electronic commerce includes the electronic provision of services, such as aftersales support or online legal advice. Finally it also includes electronic support for collaboration between companies, such as collaborative design."}
{"_id":"9f100217fbca173fe0d64d4527956d272735e41c","title":"Geometric mechanics of curved crease origami.","text":"Folding a sheet of paper along a curve can lead to structures seen in decorative art and utilitarian packing boxes. Here we present a theory for the simplest such structure: an annular circular strip that is folded along a central circular curve to form a three-dimensional buckled structure driven by geometrical frustration. We quantify this shape in terms of the radius of the circle, the dihedral angle of the fold, and the mechanical properties of the sheet of paper and the fold itself. When the sheet is isometrically deformed everywhere except along the fold itself, stiff folds result in creases with constant curvature and oscillatory torsion. However, relatively softer folds inherit the broken symmetry of the buckled shape with oscillatory curvature and torsion. Our asymptotic analysis of the isometrically deformed state is corroborated by numerical simulations that allow us to generalize our analysis to study structures with multiple curved creases."}
{"_id":"c979efe1f0a8b0b343ea332368e5b51dc153c522","title":"Policy Optimization with Demonstrations","text":"Exploration remains a significant challenge to reinforcement learning methods, especially in environments where reward signals are sparse. Recent methods of learning from demonstrations have shown to be promising in overcoming exploration difficulties but typically require considerable highquality demonstrations that are difficult to collect. We propose to effectively leverage available demonstrations to guide exploration through enforcing occupancy measure matching between the learned policy and current demonstrations, and develop a novel Policy Optimization from Demonstration (POfD) method. We show that POfD induces implicit dynamic reward shaping and brings provable benefits for policy improvement. Furthermore, it can be combined with policy gradient methods to produce state-of-the-art results, as demonstrated experimentally on a range of popular benchmark sparse-reward tasks, even when the demonstrations are few and imperfect."}
{"_id":"17ee773e6b2baecba638e83ffbcbb4f103231236","title":"Are You Asking the Right Questions? Teaching Machines to Ask Clarification Questions","text":"Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this thesis work, we explore how can we teach machines to ask clarification questions when faced with uncertainty, a goal of increasing importance in today\u2019s automated society. We do a preliminary study using data from StackExchange, a plentiful online resource where people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We build neural network models inspired by the idea of the expected value of perfect information: a good question is one whose expected answer is going to be most useful. To build generalizable systems, we propose two future research directions: a template-based model and a sequence-to-sequence based neural generative model."}
{"_id":"b305233e5b300d7fc77fb9595c2c3ff7a6a9b91a","title":"Sentiment Adaptive End-to-End Dialog Systems","text":"v This work focuses on incorporating sentiment information into task-oriented dialogue systems. v Current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. v But the ultimate evaluator of dialog systems is the end-users and their sentiment is a direct reflection of user satisfaction and should be taken into consideration. v Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. v We incorporated user sentiment information in both supervised and reinforcement learning settings. v In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. Multimodal Sentiment Detector v We manually annotated 50 dialogs with 517 conversation turns to train this sentiment detector. The annotated set is open to public. v Prediction made by the detector will be used in the supervised learning and reinforcement learning. v Three sets of features: 1) Acoustic features; 2) Dialogic features; 3) Textual features. v Dialogic features include: 1) Interruption; 2) Button usage; 3) Repetitions; 4) Start over. These four categories of dialog features are chosen based on the previous literature and the observed statistics in the dataset."}
{"_id":"e3adb12fbd126ce9abbbec86c6ac1642e61ded34","title":"Social Media for Opioid Addiction Epidemiology: Automatic Detection of Opioid Addicts from Twitter and Case Studies","text":"Opioid (e.g., heroin and morphine) addiction has become one of the largest and deadliest epidemics in the United States. To combat such deadly epidemic, there is an urgent need for novel tools and methodologies to gain new insights into the behavioral processes of opioid abuse and addiction. The role of social media in biomedical knowledge mining has turned into increasingly significant in recent years. In this paper, we propose a novel framework named AutoDOA to automatically detect the opioid addicts from Twitter, which can potentially assist in sharpening our understanding toward the behavioral process of opioid abuse and addiction. In AutoDOA, to model the users and posted tweets as well as their rich relationships, a structured heterogeneous information network (HIN) is first constructed. Then meta-path based approach is used to formulate similarity measures over users and different similarities are aggregated using Laplacian scores. Based on HIN and the combined meta-path, to reduce the cost of acquiring labeled examples for supervised learning, a transductive classification model is built for automatic opioid addict detection. To the best of our knowledge, this is the first work to apply transductive classification in HIN into drug-addiction domain. Comprehensive experiments on real sample collections from Twitter are conducted to validate the effectiveness of our developed system AutoDOA in opioid addict detection by comparisons with other alternate methods. The results and case studies also demonstrate that knowledge from daily-life social media data mining could support a better practice of opioid addiction prevention and treatment."}
{"_id":"5c6f5ed9a3d7b1754d26157844511a8f8a8e76f3","title":"Integrated Wideband Self-Interference Cancellation in the RF Domain for FDD and Full-Duplex Wireless","text":"A fully integrated technique for wideband cancellation of transmitter (TX) self-interference (SI) in the RF domain is proposed for multiband frequency-division duplexing (FDD) and full-duplex (FD) wireless applications. Integrated wideband SI cancellation (SIC) in the RF domain is accomplished through: 1) a bank of tunable, reconfigurable second-order high-Q RF bandpass filters in the canceller that emulate the antenna interface's isolation (essentially frequency-domain equalization in the RF domain) and 2) a linear N-path Gm-C filter implementation with embedded variable attenuation and phase shifting. A 0.8-1.4 GHz receiver (RX) with the proposed wideband SIC circuits is implemented in a 65 nm CMOS process. In measurement, >20 MHz 20 dB cancellation bandwidth (BW) is achieved across frequency-selective antenna interfaces: 1) a custom-designed LTElike 0.780\/0.895 GHz duplexer with TX\/RX isolation peak magnitude of 30 dB, peak group delay of 11 ns, and 7 dB magnitude variation across the TX band for FDD and 2) a 1.4 GHz antenna pair for FD wireless with TX\/RX isolation peak magnitude of 32 dB, peak group delay of 9 ns, and 3 dB magnitude variation over 1.36-1.38 GHz. For FDD, SIC enhances the effective outof-band (OOB) IIP3 and IIP2 to +25-27 dBm and +90 dBm, respectively (enhancements of 8-10 and 29 dB, respectively). For FD, SIC eliminates RX gain compression for as high as -8 dBm of peak in-band (IB) SI, and enhances effective IB IIP3 and IIP2 by 22 and 58 dB."}
{"_id":"2dc1775eefe3dc9f43ae25d3c7335378615fa33d","title":"The analyse of the various methods for location of Data Matrix codes in images","text":"Data Matrix codes belong to the group of 2-D bar codes, which are widely used in marking the products in storing, production, distribution and sales processes. Their compact matrix structure enables to store big amount of information in a very small area compared to conventional 1-D bar codes. In following paper we compare several methods for detection of Data Matrix codes in images. When locating the position of the code we start from typical bordering of Data Matrix code \u2014 which forms \u201cL\u201d so called Finder Pattern and to it the parallel dotting so called Timing Pattern. On the first stage we try to locate Finder Pattern using the edge detection or adaptive thresholding, then continuing with connecting the points into the continuous regions, we are searching for the regions, which could represent the Finder Pattern. On the second stage we verify Timing Pattern where the number of crossing between the background and foreground must be even."}
{"_id":"24bbccc21bd4826365460939a5ca295f690862c5","title":"Quorum responses and consensus decision making.","text":"Animal groups are said to make consensus decisions when group members come to agree on the same option. Consensus decisions are taxonomically widespread and potentially offer three key benefits: maintenance of group cohesion, enhancement of decision accuracy compared with lone individuals and improvement in decision speed. In the absence of centralized control, arriving at a consensus depends on local interactions in which each individual's likelihood of choosing an option increases with the number of others already committed to that option. The resulting positive feedback can effectively direct most or all group members to the best available choice. In this paper, we examine the functional form of the individual response to others' behaviour that lies at the heart of this process. We review recent theoretical and empirical work on consensus decisions, and we develop a simple mathematical model to show the central importance to speedy and accurate decisions of quorum responses, in which an animal's probability of exhibiting a behaviour is a sharply nonlinear function of the number of other individuals already performing this behaviour. We argue that systems relying on such quorum rules can achieve cohesive choice of the best option while also permitting adaptive tuning of the trade-off between decision speed and accuracy."}
{"_id":"d663e0f387bc9dc826b2a8594f7967c3d6a0e804","title":"Single-unit pattern generators for quadruped locomotion","text":"Legged robots can potentially venture beyond the limits of wheeled vehicles. While creating controllers for such robots by hand is possible, evolutionary algorithms are an alternative that can reduce the burden of hand-crafting robotic controllers. Although major evolutionary approaches to legged locomotion can generate oscillations through popular techniques such as continuous time recurrent neural networks (CTRNNs) or sinusoidal input, they typically face a challenge in maintaining long-term stability. The aim of this paper is to address this challenge by introducing an effective alternative based on a new type of neuron called a single-unit pattern generator (SUPG). The SUPG, which is indirectly encoded by a compositional pattern producing network (CPPN) evolved by HyperNEAT, produces a flexible temporal activation pattern that can be reset and repeated at any time through an explicit trigger input, thereby allowing it to dynamically recalibrate over time to maintain stability. The SUPG approach, which is compared to CTRNNs and sinusoidal input, is shown to produce natural-looking gaits that exhibit superior stability over time, thereby providing a new alternative for evolving oscillatory locomotion."}
{"_id":"cb602795e6bdcf0938761ca08e77df4e9842ba9f","title":"A modular control scheme for PMSM speed control with pulsating torque minimization","text":"In this paper, a modular control approach is applied to a permanent-magnet synchronous motor (PMSM) speed control. Based on the functioning of the individual module, the modular approach enables the powerfully intelligent and robust control modules to easily replace any existing module which does not perform well, meanwhile retaining other existing modules which are still effective. Property analysis is first conducted for the existing function modules in a conventional PMSM control system: proportional-integral (PI) speed control module, reference current-generating module, and PI current control module. Next, it is shown that the conventional PMSM controller is not able to reject the torque pulsation which is the main hurdle when PMSM is used as a high-performance servo. By virtue of the internal model, to nullify the torque pulsation it is imperative to incorporate an internal model in the feed-through path. This is achieved by replacing the reference current-generating module with an iterative learning control (ILC) module. The ILC module records the cyclic torque and reference current signals over one entire cycle, and then uses those signals to update the reference current for the next cycle. As a consequence, the torque pulsation can be reduced significantly. In order to estimate the torque ripples which may exceed certain bandwidth of a torque transducer, a novel torque estimation module using a gain-shaped sliding-mode observer is further developed to facilitate the implementation of torque learning control. The proposed control system is evaluated through real-time implementation and experimental results validate the effectiveness."}
{"_id":"f2d9f9beb61f61edc50257292c63dc9e7c6339c4","title":"LIP ACTIVITY DETECTION FOR TALKING FACES CLASSIFICATION IN TV-CONTENT","text":"Our objective is to index people in a TV-Content. In this context, because of multi-face shots and non-speaking face shots, it is difficult to determine which face is speaking. There is no guaranteed synchronization between sequences of a person\u2019s appearance and sequences of his or her speech. In this work, we want to separate talking and non-talking faces by detecting lip motion. We propose a method to detect the lip motion by measuring the degree of disorder of pixel directions around the lip. Results of experiments on a TV-Show database show that a high correct classification rate can be achieved by the proposed method."}
{"_id":"f9791399e87bba3f911fd8f570443cf721cf7b1e","title":"Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network","text":"Capturing the compositional process which maps the meaning of words to that of documents is a central challenge for researchers in Natural Language Processing and Information Retrieval. We introduce a model that is able to represent the meaning of documents by embedding them in a low dimensional vector space, while preserving distinctions of word and sentence order crucial for capturing nuanced semantics. Our model is based on an extended Dynamic Convolution Neural Network, which learns convolution filters at both the sentence and document level, hierarchically learning to capture and compose low level lexical features into high level semantic concepts. We demonstrate the effectiveness of this model on a range of document modelling tasks, achieving strong results with no feature engineering and with a more compact model. Inspired by recent advances in visualising deep convolution networks for computer vision, we present a novel visualisation technique for our document networks which not only provides insight into their learning process, but also can be interpreted to produce a compelling automatic summarisation system for texts."}
{"_id":"6f2cdce2eb8e6afdfd9e81316ff08f80e972cc47","title":"The Computable News project: Research in the Newsroom","text":"We report on a four year academic research project to build a natural language processing platform in support of a large media company. The Computable News platform processes news stories, producing a layer of structured data that can be used to build rich applications. We describe the underlying platform and the research tasks that we explored building it. The platform supports a wide range of prototype applications designed to support different newsroom functions. We hope that this qualitative review provides some insight into the challenges involved in this type of project."}
{"_id":"1cc7013247056e45264de9817171d72690181692","title":"A language modeling framework for resource selection and results merging","text":"Statistical language models have been proposed recently for several information retrieval tasks, including the resource selection task in distributed information retrieval. This paper extends the language modeling approach to integrate resource selection, ad-hoc searching, and merging of results from different text databases into a single probabilistic retrieval model. This new approach is designed primarily for Intranet environments, where it is reasonable to assume that resource providers are relatively homogeneous and can adopt the same kind of search engine. Experiments demonstrate that this new, integrated approach is at least as effective as the prior state-of-the-art in distributed IR."}
{"_id":"27d205314a4e416a685d153a8c7bd65966f9f7d1","title":"Frontal fibrosing alopecia: a clinical review of 36 patients.","text":"BACKGROUND\nFrontal fibrosing alopecia (FFA) is a primary lymphocytic cicatricial alopecia with a distinctive clinical pattern of progressive frontotemporal hairline recession. Currently, there are no evidence-based studies to guide treatment for patients with FFA; thus, treatment options vary among clinicians.\n\n\nOBJECTIVES\nWe report clinical findings and treatment outcomes of 36 patients with FFA, the largest cohort to date. Further, we report the first evidence-based study of the efficacy of hydroxychloroquine in FFA using a quantitative clinical score, the Lichen Planopilaris Activity Index (LPPAI).\n\n\nMETHODS\nA retrospective case note review was performed of 36 adult patients with FFA. Data were collected on demographics and clinical findings. Treatment responses to hydroxychloroquine, doxycycline and mycophenolate mofetil were assessed using the LPPAI. Adverse events were monitored.\n\n\nRESULTS\nMost patients in our cohort were female (97%), white (92%) and postmenopausal (83%). Apart from hairline recession, 75% also reported eyebrow loss. Scalp pruritus (67%) and perifollicular erythema (86%) were the most common presenting symptom and sign, respectively. A statistically significant reduction in signs and symptoms in subjects treated with hydroxychloroquine (P < 0\u00b705) was found at both 6- and 12-month follow up.\n\n\nCONCLUSIONS\nIn FFA, hairline recession, scalp pruritus, perifollicular erythema and eyebrow loss are common at presentation. Despite the limitations of a retrospective review, our data reveal that hydroxychloroquine is significantly effective in reducing signs and symptoms of FFA after both 6 and 12 months of treatment. However, the lack of a significant reduction in signs and symptoms between 6 and 12 months indicates that the maximal benefits of hydroxychloroquine are evident within the first 6 months of use."}
{"_id":"94ee91bc1ec67bb2fe8a1d5b48713f19db98be54","title":"Incorporating Entity Correlation Knowledge into Topic Modeling","text":"Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for exploring hidden topics in text corpora. Standard LDA model suffers the problem that the topic assignment of each word is independent and lacks the mechanism to utilize the rich prior background knowledge to learn semantically coherent topics. To address this problem, in this paper, we propose a model called Entity Correlation Latent Dirichlet Allocation (EC-LDA) by incorporating constraints derived from entity correlations as the prior knowledge into LDA topic model. Different from other knowledge-based topic models which extract the knowledge information directly from the train dataset itself or even from the human judgements, for our work, we take advantage of the prior knowledge from the external knowledge base (Freebase 1, in our experiment). Hence, our approach is more suitable to widely kinds of text corpora in different scenarios. We fit our proposed model using Gibbs sampling. Experiment results demonstrate the effectiveness of our model compared with standard LDA."}
{"_id":"8b024d8b0b62593d44306613860a4bedc857a021","title":"Learning by Googling","text":"The goal of giving a well-defined meaning to information is currently shared by endeavors such as the Semantic Web as well as by current trends within Knowledge Management. They all depend on the large-scale formalization of knowledge and on the availability of formal metadata about information resources. However, the question how to provide the necessary formal metadata in an effective and efficient way is still not solved to a satisfactory extent. Certainly, the most effective way to provide such metadata as well as formalized knowledge is to let humans encode them directly into the system, but this is neither efficient nor feasible. Furthermore, as current social studies show, individual knowledge is often less powerful than the collective knowledge of a certain community.As a potential way out of the knowledge acquisition bottleneck, we present a novel methodology that acquires collective knowledge from the World Wide Web using the GoogleTM API. In particular, we present PANKOW, a concrete instantiation of this methodology which is evaluated in two experiments: one with the aim of classifying novel instances with regard to an existing ontology and one with the aim of learning sub-\/superconcept relations."}
{"_id":"27e13389203b2f8f6138afed867965a3a38cbd8e","title":"EEG-GAN: Generative adversarial networks for electroencephalograhic (EEG) brain signals","text":"Generative adversarial networks (GANs) are recently highly successful in generative applications involving images and start being applied to time series data. Here we describe EEG-GAN as a framework to generate electroencephalographic (EEG) brain signals. We introduce a modification to the improved training of Wasserstein GANs to stabilize training and investigate a range of architectural choices critical for time series generation (most notably upand down-sampling). For evaluation we consider and compare different metrics such as Inception score, Frechet inception distance and sliced Wasserstein distance, together showing that our EEG-GAN framework generated naturalistic EEG examples. It thus opens up a range of new generative application scenarios in the neuroscientific and neurological context, such as data augmentation in brain-computer interfacing tasks, EEG super-sampling, or restoration of corrupted data segments. The possibility to generate signals of a certain class and\/or with specific properties may also open a new avenue for research into the underlying structure of brain signals."}
{"_id":"1af5293e7e270d35be5eca28cd904d1e8fc9219c","title":"CEDD: Color and Edge Directivity Descriptor: A Compact Descriptor for Image Indexing and Retrieval","text":"This paper deals with a new low level feature that is extracted from the images and can be used for indexing and retrieval. This feature is called \u201cColor and Edge Directivity Descriptor\u201d and incorporates color and texture information in a histogram. CEDD size is limited to 54 bytes per image, rendering this descriptor suitable for use in large image databases. One of the most important attribute of the CEDD is the low computational power needed for its extraction, in comparison with the needs of the most MPEG-7 descriptors. The objective measure called ANMRR is used to evaluate the performance of the proposed feature. An online demo that implements the proposed feature in an image retrieval system is available at: http:\/\/orpheus.ee.duth.gr\/image_retrieval."}
{"_id":"597edc3174dc9f26badd67c4e81d0e8a58f9dbb3","title":"Textural Features Corresponding to Visual Perception","text":"Textural features corresponding to human visual perception are very useful for optimum feature selection and texture analyzer design. We approximated in computational form six basic textural features, namely, coarseness, contrast, directionality, line-likeness, regularity, and roughness. In comparison with psychological measurements for human subjects, the computational measures gave good correspondences in rank correlation of 16 typical texture patterns. Similarity measurements using these features were attempted. The discrepancies between human vision and computerized techniques that we encountered in this study indicate fundamental problems in digital analysis of textures. Some of them could be overcome by analyzing their causes and using more sophisticated techniques."}
{"_id":"6646c3e932c69c6c2f317d615c55ccc22e21b4be","title":"FCTH: Fuzzy Color and Texture Histogram - A Low Level Feature for Accurate Image Retrieval","text":"This paper deals with the extraction of a new low level feature that combines, in one histogram, color and texture information. This feature is named FCTH - Fuzzy Color and Texture Histogram - and results from the combination of 3 fuzzy systems. FCTH size is limited to 72 bytes per image, rendering this descriptor suitable for use in large image databases. The proposed feature is appropriate for accurately retrieving images even in distortion cases such as deformations, noise and smoothing. It is tested on a large number of images selected from proprietary image databases or randomly retrieved from popular search engines. To evaluate the performance of the proposed feature, the averaged normalized modified retrieval rank was used. An online demo that implements the proposed feature in an image retrieval system is available at: http:\/\/orpheus.ee.duth.gr\/image_retrieval."}
{"_id":"1b8fb3367b2527b53eda74c7966db809172eed28","title":"M-tree: An Efficient Access Method for Similarity Search in Metric Spaces","text":"A new access method called M tree is pro posed to organize and search large data sets from a generic metric space i e where ob ject proximity is only de ned by a distance function satisfying the positivity symmetry and triangle inequality postulates We detail algorithms for insertion of objects and split management which keep the M tree always balanced several heuristic split alternatives are considered and experimentally evaluated Algorithms for similarity range and k nearest neighbors queries are also described Re sults from extensive experimentation with a prototype system are reported considering as the performance criteria the number of page I O s and the number of distance computa tions The results demonstrate that the M tree indeed extends the domain of applica bility beyond the traditional vector spaces performs reasonably well in high dimensional data spaces and scales well in case of growing les"}
{"_id":"562283bfb84f6b7a0b881a9dcf5b713e1c1f57bf","title":"Normalized cuts in 3-D for spinal MRI segmentation","text":"Segmentation of medical images has become an indispensable process to perform quantitative analysis of images of human organs and their functions. Normalized Cuts (NCut) is a spectral graph theoretic method that readily admits combinations of different features for image segmentation. The computational demand imposed by NCut has been successfully alleviated with the Nystro\/spl uml\/m approximation method for applications different than medical imaging. In this paper we discuss the application of NCut with the Nystro\/spl uml\/m approximation method to segment vertebral bodies from sagittal T1-weighted magnetic resonance images of the spine. The magnetic resonance images were preprocessed by the anisotropic diffusion algorithm, and three-dimensional local histograms of brightness was chosen as the segmentation feature. Results of the segmentation as well as limitations and challenges in this area are presented."}
{"_id":"9970938755296a73e48b362f1c29c70690e769b7","title":"Mining Efficient Taxi Operation Strategies From Large Scale Geo-Location Data","text":"Taxi drivers always look for strategies to locate passengers quickly and therefore increase their profit margin. In reality, the passenger seeking strategies are mostly empirical and substantially vary among taxi drivers. From the history taxi data, the top performing taxi drivers can earn 25% more than the ones with mediocre seeking strategy in the same period of time. A better strategy not only helps taxi drivers earn more with less effort, but also reduce fuel consumption and carbon emissions. It is interesting to examine the influential factors in passenger seeking strategies and find algorithms to guide taxi drivers to passenger hotspots with the right timing. With the abundant availability of history taxicab traces, the existing methods of doing taxi business have been radically changed. This paper focuses on the problem of mining efficient operation strategies from a large-scale history taxi traces collected over one year. Our approach presents generic insights into the dynamics of taxicab services with the objective of maximizing the profit margins for the concerned parties. We propose important metrics, such as trip frequency, hot spots, and taxi mileage, and provide valuable insights toward more efficient operation strategies. We analyze these metrics using techniques, such as Newton\u2019s polynomial interpolation and Gamma distribution, to understand their dynamics. Our strategies use the real taxicab traces from the city of Changsha (P.R.China), may predict the taxi rides at different times by 90.68% per day, and increase the taxi drivers income levels up to 19.38% by controlling appropriate mileage per trip and following the route across more urban hot spots."}
{"_id":"ea5e5d3cbd9898a174f8b8e7f764340594533e0b","title":"Tracking Depression Dynamics in College Students Using Mobile Phone and Wearable Sensing","text":"There are rising rates of depression on college campuses. Mental health services on our campuses are working at full stretch. In response researchers have proposed using mobile sensing for continuous mental health assessment. Existing work on understanding the relationship between mobile sensing and depression, however, focuses on generic behavioral features that do not map to major depressive disorder symptoms defined in the standard mental disorders diagnostic manual (DSM-5). We propose a new approach to predicting depression using passive sensing data from students' smartphones and wearables. We propose a set of symptom features that proxy the DSM-5 defined depression symptoms specifically designed for college students. We present results from a study of 83 undergraduate students at Dartmouth College across two 9-week terms during the winter and spring terms in 2016. We identify a number of important new associations between symptom features and student self reported PHQ-8 and PHQ-4 depression scores. The study captures depression dynamics of the students at the beginning and end of term using a pre-post PHQ-8 and week by week changes using a weekly administered PHQ-4. Importantly, we show that symptom features derived from phone and wearable sensors can predict whether or not a student is depressed on a week by week basis with 81.5% recall and 69.1% precision."}
{"_id":"020faf613a105bbdd2ebd32ff78fe018c1e7f912","title":"Real-time super-resolution Sound Source Localization for robots","text":"Sound Source Localization (SSL) is an essential function for robot audition and yields the location and number of sound sources, which are utilized for post-processes such as sound source separation. SSL for a robot in a real environment mainly requires noise-robustness, high resolution and real-time processing. A technique using microphone array processing, that is, Multiple Signal Classification based on Standard EigenValue Decomposition (SEVD-MUSIC) is commonly used for localization. We improved its robustness against noise with high power by incorporating Generalized EigenValue Decomposition (GEVD). However, GEVD-based MUSIC (GEVD-MUSIC) has mainly two issues: 1) the resolution of pre-measured Transfer Functions (TFs) determines the resolution of SSL, 2) its computational cost is expensive for real-time processing. For the first issue, we propose a TF interpolation method integrating time-domain-based and frequency-domain-based interpolation. The interpolation achieves super-resolution SSL, whose resolution is higher than that of the pre-measured TFs. For the second issue, we propose two methods, MUSIC based on Generalized Singular Value Decomposition (GSVD-MUSIC), and Hierarchical SSL (H-SSL). GSVD-MUSIC drastically reduces the computational cost while maintaining noise-robustness in localization. H-SSL also reduces the computational cost by introducing a hierarchical search algorithm instead of using greedy search in localization. These techniques are integrated into an SSL system using a robot embedded microphone array. The experimental result showed: the proposed interpolation achieved approximately 1 degree resolution although we have only TFs at 30 degree intervals, GSVD-MUSIC attained 46.4% and 40.6% of the computational cost compared to SEVD-MUSIC and GEVD-MUSIC, respectively, H-SSL reduces 59.2% computational cost in localization of a single sound source."}
{"_id":"dd008f0ed8b1be16850036fd19809674889a1c5f","title":"Medical image segmentation on GPUs - A comprehensive review","text":"Segmentation of anatomical structures, from modalities like computed tomography (CT), magnetic resonance imaging (MRI) and ultrasound, is a key enabling technology for medical applications such as diagnostics, planning and guidance. More efficient implementations are necessary, as most segmentation methods are computationally expensive, and the amount of medical imaging data is growing. The increased programmability of graphic processing units (GPUs) in recent years have enabled their use in several areas. GPUs can solve large data parallel problems at a higher speed than the traditional CPU, while being more affordable and energy efficient than distributed systems. Furthermore, using a GPU enables concurrent visualization and interactive segmentation, where the user can help the algorithm to achieve a satisfactory result. This review investigates the use of GPUs to accelerate medical image segmentation methods. A set of criteria for efficient use of GPUs are defined and each segmentation method is rated accordingly. In addition, references to relevant GPU implementations and insight into GPU optimization are provided and discussed. The review concludes that most segmentation methods may benefit from GPU processing due to the methods' data parallel structure and high thread count. However, factors such as synchronization, branch divergence and memory usage can limit the speedup."}
{"_id":"771b9855ef09cf6106700a7cd2c325e3459db429","title":"A compact printed monopole antenna with symmetrical i and rectangular shaped slots for bluetooth\/WLAN\/WIMAX applications","text":"A compact dual band microstrip fed printed monopole antenna is designed and presented in this paper for Bluetooth\/WLAN\/WiMAX applications. Two I with rectangular shape slot have been etched on a Roger RT\/duroid substrate to form desired monopole antenna. The radiator of the antenna is very compact with an area of only 9.1 \u00d7 10.2 mm2. The size of the I & rectangular slots of dual radiating elements were adjusted so as to achieve two different resonant modes for dual band operation. The simulation result shows that the proposed antenna has 10-dB impedance bandwidth of 219.6 MHz and 3.20 GHz which cover the Bluetooth, WLAN 2.4\/5.8 GHz and the WiMAX 2.5\/3.5 GHz bands. The overall proposed antenna size is 15 \u00d7 17 \u00d7 0.762 mm 3 with the obtained VSWR less than 1.5."}
{"_id":"062397dbc063312de6b006b49acfa76e3ee27e59","title":"Phase statistics of interferograms with applications to synthetic aperture radar.","text":"Interferometric methods are well established in optics and radio astronomy. In recent years, interferometric concepts have been applied successfully to synthetic aperture radar (SAR) and have opened up new possibilities in the area of earth remote sensing. However interferometric SAR applications require thorough phase control through the imaging process. The phase accuracy of SAR images is affected by decorrelation effects between the individual surveys. We analyze quantitatively the influence of decorrelation on the phase statistics of SAR interferograms. In particular, phase aberrations as they occur in typical SAR processors are studied in detail. The dependence of the resulting phase bias and variance on processor parameters is presented in several diagrams."}
{"_id":"6a47e89f4afb3c550a5f240417590d3dce6cc6be","title":"Structured Indoor Modeling","text":"This paper presents a novel 3D modeling framework that reconstructs an indoor scene as a structured model from panorama RGBD images. A scene geometry is represented as a graph, where nodes correspond to structural elements such as rooms, walls, and objects. The approach devises a structure grammar that defines how a scene graph can be manipulated. The grammar then drives a principled new reconstruction algorithm, where the grammar rules are sequentially applied to recover a structured model. The paper also proposes a new room segmentation algorithm and an offset-map reconstruction algorithm that are used in the framework and can enforce architectural shape priors far beyond existing state-of-the-art. The structured scene representation enables a variety of novel applications, ranging from indoor scene visualization, automated floorplan generation, Inverse-CAD, and more. We have tested our framework and algorithms on six synthetic and five real datasets with qualitative and quantitative evaluations. The source code and the data are available at the project website [15]."}
{"_id":"8f9376f3b71e9182c79531551d6e953cd02d7fe6","title":"A piezoelectric vibration based generator for wireless electronics","text":"Enabling technologies for wireless sensor networks have gained considerable attention in research communities over the past few years. It is highly desirable, even necessary in certain situations, for wireless sensor nodes to be self-powered. With this goal in mind, a vibration based piezoelectric generator has been developed as an enabling technology for wireless sensor networks. The focus of this paper is to discuss the modeling, design, and optimization of a piezoelectric generator based on a two-layer bending element. An analytical model of the generator has been developed and validated. In addition to providing intuitive design insight, the model has been used as the basis for design optimization. Designs of 1 cm3 in size generated using the model have demonstrated a power output of 375 \u03bcW from a vibration source of 2.5 m s\u22122 at 120 Hz. Furthermore, a 1 cm3 generator has been used to power a custom designed 1.9 GHz radio transmitter from the same vibration source. (Some figures in this article are in colour only in the electronic version)"}
{"_id":"fdbf3fefa202391059a3802f0f288a8c7487fc76","title":"Weakly supervised user intent detection for multi-domain dialogues","text":"Users interact with mobile apps with certain intents such as finding a restaurant. Some intents and their corresponding activities are complex and may involve multiple apps; for example, a restaurant app, a messenger app and a calendar app may be needed to plan a dinner with friends. However, activities may be quite personal and third-party developers would not be building apps to specifically handle complex intents (e.g., a DinnerPlanner). Instead we want our intelligent agent to actively learn to understand these intents and provide assistance when needed. This paper proposes a framework to enable the agent to learn an inventory of intents from a small set of task-oriented user utterances. The experiments show that on previously unseen user activities, the agent is able to reliably recognize user intents using graph-based semi-supervised learning methods. The dataset, models, and the system outputs are available to research community."}
{"_id":"fede9cccd96292d3ede3a1111a87d0fce7d147d5","title":"Does telecommuting really increase productivity?","text":"As many companies have learned in the last decade, the reality of telecommuting does not reflect the hype, the expected potential, or the existing literature."}
{"_id":"5ab1d4d7aa2111e1d6254cfd81f35d4cac8d7fb8","title":"Why social preferences matter-the impact of non-selfish motives on competition , cooperation and incentives","text":"A substantial number of people exhibit social preferences, which means they are not solely motivated by material self-interest but also care positively or negatively for the material payoffs of relevant reference agents. We show empirically that economists fail to understand fundamental economic questions when they disregard social preferences, in particular, that without taking social preferences into account, it is not possible to understand adequately (i) the effects of competition on market outcomes, (ii) laws governing cooperation and collective action, (iii) effects and the determinants of material incentives, (iv) which contracts and property rights arrangements are optimal, and (v) important forces shaping social norms and market failures. a) Ernst Fehr, Institute for Empirical Research in Economics, University of Zurich, Bluemlisalpstrasse 10, CH-8006 Zurich, Switzerland, email: efehr@iew.unizh.ch. b) Urs Fischbacher, Institute for Empirical Research in Economics, University of Zurich, Bluemlisalpstrasse 10, CH-8006 Zurich, Switzerland, email: fiba@iew.unizh.ch."}
{"_id":"4b129c53e93753ec8855e5e4d7b4d1a4d586d3b1","title":"CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases","text":"Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domainindependent framework, called COTYPE, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. COTYPE, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object \"translation\" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of COTYPE across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method."}
{"_id":"c4e9e63883657895be0a3b2f78d058752bf23f41","title":"Pruning ConvNets Online for Efficient Specialist Models","text":"Convolutional neural networks (CNNs) excel in various computer vision related tasks but are extremely computationally intensive and power hungry to run on mobile and embedded devices. Recent pruning techniques can reduce the computation and memory requirements of CNNs, but a costly retraining step is needed to restore the classification accuracy of the pruned model. In this paper, we present evidence that when only a subset of the classes need to be classified, we could prune a model and achieve reasonable classification accuracy without retraining. The resulting specialist model will require less energy and time to run than the original full model. To compensate for the pruning, we take advantage of the redundancy among filters and class-specific features. We show that even simple methods such as replacing channels with mean or with the most correlated channel can boost the accuracy of the pruned model to reasonable levels."}
{"_id":"678a62aeeb8746906d5621c5b3502a25b2c4a2bf","title":"First- and second-order optimality conditions for piecewise smooth objective functions","text":"Any piecewise smooth function that is specified by an evaluation procedure involving smooth elemental functions and piecewise linear functions like min and max can be represented in the so-called abs-normal form. By an extension of algorithmic, or automatic, differentiation, one can then compute certain first and second order derivative vectors and matrices that represent a local piecewise linearization and provide additional curvature information. On the basis of these quantities we characterize local optimality by first and second order necessary and sufficient conditions, which generalize the corresponding KKT theory for smooth problems. The key assumption is the Linear Independence Kink Qualification (LIKQ), a generalization of LICQ familiar from nonlinear optimization. It implies that the objective has locally a so-called VU decomposition and renders everything tractable in terms of matrix factorizations and other simple linear algebra operations. By yielding descent directions whenever they are violated the new optimality conditions point the way to a superlinearly convergent generalized QP solver, which is currently under development. We exemplify the theory on two nonsmooth examples of Nesterov."}
{"_id":"7204441181b74ece91f442291c74fc76e7542402","title":"Precision SAR processing using chirp scaling","text":"Abstmct-A space-variant interpolation is required to compensate for the migration of signal energy through range resolution cells when processing synthetic aperture radar (SAR) data, using either the classical rangelDoppler (RID) algorithm or related frequency domain techniques. In general, interpolation requires significant computation time, and leads to loss of image quality, especially in the complex image. The new chirp scaling algorithm avoids interpolation, yet performs range cell migration correction accurately. The algorithm requires only complex multiplies and Fourier transforms to implement, is inherently phase preserving, and is suitable for wide-swath, largebeamwidth, and large-squint applications. This paper describes the chirp scaling algorithm, summarizes simulation results, presents imagery processed with the algorithm, and reviews quantitative measures of its performance. Based on quantitative comparison, the chirp scaling algorithm provides image quality equal to or better than the precision rangel Doppler processor. Over the range of parameters tested, image quality results approach the theoretical limit, as defined by the system bandwidth."}
{"_id":"6e930b5c4913f4c51dac9e88d860fb6fb7757e45","title":"The differences in motivations of online game players and offline game players: A combined analysis of three studies at higher education level","text":"Computer games have become a highly popular form of entertainment and have had a large impact on how University students spend their leisure time. Due to their highly motivating properties computer games have come to the attention of educationalists who wish to exploit these highly desirable properties for educational purposes. Several studies have been performed looking at motivations for playing computer games in a general context and in a Higher Education (HE) context. These studies did not focus on the differences in motivations between online and offline game players. Equally the studies did not look at the differences in motivations of people who prefer single player games and people who prefer multiplayer games. If games-based learning is to become a recognised teaching approach then such motivations for playing computer games must be better understood. This paper presents the combined analysis of three studies at HE level, performed over a four year period from 2005 to 2009. The paper focuses on differences of motivations in relation to single player\/multiplayer preference and online\/ offline game participation. The study found that challenge is the top ranking motivation and recognition is the lowest ranking motivation for playing computer games in general. Challenge is also the top ranking motivation for playing games in HE while fantasy and recognition are the lowest ranking motivations for playing games in HE. Multiplayer gamers derive more competition, cooperation, recognition, fantasy and curiosity from playing games and online gamers derive more challenge, cooperation, recognition and control from playing games. Multiplayer gamers and online gamers ranked competition, cooperation and recognition significantly more important for playing games in HE than single players and offline participants. 2011 Elsevier Ltd. All rights reserved."}
{"_id":"5de3ba76eeead6a6ee3295220080ee881f84bd27","title":"Homography-based 2D Visual Tracking and Servoing","text":"The objective of this paper is to propose a new homography-based approach to image-based visual tracking and servoing. The visual tracking algorithm proposed in the paper is based on a new efficient second-order minimization method. Theoretical analysis and comparative experiments with other tracking approaches show that the proposed method has a higher convergence rate than standard first-order minimization techniques. Therefore, it is well adapted to real-time robotic applications. The output of the visual tracking is a homography linking the current and the reference image of a planar target. Using the homography, a task function isomorphic to the camera pose has been designed. A new image-based control law is proposed which does not need any measure of the 3D structure of the observed target (e.g. the normal to the plane). The theoretical proof of the existence of the isomorphism between the task function and the camera pose and the theoretical proof of the stability of the control law are provided. The experimental results, obtained with a 6 d.o.f. robot, show the advantages of the proposed method with respect to the existing approaches. KEY WORDS\u2014visual tracking, visual servoing, efficient second-order minimization, homography-based control law"}
{"_id":"19b7224e55ecd61783d4f19ac3f41f1845bad189","title":"Supervised tensor learning","text":"Tensor representation is helpful to reduce the small sample size problem in discriminative subspace selection. As pointed by this paper, this is mainly because the structure information of objects in computer vision research is a reasonable constraint to reduce the number of unknown parameters used to represent a learning model. Therefore, we apply this information to the vector-based learning and generalize the vector-based learning to the tensor-based learning as the supervised tensor learning (STL) framework, which accepts tensors as input. To obtain the solution of STL, the alternating projection optimization procedure is developed. The STL framework is a combination of the convex optimization and the operations in multilinear algebra. The tensor representation helps reduce the overfitting problem in vector-based learning. Based on STL and its alternating projection optimization procedure, we generalize support vector machines, minimax probability machine, Fisher discriminant analysis, and distance metric learning, to support tensor machines, tensor minimax probability machine, tensor Fisher discriminant analysis, and the multiple distance metrics learning, respectively. We also study the iterative procedure for feature extraction within STL. To examine the effectiveness of STL, we implement the tensor minimax probability machine for image classification. By comparing with minimax probability machine, the tensor version reduces the overfitting problem."}
{"_id":"93ce62fb04283efb253b512dc3f02b1d169ee7ed","title":"Two-Stream 3-D convNet Fusion for Action Recognition in Videos With Arbitrary Size and Length","text":"3-D convolutional neural networks (3-D-convNets) have been very recently proposed for action recognition in videos, and promising results are achieved. However, existing 3-D-convNets has two \u201cartificial\u201d requirements that may reduce the quality of video analysis: 1) It requires a fixed-sized (e.g., 112  $\\times$ 112) input video; and 2) most of the 3-D-convNets require a fixed-length input (i.e., video shots with fixed number of frames). To tackle these issues, we propose an end-to-end pipeline named Two-stream 3-D-convNet Fusion, which can recognize human actions in videos of arbitrary size and length using multiple features. Specifically, we decompose a video into spatial and temporal shots. By taking a sequence of shots as input, each stream is implemented using a spatial temporal pyramid pooling (STPP) convNet with a long short-term memory (LSTM) or CNN-E model, softmax scores of which are combined by a late fusion. We devise the STPP convNet to extract equal-dimensional descriptions for each variable-size shot, and we adopt the LSTM\/CNN-E model to learn a global description for the input video using these time-varying descriptions. With these advantages, our method should improve all 3-D CNN-based video analysis methods. We empirically evaluate our method for action recognition in videos and the experimental results show that our method outperforms the state-of-the-art methods (both 2-D and 3-D based) on three standard benchmark datasets (UCF101, HMDB51 and ACT datasets)."}
{"_id":"d72458f9501963670b50ee9fe78e622425955630","title":"Fidelius Charm: Isolating Unsafe Rust Code","text":"The Rust programming language has a safe memory model that promises to eliminate critical memory bugs. While the language is strong in doing so, its memory guarantees are lost when any unsafe blocks are used. Unsafe code is often needed to call library functions written in an unsafe language inside a Rust program. We present Fidelius Charm (FC), a system that protects a programmer-specified subset of data in memory from unauthorized access through vulnerable unsafe libraries. FC does this by limiting access to the program's memory while executing unsafe libraries. FC uses standard features of Rust and utilizes the Linux kernel as a trusted base for splitting the address space into a trusted privileged region under the control of functions written in Rust and a region available to unsafe external libraries. This paper presents our design and implementation of FC, presents two case studies for using FC in Rust TLS libraries, and reports on experiments showing its performance overhead is low for typical uses."}
{"_id":"13f279a8102df272577b9fe691276d590e61311a","title":"Facial expression recognition in peripheral versus central vision: role of the eyes and the mouth.","text":"This study investigated facial expression recognition in peripheral relative to central vision, and the factors accounting for the recognition advantage of some expressions in the visual periphery. Whole faces or only the eyes or the mouth regions were presented for 150 ms, either at fixation or extrafoveally (2.5\u00b0 or 6\u00b0), followed by a backward mask and a probe word. Results indicated that (a) all the basic expressions were recognized above chance level, although performance in peripheral vision was less impaired for happy than for non-happy expressions, (b) the happy face advantage remained when only the mouth region was presented, and (c) the smiling mouth was the most visually salient and most distinctive facial feature of all expressions. This suggests that the saliency and the diagnostic value of the smile account for the advantage in happy face recognition in peripheral vision. Because of saliency, the smiling mouth accrues sensory gain and becomes resistant to visual degradation due to stimulus eccentricity, thus remaining accessible extrafoveally. Because of diagnostic value, the smile provides a distinctive single cue of facial happiness, thus bypassing integration of face parts and reducing susceptibility to breakdown of configural processing in peripheral vision."}
{"_id":"0647fe48491f82a8314453ad79912c780badddba","title":"Sentence Simplification by Monolingual Machine Translation","text":"In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems."}
{"_id":"2dbd523c9cd754c9329e722a1c33e317c6c0d53b","title":"Motivations and Methods for Text Simplification","text":"Long and complicated sentences prove to be a stumbling block for current systems relying on NL input. These systems stand to gain from methods that syntactically simplify such sentences. To simplify a sentence, we need an idea of the structure of the sentence, to identify the components to be separated out. Obviously a parser could be used to obtain the complete structure of the sentence. However, full parsing is slow and prone to failure, especially on complex sentences. In this paper, we consider two alternatives to full parsing which could be used for simpli cation. The rst approach uses a Finite State Grammar (FSG) to produce noun and verb groups while the second uses a Supertagging model to produce dependency linkages. We discuss the impact of these two input representations on the simpli cation process. 1 Reasons for Text Simpli cation Long and complicated sentences prove to be a stumbling block for current systems which rely on natural language input. These systems stand to gain from methods that preprocess such sentences so as to make them simpler. Consider, for example, the following sentence: (1) The embattled Major government survived a crucial vote on coal pits closure as its last-minute concessions curbed the extent of Tory revolt over an issue that generated unusual heat in the House of Commons and brought the miners to London streets. Such sentences are not uncommon in newswire texts. Compare this with the multi-sentence version which has been manually simpli ed: (2) The embattled Major government survived a crucial vote on coal pits closure. Its last-minute concessions curbed the extent of On leave from the National Centre for Software Technology, Gulmohar Cross Road No. 9, Juhu, Bombay 400 049, India Tory revolt over the coal-mine issue. This issue generated unusual heat in the House of Commons. It also brought the miners to"}
{"_id":"911d79afaf3ca3e3b9634ec6ed16de450cce0c8c","title":"Improving Text Simplification Language Modeling Using Unsimplified Text Data","text":"In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams."}
{"_id":"29778f86a936c5a5fbedcdffdc11d0ddfd3984f1","title":"Video In Sentences Out","text":"We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the trackto-role assignments, and changing body posture."}
{"_id":"9ed3140c455f98e07796fade833b972fc1d6c83d","title":"Multi-dimension semantic dictionary for online intelligence","text":"The proposed online intelligence (OI) is to provide popular and well-formatted reports for direct decision support based on online data. Completeness, preciseness, conciseness and smartness are proposed as the basic technical indicators to evaluate an OI application. OI requires similar data processing flow and reporting tools with traditional business intelligence (BI). However, OI collects data basically from online other than provided by enterprise application systems. The online data is usually non-structural or semi-structural. Semantics techniques are required by OI to find and process online data into decision-making knowledge automatically in many cases. To this end, a multi-dimension semantic dictionary is proposed to divide online objects' semantics into four semantic aspects, i.e., information semantics, business semantics, application semantics and structural semantics. The four semantic aspects of the semantic dictionary are developed separately and interlinked as a multi-dimension semantic matrix to conduct matchmaking when find and process online data. In this work, the related concepts and implementation framework of the multi-dimension semantic dictionary are discussed and a real application for illegal drug tracing is provided to illustrate how OI make sense and semantics are developed."}
{"_id":"66c28481397a109ff70d07e9692a41bf04ad9ae8","title":"Dead-time elimination method and current polarity detection circuit for three-phase PWM-controlled inverter","text":"This paper will present a dead-time elimination scheme and the related current polarity detection circuit without separate power sources for three-phase inverters. The presented scheme includes the freewheeling current polarity detection circuit and the PWM control generator without dead time. It will be shown that the presented scheme eliminates the dead time of PWM control for inverter and therefore dramatically improving the output voltage loss and current distortion. Experimental results derived from an FPGA-based PWM-controlled inverter are shown for confirmation."}
{"_id":"429aea828954ba77da9cb3a1a8dfd4d9ea4d7101","title":"Single-image SVBRDF capture with a rendering-aware deep network","text":"Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades. We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design. For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation. Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs."}
{"_id":"330f258e290adc2f78820eddde589946f775ae65","title":"Attribute reduction in decision-theoretic rough set models","text":"Rough set theory can be applied to rule induction. There are two different types of classification rules, positive and boundary rules, leading to different decisions and consequences. They can be distinguished not only from the syntax measures such as confidence, coverage and generality, but also the semantic measures such as decision-monotocity, cost and risk. The classification rules can be evaluated locally for each individual rule, or globally for a set of rules. Both the two types of classification rules can be generated from, and interpreted by, a decision-theoretic model, which is a probabilistic extension of the Pawlak rough set model. As an important concept of rough set theory, an attribute reduct is a subset of attributes that are jointly sufficient and individually necessary for preserving a particular property of the given information table. This paper addresses attribute reduction in decision-theoretic rough set models regarding different classification properties, such as: decisionmonotocity, confidence, coverage, generality and cost. It is important to note that many of these properties can be truthfully reflected by a single measure c in the Pawlak rough set model. On the other hand, they need to be considered separately in probabilistic models. A straightforward extension of the c measure is unable to evaluate these properties. This study provides a new insight into the problem of attribute reduction. Crown Copyright 2008 Published by Elsevier Inc. All rights reserved."}
{"_id":"26afa926e218357b04ff77bafda3fea4b24dc8fc","title":"Next-Term Student Performance Prediction: A Recommender Systems Approach","text":"An enduring issue in higher education is student retention to successful graduation. National statistics indicate that most higher education institutions have four-year degree completion rates around 50%, or just half of their student populations. While there are prediction models which illuminate what factors assist with college student success, interventions that support course selections on a semester-to-semester basis have yet to be deeply understood. To further this goal, we develop a system to predict students\u2019 grades in the courses they will enroll in during the next enrollment term by learning patterns from historical transcript data coupled with additional information about students, courses and the instructors teaching them. We explore a variety of classic and state-of-the-art techniques which have proven effective for recommendation tasks in the e-commerce domain. In our experiments, Factorization Machines (FM), Random Forests (RF), and the Personalized Linear Multiple Regression model achieve the lowest prediction error. Application of a novel feature selection technique is key to the predictive success and interpretability of the FM. By comparing feature importance across populations and across models, we uncover strong connections between instructor characteristics and student performance. We also discover key differences between transfer and non-transfer students. Ultimately we find that a hybrid FM-RF method can be used to accurately predict grades for both new and returning students taking both new and existing courses. Application of these techniques holds promise for student degree planning, instructor interventions, and personalized advising, all of which could improve retention and academic performance."}
{"_id":"c1bd75dd43ad483e93a0c915d754b15e42eeec04","title":"Semantics Extraction from Images","text":"An overview of the state-of-the-art on semantics extraction from images is presented. In this survey, we present the relevant approaches in terms of content representation as well as in terms of knowledge representation. Knowledge can be represented in either implicit or explicit fashion while the image is represented in different levels, namely, low-level, intermediate and semantic level. For each combination of knowledge and image representation, a detailed discussion is addressed that leads to fruitful conclusions for the impact of each approach. 1 Semantics extraction basic pipeline Semantics extraction refers to digital data interpretation from a human point of view. In the case that the digital data correspond to images, this usually entails an appearance-based inference using color, texture and\/or shape information along with a type of context inference (or representation) that can combine and transform these machine-extracted evidence into what we call a scene description. Following Biederman et al. [1] definitions of context in a visual scene, we can derive three types of context for real-world scene annotation problems: (i) semantic context which encodes the probability of a certain category to be present in a scene (e.g category \u201cstreets\u201d has high probability to coexist with category \u201cbuilding\u201d in the same scene ); (ii) spatial context which encodes the spatial relations of categories (e.g sky is usually above grass in a scene) and (iii) scale context which encodes the relative object size (category \u201chuman\u201d is expected to occupy a small region in a scene which includes the\u201cbuilding\u201d category). The research goals in semantics extraction are mostly a function of the granularity of the semantics in question. The goal could be the extraction of a single or multiple semantics of the entire image (e.g. indoor\/outdoor setting), or the extraction of the semantics for different objects in an image. In the latter case, the semantics could be generic (e.g. a vehicle) or specific (e.g. a motorbike). Those goals make it clear that semantics extraction is not a new research area. Depending on the goal, the task of semantics extraction can be considered as a categorization, classification, recognition and understanding task that all share in common the effort for solving the semantic gap. As stated in [2], \u201cDespite intensive recent research, the automatic establishment of a correspondence between"}
{"_id":"2ba8d5195d15c45418ee77ada809ab7e1ba0df53","title":"A Distributed Reinforcement Learning Scheme for Network Routing","text":"In this paper we describe a self-adjusting algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network. Only local communication is used to keep accurate statistics at each node on which routing policies lead to minimal delivery times. In simple experiments involving a 36-node, irregularly connected network, this learning approach proves superior to a nonadaptive algorithm based on precomputed shortest paths. The authors would like to thank for their support the Bellcore Cognitive Science Research Group, the National Defense Science and Engineering Graduate fellowship program, and National Science Foundation Grant IRI-9214873. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the o cial policies, either expressed or implied, of Bellcore, the National Science Foundation or the U.S. Government."}
{"_id":"d69bfbe6763a5ab0708c8a85eb67d5ea450177c5","title":"VADI: GPU Virtualization for an Automotive Platform","text":"Modern vehicles are evolving with more electronic components than ever before (In this paper, \u201cvehicle\u201d means \u201cautomotive vehicle.\u201d It is also equal to \u201ccar.\u201d) One notable example is graphical processing unit (GPU), which is a key component to implement a digital cluster. To implement the digital cluster that displays all the meters (such as speed and fuel gauge) together with infotainment services (such as navigator and browser), the GPU needs to be virtualized; however, GPU virtualization for the digital cluster has not been addressed yet. This paper presents a Virtualized Automotive DIsplay (VADI) system to virtualize a GPU and its attached display device. VADI manages two execution domains: one for the automotive control software and the other for the in-vehicle infotainment (IVI) software. Through GPU virtualization, VADI provides GPU rendering to both execution domains, and it simultaneously displays their images on a digital cluster. In addition, VADI isolates GPU from the IVI software in order to protect it from potential failures of the IVI software. We implement VADI with Vivante GC2000 GPU and perform experiments to ensure requirements of International Standard Organization (ISO) safety standards. The results show that VADI guarantees 30 frames per second (fps), which is the minimum frame rate for digital cluster mandated by ISO safety standards even with the failure of the IVI software. It also achieves 60 fps in a synthetic workload."}
{"_id":"c2c6a92198568be2ee891c93a3830a69bee26158","title":"An Overview of Smart Home Environments: Architectures, Technologies and Applications","text":"The aim of this research survey note is to provide a comprehensive overview of Smart Home Environments with a focus on their architectures and application areas, as well as utilized technologies, infrastructures, and standards. The main source of information was provided by past and existing research and development projects carried out in this area, while the main result of our survey is a classification of Smart Home Environments, as revealed by this projects\u2019 survey."}
{"_id":"74a308f986a8368542f6945c94559a936bc3a31e","title":"Defending against malicious peripherals","text":"Attacks on host computers by malicious peripherals are a growing problem. Inexpensive and powerful peripherals, which attach to plug-and-play buses, have made such attacks easy to mount. Making matters worse, commodity operating systems lack systematic defenses, and users are often not aware of the scope of the problem. We present Cinch, a pragmatic response to this threat. Cinch uses virtualization to place the hardware in a logically separate, untrusted machine, and includes an interposition layer between the untrusted machine and the protected one. This layer accepts or rejects interaction with devices and enforces security policies that are easily configured and extended by users. We show that Cinch integrates with existing OSes, enforces policies that thwart real world attacks, and has low overhead."}
{"_id":"9d901c1ce5e024ad22f1c2663dba8e1099b496e9","title":"HADOOP BLOCK PLACEMENT POLICY FOR DIFFERENT FILE FORMATS","text":"Now a day\u2019s Peta-Bytes of data becomes the norm in industries. Handling, analyzing such big data is challenging task. Even frameworks like Hadoop (Open Source Implementation of MapReduce Paradigm) and NoSQL databases like Cassandra, HBase can be used to analyze and store such large data; heterogeneity of data is still an issue. Data centers usually have clusters formed using heterogeneous nodes. Ecosystem like Hadoop can be used to manage such types of cluster but it can not schedule jobs (Application) efficiently on this heterogeneous cluster when data is itself heterogeneous. Heterogeneity of data may be because of data format or because of the complexity. This paper is review of systems and algorithms for distributed management of huge size data, efficiency of these approaches."}
{"_id":"a1e128a78cc9ae7aa2aaaf4c3934a0f6387b7606","title":"Simultaneous estimation of food categories and calories with multi-task CNN","text":"In this paper, we propose simultaneous estimation of food categories and calories for food photos. Since there exists strong correlation between food categories and calories in general, we expect that simultaneous training of both brings performance boosting compared to independent single training. To this end, we use a multitask CNN. In the experiments, we collected calorie-annotated recipe data from the online cooking recipe sites, and trained multi-task and single-task CNNs. As results, the multi-task CNN achieved the better performance on both food category estimation and food calorie estimation than single-task CNNs."}
{"_id":"149a43346f5a51db064be348fd266c4959e0d1f0","title":"Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation","text":"We present an approach to structured prediction from bandit feedback, called Bandit Structured Prediction, where only the value of a task loss function at a single predicted point, instead of a correct structure, is observed in learning. We present an application to discriminative reranking in Statistical Machine Translation (SMT) where the learning algorithm only has access to a 1 \u2212 BLEU loss evaluation of a predicted translation instead of obtaining a gold standard reference translation. In our experiment bandit feedback is obtained by evaluating BLEU on reference translations without revealing them to the algorithm. This can be thought of as a simulation of interactive machine translation where an SMT system is personalized by a user who provides single point feedback to predicted translations. Our experiments show that our approach improves translation quality and is comparable to approaches that employ more informative feedback in learning."}
{"_id":"c8e72009434e34e6800c54a7bb571abc8e9279ca","title":"Review of Machine Learning Algorithms in Differential Expression Analysis","text":"In biological research machine learning algorithms are part of nearly every analytical process. They are used to identify new insights into biological phenomena, interpret data, provide molecular diagnosis for diseases and develop personalized medicine that will enable future treatments of diseases. In this paper we (1) illustrate the importance of machine learning in the analysis of large scale sequencing data, (2) present an illustrative standardized workflow of the analysis process, (3) perform a Differential Expression (DE) analysis of a publicly available RNA sequencing (RNASeq) data set to demonstrate the capabilities of various algorithms at each step of the workflow, and (4) show a machine learning solution in improving the computing time, storage requirements, and minimize utilization of computer memory in analyses of RNA-Seq datasets. The source code of the analysis pipeline and associated scripts are presented in the paper appendix to allow replication of experiments."}
{"_id":"39101b6482e21eccbd77f2dee6a3710037e4ba6f","title":"As Time Goes By: Comprehensive Tagging of Textual Phrases with Temporal Scopes","text":"Temporal expressions (TempEx\u2019s for short) are increasingly important in search, question answering, information extraction, and more. Techniques for identifying and normalizing explicit temporal expressions work well, but are not designed for and cannot cope with textual phrases that denote named events, such as \u201cClinton\u2019s term as secretary of state\u201d. This paper addresses the problem of detecting such temponyms, inferring their temporal scopes, and mapping them to events in a knowledge base if present there. We present methods for this kind of temponym resolution, using an entityand TempEx-oriented document model and the Yago knowledge base for distant supervision. We develop a family of Integer Linear Programs for jointly inferring temponym mappings to the timeline and knowledge base. This enriches the document representation and also extends the knowledge base by obtaining new alias names for events. Experiments with three different corpora demonstrate the viability of our methods."}
{"_id":"5a609c164899a6e07317add1c8ae7420560c6ff1","title":"Stakeholder participation for sustainable waste management","text":"Inadequate environmental sanitation in many cities is a major cause of diseases and is a drain on the economy by way of lost workdays, cost of treatment and cleanup activities. Municipal authorities and policymakers need to act fast to address this issue. Sustainable waste management provides a comprehensive inter-disciplinary framework for addressing the problems of managing urban solid waste, in the resource constrained developing countries where quality of such services are poor and costs are high often with no effective means of recovering them. Upgrading the coverage of waste management and services and increasing their efficiency is a precondition for improving the environmental quality of cities. This paper highlights the fact that the involvement and participation of all the stakeholders such as the waste generators, waste processors, formal and informal agencies, non-governmental organisations and financing institutions is a key factor for the sustainable waste management. r 2005 Elsevier Ltd. All rights reserved."}
{"_id":"663ae1af2835f456de9f20ce6dce85ea559dad2e","title":"HILOG: A Foundation for Higher-Order Logic Programming","text":"D We describe a novel logic, called HiLog, and show that it provides a more suitable basis for logic programming than does traditional predicate logic. HiLog has a higher-order syntax and allows arbitrary terms to appear in places where predicates, functions, and atomic formulas occur in predicate calculus. But its semantics is first-order and admits a sound and complete proof procedure. Applications of HiLog are discussed, including DCG grammars, higher-order and modular logic programming, and deductive databases. a"}
{"_id":"13d79d6021ee33bb403a6ee1c5935a350b87bdba","title":"GIST: A Model for Design and Management of Content and Interactivity of Customer-Centric Web Sites","text":"Customer-centric Web-based systems, such as ecommerce Web sites, or sites that support customer relationship management (CRM) activities, are themselves information systems, but their design and maintenance need to follow vastly different approaches from the traditional systems lifecycle approach. Based on marketing frameworks that are applicable to the online world, and following design science principles, we develop a model to guide the design and the continuous management of such sites. The model makes extensive use of current technologies for tracking the customers and their behaviors, and combines elements of data mining and statistical analyses. A case study based on a financial services Web site is used to provide a preliminary validation and design evaluation of our approach. The case study showed considerable measured improvement in the effectiveness of the company\u0092s Web site. In addition, it also highlighted an important benefit of the our approach: the identification of previously unknown or unexpected segments of visitors. This finding can lead to promising new business opportunities."}
{"_id":"e76e9c99452edf1c477d4fc099fec7d607a6d65f","title":"18F-Fluciclovine PET\/MRI for preoperative lymph node staging in high-risk prostate cancer patients","text":"To investigate the diagnostic potential of simultaneous 18F-fluciclovine PET\/MRI for pelvic lymph node (LN) staging in patients with high-risk prostate cancer. High-risk prostate cancer patients (n=28) underwent simultaneous 18F-fluciclovine PET\/MRI prior to surgery. LNs were removed according to a predefined template of eight regions. PET and MR images were evaluated for presence of LN metastases according to these regions. Sensitivity\/specificity for detection of LN metastases were calculated on patient and region basis. Sizes of LN metastases in regions with positive and negative imaging findings were compared with linear mixed models. Clinical parameters of PET-positive and -negative stage N1 patients were compared with the Mann-Whitney U test. Patient- and region-based sensitivity\/specificity for detection of pelvic LN metastases was 40 %\/87.5 % and 35 %\/95.7 %, respectively, for MRI and 40 %\/100 % and 30 %\/100 %, respectively, for PET. LN metastases in true-positive regions were significantly larger than metastases in false-negative regions. PET-positive stage N1 patients had higher metastatic burden than PET-negative N1 patients. Simultaneous 18F-fluciclovine PET\/MRI provides high specificity but low sensitivity for detection of LN metastases in high-risk prostate cancer patients. 18F-Fluciclovine PET\/MRI scan positive for LN metastases indicates higher metastatic burden than negative scan. \u2022 18 F-Fluciclovine PET\/MRI has high specificity for detection of lymph node metastasis. \u2022 18 F-Fluciclovine PET\/MRI lacks sensitivity to replace ePLND. \u2022 18 F-Fluciclovine PET\/MRI may be used to aid surgery and select adjuvant therapy. \u2022 18 F-Fluciclovine PET-positive patients have more extensive disease than PET-negative patients. \u2022 Size of metastatic lymph nodes is an important factor for detection."}
{"_id":"d66edebece1f13a82a0cb68d7eaada7a83b9d77c","title":"Supporting Situationally Aware Cybersecurity Systems 30 th September 2015","text":"In this report, we describe the Unified Cyber Security ontology (UCO) to support situational awareness in cyber security systems. The ontology is an effort to incorporate and integrate heterogeneous information available from different cyber security systems and most commonly used cyber security standards for information sharing and exchange. The ontology has also been mapped to a number of existing cyber security ontologies as well as concepts in the Linked Open Data cloud. Similar to DBpedia which serves as the core for Linked Open Data cloud, we envision UCO to serve as the core for the specialized cyber security Linked Open Data cloud which would evolve and grow with the passage of time with additional cybersecurity data sets as they become available. We also present a prototype system and concrete use-cases supported by the UCO ontology. To the best of our knowledge, this is the first cyber security ontology that has been mapped to general world ontologies to support broader and diverse security use-cases. We compare the resulting ontology with previous efforts, discuss its strengths and limitations, and describe potential future work directions."}
{"_id":"0f34fcab599aabf0ab46d91c21703a9a86b5f048","title":"On computable numbers, with an application to the Entscheidungsproblem","text":"The \"computable\" numbers may be described briefly as the real numbers whose expressions as a decimal are calculable by finite means. Although the subject of this paper is ostensibly the computable numbers. it is almost equally easy to define and investigate computable functions of an integral variable or a real or computable variable, computable predicates, and so forth. The fundamental problems involved are, however, the same in each case, and I have chosen the computable numbers for explicit treatment as involving the least cumbrous technique. I hope shortly to give an account of the relations of the computable numbers, functions, and so forth to one another. This will include a development of the theory of functions of a real variable expressed in terms of computable numbers. According to my definition, a number is computable if its decimal can be written down by a machine."}
{"_id":"2586dd5514cb203f42292f25238f1537ea5e4b8c","title":"Informal social communication.","text":""}
{"_id":"89ac6035c01eb6234fe45601809e0097c4096d4a","title":"Hemihyperplasia-multiple lipomatosis syndrome (HHML): a challenge in spinal care.","text":"A 15-year-old girl developed a progressive paraparesis over a period of six months, secondary to spinal cord compression by a lipomatous mass and anomalies of the vertebral column. Clinically, a right hemihyperplasia affecting the trunk and lower limb was evident, as well as a right convex lumbar scoliosis. CT and MRI demonstrated severe spinal cord compression resulting from intraspinal lipomatosis, overgrowth of right facet joints (T8 to L5), and kyphoscoliosis. Surgical decompression was undertaken. A lumbar scoliosis of 48 degrees was partially corrected by means of dual-rod instrumentation. The neurological deficit improved significantly, and ambulation was progressively restored. The patient carried the diagnosis of Proteus syndrome for several years, but reevaluation of clinical features prompted the diagnosis of Hemihyperplasia Multiple Lipomatosis syndrome (HHML). This rare sporadic disorder is often confused with Proteus syndrome. As in Proteus syndrome, spinal cord compression in patients with HHML can result from lipomatous infiltration and\/or significant spinal abnormalities including kyphoscoliosis and overgrowth. HHML and Proteus syndrome are discussed and compared with special emphasis on spinal and orthopaedic pathologies."}
{"_id":"5cda0f3a10f2dc9aba5e5bde37d2c156e268b8f1","title":"Establishment of the Diagnosis of Follicular Occlusion Tetrad","text":"Background: Follicular occlusion tetrad (FOT) is a clinical syndrome consisting of suppurative hidradenitis, acne conglobata, dissecting cellulitis of the scalp, and pilonidal sinus. FOT is a rare case that is mostly found in severe conditions resulting in resistance to therapy. Diagnostic accuracy and therapy for all components of FOT is extremely important. Case: A 43-year old male presented with small, painful blisters and bumps filled with pus that had been ongoing for five years. We found pustules and erythematous nodules and multiple abscess with atrophic scars on the scalp, anterior and posterior trunks, abdomen, groin and gluteal regions on the facial region, we found multiple atrophic scars. The pus culture showed the presence of Escherichia coli, and fistulography examination revealed multiple cutaneous fistules and enterocutaneous sinus. The histopathological examination indicated rupture of hair follicles, follicular plugs, and infiltration of heavy-mixed infiltrate cells. Discussion: The FOT diagnosis was established through medical history recording, physical and histopathological examinations, and fistulography. Upon medical history recording and physical examination, we found pustules and nodules in several hairy areas. The fistulographic examination showed some fistules and sinus tracts, and the histopathological examination showed adnexal tissue damage caused by occlusion of hair follicles and inflammation due to accumulation of keratin and debris. All these findings led to the FOT diagnosis. ISSN 2476-2415"}
{"_id":"d39f016ef25f0ba3d9db24b94667eceeef981ecc","title":"Message Passing Inference for Large Scale Graphical Models with High Order Potentials","text":"To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods."}
{"_id":"1dc697ae0d6a1e90dc8ff061e36441b6efdcff7e","title":"A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems","text":"We present an iterative linear-quadratic-Gaussian method for locally-optimal feedback control of nonlinear stochastic systems subject to control constraints. Previously, similar methods have been restricted to deterministic unconstrained problems with quadratic costs. The new method constructs an affine feedback control law, obtained by minimizing a novel quadratic approximation to the optimal cost-to-go function. Global convergence is guaranteed through a Levenberg-Marquardt method; convergence in the vicinity of a local minimum is quadratic. Performance is illustrated on a limited-torque inverted pendulum problem, as well as a complex biomechanical control problem involving a stochastic model of the human arm, with 10 state dimensions and 6 muscle actuators. A Matlab implementation of the new algorithm is availabe at www.cogsci.ucsd.edu\/\/spl sim\/todorov."}
{"_id":"26949ee96ec22e400d8b4b264ee23360f3df1f7b","title":"Policy Improvement Methods: Between Black-Box Optimization and Episodic Reinforcement Learning","text":"Policy improvement methods seek to optimize the parameters of a policy with respect to a utility function. There are two main approaches to performing this optimization: reinforcement learning (RL) and black-box optimization (BBO). Whereas BBO algorithms are generic optimization methods that, due to there generality, may also be applied to optimizing policy parameters, RL algorithms are specifically tailored to leveraging the structure of policy improvement problems. In recent years, benchmark comparisons between RL and BBO have been made, and there has been several attempts to specify which approach works best for which types of problem classes. In this article, we make several contributions to this line of research: 1) We define four algorithmic properties that further clarify the relationship between RL and BBO: action-perturbation vs. parameter-perturbation, gradient estimation vs. rewardweighted averaging, use of only rewards vs. use of rewards and state information, actor-critic vs. direct policy search. 2) We show how the chronology of the derivation of ever more powerful algorithms displays a trend towards algorithms based on parameter-perturbation and reward-weighted averaging. A striking feature of this trend is that it has moved RL methods closer and closer to BBO. 3) We continue this trend by applying two modifications to the state-of-the-art \u201cPolicy Improvement with Path Integrals\u201d (PI), which yields an algorithm we denote PI. We show that PI is a BBO algorithm, and, more specifically, that it is a special case of the \u201cCovariance Matrix Adaptation \u2013 Evolutionary Strategy\u201d algorithm. Our empirical evaluation demonstrates that the simpler PI outperforms PI on simple evaluation tasks in terms of convergence speed and final cost. 4) Although our evaluation implies that, for these five tasks, BBO outperforms RL, we do not hold this to be a general statement, and provide an analysis of why these tasks are particularly well-suited for BBO. Thus, rather than making the case for BBO or RL, one of the main contributions of this article is rather to provide an algorithmic framework in which such cases may be made, as PI and PI use identical perturbation and parameter update methods, and differ only in being BBO and RL approaches respectively."}
{"_id":"2fd17ebb88c0748fe79f59da6d2fec51233c2dc0","title":"A dynamically stable single-wheeled mobile robot with inverse mouse-ball drive","text":"Multi-wheel statically-stable mobile robots tall enough to interact meaningfully with people must have low centers of gravity, wide bases of support, and low accelerations to avoid tipping over. These conditions present a number of performance limitations. Accordingly, we are developing an inverse of this type of mobile robot that is the height, width, and weight of a person, having a high center of gravity, that balances dynamically on a single spherical wheel. Unlike balancing 2-wheel platforms which must turn before driving in some direction, the single-wheel robot can move directly in any direction. We present the overall design, actuator mechanism based on an inverse mouse-ball drive, control system, and initial results including dynamic balancing, station keeping, and point-to-point motion"}
{"_id":"13566e7b702a156d2f0d6b2ccbcb58b95cdd41c5","title":"Evolutionary Function Approximation for Reinforcement Learning","text":"Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This thesis investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. I present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This thesis also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. I evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods."}
{"_id":"95b8c5ea18bc8532b0b725488bd2b28cebe6e760","title":"Loneliness, social contacts and Internet addiction: A cross-lagged panel study","text":"This study aims to examine the causal priority in the observed empirical relationships between Internet addiction and other psychological problems. A cross-lagged panel survey of 361 college students in Hong Kong was conducted. Results show that excessive and unhealthy Internet use would increase feelings of loneliness over time. Although depression had a moderate and positive bivariate relationship with Internet addiction at each time point, such a relationship was not significant in the cross-lagged analyses. This study also found that online social contacts with friends and family were not an effective alternative for offline social interactions in reducing feelings of loneliness. Furthermore, while an increase in face-to-face contacts could help to reduce symptoms of Internet addiction, this effect may be neutralized by the increase in online social contacts as a result of excessive Internet use. Taken as a whole, findings from the study show a worrisome vicious cycle between loneliness and Internet addiction. 2013 Elsevier Ltd. All rights reserved."}
{"_id":"8a131af3f9f27037f3e3612b84d1c508c5ac1823","title":"Real-time Vision-based Hand Gesture Recognition Using Haar-like Features","text":"This paper proposes a two level approach to solve the problem of real-time vision-based hand gesture classification. The lower level of the approach implements the posture recognition with Haar-like features and the AdaBoost learning algorithm. With this algorithm, real-time performance and high recognition accuracy can be obtained. The higher level implements the linguistic hand gesture recognition using a context-free grammar-based syntactic analysis. Given an input gesture, based on the extracted postures, the composite gestures can be parsed and recognized with a set of primitives and production rules."}
{"_id":"4d60d8d336a68008609712e3153757daefdca050","title":"DeepDefense: Training Deep Neural Networks with Improved Robustness","text":"Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in securitycritical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named \u201cdeep defense\u201d. Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial\/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results will be made publicly available."}
{"_id":"3a68b92df71637d2ba0ecc1cde8cfe5b29f2d709","title":"Ambiguity Resolution in a Cognitive Model of Language Comprehension","text":"The Lucia comprehension system attempts to model human comprehension by using the Soar cognitive architecture, Embodied Construction Grammar (ECG), and an incremental, word-by-word approach to grounded processing. Traditional approaches use techniques such as parallel paths and global optimization to resolve ambiguities. Here we describe how Lucia deals with lexical, grammatical, structural, and semantic ambiguities by using knowledge from the surrounding linguistic and environmental context. It uses a local repair mechanism to maintain a single path, and shows a garden path effect when local repair breaks down. Data on adding new linguistic knowledge shows that the ECG grammar grows faster than the knowledge for handling context, and that lowlevel grammar items grow faster than more general ones."}
{"_id":"0fd49277e7ad0b0ef302f05ebcca772d28e34292","title":"Personalized Reliability Prediction of Web Services","text":"Service Oriented Architecture (SOA) is a business-centric IT architectural approach for building distributed systems. Reliability of service-oriented systems heavily depends on the remote Web services as well as the unpredictable Internet connections. Designing efficient and effective reliability prediction approaches of Web services has become an important research issue. In this article, we propose two personalized reliability prediction approaches of Web services, that is, neighborhood-based approach and model-based approach. The neighborhood-based approach employs past failure data of similar neighbors (either service users or Web services) to predict the Web service reliability. On the other hand, the model-based approach fits a factor model based on the available Web service failure data and use this factor model to make further reliability prediction. Extensive experiments are conducted with our real-world Web service datasets, which include about 23 millions invocation results on more than 3,000 real-world Web services. The experimental results show that our proposed reliability prediction approaches obtain better reliability prediction accuracy than other competing approaches."}
{"_id":"24ecb323657c8bc581c34ef26e07c58bb237922b","title":"A Study on the Effective Permittivity of Carbon\/PI Honeycomb Composites for Radar Absorbing Design","text":"Radar absorbing honeycomb composites with different coating thicknesses are prepared by impregnation of aramid paper frame with solutions containing conductive carbon blacks (non-magnetic) and PI (polyimide). Expressions for the effective permittivity of the composites are studied and validated both in theory and experiment. It is found that a theoretical equivalent panel with given permittivity can be obtained to represent the honeycomb structure in the quasistatic approximation, which provides a feasible way to optimize the design of radar absorbing honeycomb structure by connecting the effective electromagnetic parameters with the unit cell dimensions. The effective permittivity is measured by a network analyzer system in the frequency range of 8-12 GHz and compared with the theoretical result."}
{"_id":"9af5d0d7d6106863629ea0a643ffa05f934e0ee7","title":"SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems","text":"Direct methods for visual odometry (VO) have gained popularity for their capability to exploit information from all intensity gradients in the image. However, low computational speed as well as missing guarantees for optimality and consistency are limiting factors of direct methods, in which established feature-based methods succeed instead. Based on these considerations, we propose a semidirect VO (SVO) that uses direct methods to track and triangulate pixels that are characterized by high image gradients, but relies on proven feature-based methods for joint optimization of structure and motion. Together with a robust probabilistic depth estimation algorithm, this enables us to efficiently track pixels lying on weak corners and edges in environments with little or high-frequency texture. We further demonstrate that the algorithm can easily be extended to multiple cameras, to track edges, to include motion priors, and to enable the use of very large field of view cameras, such as fisheye and catadioptric ones. Experimental evaluation on benchmark datasets shows that the algorithm is significantly faster than the state of the art while achieving highly competitive accuracy."}
{"_id":"f6e5e70860080a69e232d14a98bf20128957b9b5","title":"Fuzzy memoization for floating-point multimedia applications","text":"Instruction memoization is a promising technique to reduce the power consumption and increase the performance of future low-end\/mobile multimedia systems. Power and performance efficiency can be improved by reusing instances of an already executed operation. Unfortunately, this technique may not always be worth the effort due to the power consumption and area impact of the tables required to leverage an adequate level of reuse. In this paper, we introduce and evaluate a novel way of understanding multimedia floating-point operations based on the fuzzy computation paradigm: performance and power consumption can be improved at the cost of small precision losses in computation. By exploiting this implicit characteristic of multimedia applications, we propose a new technique called tolerant memoization. This technique expands the capabilities of classic memoization by associating entries with similar inputs to the same output. We evaluate this new technique by measuring the effect of tolerant memoization for floating-point operations in a low-power multimedia processor and discuss the trade-offs between performance and quality of the media outputs. We report energy improvements of 12 percent for a set of key multimedia applications with small LUT of 6 Kbytes, compared to 3 percent obtained using previously proposed techniques."}
{"_id":"b4d6f5a6471eef2f2943ade0dec77d122720d719","title":"Energy-Efficient Multiple-Precision Floating-Point Multiplier for Embedded Applications","text":"Floating-point (FP) multipliers are the main energy consumers in many modern embedded digital signal processing (DSP) and multimedia systems. For lossy applications, minimizing the precision of FP multiplication operations under the acceptable accuracy loss is a well-known approach for reducing the energy consumption of FP multipliers. This paper proposes a multiple-precision FP multiplier to efficiently trade the energy consumption with the output quality. The proposed FP multiplier can perform lowprecision multiplication that generates 8\u2212, 14\u2212, 20\u2212, or 26bit mantissa product through an iterative and truncated modified Booth multiplier. Energy saving for lowprecision multiplication is achieved by partially suppressing the computation of mantissa multiplier. In addition, the proposed multiplier allows the bitwidth of mantissa in the multiplicand, multiplier, and output product to be dynamically changed when it performs different FP multiplication operations to further reduce energy consumption. Experimental results show that the proposed multiplier can achieve 59 %, 71 %, 73 %, and 82 % energy saving under 0.1 %, 1 %, 5 %, and 11 % accuracy loss, respectively, for the RGB-to-YUV & YUV-to-RGB conversion when compared to the conventional IEEE single-precision multiplier. In addition, the results also exhibit that the proposed multiplier can obtain more energy reduction than previous multipleprecision iterative FP multipliers."}
{"_id":"3d0e07fc681428f230d1be6a79c5b40450e2e332","title":"Bootstrapping a Neural Conversational Agent with Dialogue Self-Play, Crowdsourcing and On-Line Reinforcement Learning","text":"End-to-end neural models show great promise towards building conversational agents that are trained from data and on-line experience using supervised and reinforcement learning. However, these models require a large corpus of dialogues to learn effectively. For goal-oriented dialogues, such datasets are expensive to collect and annotate, since each task involves a separate schema and database of entities. Further, the Wizard-of-Oz approach commonly used for dialogue collection does not provide sufficient coverage of salient dialogue flows, which is critical for guaranteeing an acceptable task completion rate in consumer-facing conversational agents. In this paper, we study a recently proposed approach for building an agent for arbitrary tasks by combining dialogue self-play and crowd-sourcing to generate fully-annotated dialogues with diverse and natural utterances. We discuss the advantages of this approach for industry applications of conversational agents, wherein an agent can be rapidly bootstrapped to deploy in front of users and further optimized via interactive learning from actual users of the system."}
{"_id":"355887cec4b19668adb7db0a72c93f4ad3e1ea09","title":"Characterization and Implementation of Dual-SiC MOSFET Modules for Future Use in Traction Converters","text":"Silicon (Si) insulated-gate bipolar transistors are widely used in railway traction converters. In the near future, silicon carbide (SiC) technology will push the limits of switching devices in three directions: higher blocking voltage, higher operating temperature, and higher switching speeds. The first silicon carbide (SiC) MOSFET modules are available on the market and look promising. Although they are still limited in breakdown voltage, these wide-bandgap components should improve traction-chain efficiency. Particularly, a significant reduction in the switching losses is expected which should lead to improvements in power-weight ratios. Nevertheless, because of the high switching speed and the high current levels required by traction applications, the implementation of these new modules is critical. An original method is proposed to compare, in terms of stray inductance, several dc bus-bar designs. To evaluate the potential of these new devices, a first set of measurements, based on a single-pulse test-bench, was obtained. The switching behavior of SiC devices was well understood at turn-off and turn-on. To complete this work, the authors use an opposition method to compare Si-IGBT and SiC-MOSFET modules in voltage source inverter operation. For this purpose, a second test-bench, allowing electrical and thermal measurements, was developed. Experimental results confirm the theoretical loss-calculation of the single-pulse tests and the correct operation of up to three modules directly connected in parallel. This analysis provides guidelines for a full SiC inverter design, and prospects for developments in traction applications are presented."}
{"_id":"48fc3f5091faff4d17572ed49a24075d1d9da33f","title":"A High-Speed Mesh of Tactile Sensors Fitting Arbitrary Surfaces","text":"A tactile sensor is developed with the aim of covering a robot's entire structure, while reducing wiring requirement and ensuring high-speed response. The sensor detects the center point of load distribution on 2-D surfaces as well as the overall load. There are only four signal wires from the sensor. The sensor response time is nearly constant (within 1 ms) regardless of the number of detection elements, their placements or sensor areas. In this paper, the principles behind the operation of this sensor and the results of experiments using the sensor are described."}
{"_id":"49aba86ad0178ae9f85f6e40b625fc8b91410874","title":"A PPG sensor for continuous cuffless blood pressure monitoring with self-adaptive signal processing","text":"A new portable PPG-BP device designed for continuously measuring blood pressure (BP) without a cuff is proposed in this study. This continuous and long-time BP monitoring enabled herein by the proposed portable cuffless BP sensor. Towards the aforementioned goal, the sensor is designed capable of detecting in real time, non-invasively and continuously the temporal intravascular blood volume change based on the principle of photoplethysmograph (PPG) for estimating BP. The hardware of the sensor consists mainly of light emitting diodes (LEDs) in wavelengths of 660 nm, a photo detectors (PD), and also signal processing chips to read output signals of the PD. The PD readout circuit includes a PD pre-amplifier, a band-pass filter, a programmable gain amplifier (PGA), a microcontroller unit for calculation and a wireless module for communication. A laptop is also used to display continuous BPs and conducts statistical analysis and displaying results. 27 subjects participated in the experimental validation, in which the obtained BPs are calibrated by and then compared with the results from a commercial blood pressure monitor by OMRON. The resultant signal-to-noise ratio (SNR) is capable of rising more than 15%, correlation coefficient, R2, for systolic blood pressure (SBP) and diastolic blood pressure (DBP) are 0.89 and 0.98, respectively."}
{"_id":"35b7e8ad60cb357cd9457ca2687eb2ba37068bd6","title":"How Visualization Layout Relates to Locus of Control and Other Personality Factors","text":"Existing research suggests that individual personality differences are correlated with a user's speed and accuracy in solving problems with different types of complex visualization systems. We extend this research by isolating factors in personality traits as well as in the visualizations that could have contributed to the observed correlation. We focus on a personality trait known as \"locus of control\u201d (LOC), which represents a person's tendency to see themselves as controlled by or in control of external events. To isolate variables of the visualization design, we control extraneous factors such as color, interaction, and labeling. We conduct a user study with four visualizations that gradually shift from a list metaphor to a containment metaphor and compare the participants' speed, accuracy, and preference with their locus of control and other personality factors. Our findings demonstrate that there is indeed a correlation between the two: participants with an internal locus of control perform more poorly with visualizations that employ a containment metaphor, while those with an external locus of control perform well with such visualizations. These results provide evidence for the externalization theory of visualization. Finally, we propose applications of these findings to adaptive visual analytics and visualization evaluation."}
{"_id":"822544a8601845a758255975f72cd91c24f705d2","title":"MVC architecture driven restructuring to achieve client-side web page composition","text":"This paper presents a restructuring approach to relocating web page composition from servers to browsers for Java web applications. The objective is to reduce redundant manipulation and transfer of code\/data that are shared by web pages. The reduction is carried out through a restructuring algorithm, effectively keeping consistency between source and target applications from the perspective of the model-view-controller (MVC) architecture, because the problem requires the target application to preserve the observable behavior of its source application. Case studies show that our restructuring tool can efficiently support the restructuring process."}
{"_id":"e321c0f7ccc738d3ad00ea92e9cf16a32a7aa071","title":"High speed cascode flyback converter using multilayered coreless printed circuit board (PCB) step-down power transformer","text":"In this paper, design and analysis of the high speed isolated cascode flyback converter using multilayered coreless PCB step down power transformer is presented. The converter is tested for the input voltage variation of 60\u2013120V with a nominal DC input voltage of 90V. The designed converter was simulated and tested successfully up to the output power level of 30W within the switching frequency range of 2.6\u20133.7MHz. The cascode flyback converter is compared with the single switch flyback converter in terms of operating frequency, gate drive power consumption, conduction losses and stresses on MOSFETs. The maximum energy efficiency of the cascode converter is approximately 81% with a significant improvement of about 3\u20134% compared to single switch flyback converter. The gate drive power consumption which is more dominant compared to conduction losses of the cascode converter using GaN MOSFET is found to be negligible compared to single switch flyback converter."}
{"_id":"43f87f70c86398ecd2093723f8fb9d5024a57b68","title":"Optimal integration of energy storage in distribution networks","text":"Energy storage, traditionally well established in the form of large scale pumped-hydro systems, is finding increased attraction in medium and smaller scale systems. Such expansion is entirely complementary to the wider uptake of intermittent renewable resources and to distributed generation in general, which are likely to present a whole range of new business opportunities for storage systems and their suppliers. In the paper, by assuming that Distribution System Operator has got the ownership and operation of storage, a new software planning tool for distribution networks able to define the optimal placement, rating and control strategies of distributed storage systems that minimize the overall network cost is proposed. This tool will assist the System Operators in defining the better integration strategies of distributed storage systems in distribution networks and in assessing their potential as an option for a more efficient operation and development of future electricity distribution networks."}
{"_id":"895719a72d72cfaf4b47d95c474a708934b9d42d","title":"ACES: Automatic Compartments for Embedded Systems","text":"Securing the rapidly expanding Internet of Things (IoT) is critical. Many of these \u201cthings\u201d are vulnerable baremetal embedded systems where the application executes directly on hardware without an operating system. Unfortunately, the integrity of current systems may be compromised by a single vulnerability, as recently shown by Google\u2019s P0 team against Broadcom\u2019s WiFi SoC. We present ACES (Automatic Compartments for Embedded Systems), an LLVM-based compiler that automatically infers and enforces inter-component isolation on bare-metal systems, thus applying the principle of least privileges. ACES takes a developer-specified compartmentalization policy and then automatically creates an instrumented binary that isolates compartments at runtime, while handling the hardware limitations of baremetal embedded devices. We demonstrate ACES\u2019 ability to implement arbitrary compartmentalization policies by implementing three policies and comparing the compartment isolation, runtime overhead, and memory overhead. Our results show that ACES\u2019 compartments can have low runtime overheads (13% on our largest test application), while using 59% less Flash, and 84% less RAM than the Mbed \u03bcVisor\u2014the current state-of-theart compartmentalization technique for bare-metal systems. ACES \u2018 compartments protect the integrity of privileged data, provide control-flow integrity between compartments, and reduce exposure to ROP attacks by 94.3% compared to \u03bcVisor."}
{"_id":"268a88fedcf949ffda3bc0f5573ad5f1c8b0c29d","title":"On the speedup of single-disk failure recovery in XOR-coded storage systems: Theory and practice","text":"Modern storage systems stripe redundant data across multiple disks to provide availability guarantees against disk failures. One form of data redundancy is based on XOR-based erasure codes, which use only XOR operations for encoding and decoding. In addition to providing failure tolerance, a storage system must also provide fast failure recovery to avoid data unavailability. We consider the problem of speeding up the recovery of a single-disk failure for arbitrary XOR-based erasure codes. We address this problem from both theoretical and practical perspectives. We propose a replace recovery algorithm, which uses a hill-climbing technique to search for a fast recovery solution, such that the solution search can be completed within a short time period. We further implement our replace recovery algorithm atop a parallelized architecture to justify its practicality. We experiment our replace recovery algorithm and its parallelized implementation on a networked storage system testbed, and demonstrate that our replace recovery algorithm uses less recovery time than the conventional approach."}
{"_id":"c7f0ecde0907abfe033d0b347c62ec2b5761043a","title":"Internet of Cloud: Security and Privacy issues","text":"The synergy between the cloud and the IoT has emerged largely due to the cloud having attributes which directly benefit the IoT and enable its continued growth. IoT adopting Cloud services has brought new security challenges. In this book chapter, we pursue two main goals: 1) to analyse the different components of Cloud computing and the IoT and 2) to present security and privacy problems that these systems face. We thoroughly investigate current security and privacy preservation solutions that exist in this area, with an eye on the Industrial Internet of Things, discuss open issues and propose future directions."}
{"_id":"def29939ce675ae9d9d855c661cbf85e34a1daaf","title":"Detecting Clickbait in Online Social Media: You Won't Believe How We Did It","text":"In this paper, we propose an approach for the detection of clickbait posts in online social media (OSM). Clickbait posts are short catchy phrases that attract a user\u2019s attention to click to an article. The approach is based on a machine learning (ML) classifier capable of distinguishing between clickbait and legitimate posts published in OSM. The suggested classifier is based on a variety of features, including image related features, linguistic analysis, and methods for abuser detection. In order to evaluate our method, we used two datasets provided by Clickbait Challenge 2017. The best performance obtained by the ML classifier was an AUC of 0.8, accuracy of 0.812, precision of 0.819, and recall of 0.966. In addition, as opposed to previous studies, we found that clickbait post titles are statistically significant shorter than legitimate post titles. Finally, we found that counting the number of formal English words in the given contentis useful for clickbait detection."}
{"_id":"3b632a49509c0c46ab0e0c0780b2170524f7c0ac","title":"Demand Response as a Market Resource Under the Smart Grid Paradigm","text":"Demand response (DR), distributed generation (DG), and distributed energy storage (DES) are important ingredients of the emerging smart grid paradigm. For ease of reference we refer to these resources collectively as distributed energy resources (DER). Although much of the DER emerging under smart grid are targeted at the distribution level, DER, and more specifically DR resources, are considered important elements for reliable and economic operation of the transmission system and the wholesale markets. In fact, viewed from transmission and wholesale operations, sometimes the term \u00bfvirtual power plant\u00bf is used to refer to these resources. In the context of energy and ancillary service markets facilitated by the independent system operators (ISOs)\/regional transmission organizations (RTOs), the market products DER\/DR can offer may include energy, ancillary services, and\/or capacity, depending on the ISO\/RTO market design and applicable operational standards. In this paper we first explore the main industry drivers of smart grid and the different facets of DER under the smart grid paradigm. We then concentrate on DR and summarize the existing and evolving programs at different ISOs\/RTOs and the product markets they can participate in. We conclude by addressing some of the challenges and potential solutions for implementation of DR under smart grid and market paradigms."}
{"_id":"af954a7a097abaa828bc2b4080c2d8c1b6acb853","title":"A survey of communication\/networking in Smart Grids","text":"Smart Grid is designed to integrate advanced communication\/networking technologies into electrical power grids to make them \u2018\u2018smarter\u2019\u2019. Current situation is that most of the blackouts and voltage sags could be prevented if we have better and faster communication devices and technologies for the electrical grid. In order to make the current electrical power grid a Smart Grid, the design and implementation of a new communication infrastructure for the grid are two important fields of research. However, Smart Grid projects have only been proposed in recent years and only a few proposals for forwardlooking requirements and initial research work have been offered in this field. No any systematic reviews of communication\/networking in Smart Grids have been conducted yet. Therefore, we conduct a systematic review of communication\/networking technologies in Smart Grid in this paper, including communication\/networking architecture, different communication technologies thatwould be employed into this architecture, quality of service (QoS), optimizing utilization of assets, control and management, etc. \u00a9 2011 Elsevier B.V. All rights reserved."}
{"_id":"b746fc72c0bd3f0a94145e375cc267e1128ba32e","title":"An Optimal Power Scheduling Method for Demand Response in Home Energy Management System","text":"With the development of smart grid, residents have the opportunity to schedule their power usage in the home by themselves for the purpose of reducing electricity expense and alleviating the power peak-to-average ratio (PAR). In this paper, we first introduce a general architecture of energy management system (EMS) in a home area network (HAN) based on the smart grid and then propose an efficient scheduling method for home power usage. The home gateway (HG) receives the demand response (DR) information indicating the real-time electricity price that is transferred to an energy management controller (EMC). With the DR, the EMC achieves an optimal power scheduling scheme that can be delivered to each electric appliance by the HG. Accordingly, all appliances in the home operate automatically in the most cost-effective way. When only the real-time pricing (RTP) model is adopted, there is the possibility that most appliances would operate during the time with the lowest electricity price, and this may damage the entire electricity system due to the high PAR. In our research, we combine RTP with the inclining block rate (IBR) model. By adopting this combined pricing model, our proposed power scheduling method would effectively reduce both the electricity cost and PAR, thereby, strengthening the stability of the entire electricity system. Because these kinds of optimization problems are usually nonlinear, we use a genetic algorithm to solve this problem."}
{"_id":"d117ab7678e040d403f381505c22f72cb4c2b5ed","title":"Optimal Residential Load Control With Price Prediction in Real-Time Electricity Pricing Environments","text":"Real-time electricity pricing models can potentially lead to economic and environmental advantages compared to the current common flat rates. In particular, they can provide end users with the opportunity to reduce their electricity expenditures by responding to pricing that varies with different times of the day. However, recent studies have revealed that the lack of knowledge among users about how to respond to time-varying prices as well as the lack of effective building automation systems are two major barriers for fully utilizing the potential benefits of real-time pricing tariffs. We tackle these problems by proposing an optimal and automatic residential energy consumption scheduling framework which attempts to achieve a desired trade-off between minimizing the electricity payment and minimizing the waiting time for the operation of each appliance in household in presence of a real-time pricing tariff combined with inclining block rates. Our design requires minimum effort from the users and is based on simple linear programming computations. Moreover, we argue that any residential load control strategy in real-time electricity pricing environments requires price prediction capabilities. This is particularly true if the utility companies provide price information only one or two hours ahead of time. By applying a simple and efficient weighted average price prediction filter to the actual hourly-based price values used by the Illinois Power Company from January 2007 to December 2009, we obtain the optimal choices of the coefficients for each day of the week to be used by the price predictor filter. Simulation results show that the combination of the proposed energy consumption scheduling design and the price predictor filter leads to significant reduction not only in users' payments but also in the resulting peak-to-average ratio in load demand for various load scenarios. Therefore, the deployment of the proposed optimal energy consumption scheduling schemes is beneficial for both end users and utility companies."}
{"_id":"15da2e12df4168a5afe6bb897e1f52a47451b0cd","title":"A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks","text":"We present the soft exponential activation function for artificial neural networks that continuously interpolates between logarithmic, linear, and exponential functions. This activation function is simple, differentiable, and parameterized so that it can be trained as the rest of the network is trained. We hypothesize that soft exponential has the potential to improve neural network learning, as it can exactly calculate many natural operations that typical neural networks can only approximate, including addition, multiplication, inner product, distance, and sinusoids."}
{"_id":"e78ac6617fee67cfb981423cb6d42526b51bb9db","title":"Low-Resolution Face Recognition of Multi-Scale Blocking CS-LBP and Weighted PCA","text":"A novel method is proposed in this paper to improve the recognition accuracy of Local Binary Pattern (LBP) on low-resolution face recognition. More precise descriptors and e\u00aeectively face features can be extracted by combining multi-scale blocking center symmetric local binary pattern (CS-LBP) based on Gaussian pyramids and weighted principal component analysis (PCA) on low-resolution condition. Firstly, the features statistical histograms of face images are calculated by multi-scale blocking CS-LBP operator. Secondly, the stronger classi \u0304cation and lower dimension features can be got by applying weighted PCA algorithm. Finally, the di\u00aeerent classi \u0304ers are used to select the optimal classi \u0304cation categories of low-resolution face set and calculate the recognition rate. The results in the ORL human face databases show that recognition rate can get 89.38% when the resolution of face image drops to 12 10 pixel and basically satisfy the practical requirements of recognition. The further comparison of other descriptors and experiments from videos proved that the novel algorithm can improve recognition accuracy."}
{"_id":"290f6b98d15800753329b156560c32ffae9ba166","title":"Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems","text":"This paper proposes a new dataset, Frames, composed of 1369 human-human dialogues with an average of 15 turns per dialogue. This corpus contains goal-oriented dialogues between users who are given some constraints to book a trip and assistants who search a database to find appropriate trips. The users exhibit complex decision-making behaviour which involve comparing trips, exploring different options, and selecting among the trips that were discussed during the dialogue. To drive research on dialogue systems towards handling such behaviour, we have annotated and released the dataset and we propose in this paper a task called frame tracking. This task consists of keeping track of different semantic frames throughout each dialogue. We propose a rule-based baseline and analyse the frame tracking task through this baseline."}
{"_id":"14815c67e4d215acf9558950e2762759229fe277","title":"Beyond 'Caveman Communities': Hubs and Spokes for Graph Compression and Mining","text":"Given a real world graph, how should we lay-out its edges? How can we compress it? These questions are closely related, and the typical approach so far is to find clique-like communities, like the `cavemen graph', and compress them. We show that the block-diagonal mental image of the `cavemen graph' is the wrong paradigm, in full agreement with earlier results that real world graphs have no good cuts. Instead, we propose to envision graphs as a collection of hubs connecting spokes, with super-hubs connecting the hubs, and so on, recursively. Based on the idea, we propose the Slash Burn method (burn the hubs, and slash the remaining graph into smaller connected components). Our view point has several advantages: (a) it avoids the `no good cuts' problem, (b) it gives better compression, and (c) it leads to faster execution times for matrix-vector operations, which are the back-bone of most graph processing tools. Experimental results show that our Slash Burn method consistently outperforms other methods on all datasets, giving good compression and faster running time."}
{"_id":"cced0f6594f90a1702ebefd233daea4af36ded5e","title":"Advanced universal remote controller for home automation and security","text":"There has been inconvenience in controlling each digital home appliance which requires its own remote controller. In this paper, we present an advanced universal remote controller (URC) with the total solution for home automation and security. All kinds of home appliances can be controlled with the URC, which can be also connected to a PC dealing with Internet as well. To use the URC, we need several receivers with wired or wireless communication methods to be connected to all appliances. The receivers have many channels and IDs to control many appliances at the same time and to support multi-zone services. In addition, we propose a PC-based interface for end-users to use the URC conveniently. With the proposed URC, we can easily construct a ubiquitous home automation and security environment with the total solution. Furthermore, this solution can be applied to the automated control of all kinds of appliances installed within buildings for companies, schools, hospitals, and so on."}
{"_id":"69f955a0a43b79790a061cca5470abebde577c06","title":"LSDSCC: a Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity Oriented Evaluation Metrics","text":"It has been proven that automatic conversational agents can be built up using the Endto-End Neural Response Generation (NRG) framework, and such a data-driven methodology requires a large number of dialog pairs for model training and reasonable evaluation metrics for testing. This paper proposes a Large Scale Domain-Specific Conversational Corpus (LSDSCC) composed of high-quality queryresponse pairs extracted from the domainspecific online forum, with thorough preprocessing and cleansing procedures. Also, a testing set, including multiple diverse responses annotated for each query, is constructed, and on this basis, the metrics for measuring the diversity of generated results are further presented. We evaluate the performances of neural dialog models with the widely applied diversity boosting strategies on the proposed dataset. The experimental results have shown that our proposed corpus can be taken as a new benchmark dataset for the NRG task, and the presented metrics are promising to guide the optimization of NRG models by quantifying the diversity of the generated responses reasonably."}
{"_id":"1057a289a1438a6a7caccb84d4fff7daa7b779d9","title":"Litz wire design for wireless power transfer in electric vehicles","text":"Eddy current losses in wireless charging systems for electric vehicles must be minimized in consideration of constraints like the available space, the strand diameter in litz wire and the cost. A mathematical model for eddy current losses is needed to find the optimum geometrical design for a given application. This paper gives the fundamental equations to calculate skin effect, proximity effect and dc losses in litz wire. Litz wire consists of strands, which are twisted to form multiple bundles. Therefore eddy current losses occur not only on the strand level but the bundle level as well. The losses caused by different effects and bundle levels are compared, and the influence of strand diameter and strand number on the total loss is examined. Finally, aluminum litz wire is analyzed. It becomes apparent that the overall loss can be reduced with the aid of replacing copper litz wire by aluminum in some areas of application. Material choice and geometrical design require detailed knowledge of the systems nominal operation point."}
{"_id":"760206dbc361642fc82eaf254d61d68b3ff0eedf","title":"The New DLR Flight Dynamics Library Gertjan Looye","text":"An overview of the new Modelica Flight Dynamics Library of the German Aerospace Center DLR is given. This library is intended for construction of multi-disciplinary flight dynamics models of rigid and flexible flight vehicles. The environment models provide the functionality to cover on-ground operations up to flight at high speeds and high altitudes. The resulting models may be used in various fields and stages of the aircraft development process, like flight control law design, as well as for real-time flight simulation."}
{"_id":"cd8db517b9a73274c0d69c39db44d4be9bd57f0c","title":"The Marulan Data Sets: Multi-sensor Perception in a Natural Environment with Challenging Conditions","text":"This paper presents large, accurately calibrated and timesynchronised data sets, gathered outdoors in controlled and variable environmental conditions, using an unmanned ground vehicle (UGV), equipped with a wide variety of sensors. These include four 2D laser scanners, a radar scanner, a colour camera and an infrared camera. It provides a full description of the system used for data collection and the types of environments and conditions in which these data sets have been gathered, which include the presence of airborne dust, smoke and rain."}
{"_id":"c3fc1ef004edf47d494bae14cb5f0bd5f663a222","title":"Boosting in the presence of label noise","text":"Boosting is known to be sensitive to label noise. We studied two approaches to improve AdaBoost\u2019s robustness against labelling errors. One is to employ a label-noise robust classifier as a base learner, while the other is to modify the AdaBoost algorithm to be more robust. Empirical evaluation shows that a committee of robust classifiers, although converges faster than non label-noise aware AdaBoost, is still susceptible to label noise. However, pairing it with the new robust Boosting algorithm we propose here results in a more resilient algorithm under mislabelling."}
{"_id":"b46b72c02035a80afbcb9d072405661be5f48b31","title":"Seaweed extract improve drought tolerance of soybean by regulating stress-response genes","text":"There is an increasing global concern about the availability of water for agricultural use. Drought stress negatively impacts plant physiology and crop productivity. Soybean (Glycine max) is one of the important oilseed crops, and its productivity is often reduced by drought. In this study, a commercial extract of Ascophyllum nodosum (ANE) was evaluated for its potential to alleviate drought stress in soybean. The aim of this study was to determine the effects of ANE on the response of soybean plants to drought stress by monitoring stomatal conductance, relative leaf water content, antioxidant activity and expression of stress-responsive genes. Plants treated with ANE had higher relative water content and higher stomatal conductance under drought stress. During early recovery in the post-drought phase, ANE treated plants had significantly higher stomatal conductance. The antioxidant activity was also found higher in the plants treated with ANE. In addition, ANE-treatment led to changes in the expression of stress-responsive genes: GmCYP707A1a, GmCYP707A3b, GmRD22, GmRD20, GmDREB1B, GmERD1, GmNFYA3, FIB1a, GmPIP1b, GmGST, GmBIP and GmTp55. Taken together, these results suggest that applications of ANE improve the drought tolerance of soybean by changing physiology and gene expression."}
{"_id":"ddb98f69b500336525c7cf73ec7ac5a9bbc417dc","title":"Efficient detail-enhanced exposure correction based on auto-fusion for LDR image","text":"We consider the problem of how to simultaneously and well correct the over- and under-exposure regions in a single low dynamic range (LDR) image. Recent methods typically focus on global visual quality but cannot well-correct much potential details in extremely wrong exposure areas, and some are also time consuming. In this paper, we propose a fast and detail-enhanced correction method based on automatic fusion which combines a pair of complementarily corrected images, i.e. backlight & highlight correction images (BCI &HCI). A BCI with higher visual quality in details is quickly produced based on a proposed faster multi-scale retinex algorithm; meanwhile, a HCI is generated through contrast enhancement method. Then, an automatic fusion algorithm is proposed to create a color-protected exposure mask for fusing BCI and HCI when avoiding potential artifacts on the boundary. The experiment results show that the proposed method can fast correct over\/under-exposed regions with higher detail quality than existing methods."}
{"_id":"fb153ce56e0040214146809cd8c16c0e171e43e6","title":"Minimum PCB footprint point-of-load DC-DC converter realized with Switched-Capacitor architecture","text":"This work reports on the design and test of a CMOS-based Switched Capacitor (SC) DC-DC conversion integrated circuit (IC) for the point-of-load (POL) application conventionally addressed with the buck converter. A 12V-to-1.5V converter is fabricated in a 0.18\u00b5m technology, with an active die area of 3mm2. There is a significant reduction in printed circuit board (PCB) footprint, passive component height and cost when compared to surveyed buck converters. The converter achieves 93% peak efficiency and an efficiency of 80% over an output current range of 7mA to 1A. This work demonstrates the vast potential of SC converters for wide-range voltage conversion in deep-submicron CMOS technologies, when cost and efficiency are of critical importance."}
{"_id":"dc442551c2e59f8bf09e1bddd8500bb5b2176d6f","title":"Towards automatic assessment of government web sites","text":"This paper presents an approach for automatic assessment of web sites in large scale e-Government surveys. The approach aims at supplementing and to some extent replacing human evaluation which is typically the core part of these surveys.\n The heart of the solution is a colony inspired algorithm, called the lost sheep, which automatically locates targeted governmental material online. The algorithm centers around classifying link texts to determine if a web page should be downloaded for further analysis.\n The proposed algorithm is designed to work with minimum human interaction and utilize the available resources as best possible. Using the lost sheep, the people carrying out a survey will only provide sample data for a few web sites for each type of material sought after. The algorithm will automatically locate the same type of material in the other web sites part of the survey. This way it significantly reduces the need for manual work in large scale e-Government surveys."}
{"_id":"0b52e925b8164cc39a4ec222e128da8f216ceb28","title":"A parallel and efficient approach to large scale clone detection","text":"Over the past few years, researchers have implemented various algorithms to improve the scalability of clone detection. Most of these algorithms focus on scaling vertically on a single machine, and require complex intermediate data structures (e.g., suffix tree, etc.). However, several new use-cases of clone detection have emerged, which are beyond the computational capacity of a single machine. Moreover, for some of these use-cases it may be expensive to invest upfront in the cost of building these data structures.\n In this paper, we propose a technique to horizontally scale clone detection across multiple machines using the popular MapReduce framework. The technique does not require building any complex intermediate data structures. Moreover, in order to increase the efficiency, the technique uses a filtering heuristic to prune the number of code block comparisons. The filtering heuristic is independent of our approach and it can be used in conjunction with other approaches to increase their efficiency. In our experiments, we found that: (i) the computation time to detect clones decreases by almost half every time we double the number of nodes; and (ii) the scaleup is linear, with a decline of not more than 70% compared to the ideal case, on a cluster of 2-32 nodes for 150-2800 projects."}
{"_id":"5f09cb313b6fb14877c6b5be79294faf1f4f7f02","title":"Thinking inside the Box: five Organizational Strategies Enabled through Information Systems","text":"The relationship between information systems (IS) and organizational strategies has been a much discussed topic with most of the prior studies taking a highly positive view of technology\u2019s role in enabling organizational strategies. Despite this wealth of studies, there is a dearth of empirical investigations on how IS enable specific organizational strategies. Through a qualitative empirical investigation of five case organizations this research derives five organizational strategies that are specifically enabled through IS. The five strategies; (i) generic-heartland, (ii) craft-based selective, (iii) adhoc, IT-driven, (iv) corporative-orchestrated and (v) transformative provide a unique perspective of how IS enable organizational strategy."}
{"_id":"9f4ec4dd3377612383057c3387421df65ab19cce","title":"Microstrip patch antenna array at 3.8 GHz for WiMax and UAV applications","text":"This paper presents the design of a rectangular microstrip line-fed patch antenna array with a centre frequency of 3.8 GHz for WiMAX and Unmanned Air Vehicle (UAV) applications. A single element, 1\u00d72 and 2\u00d72 microstrip rectangular patch antennas were designed and simulated in Computer Simulation Tool (CST) Microwave Studio environment. The results of designed antennas were compared in terms of Return Loss (S11 parameters), bandwidth, directivity, gain and radiation pattern. Compared to traditional microstrip antennas the proposed array structure achieved a gain and directivity of 13.2 dB and 13.5 dBi respectively. The antenna was fabricated using Rogers Duroid RT-5880 substrate with a dielectric constant er of 2.2 and a thickness of 1.574 mm respectively. The array antennas were measured in the laboratory using Vector Network Analyser (VNA) and the results show good agreement with the array antenna simulation."}
{"_id":"e19c4e1fbe2bfecba7839f934faf6be0319476b5","title":"Modeling and simulation of DC rail traction systems for energy saving","text":"The modeling and simulation of the electrified transit system is an essential element in the design process of a new railway, or an existing one being modernized, particularly in DC powered railway systems which have significant losses in the power network. With the continuing focus on environmental concerns and rising energy prices, energy-saving operation technology for railway systems has been paid more and more attention. Previous work on energy optimization techniques mainly focuses on optimizing driving strategies subject to geographic and physical constraints, and kinematic equations, which only minimizes the mechanical energy consumption without considering the loss from the power supply network. This paper proposes a DC power network modeling technique and extends the traditional energy-saving methods to develop a novel approach which combines traction power supply network calculations and numerical algorithms to minimize the electrical energy delivered from substations. As train resistance is time-varying with the train movement, iterative algorithms are presented in order to calculate the energy consumption dynamically. Some case studies based on the Beijing Yizhuang Subway Line are presented to illustrate the proposed approach for power network simulation and energy-saving, in which the energy consumption of both the practical operation and optimal operation are compared."}
{"_id":"ad384ff98f002c16ccdb8264a631068f2c3287f2","title":"Consensus in the Age of Blockchains","text":"The blockchain initially gained traction in 2008 as the technology underlying Bitcoin [105], but now has been employed in a diverse range of applications and created a global market worth over $150B as of 2017. What distinguishes blockchains from traditional distributed databases is the ability to operate in a decentralized setting without relying on a trusted third party. As such their core technical component is consensus: how to reach agreement among a group of nodes. This has been extensively studied already in the distributed systems community for closed systems, but its application to open blockchains has revitalized the field and led to a plethora of new designs. The inherent complexity of consensus protocols and their rapid and dramatic evolution makes it hard to contextualize the design landscape. We address this challenge by conducting a systematic and comprehensive study of blockchain consensus protocols. After first discussing key themes in classical consensus protocols, we describe: (i) protocols based on proof-of-work (PoW), (ii) proof-of-X (PoX) protocols that replace PoW with more energy-efficient alternatives, and (iii) hybrid protocols that are compositions or variations of classical consensus protocols. We develop a framework to evaluate their performance, security and design properties, and use it to systematize key themes in the protocol categories described above. This evaluation leads us to identify research gaps and challenges for the community to consider in future research endeavours."}
{"_id":"d60c52a77e974797b14774f22bff06faa1e03003","title":"What influences literacy outcome in children with speech sound disorder?","text":"PURPOSE\nIn this study, the authors evaluated literacy outcome in children with histories of speech sound disorder (SSD) who were characterized along 2 dimensions: broader language function and persistence of SSD. In previous studies, authors have demonstrated that each dimension relates to literacy but have not disentangled their effects. Methods Two groups of children (86 SSD and 37 controls) were recruited at ages 5-6 and were followed longitudinally. The authors report the literacy of children with SSD at ages 7-9, compared with controls and national norms, and relative to language skill and SSD persistence (both measured at age 5-6).\n\n\nRESULTS\nThe SSD group demonstrated elevated rates of reading disability. Language skill but not SSD persistence predicted later literacy. However, SSD persistence was associated with phonological awareness impairments. Phonological awareness alone predicted literacy outcome less well than a model that also included syntax and nonverbal IQ.\n\n\nCONCLUSIONS\nResults support previous literature findings that SSD history predicts literacy difficulties and that the association is strongest for SSD + language impairment (LI). Magnitude of phonological impairment alone did not determine literacy outcome, as predicted by the core phonological deficit hypothesis. Instead, consistent with a multiple deficit approach, phonological deficits appeared to interact with other cognitive factors in literacy development."}
{"_id":"bbaac3ff0eb5b661faead2748313834c9cf771cf","title":"Outline for a Logical Theory of Adaptive Systems","text":"The purpose of this paper is to outline a theory of automata appropriate to the properties, requirements and questions of adaptation. The conditions that such a theory should satisfy come from not one but several fields: It should be possible to formulate, at least in an abstract version, some of the key hypotheses and problems from relevant parts of biology, particularly the areas concerned with molecular control and neurophysiology. The work in theoretical genetics initiated by R. A. Fisher [5] and Sewall Wright [24] should find a natural place in the theory. At the same time the rigorous methods of automata theory should be brought to bear (particularly those parts concerned with growing automata [1, 2, 3, 7, 8, 12, 15, 18, 23]). Finally the theory should include among its models abstract counterparts of artificial adaptive systems currently being studied, systems such as Newell-Shaw-Simon's \"General Problem Solver\" [13], Selfridge's \"Pandemonium\" [17], von Neumann's self-reproducing automata [22] and Turing's morphogenetic systems [19, 20]. The theory outlined here (which is intended as a theory and not the theory) is presented in four main parts. Section 2 discusses the study of adaptation via generation procedures and generated populations. Section 3 defines a continuum of generation procedures realizable in a reasonably direct fashion. Section 4 discusses the realization of generation procedures as populations of interacting programs in an iterative circuit computer. Section 5 discusses the process of adaptation in the context of the earlier sections. The paper concludes with a discussion of the nature of the theorems of this theory. Before entering upon the detailed discussion, one general feature of the theory should be noted. The interpretations or models of the theory divide into two broad categories: \"complete\" models and \"incomplete\" models. The \"complete\" models comprise the artificial systems--systems with properties and specifications completely delimited at the outset (cf. the rules of a game). One set of \"complete\" models for the theory consists of various programmed parallel computers. The \"incomplete\" models encompass natural systems. Any natural system involves an unlimited number of factors and, inevitably, the theory can handle only a selected few of these. Because there will always be variables which do not have explicit counterparts in the theory, the derived statements must be approximate relative to natural systems. For this reason it helps greatly that"}
{"_id":"0f3b8eba6e536215b6f6c727a009c8b44cda0a91","title":"A Convolutional Attention Network for Extreme Summarization of Source Code","text":"Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model\u2019s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network\u2019s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms."}
{"_id":"049b30d8aaedc87c9b66dac2e607ea0cf4e87b56","title":"'C and tcc: A Language and Compiler for Dynamic Code Generation","text":"Dynamic code generation allows programmers to use run-time information in order to achieve performance and expressiveness superior to those of static code. The 'C(Tick C) language is a superset of ANSI C that supports efficient and high-level use of dynamic code generation. 'C provides dynamic code generation at the level of C expressions and statements and supports the composition of dynamic code at run time. These features enable programmers to add dynamic code generation to existing C code incrementally and to write important applications (such as \u201cjust-in-time\u201d compilers) easily. The article presents many examples of how 'C can be used to solve practical problems. The tcc compiler is an efficient, portable, and freely available implementation of 'C. tcc allows programmers to trade dynamic compilation speed for dynamic code quality: in some aplications, it is most important to generate code quickly, while in others code quality matters more than compilation speed. The overhead of dynamic compilation is on the order of 100 to 600 cycles per generated instruction, depending on the level of dynamic optimizaton. Measurements show that the use of dynamic code generation can improve performance by almost an order of magnitude; two- to four-fold speedups are common. In most cases, the overhead of dynamic compilation is recovered in under 100 uses of the dynamic code; sometimes it can be recovered within one use."}
{"_id":"83cfac5ff6c789089b852c11106eb44500567078","title":"The dark side of smartphone usage: Psychological traits, compulsive behavior and technostress","text":"0747-5632\/$ see front matter 2013 Elsevier Ltd. All rights reserved. http:\/\/dx.doi.org\/10.1016\/j.chb.2013.10.047 \u21d1 Corresponding author. Address: Department of Business Management, National Sun Yat-sen University, No. 70, Lianhai Rd., Gushan District, Kaohsiung City 804, Taiwan, Tel.: +886 7 525 2000x4627. E-mail address: ctchang@faculty.nsysu.edu.tw (C.-T. Chang). Yu-Kang Lee, Chun-Tuan Chang \u21d1, You Lin, Zhao-Hong Cheng"}
{"_id":"89d1b0072e6d3ff476cef0ad5224786851ae144f","title":"Fourier Lucas-Kanade Algorithm","text":"In this paper, we propose a framework for both gradient descent image and object alignment in the Fourier domain. Our method centers upon the classical Lucas &#x0026; Kanade (LK) algorithm where we represent the source and template\/model in the complex 2D Fourier domain rather than in the spatial 2D domain. We refer to our approach as the Fourier LK (FLK) algorithm. The FLK formulation is advantageous when one preprocesses the source image and template\/model with a bank of filters (e.g., oriented edges, Gabor, etc.) as 1) it can handle substantial illumination variations, 2) the inefficient preprocessing filter bank step can be subsumed within the FLK algorithm as a sparse diagonal weighting matrix, 3) unlike traditional LK, the computational cost is invariant to the number of filters and as a result is far more efficient, and 4) this approach can be extended to the Inverse Compositional (IC) form of the LK algorithm where nearly all steps (including Fourier transform and filter bank preprocessing) can be precomputed, leading to an extremely efficient and robust approach to gradient descent image matching. Further, these computational savings translate to nonrigid object alignment tasks that are considered extensions of the LK algorithm, such as those found in Active Appearance Models (AAMs)."}
{"_id":"a0bbd8ca2a7d881158a71ee6b6b1b586fe254a44","title":"Sensor Reduction for Driver-Automation Shared Steering Control via an Adaptive Authority Allocation Strategy","text":"This paper presents a new shared control method for lane keeping assist (LKA) systems of intelligent vehicles. The proposed method allows the LKA system to effectively share the control authority with a human driver by avoiding or minimizing the conflict situations between these two driving actors. To realize the shared control scheme, the unpredictable driver-automation interaction is explicitly taken into account in the control design via a fictive driver activity variable. This latter is judiciously introduced into the driver\u2013road\u2013vehicle system to represent the driver's need for assistance in accordance with his\/her real-time driving activity. Using Lyapunov stability arguments, Takagi\u2013Sugeno fuzzy model-based design conditions are derived to handle not only the time-varying driver activity variable, but also a large variation range of vehicle speed. Both simulation and hardware experiments are presented to demonstrate that the proposed control strategy together with a linear matrix inequality design formulation provide an effective tool to deal with the challenging shared steering control issue. In particular, a fuzzy output feedback control scheme is exploited to achieve the shared control goal without at least two important vehicle sensors. These physical sensors are widely employed in previous works to measure the lateral speed and the steering rate for the control design and real-time implementation. The successful results of this idea of sensor-reduction control has an obvious interest from practical viewpoint."}
{"_id":"147ba62f2e7af3a5b85781941227132a9ec3535b","title":"Predictability, Complexity, and Learning","text":"We define predictive information Ipred(T) as the mutual information between the past and the future of a time series. Three qualitatively different behaviors are found in the limit of large observation times T: Ipred(T) can remain finite, grow logarithmically, or grow as a fractional power law. If the time series allows us to learn a model with a finite number of parameters, then Ipred(T) grows logarithmically with a coefficient that counts the dimensionality of the model space. In contrast, power-law growth is associated, for example, with the learning of infinite parameter (or non-parametric) models such as continuous functions with smoothness constraints. There are connections between the predictive information and measures of complexity that have been defined both in learning theory and the analysis of physical systems through statistical mechanics and dynamical systems theory. Furthermore, in the same way that entropy provides the unique measure of available information consistent with some simple and plausible conditions, we argue that the divergent part of Ipred(T) provides the unique measure for the complexity of dynamics underlying a time series. Finally, we discuss how these ideas may be useful in problems in physics, statistics, and biology."}
{"_id":"47f193b934e38fb3d056aa75fabaca4d60e93055","title":"Modeling Response Time in Digital Human Communication","text":"Our daily lives increasingly involve interactions with other individuals via different communication channels, such as email, text messaging, and social media. In this paper we focus on the problem of modeling and predicting how long it takes an individual to respond to an incoming communication event, such as receiving an email or a text. In particular, we explore the effect on response times of an individual\u2019s temporal pattern of activity, such as circadian and weekly patterns which are typically present in individual data. A probabilistic time-warping approach is used, considering linear time to be a transformation of \u201ceffective time,\u201d where the transformation is a function of an individual\u2019s activity rate. We apply this transformation of time to two different types of temporal event models, the first for modeling response times directly, and the second for modeling event times via a Hawkes process. We apply our approach to two different sets of real-world email histories. The experimental results clearly indicate that the transformation-based approach produces systematically better models and predictions compared to simpler methods that ignore circadian and weekly patterns. Current technology allows us to collect large quantities of time-stamped individual-level event data characterizing our \u201cdigital behavior\u201d in contexts such as texting, email activity, microblogging, social media interactions, and more \u2014 and the volume and variety of this type of data is continually increasing. The resulting time-series of events are rich in behavioral information about our daily lives. Tools for obtaining and visualizing such information are becoming increasingly popular, such as the ability to download your entire email history for mail applications such as Gmail, and various software packages for tracking personal fitness using data from devices such as Fitbit. This paper is focused on modeling the temporal aspects of how an individual (also referred to as the \u201cego\u201d) responds to others, given a sequence of timestamped events (e.g. communication messages via email or text). What can we learn from the way we respond to others? Are there systematic \u2217Currently employed at Google. The research described in this paper was conducted while the author was a graduate student at UC Irvine. Copyright c \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Mon Tue Wed Thu Fri Sat Sun 0 1 a (t ) Mon Tue Wed Thu Fri Sat Sun 0 1 a (t ) Figure 1: Example of smoothed daily and weekly patterns over two separate individuals\u2019 email histories. a(t) on the yaxis represents an individual\u2019s typical activity as a function of time. patterns that can be extracted and used predictively? How do our daily sleep and work patterns factor in to our response patterns? These are common issues that arise when modeling communication events, as the ego\u2019s behavior typically changes significantly over the course of a day or week. Examples of such patterns are shown in Figure 1. We propose a novel method for parameterizing these circadian (daily) and weekly patterns, allowing the time dimension to be transformed according to such patterns. This transformation of time will allow models to describe and predict behavioral patterns which are invariant to the ego\u2019s routine patterns. We apply this approach to predicting the ego\u2019s response time to an event. Learning such response patterns is useful not only for identifying relationships between pairs of individuals (Halpin and De Boeck 2013), but also as features for prioritization of events, e.g., the priority inbox implemented in Gmail (Aberdeen, Pacovsky, and Slater 2010). Our experimental results show clear advantages in terms of predictive power when modeling response times in the transformed time domain. \u03c40 \u03c41 \u03c42 t0 t1 t2 \u22060"}
{"_id":"ac29f5feff6c2bab6fc5147fd8cb6d327e1dc1d5","title":"An empirical investigation of mobile ticketing service adoption in public transportation","text":"In this paper, we present results from a study of mobile ticketing service adoption in public transportation. The theoretical background of the study is based on technology adoption and trust theories, which are augmented with concepts of mobile use context and mobility. Our empirical findings from analyses of a survey data suggest that compatibility of the mobile ticketing service with consumer behavior is a major determinant of adoption. Mobility and contextual factors, including budget constraints, availability of other alternatives, and time pressure in the service use situation were also found to have a strong effect on the adoption decision. Our findings suggest that contextual and mobile service-specific features are important determinants of mobile service adoption and should thus be integrated into the traditional adoption models."}
{"_id":"26c2d005898b5055420a31d96b73d7b4830ba3c4","title":"The obstacle-restriction method for robot obstacle avoidance in difficult environments","text":"This paper addresses the obstacle avoidance problem in difficult scenarios that usually are dense, complex and cluttered. The proposal is a method called the obstacle-restriction. At each iteration of the control cycle, this method addresses the obstacle avoidance in two steps. First there is procedure to compute instantaneous subgoals in the obstacle structure (obtained by the sensors). The second step associates a motion restriction to each obstacle, which are managed next to compute the most promising motion direction. The advantage of this technique is that it avoids common limitations of previous obstacle avoidance methods, improving their navigation performance in difficult scenarios. Furthermore, we obtain similar results to the recent methods that achieve navigation in troublesome scenarios. However, the new method improves their behavior in open spaces. The performance of this method is illustrated with experimental results obtained with a robotic wheelchair vehicle."}
{"_id":"40f5d252394c8dd98dd34c2a3d047da28d1636eb","title":"Perceived aggressiveness predicts fighting performance in mixed-martial-arts fighters.","text":"Accurate assessment of competitive ability is a critical component of contest behavior in animals, and it could be just as important in human competition, particularly in human ancestral populations. Here, we tested the role that facial perception plays in this assessment by investigating the association between both perceived aggressiveness and perceived fighting ability in fighters' faces and their actual fighting success. Perceived aggressiveness was positively associated with the proportion of fights won, after we controlled for the effect of weight, which also independently predicted perceived aggression. In contrast, perception of fighting ability was confounded by weight, and an association between perceived fighting ability and actual fighting success was restricted to heavyweight fighters. Shape regressions revealed that aggressive-looking faces are generally wider and have a broader chin, more prominent eyebrows, and a larger nose than less aggressive-looking faces. Our results indicate that perception of aggressiveness and fighting ability might cue different aspects of success in male-male physical confrontation."}
{"_id":"6eca84343280362cfa9fe227921a316533adaa49","title":"Positive psychology : Past , present , and ( possible ) future","text":"What is positive psychology? Where has it come from? Where is it going? These are the questions we address in this article. In defining positive psychology, we distinguish between the meta-psychological level, where the aim of positive psychology is to redress the imbalance in psychology research and practice, and the pragmatic level, which is concerned with what positive psychologists do, in terms of their research, practice, and areas of interest. These distinctions in how we understand positive psychology are then used to shape conceptions of possible futures for positive psychology. In conclusion, we identify several pertinent issues for the consideration of positive psychology as it moves forward. These include the need to synthesize the positive and negative, build on its historical antecedents, integrate across levels of analysis, build constituency with powerful stakeholders, and be aware of the implications of description versus prescription."}
{"_id":"d8354a6d188a9bfca01586a5467670650f3e3a8a","title":"Design and Optimization of Printed Spiral Coils for Efficient Transcutaneous Inductive Power Transmission","text":"The next generation of implantable high-power neuroprosthetic devices such as visual prostheses and brain computer interfaces are going to be powered by transcutaneous inductive power links formed between a pair of printed spiral coils (PSC) that are batch-fabricated using micromachining technology. Optimizing the power efficiency of the wireless link is imperative to minimize the size of the external energy source, heating dissipation in the tissue, and interference with other devices. Previous design methodologies for coils made of 1-D filaments are not comprehensive and accurate enough to consider all geometrical aspects of PSCs with planar 3-D conductors as well as design constraints imposed by implantable device application and fabrication technology. We have outlined the theoretical foundation of optimal power transmission efficiency in an inductive link, and combined it with semi-empirical models to predict parasitic components in PSCs. We have used this foundation to devise an iterative PSC design methodology that starts with a set of realistic design constraints and ends with the optimal PSC pair geometries. We have executed this procedure on two design examples at 1 and 5 MHz achieving power transmission efficiencies of 41.2% and 85.8%, respectively, at 10-mm spacing. All results are verified with simulations using a commercial field solver (HFSS) as well as measurements using PSCs fabricated on printed circuit boards."}
{"_id":"585dcc735dc74e50c7a39f3c02f5ed36f3ef8fd1","title":"Mapping distributed brain function and networks with diffuse optical tomography.","text":"Mapping of human brain function has revolutionized systems neuroscience. However, traditional functional neuroimaging by positron emission tomography or functional magnetic resonance imaging cannot be used when applications require portability, or are contraindicated because of ionizing radiation (positron emission tomography) or implanted metal (functional magnetic resonance imaging). Optical neuroimaging offers a non-invasive alternative that is radiation free and compatible with implanted metal and electronic devices (for example, pacemakers). However, optical imaging technology has heretofore lacked the combination of spatial resolution and wide field of view sufficient to map distributed brain functions. Here, we present a high-density diffuse optical tomography imaging array that can map higher-order, distributed brain function. The system was tested by imaging four hierarchical language tasks and multiple resting-state networks including the dorsal attention and default mode networks. Finally, we imaged brain function in patients with Parkinson's disease and implanted deep brain stimulators that preclude functional magnetic resonance imaging."}
{"_id":"77faf16660b785f565778ac920dc1f94e06c252c","title":"On the Performance of Intel SGX","text":"As cloud computing is widely used in various fields, more and more individuals and organizations are considering outsourcing data to the public cloud. However, the security of the cloud data has become the most prominent concern of many customers, especially those who possess a large volume of valuable and sensitive data. Although some technologies like Homomorphic Encryption were proposed to solve the problem of secure data, the result is still not satisfying. With the advent of Intel SGX processor, which aims to thoroughly eliminate the security concern of cloud environment in a hardware-assisted approach, it brings us a number of questions on its features and its practicability for the current cloud platform. To evaluate the potential impact of Intel SGX, we analyzed the current SGX programming mode and inferred some possible factors that may arise the overhead. To verify our performance hypothesis, we conducted a systematic study on SGX performance by a series of benchmark experiments. After analyzing the experiment result, we performed a workload characterization to help programmer better exploit the current availability of Intel SGX and identify feasible research directions."}
{"_id":"400f329c0c411507285cc801c1aee8e49f6329e3","title":"A Generative Modeling Approach to Limited Channel ECG Classification","text":"Processing temporal sequences is central to a variety of applications in health care, and in particular multichannel Electrocardiogram (ECG) is a highly prevalent diagnostic modality that relies on robust sequence modeling. While Recurrent Neural Networks (RNNs) have led to significant advances in automated diagnosis with time-series data, they perform poorly when models are trained using a limited set of channels. A crucial limitation of existing solutions is that they rely solely on discriminative models, which tend to generalize poorly in such scenarios. In order to combat this limitation, we develop a generative modeling approach to limited channel ECG classification. This approach first uses a Seq2Seq model to implicitly generate the missing channel information, and then uses the latent representation to perform the actual supervisory task. This decoupling enables the use of unsupervised data and also provides highly robust metric spaces for subsequent discriminative learning. Our experiments with the Physionet dataset clearly evidence the effectiveness of our approach over standard RNNs in disease prediction."}
{"_id":"540933a2858483f2e71eea58026d011fb4bba040","title":"A Boosted Bayesian Multiresolution Classifier for Prostate Cancer Detection From Digitized Needle Biopsies","text":"Diagnosis of prostate cancer (CaP) currently involves examining tissue samples for CaP presence and extent via a microscope, a time-consuming and subjective process. With the advent of digital pathology, computer-aided algorithms can now be applied to disease detection on digitized glass slides. The size of these digitized histology images (hundreds of millions of pixels) presents a formidable challenge for any computerized image analysis program. In this paper, we present a boosted Bayesian multiresolution (BBMR) system to identify regions of CaP on digital biopsy slides. Such a system would serve as an important preceding step to a Gleason grading algorithm, where the objective would be to score the invasiveness and severity of the disease. In the first step, our algorithm decomposes the whole-slide image into an image pyramid comprising multiple resolution levels. Regions identified as cancer via a Bayesian classifier at lower resolution levels are subsequently examined in greater detail at higher resolution levels, thereby allowing for rapid and efficient analysis of large images. At each resolution level, ten image features are chosen from a pool of over 900 first-order statistical, second-order co-occurrence, and Gabor filter features using an AdaBoost ensemble method. The BBMR scheme, operating on 100 images obtained from 58 patients, yielded: 1) areas under the receiver operating characteristic curve (AUC) of 0.84, 0.83, and 0.76, respectively, at the lowest, intermediate, and highest resolution levels and 2) an eightfold savings in terms of computational time compared to running the algorithm directly at full (highest) resolution. The BBMR model outperformed (in terms of AUC): 1) individual features (no ensemble) and 2) a random forest classifier ensemble obtained by bagging multiple decision tree classifiers. The apparent drop-off in AUC at higher image resolutions is due to lack of fine detail in the expert annotation of CaP and is not an artifact of the classifier. The implicit feature selection done via the AdaBoost component of the BBMR classifier reveals that different classes and types of image features become more relevant for discriminating between CaP and benign areas at different image resolutions."}
{"_id":"8acf78df5aa283f02d3805867e1dd1c6a97f389b","title":"Innovation and practice of continuous auditing","text":"Article history: Received 27 August 2010 Received in revised form 23 December 2010 Accepted 6 January 2011 The traditional audit paradigm is outdated in the real time economy. Innovation of the traditional audit process is necessary to support real time assurance. Practitioners and academics are exploring continuous auditing as a potential successor to the traditional audit paradigm. Using technology and automation, continuous auditing methodology enhances the efficiency and effectiveness of the audit process to support real time assurance. This paper defines how continuous auditing methodology introduces innovation to practice in seven dimensions and proposes a four-stage paradigm to advance future research. In addition, we formulate a set of methodological propositions concerning the future of assurance for practitioners and academic researchers. \u00a9 2011 Elsevier Inc. All rights reserved."}
{"_id":"d2f1e9129030dbd1900a210bb4196112fba59103","title":"Severity Analyses of Single-Vehicle Crashes Based on Rough Set Theory","text":"A single-vehicle crash is a typical pattern of traffic accidents and tends to cause heavy loss. The purpose of this study is to identify the factors significantly influencing single-vehicle crash injury severity, using a data selected from Beijing city for a 4-year period. Rough set theory was applied to complete the injury severity analysis, and followed by applying cross-validation method to estimate the prediction accuracy of extraction rules. Results show that it is effective for analyzing the severity of Single-vehicle crashes with rough set theory."}
{"_id":"f693699344261d0d7b5f57b6c402fd8d26781ba5","title":"Galvanometric optical laser beam steering system for microfactory application","text":"This article presents a kinematic model and control of a galvanometric laser beam steering system for high precision marking, welding or soldering applications as a microfactory module. Galvo systems are capable of scanning laser beam with relatively high frequencies that makes them suitable for fast processing applications. For the sake of flexibility and ease of use 2D reference shapes to be processed are provided as CAD drawings. Drawings are parsed and interpolated to x - y reference data points on MATLAB then stored as arrays in C code. C header file is further included as reference data points to be used by the system. Theoretical kinematic model of the system is derived and model parameters are tuned for practical implementation and validated with respect to measured positions on rotation space with optical position sensor and image field with position sensitive device. Machining with material removal requires high power laser to be employed that makes position measurement on image field unfeasible. Therefore for closed loop applications optical position sensor embedded in galvo motors is used for position feedback. Since the model approved to be approximately linear in the range of interest by simulations, a PI controller is used for precise positioning of the galvo motors. Experimental results for tracking circular and rectangular shape references are proved to be precise with errors of less than 2%."}
{"_id":"479fb8640836baa92fe99de50adbe6af44bc5444","title":"Local potassium signaling couples neuronal activity to vasodilation in the brain","text":"The mechanisms by which active neurons, via astrocytes, rapidly signal intracerebral arterioles to dilate remain obscure. Here we show that modest elevation of extracellular potassium (K+) activated inward rectifier K+ (Kir) channels and caused membrane potential hyperpolarization in smooth muscle cells (SMCs) of intracerebral arterioles and, in cortical brain slices, induced Kir-dependent vasodilation and suppression of SMC intracellular calcium (Ca2+) oscillations. Neuronal activation induced a rapid (<2 s latency) vasodilation that was greatly reduced by Kir channel blockade and completely abrogated by concurrent cyclooxygenase inhibition. Astrocytic endfeet exhibited large-conductance, Ca2+-sensitive K+ (BK) channel currents that could be activated by neuronal stimulation. Blocking BK channels or ablating the gene encoding these channels prevented neuronally induced vasodilation and suppression of arteriolar SMC Ca2+, without affecting the astrocytic Ca2+ elevation. These results support the concept of intercellular K+ channel\u2013to\u2013K+ channel signaling, through which neuronal activity in the form of an astrocytic Ca2+ signal is decoded by astrocytic BK channels, which locally release K+ into the perivascular space to activate SMC Kir channels and cause vasodilation."}
{"_id":"0d99a8787bd3abe24c7737775da4d842bb86e4ab","title":"Winnowing: Local Algorithms for Document Fingerprinting","text":"Digital content is for copying: quotation, revision, plagiarism, and file sharing all create copies. Document fingerprinting is concerned with accurately identifying copying, including small partial copies, within large sets of documents.We introduce the class of local document fingerprinting algorithms, which seems to capture an essential property of any finger-printing technique guaranteed to detect copies. We prove a novel lower bound on the performance of any local algorithm. We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33% of the lower bound. Finally, we also give experimental results on Web data, and report experience with MOSS, a widely-used plagiarism detection service."}
{"_id":"310974f3c1fc87120414eceba7f07b31885f1292","title":"Analysis of Metastability Errors in Conventional, LSB-First, and Asynchronous SAR ADCs","text":"A practical model for characterizing comparator metastability errors in SAR ADCs is presented, and is used to analyze not only the conventional SAR but also LSB-first and asynchronous versions. This work makes three main contributions: first, it is shown that for characterizing metastability it is more reasonable to use input signals with normal or Laplace distributions. Previous work used uniformly-distributed signals in the interest of making derivations easier, but this simplifying assumption overestimated SMR by as much as 18 dB compared to the more reasonable analysis presented here. Second, this work shows that LSB-first SAR ADCs achieve SMR performance equal to or better than conventional SARs with the same metastability window, depending on bandwidth. Finally, the analysis is used to develop a framework for calculating the maximum effective sample rate for asynchronous SAR ADCs, and in doing so demonstrates that proximity detectors are not effective solutions to improving metastability performance."}
{"_id":"ec3472acc24fe5ef9eb07a31697f2cd446c8facc","title":"PixelNet: Representation of the pixels, by the pixels, and for the pixels","text":"We explore design principles for general pixel-level prediction problems, from low-level edge detection to midlevel surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fullyconvolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that stratified sampling of pixels allows one to (1) add diversity during batch updates, speeding up learning; (2) explore complex nonlinear predictors, improving accuracy; and (3) efficiently train state-of-the-art models tabula rasa (i.e., \u201cfrom scratch\u201d) for diverse pixel-labeling tasks. Our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset, and edge detection on BSDS."}
{"_id":"48cfc4b36202f2cf10a70edac9490c5234d603b3","title":"Culture Wires the Brain: A Cognitive Neuroscience Perspective.","text":"There is clear evidence that sustained experiences may affect both brain structure and function. Thus, it is quite reasonable to posit that sustained exposure to a set of cultural experiences and behavioral practices will affect neural structure and function. The burgeoning field of cultural psychology has often demonstrated the subtle differences in the way individuals process information-differences that appear to be a product of cultural experiences. We review evidence that the collectivistic and individualistic biases of East Asian and Western cultures, respectively, affect neural structure and function. We conclude that there is limited evidence that cultural experiences affect brain structure and considerably more evidence that neural function is affected by culture, particularly activations in ventral visual cortex-areas associated with perceptual processing."}
{"_id":"4fba12c75d28e46e3ea93102499fb0c27d360e17","title":"Low-Quality Product Review Detection in Opinion Summarization","text":"Product reviews posted at online shopping sites vary greatly in quality. This paper addresses the problem of detecting lowquality product reviews. Three types of biases in the existing evaluation standard of product reviews are discovered. To assess the quality of product reviews, a set of specifications for judging the quality of reviews is first defined. A classificationbased approach is proposed to detect the low-quality reviews. We apply the proposed approach to enhance opinion summarization in a two-stage framework. Experimental results show that the proposed approach effectively (1) discriminates lowquality reviews from high-quality ones and (2) enhances the task of opinion summarization by detecting and filtering lowquality reviews."}
{"_id":"645d90ede11053f795ad2e6c142d493a5a793303","title":"Holonomic Control of a robot with an omni-directional drive .","text":"This paper shows how to control a robot with omnidirectional wheels, using as example robots with four motors, and generalizing to n motors. More than three wheels provide redundancy: many combinations of motors speeds can provide the same Euclidean movement. Since the system is over-determined, we show how to compute a set of consistent and optimal motor forces and speeds using the pseudoinverse of coupling matrices. This approach allows us also to perform a consistency check to determine whether a wheel is slipping on the floor or not. We show that it is possible to avoid wheel slippage by driving the robot with a motor torque under a certain threshold or handle it and make high accelerations possible."}
{"_id":"be227e18173398ba88621c4168258c231be44273","title":"AdGraph: A Machine Learning Approach to Automatic and Effective Adblocking","text":"Filter lists are widely deployed by adblockers to block ads and other forms of undesirable content in web browsers. However, these filter lists are manually curated based on informal crowdsourced feedback, which brings with it a significant number of maintenance challenges. To address these challenges, we propose a machine learning approach for automatic and effective adblocking called AdGraph. Our approach relies on information obtained from multiple layers of the web stack (HTML, HTTP, and JavaScript) to train a machine learning classifier to block ads and trackers. Our evaluation on Alexa top-10K websites shows that AdGraph automatically and effectively blocks ads and trackers with 97.7% accuracy. Our manual analysis shows that AdGraph has better recall than filter lists, it blocks 16%more ads and trackers with 65% accuracy. We also show that AdGraph is fairly robust against adversarial obfuscation by publishers and advertisers that bypass filter lists."}
{"_id":"8077a34c426acff10f0717c0cf0b99958fc3c5ed","title":"Automatic Text Classification: A Technical Review","text":"Automatic Text Classification is a semi-supervised machine learning task that automatically assigns a given document to a set of pre-defined categories based on its textual content and extracted features. Automatic Text Classification has important applications in content management, contextual search, opinion mining, product review analysis, spam filtering and text sentiment mining. This paper explains the generic strategy for automatic text classification and surveys existing solutions to major issues such as dealing with unstructured text, handling large number of attributes and selecting a machine learning technique appropriate to the text-classification application."}
{"_id":"7752e0835506a6629c1b06e67f2afb1e5d2bb714","title":"Convergent and discriminant validation by the multitrait-multimethod matrix.","text":"Content Memory (Learning Ability) As Comprehension 82 Vocabulary Cs .30 ( ) .23 .31 ( ) .31 .31 .35 ( ) .29 .48 .35 .38 ( ) .30 .40 .47 .58 .48 ( ) As judged against these latter values, comprehension (.48) and vocabulary (.47), but not memory (.31), show some specific validity. This transmutability of the validation matrix argues for the comparisons within the heteromethod block as the most generally relevant validation data, and illustrates the potential interchangeability of trait and method components. Some of the correlations in Chi's (1937) prodigious study of halo effect in ratings are appropriate to a multitrait-multimethod matrix in which each rater might be regarded as representing a different method. While the published report does not make these available in detail because it employs averaged values, it is apparent from a comparison of his Tables IV and VIII that the ratings generally failed to meet the requirement that ratings of the same trait by different raters should correlate higher than ratings of different traits by the same rater. Validity is shown to the extent that of the correlations in the heteromethod block, those in the validity diagonal are higher than the average heteromethod-heterotrait values. A conspicuously unsuccessful multitrait-multimethod matrix is provided by Campbell (1953, 1956) for rating of the leadership behavior of officers by themselves and by their subordinates. Only one of 11 variables (Recognition Behavior) met the requirement of providing a validity diagonal value higher than any of the heterotrait-heteromethod values, that validity being .29. For none of the variables were the validities higher than heterotrait-monomethod values. A study of attitudes toward authority and nonauthority figures by Burwen and Campbell (1957) contains a complex multitrait-multimethod matrix, one symmetrical excerpt from which is shown in Table 6. Method variance was strong for most of the procedures in this study. Where validity was found, it was primarily at the level of validity diagonal values higher than heterotrait-heteromethod values. As illustrated in Table 6, attitude toward father showed this kind of validity, as did attitude toward peers to a lesser degree. Attitude toward boss showed no validity. There was no evidence of a generalized attitude toward authority which would include father and boss, although such values as the VALIDATION BY THE MULTITRAIT-MULTIMETHOD MATRIX"}
{"_id":"9a756fa7e7c8afa53ada2201bcea38a095425a8e","title":"The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation","text":""}
{"_id":"e0f22e9eee05566a4fdd9b5c0fae7e2f8b6802f4","title":"A Latent Semantic Indexing-based approach to multilingual document clustering","text":"The creation and deployment of knowledge repositories formanaging, sharing, and reusing tacit knowledgewithin an organization has emerged as a prevalent approach in current knowledge management practices. A knowledge repository typically contains vast amounts of formal knowledge elements, which generally are available as documents. To facilitate users' navigation of documents within a knowledge repository, knowledge maps, often created by document clustering techniques, represent an appealing and promising approach. Various document clustering techniques have been proposed in the literature, but most deal with monolingual documents (i.e., written in the same language). However, as a result of increased globalization and advances in Internet technology, an organization often maintains documents in different languages in its knowledge repositories, which necessitates multilingual document clustering (MLDC) to create organizational knowledge maps. Motivated by the significance of this demand, this study designs a Latent Semantic Indexing (LSI)-based MLDC technique capable of generating knowledge maps (i.e., document clusters) from multilingual documents. The empirical evaluation results show that the proposed LSI-based MLDC technique achieves satisfactory clustering effectiveness, measured by both cluster recall and cluster precision, and is capable of maintaining a good balance between monolingual and cross-lingual clustering effectiveness when clustering a multilingual document corpus. \u00a9 2007 Elsevier B.V. All rights reserved."}
{"_id":"3fb4f9bb4a82945558c1b92f00f82fc38f160155","title":"Interworking of DSRC and Cellular Network Technologies for V2X Communications: A Survey","text":"Vehicle-to-anything (V2X) communications refer to information exchange between a vehicle and various elements of the intelligent transportation system (ITS), including other vehicles, pedestrians, Internet gateways, and transport infrastructure (such as traffic lights and signs). The technology has a great potential of enabling a variety of novel applications for road safety, passenger infotainment, car manufacturer services, and vehicle traffic optimization. Today, V2X communications is based on one of two main technologies: dedicated short-range communications (DSRC) and cellular networks. However, in the near future, it is not expected that a single technology can support such a variety of expected V2X applications for a large number of vehicles. Hence, interworking between DSRC and cellular network technologies for efficient V2X communications is proposed. This paper surveys potential DSRC and cellular interworking solutions for efficient V2X communications. First, we highlight the limitations of each technology in supporting V2X applications. Then, we review potential DSRC-cellular hybrid architectures, together with the main interworking challenges resulting from vehicle mobility, such as vertical handover and network selection issues. In addition, we provide an overview of the global DSRC standards, the existing V2X research and development platforms, and the V2X products already adopted and deployed in vehicles by car manufactures, as an attempt to align academic research with automotive industrial activities. Finally, we suggest some open research issues for future V2X communications based on the interworking of DSRC and cellular network technologies."}
{"_id":"3ae1699ac44725f9eeba653aeb59d4c35a498b12","title":"Dual band compact printed monopole antenna for Bluetooth (2.54GHz) and WLAN (5.2GHz) applications","text":"The printed monopole antenna having dual band is proposed which operates at frequency 2.54 GHz (Bluetooth) and 5.2GHz (WLAN). Bluetooth and WLAN have been widely applied in all new trending laptops and smart phones. These two technologies are well known for its effective cost and high-speed data connection. The antenna comprises of two rectangular patches of different sizes for the required dual-band operations. The presented antenna is fed by corporate feed network which improves impedance bandwidth. The prime motto of this project is to make the smallest (compact) possible antenna so that it can be placed in the limited area of handheld devices. Simulated percentage impedance bandwidth of the antenna are 46.25 (1.958 GHz to 3.13 GHz) and 31.30 (4.15 GHz to 5.69 GHz) respectively. Good return loss and VSWR (less than 2) of the designed antenna is obtained by simulating on IE3D software."}
{"_id":"c7fd5705cf27f32cf36d718fe5ab499fec2d02e3","title":"Tibial periosteal ganglion cyst: The ganglion in disguise","text":"Soft tissue ganglions are commonly encountered cystic lesions around the wrist presumed to arise from myxomatous degeneration of periarticular connective tissue. Lesions with similar pathology in subchondral location close to joints, and often simulating a geode, is the less common entity called intraosseous ganglion. Rarer still is a lesion produced by mucoid degeneration and cyst formation of the periostium of long bones, rightly called the periosteal ganglion. They are mostly found in the lower extremities at the region of pes anserinus, typically limited to the periosteum and outer cortex without any intramedullary component. We report the case of a 62 year-old male who presented with a tender swelling on the mid shaft of the left tibia, which radiologically suggested a juxtacortical lesion extending to the soft tissue or a soft tissue neoplasm eroding the bony cortex of tibia. It was later diagnosed definitively as a periosteal ganglion in an atypical location, on further radiologic work-up and histopathological correlation."}
{"_id":"f456ca5d759ba05402721125fe4ce0da1730f683","title":"CHAPTER 12 WORKFLOW ENGINE FOR CLOUDS","text":"A workflow models a process as consisting of a series of steps that simplifies the complexity of execution and management of applications. Scientific workflows in domains such as high-energy physics and life sciences utilize distributed resources in order to access, manage, and process a large amount of data from a higher level. Processing and managing such large amounts of data require the use of a distributed collection of computation and storage facilities. These resources are often limited in supply and are shared among many competing users. The recent progress in virtualization technologies and the rapid growth of cloud computing services have opened a new paradigm in distributed computing for utilizing existing (and often cheaper) resource pools for ondemand and scalable scientific computing. Scientific Workflow Management Systems (WfMS) need to adapt to this new paradigm in order to leverage the benefits of cloud services. Cloud services vary in the levels of abstraction and hence the type of service they present to application users. Infrastructure virtualization enables providers such as Amazon to offer virtual hardware for use in computeand dataintensive workflow applications. Platform-as-a-Service (PaaS) clouds expose a higher-level development and runtime environment for building and deploying workflow applications on cloud infrastructures. Such services may also expose domain-specific concepts for rapid-application development. Further up in the cloud stack are Software-as-a-Service providers who offer end users with"}
{"_id":"2a70fbec8ca4e271d67c565a631ea05d3364164a","title":"Coding polygon meshes as compressable ASCII","text":"Because of the convenience of a text-based format 3D content is often published in form of a gzipped file that contains an ASCII description of the scene graph. While compressed image, audio, and video data is kept in seperate binary files, polygonal data is usually included uncompressed into the ASCII description, as there is no widely-accepted standard for compressed polygon meshes.In this paper we show how to incorporate compression of polygonal data into a purely text-based scene graph description. Our scheme codes polygon meshes as ASCII strings that compress well with standard compression schemes such as gzip. The coder is lossless when only the position and texture coordinate {\\em indices} are coded. If loss is acceptable, positions and texture coordinates can be quantized and delta coded, which reduces the file size further. The gzipped scene graph description files decrease by a factor of two (six) in size when the polygon meshes they contain are coded with the lossless (lossy) ASCII coder.Furthermore we describe in detail a proof-of-concept implementation that uses the Shout3D~\\cite{shout3d} pure java API---a plugin-less Web3D player that downloads all required java classes on demand. Our prototype is an extremely light-weight implementation of the decoder that can be distributed at minimal additional cost. The size of the compiled decoder class is less than 6KB by itself and less than 3KB if included into a compressed archive of java class files. It makes no use of specific features of the Shout3D API. Hence, our method will work for any scene graph API that allows (a) to extend the node set and (b) to store the scene graph as ASCII."}
{"_id":"3fb1cbc03bbb237343d1c1298e2d380c1b38e61d","title":"Analyzing and Comparing the Protection Quality of Security Enhanced Operating Systems","text":"Host compromise is a serious computer security problem today. To better protect hosts, several Mandatory Access Control systems, such as Security Enhanced Linux (SELinux) and AppArmor, have been introduced. In this paper we propose an approach to analyze and compare the quality of protection offered by these different MAC systems. We introduce the notion of vulnerability surfaces under attack scenarios as the measurement of protection quality, and implement a tool called VulSAN for computing such vulnerability surfaces. In VulSAN, we encode security policies, system states, and system rules using logic programs. Given an attack scenario, VulSAN computes a host attack graph and the vulnerability surface. We apply our approach to compare SELinux and AppArmor policies in several Linux distributions and discuss the results. Our tool can also be used by Linux system administrators as a system hardening tool. Because of its ability to analyze SELinux as well as AppArmor policies, it can be used for most enterprise Linux distributions and home user distributions."}
{"_id":"10987d17af1245d49139fce9feae0e0afa71b6f2","title":"Continual Learning in Generative Adversarial Nets","text":"Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling."}
{"_id":"e416e9a7bb3d27086ff0487894ba636b4123bfe5","title":"Inverse reinforcement learning for interactive systems","text":"Human machine interaction is a field where machine learning is present at almost any level, from human activity recognition to natural language generation. The interaction manager is probably one of the latest components of an interactive system that benefited from machine learning techniques. In the late 90's, sequential decision making algorithms like reinforcement learning have been introduced in the field with the aim of making the interaction more natural in a measurable way. Yet, these algorithms require providing the learning agent with a reward after each interaction. This reward is generally handcrafted by the system designer who introduces again some expertise in the system. In this paper, we will discuss a method for learning a reward function by observing expert humans, namely inverse reinforcement learning (IRL). IRL will then be applied to several steps of the spoken dialogue management design such as user simulation and clustering but also to co-adaptation of human user and machine."}
{"_id":"41f89ee0e3b4d039e678fb197f69e87820f3f171","title":"CS224N Project: Let Computers Do Reading Comprehension","text":"In this work, we want to teach the computers to do reading comprehension. More specifically, the task is to let computers read a short passage and then answer some simple questions based on the passage.[1] proposes an end-to-end, RNN-like approach which, according to them, needs significantly less supervision but performs comparatively good to benchmarks for many applications. However, we found that their Q&A result is based on a small synthetic data set defined in [2]. We could see that the dataset is quite small and simple with a vocabulary size of 177. Here is a sample from the dataset."}
{"_id":"bed4a1ffc5f7fc5ad3a1298f0dd82893dcade055","title":"Performance Management : A model and research agenda","text":"\u00a9 International Association for Applied Psychology, 2004. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main Street, Malden, MA 02148, USA. Blackwell Publishing, Ltd. Oxford, UK APPS pplied Psychology: an International Review 0269-994X \u00a9 Blackwell Publishing 2004 ct ber 2004 53 4 riginal Arti le PERFORMANCE MANAGEMENT D N HARTOG ET AL. Performance Management: A Model and Research Agenda"}
{"_id":"ced4d6cf95921608c91b3fcf52e96fc758f0912d","title":"Insulin detemir versus insulin glargine for type 2 diabetes mellitus.","text":"BACKGROUND\nChronically elevated blood glucose levels are associated with significant morbidity and mortality. Many diabetes patients will eventually require insulin treatment to maintain good glycaemic control. There are still uncertainties about the optimal insulin treatment regimens for type 2 diabetes, but the long-acting insulin analogues seem beneficial. Several reviews have compared either insulin detemir or insulin glargine to NPH insulin, but research directly comparing both insulin analogues is limited.\n\n\nOBJECTIVES\nTo assess the effects of insulin detemir and insulin glargine compared with each other in the treatment of type 2 diabetes mellitus.\n\n\nSEARCH STRATEGY\nWe searched MEDLINE, EMBASE, The Cochrane Library, online registries of ongoing trials and abstract books. Date of last search was January 2011.\n\n\nSELECTION CRITERIA\nAll randomised controlled trials comparing insulin detemir with insulin glargine with a duration of 12 weeks or longer were included.\n\n\nDATA COLLECTION AND ANALYSIS\nTwo authors independently selected the studies and extracted the data. Pooling of studies by means of random-effects meta-analysis was performed.\n\n\nMAIN RESULTS\nThis review examined four trials lasting 24 to 52 weeks involving 2250 people randomised to either insulin detemir or glargine. Overall, risk of bias of the evaluated studies was high. Insulin glargine was dosed once-daily in the evening. Insulin detemir was initiated once-daily in the evening with the option of an additional dose in the morning in three studies and initiated twice-daily in one study. Of randomised patients 13.6% to 57.2% were injecting insulin detemir twice-daily at the end of trial.Glycaemic control, measured by glycosylated haemoglobin A1c (HbA1c) and HbA1c equal to or less than 7% with or without hypoglycaemia, did not differ statistically significantly between treatment groups.The results showed no significant differences in overall, nocturnal and severe hypoglycaemia between treatment groups.Insulin detemir was associated with less weight gain. Treatment with insulin glargine resulted in a lower daily basal insulin dose and a lower number of injection site reactions.There was no significant difference in the variability of FPG or glucose values in 24-hour profiles between treatment groups. It was not possible to draw conclusions on quality of life, costs or mortality. Only one trial reported results on health-related quality of life and showed no significant differences between treatment groups.\n\n\nAUTHORS' CONCLUSIONS\nOur analyses suggest that there is no clinically relevant difference in efficacy or safety between insulin detemir and insulin glargine for targeting hyperglycaemia. However, to achieve the same glycaemic control insulin detemir was often injected twice-daily in a higher dose but with less weight gain, while insulin glargine was injected once-daily, with somewhat fewer injection site reactions."}
{"_id":"f7fc50c74863441ec375a5cf26154ede7e22b9d5","title":"Depth From Defocus in Presence of Partial Self Occlusion","text":"Contrary to the normal belief we show that selfocclusion is present in any real aperture image and we present a method on how we can take care of the occlusion while recovering the depth using the defocus as the cue. The spacevariant blur is modeled as an MRF and the MAP estimates are obtained f o r both the depth map and the everywhere f o cused intensity image. The blur kernel is adjusted in the regions where occlusion is present, particularly at the regions of discontinuities in the scene. The performance of the proposed algorithm is tested over synthetic data and the estimates are found to be better than the earlier schemes where such subtle effects were ignored."}
{"_id":"c0555be12364c6621bf591a1403da842df492f45","title":"QUOTA: The Quantile Option Architecture for Reinforcement Learning","text":"In this paper, we propose the Quantile Option Architecture (QUOTA) for exploration based on recent advances in distributional reinforcement learning (RL). In QUOTA, decision making is based on quantiles of a value distribution, not only the mean. QUOTA provides a new dimension for exploration via making use of both optimism and pessimism of a value distribution. We demonstrate the performance advantage of QUOTA in both challenging video games and physical robot simulators."}
{"_id":"3296c12d8edfdd0ed867273458d6a4f683e1c30d","title":"Acquiring a Dictionary of Emotion-Provoking Events","text":"This paper is concerned with the discovery and aggregation of events that provoke a particular emotion in the person who experiences them, or emotion-provoking events. We first describe the creation of a small manually-constructed dictionary of events through a survey of 30 subjects. Next, we describe first attempts at automatically acquiring and aggregating these events from web data, with a baseline from previous work and some simple extensions using seed expansion and clustering. Finally, we propose several evaluation measures for evaluating the automatically acquired events, and perform an evaluation of the effectiveness of automatic event extraction."}
{"_id":"db77e6b8030e7f8f2c1503b99fc88ab002b84cb4","title":"Multi-linear polarization reconfigurable center-fed circular patch antenna with shorting posts","text":"In this paper, a novel multi-linear polarization reconfigurable antenna with shorting posts, which can achieve four linear polarizations (0\u00b0, 45\u00b0, 90\u00b0, 135\u00b0), has been proposed. By switching the diodes between two groups of shorting posts, four linear polarizations can be realized. The dimensions of the proposed antenna are about 0.56\u03bb\u00d7 0.56\u03bb\u00d7 0.07\u03bb at 2.4 GHz. The measured results agree well with the simulated ones."}
{"_id":"b8e202587a83d13fcabeb9d2e21df6b12150a8e2","title":"Description of Luciola aquatilis sp. nov., a new aquatic firefly (Coleoptera: Lampyridae: Luciolinae) from Thailand","text":"A new species of aquatic firefly belonging to Luciola Laporte is described and illustrated based on external morphology of both males and females, and the genitalia of males. Luciola aquatilis sp. nov., a common firefly in Thailand was formerly commonly misidentified as Luciola brahmina Bourgeois. Other Luciola species that resemble L. aquatilis are discussed, as well as past confusion concerning their taxonomic affinities."}
{"_id":"086a6b1a757c25aa09e2a5bbdc39ae27d6e7ec9c","title":"Comparing two evolutionary algorithm based methods for layout generation: Dense packing versus subdivision","text":"We present and compare two evolutionary algorithm based methods for rectangular architectural layout generation: dense packing and subdivision algorithms. We analyze the characteristics of the two methods on the basis of three floor plan scenarios. Our analyses include the speed with which solutions are generated, the reliability with which optimal solutions can be found, and the number of different solutions that can be found overall. In a following step, we discuss the methods with respect to their different user interaction capabilities. In addition, we show that each method has the capability to generate more complex L-shaped layouts. Finally, we conclude that neither of the methods is superior but that each of them is suitable for use in distinct application scenarios because of its different properties."}
{"_id":"0d3bb75852098b25d90f31d2f48fd0cb4944702b","title":"A data-driven approach to cleaning large face datasets","text":"Large face datasets are important for advancing face recognition research, but they are tedious to build, because a lot of work has to go into cleaning the huge amount of raw data. To facilitate this task, we describe an approach to building face datasets that starts with detecting faces in images returned from searches for public figures on the Internet, followed by discarding those not belonging to each queried person. We formulate the problem of identifying the faces to be removed as a quadratic programming problem, which exploits the observations that faces of the same person should look similar, have the same gender, and normally appear at most once per image. Our results show that this method can reliably clean a large dataset, leading to a considerable reduction in the work needed to build it. Finally, we are releasing the FaceScrub dataset that was created using this approach. It consists of 141,130 faces of 695 public figures and can be obtained from http:\/\/vintage.winklerbros.net\/facescrub.html."}
{"_id":"efe573cbfa7f4de4fd31eda183fefa8a7aa80888","title":"Blockchain beyond bitcoin","text":"Blockchain technology has the potential to revolutionize applications and redefine the digital economy."}
{"_id":"acbfe25e9f78a1a1b5bb084b0455b6e88f6d3b4a","title":"AcuBot: a robot for radiological interventions","text":"We report the development of a robot for radiological percutaneous interventions using uniplanar fluoroscopy, biplanar fluoroscopy, or computed tomography (CT) for needle biopsy, radio frequency ablation, cryotherapy, and other needle procedures. AcuBot is a compact six-degree-of-freedom robot for manipulating a needle or other slender surgical instrument in the confined space of the imager without inducing image artifacts. Its distinctive characteristic is its decoupled motion capability correlated to the positioning, orientation, and instrument insertion steps of the percutaneous intervention. This approach allows each step of the intervention to be performed using a separate mechanism of the robot. One major advantage of this kinematic approach is patient safety. The first feasibility experiment performed with the robot, a cadaver study of perispinal blocks under biplanar fluoroscopy, is presented. The main expected application of this system is to CT-based procedures. AcuBot has received Food and Drug Administration clearance (IDE G010331\/S1), and a clinical trial of using the robot for perispinal nerve and facet blocks is presently underway at Georgetown University, Washington, DC."}
{"_id":"39ee50989613888ff23bdac0d6711ca8eefe658b","title":"Visualizing the Hidden Activity of Artificial Neural Networks","text":"In machine learning, pattern classification assigns high-dimensional vectors (observations) to classes based on generalization from examples. Artificial neural networks currently achieve state-of-the-art results in this task. Although such networks are typically used as black-boxes, they are also widely believed to learn (high-dimensional) higher-level representations of the original observations. In this paper, we propose using dimensionality reduction for two tasks: visualizing the relationships between learned representations of observations, and visualizing the relationships between artificial neurons. Through experiments conducted in three traditional image classification benchmark datasets, we show how visualization can provide highly valuable feedback for network designers. For instance, our discoveries in one of these datasets (SVHN) include the presence of interpretable clusters of learned representations, and the partitioning of artificial neurons into groups with apparently related discriminative roles."}
{"_id":"41e2840d51e23727ec1c133c6fc924fa199b5f82","title":"Forward and inverse kinematics model for robotic welding process using KR-16KS KUKA robot","text":"This paper aims to model the forward and inverse kinematics of a KUKA KR-16KS robotic arm in the application of a simple welding process. A simple welding task to weld a block onto a metal sheet is carried out in order to investigate the forward and inverse kinematics models of KR-16KS. A movement flow planning is designed and further developed into the KR-16KS programming. Eleven points of movement are studied for the forward kinematic modeling. A summary of calculation is obtained. A general D-H representation of forward and inverse matrix is obtained. This can be used in each of the welding operation movement based on KUKA KR-16KS robotic arm. A forward kinematic and an inverse kinematic aspect of KUKA KR-16KS is successfully modeled based on a simple welding task."}
{"_id":"3393866fbeb1f8510ff458b19f6e98b7f4a902ec","title":"Vision based distance measurement system using single laser pointer design for underwater vehicle","text":"As part of a continuous research and development of underwater robotics technology at ITB, a visionbased distance measurement system for an Unmanned Underwater vehicle (UUV) has been designed. The proposed system can be used to predict horizontal distance between underwater vehicle and wall in front of vehicle. At the same time, it can be used to predict vertical distance between vehicle and the surface below it as well. A camera and a single laser pointer are used to obtain data needed by our algorithm. The vision-based navigation consists of two main processes which are the detection of a laser spot using image processing and the calculation of the distance based on laser spot position on the image."}
{"_id":"ac6574a51108e57d1265b97fcd78992bc6360ed0","title":"Interactive Storytelling with Literary Feelings","text":"In this paper, we describe the integration of Natural Language Processing (NLP) within an emotional planner to support Interactive Storytelling. Our emotional planner is based on a standard HSP planner, whose originality is drawn from altering the agents\u2019 beliefs and emotional states. Each character is driven by its own planner, while characters are able to operate on their reciprocal feelings thus affecting each other. Our baseline story is constituted by a classic XIX century French novel from Gustave Flaubert in which characters feelings play a dominant role. This approach benefits from the fact that Flaubert has described a specific ontology for his characters feelings. The objective of NLP should be to uncover from natural language utterances the same kind of affective elements, which requires an integration between NLP and the planning component at the level of semantic content. This research is illustrated with examples from a first fully integrated prototype comprising NLP, emotional planning and real-time 3D animation."}
{"_id":"7534ac4b42d39e2033e32cb4a38a9dc95070b50d","title":"Where the Action Is - The Foundations of Embodied Interaction","text":"Most textbooks in HCI and CSCW do not offer a coherent and over\u2013arching understanding of social and technological issues. They present a variety of techniques and technologies, and outline a little history, but offer little in terms of theory that addresses the complexity of collaborative systems\u2019 structure and use. The majority of practitioners and researchers do not see theory as one of the things that they do. Immersed in their craft, focusing on technological innovation or ethnomethodological detail, they do not engage in theoretical abstraction. Technologists contentedly explore new tools and devices, with little heed to older disciplines or deeper discussion about the limits and assumptions inherent in their craft. The area of sociology most influential in CSCW, ethnomethodology, deliberately keeps theorising and generalisation at a distance, seeing abstraction as brutish and creativity as foreign. In our field, theory is like the public library. If asked, most of us would say that we are glad that it is around\u2014but few of us actually go there. Most see it as a haven for the old, the unemployed and the eccentric. Paul Dourish is a card\u2013carrying member, however. He reads avidly and widely, but is also a skilful system designer and developer. The result is a book that is deep, accessible and useful, which is a rare thing nowadays. In the introduction, Dourish lays out the structure of the book. He describes and reflects on two current trends in system design: tangible and social computing. By looking at the usually hidden assumptions in system design, he makes his later theoretical discussion more relevant and accessible to a computer science audience. He does not develop new philosophy or social theory, but draws upon established 20th century philosophy of language and phenomenology. Much of this material is likely to be unfamiliar to \u2018the average programmer\u2019 and, although Dourish presents it well, it may still be challenging to the reader. However, its use is one of the book\u2019s main contributions. He uses it to ground a conceptual framework and a corresponding set of principles for system design practice. He aims to do justice to the sociality and heterogeneity of interactive media, and to avoid putting theory above practice\u2014or vice versa. His ideal is balance: \u201cthe ability to develop systemsthat resonate with, rather than restrict (or, worse, refute), the social organization of action\u201d. The term \u2018tangible computing\u2019 is used very broadly in this book. The chapter on this theme covers the designs of Hiroshi Ishii\u2019s group at MIT\u2019s Media Lab, but also ubiquitous computing work such as the badges, tabs and LiveBoards of Xerox PARC, and augmented reality systems such as Pierre Wellner\u2019s DigitalDesk. Although \u2018UbiComp\u2019 is more fashionable nowadays, Dourish chooses a term that helps him focus more on our perception and \u201cthe ways we experience the everyday world\u201d than on computation and technology. Much of this discussion centres on exemplary systems, and the way that an increasing number and variety of computational devices and sensors are distributed in our environment. This contrasts with older systems that used very few media and which were encapsulated in the beige box of the traditional PC. The following chapter is on social computing: \u201cthe application of sociological understanding to the design of interactive systems\u201d. This stands in contrast to the more traditional tendency for designers to treat people as isolated system users, with little account taken of organisational and social context. Dourish draws upon some influential studies of existing technology such as that of an air traffic control room by Hughes et al., and the ethnography of a print shop by Bowers, Button and Sharrock. Compared to the previous chapter, exemplary systems are scarce. There are none of the previous chapter\u2019s attractive images of exotic displays and devices, so beloved in undergraduate lectures and conference presentations, as this chapter addresses systems\u2019 internal structure rather than external interaction. Dourish has developed more systems in the \u2018social\u2019 category than the \u2018tangible\u2019, and here he offers as examples Mansfield et al.\u2019s Orbit and some of his own work: using a system design technique, computational reflection, to offer users an account of deep system structure. In describing this particular technique, Dourish touches on the essence of the entire book: What is radical is the relationship it proposes between technical design and social understandings. It argues that the most fruitful place to forge these relationships is at a foundational level, one that attempts to take sociological insights into the heart of the process and fabric of design. (p. 87)"}
{"_id":"1bc6ea86a8ed0a80406404693c675f11f6b8e454","title":"Towards a Trust Management System for Cloud Computing","text":"Cloud computing provides cost-efficient opportunities for enterprises by offering a variety of dynamic, scalable, and shared services. Usually, cloud providers provide assurances by specifying technical and functional descriptions in Service Level Agreements (SLAs) for the services they offer. The descriptions in SLAs are not consistent among the cloud providers even though they offer services with similar functionality. Therefore, customers are not sure whether they can identify a trustworthy cloud provider only based on its SLA. To support the customers in reliably identifying trustworthy cloud providers, we propose a multi-faceted Trust Management (TM) system architecture for a cloud computing marketplace. This system provides means to identify the trustworthy cloud providers in terms of different attributes (e.g., security, performance, compliance) assessed by multiple sources and roots of trust information."}
{"_id":"00cf965e32f89e475e076aab5db97c8b3b36fa63","title":"A tutorial on support vector regression","text":"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention somemodifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective."}
{"_id":"385622a5862c989653a648ac8abc59ae3fe785f7","title":"An introduction to hidden Markov models","text":"The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition."}
{"_id":"c48d3d566dfcb63da6ad9aab48783c5e80adb771","title":"A sample size calculator for SMART pilot studies","text":"In clinical practice, as well as in other areas where interventions are provided, a sequential individualized approach to treatment is often necessary, whereby each treatment is adapted based on the object \u2032s response. An adaptive intervention is a sequence of decision rules which formalizes the provision of treatment at critical decision points in the care of an individual. In order to inform the development of an adaptive intervention, scientists are increasingly interested in the use of sequential multiple assignment randomized trials (SMART), which is a type of multistage randomized trial where individuals are randomized repeatedly at critical decision points to a set treatment options. While there is great interest in the use of SMART and in the development of adaptive interventions, both are relatively new to themedical and behavioral sciences. As a result, many clinical researchers will first implement a SMART pilot study (i.e., a small-scale version of a SMART) to examine feasibility and acceptability considerations prior to conducting a full-scale SMART study. A primary aim of this paper is to introduce a new methodology to calculate minimal sample size necessary for conducting a SMART pilot."}
{"_id":"cf17c2eadd207cdc1e06b6fb0fa0881450e686a2","title":"A Quantitative and Qualitative Assessment of Automatic Text Summarization Systems","text":"Text summarization is the process of automatically creating a shorter version of one or more text documents. This paper presents a qualitative and quantitative assessment of the 22 state-of-the-art extractive summarization systems using the CNN corpus, a dataset of 3,000 news articles."}
{"_id":"32ca9f202b17a485625909213c23be8b416f9a6a","title":"A foundation for representing and querying moving objects","text":"Spatio-temporal databases deal with geometries changing over time. The goal of our work is to provide a DBMS data model and query language capable of handling such time-dependent geometries, including those changing continuously that describe moving objects. Two fundamental abstractions are moving point and moving region, describing objects for which only the time-dependent position, or position and extent, respectively, are of interest. We propose to present such time-dependent geometries as attribute data types with suitable operations, that is, to provide an abstract data type extension to a DBMS data model and query language. This paper presents a design of such a system of abstract data types. It turns out that besides the main types of interest, moving point and moving region, a relatively large number of auxiliary data types are needed. For example, one needs a line type to represent the projection of a moving point into the plane, or a \u201cmoving real\u201d to represent the time-dependent distance of two points. It then becomes crucial to achieve (i) orthogonality in the design of the system, i.e., type constructors can be applied unifomly; (ii) genericity and consistency of operations, i.e., operations range over as many types as possible and behave consistently; and (iii) closure and consistency between structure and operations of nontemporal and related temporal types. Satisfying these goal leads to a simple and expressive system of abstract data types that may be integrated into a query language to yield a powerful language for querying spatio-temporal data, including moving objects. The paper formally defines the types and operations, offers detailed insight into the considerations that went into the design, and exemplifies the use of the abstract data types using SQL. The paper offers a precise and conceptually clean foundation for implementing a spatio-temporal DBMS extension."}
{"_id":"9b1918aea35650e8fc3b295417b755d4c5ed748c","title":"Free Form based active contours for image segmentation and free space perception","text":"In this paper we present a novel approach for representing and evolving deformable active contours. The method combines piecewise regular B\u00e9zier models and curve evolution defined by local Free Form Deformation. The contour deformation is locally constrained which allows contour convergence with almost linear complexity while adapting to various shape settings and handling topology changes of the active contour. We demonstrate the effectiveness of the new active contour scheme for visual free space perception and segmentation using omnidirectional images acquired by a robot exploring unknown indoor and outdoor environments. Several experiments validate the approach with comparison to state-of-the art parametric and geometric active contours and provide fast and real-time robot free space segmentation and navigation."}
{"_id":"509bdbacd064debc94a67f5b2e79dab070cad81c","title":"Neon: A (Big) (Fast) Single-Chip 3D Workstation Graphics Accelerator","text":"High-performance 3D graphics accelerators traditionally require multiple chips on multiple boards. Specialized chips perform geometry transformations and lighting computations, rasterizing, pixel processing, and texture mapping. Multiple chip designs are often scalable: they can increase performance by using more chips. Scalability has obvious costs: a minimal configuration needs several chips, and some configurations must replicate texture maps. A less obvious cost is the almost irresistible temptation to replicate chips to increase performance, rather than to design individual chips for higher performance in the first place. In contrast, Neon is a single chip that performs like a multichip design. Neon accelerates OpenGL 3D rendering, as well as X11 and Windows\/NT 2D rendering. Since our pin budget limited peak memory bandwidth, we designed Neon from the memory system upward in order to reduce bandwidth requirements. Neon has no special-purpose memories; its eight independent 32-bit memory controllers can access color buffers, Z depth buffers, stencil buffers, and texture data. To fit our gate budget, we shared logic among different operations with similar implementation requirements, and left floating point calculations to Digital's Alpha CPUs. Neon\u2019s performance is between HP\u2019s Visualize fx and fx, and is well above SGI\u2019s MXE for most operations. Neon-based boards cost much less than these competitors, due to a small part count and use of commodity SDRAMs."}
{"_id":"4509771bb71500d411ced0d1cb53722fb73c9716","title":"Boosting for transfer learning","text":"Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model."}
{"_id":"01cdbb5be2c6485f5b5dbc29417498e33e18f5e4","title":"Publicity Trends in Arrests of \" Online Predators \" How the National Juvenile Online Victimization (n\u2010jov) Study Was Conducted","text":"about \" online predators \" * \u2013 sex of\u2010 fenders who use the Internet to meet juvenile victims \u2013 has raised considerable alarm about the extent to which Internet use may be put\u2010 ting children and adolescents at risk for sexual abuse and exploitation. Media stories and Internet safety messages have raised fears by describing violent offenders who use the Inter\u2010 net to prey on na\u00efve children by tricking them into face\u2010to\u2010face meetings or tracking them down through information posted online. Law enforcement has mobilized on a number of fronts, setting up task forces to identify and prosecute online predators, developing under\u2010 cover operations, and urging social networking sites to protect young users. Unfortunately, however, reliable information on the scope and nature of the online predator problem remains scarce. Established criminal justice data collection systems do not gather detailed data on such crimes that could help inform public policy and education. To remedy this information vacuum, the Crimes against Children Research Center at the University of New Hampshire conducted two waves of a The N\u2010JOV Study collected information from a national sample of law en\u2010 forcement agencies about the prevalence of arrests for and characteristics of online sex crimes against minors during two 12 month periods: July 1, 2000 through June 30, 2001 (Wave 1) and calendar year 2006 (Wave 2). For both Waves, we used a two\u2010phase process of mail surveys followed by telephone interviews to collect data from a national sample of the same lo\u2010 cal, county, state, and federal law enforcement agencies. First, we sent the mail surveys to a national sample of more than 2,500 agencies. These sur\u2010 veys asked if agencies had made arrests for online sex crimes against minors during the respective one\u2010year timeframes. Then we conducted detailed telephone interviews with law enforcement investigators about a random sample of arrest cases reported in the mail surveys. For the telephone interviews, we designed a sampling procedure that took into account the number of arrests reported by an agency, so that we would not unduly burden respondents in agencies with many cases. If an agency reported between one and three arrests for online sex crimes, we conducted follow\u2010up interviews for every case. For agencies that reported more than three arrests, we conducted interviews for all cases that involved youth vic\u2010 tims (victims who were located and contacted during the investigation), and sampled other arrest cases (i.e., \u2026"}
{"_id":"386596604cce7bf86c542edbdff32781a6854889","title":"AutoBrief: an experimental system for the automatic generation of briefings in integrated text and information graphics","text":"This paper describes AutoBrief, an experimental intelligent multimedia presentation system that generates presentations in text and information graphics in the domain of transportation scheduling. Acting as an intelligent assistant, AutoBrief creates a presentation to communicate its analysis of alternative schedules. In addition, the multimedia presentation facilitates data exploration through its complex information visualizations and support for direct manipulation of presentation elements. AutoBrief\u2019s research contributions include (1) a design enabling a new human\u2013computer interaction style in which intelligent multimedia presentation objects (textual or graphic) can be used by the audience in direct manipulation operations for data exploration, (2) an application-independent approach to multimedia generation based on the representation of communicative goals suitable for both generation of text and of complex information graphics, and (3) an application-independent approach to intelligent graphic design based upon communicative goals. This retrospective overview paper, aimed at a multidisciplinary audience from the fields of human\u2013computer ARTICLE IN PRESS *Corresponding author. Tel.: +1-336-2561133; fax: +1-336-3345949. E-mail addresses: nlgreen@uncg.edu (N.L. Green), carenini@cs.ubc.ca (G. Carenini), kerpedjiev@maya.com (S. Kerpedjiev), mattis@mayaviz.com (J. Mattis), j.moore@ed.ac.uk (J.D. Moore), roth+@cs.cmu.edu (S.F. Roth). 1071-5819\/$ see front matter r 2003 Elsevier Ltd. All rights reserved. doi:10.1016\/j.ijhcs.2003.10.007 interaction and natural language generation, presents AutoBrief\u2019s design and design rationale. r 2003 Elsevier Ltd. All rights reserved."}
{"_id":"2065d635b97b3bc4980432426822b543a909ce4c","title":"Failure Analysis of SGI XFS File System","text":"Commodity file systems expect a fail stop disk. But todays disks fail in unexpected ways. Disks exhibit latent sector errors, silent corruption and transient failures to name a few. In this paper we study the behavior of SGI XFS to such errors. File systems play a key role in handling most data and hence the failure handling policy of the file system plays a major role in ensuring the integrity of the data. XFS is a highly scalable journaling file system developed by SGI. We analyze the failure handling policy of XFS file system. We fail reads and writes of various blocks that originate from the file system and analyze the file system behavior. Some such blocks include super block, journal header block, commit block, data block, index blocks. We classify our errors according to the extended IRON Taxonomy. We see that XFS is vulnerable to these failures and does not possess a uniform failure handling policy. Further the file system at times silently corrupts data. We also see that XFS has very little internal redundancy even for critical structures such as the super block and B+Tree root. Finally we see that XFS, like most commodity file systems, fails to recover from these failures putting the data at risk."}
{"_id":"76c366859763ace4bbd5d0fbee8f9f094c8dc3d5","title":"The Wnt signaling pathway in development and disease.","text":"Tight control of cell-cell communication is essential for the generation of a normally patterned embryo. A critical mediator of key cell-cell signaling events during embryogenesis is the highly conserved Wnt family of secreted proteins. Recent biochemical and genetic analyses have greatly enriched our understanding of how Wnts signal, and the list of canonical Wnt signaling components has exploded. The data reveal that multiple extracellular, cytoplasmic, and nuclear regulators intricately modulate Wnt signaling levels. In addition, receptor-ligand specificity and feedback loops help to determine Wnt signaling outputs. Wnts are required for adult tissue maintenance, and perturbations in Wnt signaling promote both human degenerative diseases and cancer. The next few years are likely to see novel therapeutic reagents aimed at controlling Wnt signaling in order to alleviate these conditions."}
{"_id":"19003129b41298dca0843f7c9c7fd6ec4bae0556","title":"Magnetic Optimization Algorithms a new synthesis","text":"A novel optimization algorithm is proposed here that is inspired by the principles of magnetic field theory. In the proposed Magnetic Optimization Algorithm (MOA) the possible solutions are magnetic particles scattered in the search space. Each magnetic particle has a measure of mass and magnetic field according to its fitness. The fitter magnetic particles are those with higher magnetic field and higher mass. These particles are located in a lattice-like environment and apply a force of attraction to their neighbors. The proposed cellular structure allows a better exploitation of local neighborhoods before they move towards the global best, hence it increases population diversity. Experimental results on 14 numerical benchmark functions show that MOA in some benchmark functions can work better than GA and PSO."}
{"_id":"9f832bdcbc9d9566f7ab07b7455364bee62086fb","title":"Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code","text":"Understanding the behavior of source code written in an unfamiliar programming language is difficult. One way to aid understanding of difficult code is to add corresponding pseudo-code, which describes in detail the workings of the code in a natural language such as English. In spite of its usefulness, most source code does not have corresponding pseudo-code because it is tedious to create. This paper demonstrates a tool Pseudogen that makes it possible to automatically generate pseudo-code from source code using statistical machine translation (SMT). Pseudogen currently supports generation of English or Japanese pseudo-code from Python source code, and the SMT framework makes it easy for users to create new generators for their preferred source code\/pseudo-code pairs."}
{"_id":"a8cb524cb9b052ce5ca2220d4197aa6e86f86036","title":"Big Data Analytics for Air Quality Monitoring at a Logistics Shipping Base via Autonomous Wireless Sensor Network Technologies","text":"The indoor air quality in industrial workplace buildings, e.g. air temperature, humidity and levels of carbon dioxide (CO2), play a critical role in the perceived levels of workers' comfort and in reported medical health. CO2 can act as an oxygen displacer, and in confined spaces humans can have, for example, reactions of dizziness, increased heart rate and blood pressure, headaches, and in more serious cases loss of consciousness. Specialized organizations can be brought in to monitor the work environment for limited periods. However, new low cost wireless sensor network (WSN) technologies offer potential for more continuous and autonomous assessment of industrial workplace air quality. Central to effective decision making is the data analytics approach and visualization of what is potentially, big data (BD) in monitoring the air quality in industrial workplaces. This paper presents a case study that monitors air quality that is collected with WSN technologies. We discuss the potential BD problems. The case trials are from two workshops that are part of a large on-shore logistics base a regional shipping industry in Norway. This small case study demonstrates a monitoring and visualization approach for facilitating BD in decision making for health and safety in the shipping industry. We also identify other potential applications of WSN technologies and visualization of BD in the workplace environments; for example, for monitoring of other substances for worker safety in high risk industries and for quality of goods in supply chain management."}
{"_id":"c1b72345ca83f452c73c8c1f7a3bbb900248dc90","title":"Automated Verification of Electrum Wallet","text":"We introduce a formal modeling in ASLan++ of the twofactor authentication protocol used by the Electrum Bitcoin wallet. This allows us to perform an automatic analysis of the wallet and show that it is secure for standard scenarios in Dolev Yao model [Dolev 1981]. The result could be derived thanks to some advanced features of the protocol analyzer such as the possibility to specify i) new intruder deduction rules with clauses and ii) non-deducibility constraints."}
{"_id":"51796da9284df8eae3a9ab6f2731f27fa8900095","title":"A Semi-Supervised Approach for Kernel-Based Temporal Clustering","text":"Temporal clustering refers to the partitioning of a time series into multiple nonoverlapping segments that belong to k temporal clusters, in such a way that segments in the same cluster are more similar to each other than to those in other clusters. Temporal clustering is a fundamental task in many fields, such as computer animation, computer vision, health care, and robotics. The applications of temporal clustering in those areas are diverse, and include human-motion imitation and recognition, emotion analysis, human activity segmentation, automated rehabilitation exercise analysis, and human-computer interaction. However, temporal clustering using a completely unsupervised method may not produce satisfactory results. Similar to regular clustering, temporal clustering also benefits from some expert knowledge that may be available. The type of approach that utilizes a small amount of knowledge to \u201cguide\u201d the clustering process is known as \u201csemi-supervised clustering.\u201d Semi-supervised temporal clustering is a strategy in which extra knowledge, in the form of pairwise constraints, is incorporated into the temporal data to help with the partitioning problem. This thesis proposes a process to adapt and transform two kernel-based methods into semi-supervised temporal clustering methods. The proposed process is exclusive to kernel-based clustering methods, and is based on two concepts. First, it uses the idea of instance-level constraints, in the form of must-link and cannot-link, to supervise the clustering methods. Second, it uses a dynamic-programming method to search for the optimal temporal clusters. The proposed process is applied to two algorithms, aligned cluster analysis (ACA) and spectral clustering. To validate the advantages of the proposed temporal semi-supervised clustering methods, a comparative analysis was performed, using the original versions of the algorithm and another semi-supervised temporal cluster. This evaluation was conducted with both synthetic data and two real-world applications. The first application includes two naturalistic audio-visual human emotion datasets, and the second application focuses on human-motion segmentation. Results show substantial improvements in accuracy, with minimal supervision, compared to unsupervised and other temporal semi-supervised approaches, without compromising time performance."}
{"_id":"18bfccf0e0383f5867e60d35759091e5a25099e1","title":"BRAINIAC: Bringing reliable accuracy into neurally-implemented approximate computing","text":"Applications with large amounts of data, real-time constraints, ultra-low power requirements, and heavy computational complexity present significant challenges for modern computing systems, and often fall within the category of high performance computing (HPC). As such, computer architects have looked to high performance single instruction multiple data (SIMD) architectures, such as accelerator-rich platforms, for handling these workloads. However, since the results of these applications do not always require exact precision, approximate computing may also be leveraged. In this work, we introduce BRAINIAC, a heterogeneous platform that combines precise accelerators with neural-network-based approximate accelerators. These reconfigurable accelerators are leveraged in a multi-stage flow that begins with simple approximations and resorts to more complex ones as needed. We employ high-level, application-specific light-weight checks (LWCs) to throttle this multi-stage acceleration flow and reliably ensure user-specified accuracy at runtime. Evaluation of the performance and energy of our heterogeneous platform for error tolerance thresholds of 5%-25% demonstrates an average of 3\u00d7 gain over computation that only includes precise acceleration, and 15\u00d7-35\u00d7 gain over software-based computation."}
{"_id":"f01fbb6236bbf3bddc90ac14a36811f09f45982b","title":"The folly of forecasting : The effects of a disaggregated sales forecasting system on sales forecast error , sales forecast positive bias , and inventory levels","text":"In this study we provide field evidence of the role that sales forecasts play as the coordination mechanism between sales managers and production managers. An extensive body of operations research documents the negative consequences of sales forecast error and investigates how to respond to sales forecast error by optimizing inventory \u201cbuffer\u201d stocks. In contrast, we focus on whether a change in the sales forecasting information environment implemented at our research site reduces forecast error and, hence, the need for those buffer stocks. The newly implemented sales forecast \u201ccontingency system\u201d disaggregated the sales forecast into two components: (i) an official sales forecast that reflected relatively more certain expected demand, and (ii) a separate report that provided the probability of a contingent demand \u201cevent\u201d occurring and the expected volume impact of that event. We predict and find that the system had the intended effect of a reduction in inventory levels, both through better timing of production via a production \u201cpostponement\u201d strategy and through a decrease in absolute forecast error. We further consider the incentives of the self-interested sales managers and predict and find that the inventory reduction benefits of the sales forecast system gained through postponement and a decline in absolute forecast error were partially offset by an increase in positive sales forecast bias. Our study provides novel insights regarding the role of forecasting within the organizational context. While the operations literature uses analytic and simulation methods to examine sales and operations planning in a very mechanistic way, we examine the role that changes in the sales forecast information environment play and the opportunistic responses of self-interested managers. (266 words)"}
{"_id":"798133be9e9a3868ef488395bbe2c644417a6ae7","title":"Creating people-aware IoT applications by combining design thinking and user-centered design methods","text":"This article presents a methodology based on design thinking and user experience design methods for creating what we call `people-aware' IoT applications, where user needs, not technological opportunities, drive the development. This methodology is divided into 7 steps: discovery, capturing, research, design, prototype, evaluate and refine. The tools used include conventional user experience procedures such as problem identification, group brainstorming, surveys, or interviews, mixed with more IoT-specific design specificities. The results of the methodology include well-described and user-oriented scenarios meeting user's needs and also a complete toolbox to assist the implementation and the testing of abovementioned scenarios in an IoT perspective. The article describes the methodology in detail with the help of a use case conducted in a business environment available for the project that leads to the identification and partial design of concrete people-aware IoT applications in the context of a smart meeting room."}
{"_id":"1b1a3ab704ffacdf20062e4b5bbc1f39d46a26c9","title":"A Survey of Surface-Based Illustrative Rendering for Visualization","text":"In this paper, we survey illustrative rendering techniques for 3D surface models. We first discuss the field of illustrative visualization in general and provide a new definition for this sub-area of visualization. For the remainder of the survey, we then focus on surface-based models. We start by briefly summarizing the differential geometry fundamental to many approaches and discuss additional general requirements for the underlying models and the methods\u2019 implementations. We then provide an overview of low-level illustrative rendering techniques including sparse lines, stippling and hatching, and illustrative shading, connecting each of them to practical examples of visualization applications. We also mention evaluation approaches and list various application fields, before we close with a discussion of the state of the art and future work."}
{"_id":"c28db43fe2bd60c14824c4dc48b7363643a99eee","title":"Recovering the interleaver of an unknown turbo-code","text":"We give here an efficient algorithm for recovering the permutation of an unknown turbo-code when several noisy codewords are given. The algorithm presented here uses the same information as some other algorithms given previously for this problem but in an optimal fashion. This paper also clarifies the link between this problem and the BCJR decoding algorithm."}
{"_id":"5ff520eacf2c3fa22ccad53d7a97950fd34ddf0e","title":"On Quaternions and Octonions : Their Geometry , Arithmetic , and Symmetry","text":"Conway and Smith\u2019s book is a wonderful introduction to the normed division algebras: the real numbers (R), the complex numbers (C), the quaternions (H), and the octonions (O). The first two are well-known to every mathematician. In contrast, the quaternions and especially the octonions are sadly neglected, so the authors rightly concentrate on these. They develop these number systems from scratch, explore their connections to geometry, and even study number theory in quaternionic and octonionic versions of the integers. Conway and Smith warm up by studying two famous subrings of C: the Gaussian integers and Eisenstein integers. The Gaussian integers are the complex numbers x + iy for which x and y are integers. They form a square lattice:"}
{"_id":"c63a8640e5d426b1c8b0ca2ea45c20c265b3f2ad","title":"Class Noise vs. Attribute Noise: A Quantitative Study","text":"Real-world data is never perfect and can often suffer from corruptions (noise) that may impact interpretations of the data, models created from the data and decisions made based on the data. Noise can reduce system performance in terms of classification accuracy, time in building a classifier and the size of the classifier. Accordingly, most existing learning algorithms have integrated various approaches to enhance their learning abilities from noisy environments, but the existence of noise can still introduce serious negative impacts. A more reasonable solution might be to employ some preprocessing mechanisms to handle noisy instances before a learner is formed. Unfortunately, rare research has been conducted to systematically explore the impact of noise, especially from the noise handling point of view. This has made various noise processing techniques less significant, specifically when dealing with noise that is introduced in attributes. In this paper, we present a systematic evaluation on the effect of noise in machine learning. Instead of taking any unified theory of noise to evaluate the noise impacts, we differentiate noise into two categories: class noise and attribute noise, and analyze their impacts on the system performance separately. Because class noise has been widely addressed in existing research efforts, we concentrate on attribute noise. We investigate the relationship between attribute noise and classification accuracy, the impact of noise at different attributes, and possible solutions in handling attribute noise. Our conclusions can be used to guide interested readers to enhance data quality by designing various noise handling mechanisms."}
{"_id":"16a710b41c3bd5fa7051763c509ad0ff814e0e6c","title":"Enabling \u223c100fps detection on a landing unmanned aircraft for its on-ground vision-based recovery","text":"In this paper, a deep learning inspired solution is proposed and developed to enable timeliness and practicality of the pre-existing ground stereo vision guidance system for flxed-wing UAVs' safe landing. Since the ground guidance prototype was restricted within applications due to its untimeliness, eventually the vision-based detection less than 15 fps (frame per second). Under such circumstances, we employ a regression based deep learning algorithm into automatic detection on the flying aircraft in the landing sequential images. The system architecture is upgraded so as to be compatible with the novel deep learning requests, and furthermore, annotated datasets are conducted to support training and testing of the regression-based learning detection algorithm. Experimental results validate that the detection attaches 100 fps or more while the localization accuracy is kept in the same level."}
{"_id":"df2ae4effe67763a3505a30c2b05df6a3efbb93f","title":"Challenges and business models for mobile location-based services and advertising","text":"Mobile advertising will become more pervasive and profitable, but not before addressing key technical and business challenges."}
{"_id":"dc2d5a12c9b6c76e93658c3343f854d2bc65b577","title":"Map Matching with Inverse Reinforcement Learning","text":"We study map-matching, the problem of estimating the route that is traveled by a vehicle, where the points observed with the Global Positioning System are available. A state-of-the-art approach for this problem is a Hidden Markov Model (HMM). We propose a particular transition probability between latent road segments by the use of the number of turns in addition to the travel distance between the latent road segments. We use inverse reinforcement learning to estimate the importance of the number of turns relative to the travel distance. This estimated importance is incorporated in the transition probability of the HMM. We show, through numerical experiments, that the error of map-matching can be reduced substantially with the proposed transition probability."}
{"_id":"cedeeccea19b851cdfa3cd8ce753226c2bf55dd8","title":"High Power Outphasing Modulation","text":""}
{"_id":"f96e93e705ebe287d173857d7469b43e5a1d55dc","title":"Performance Evaluation and Comparative Analysis of SubCarrier Modulation Wake-up Radio Systems for Energy-Efficient Wireless Sensor Networks","text":"Energy-efficient communication is one of the main concerns of wireless sensor networks nowadays. A commonly employed approach for achieving energy efficiency has been the use of duty-cycled operation of the radio, where the node's transceiver is turned off and on regularly, listening to the radio channel for possible incoming communication during its on-state. Nonetheless, such a paradigm performs poorly for scenarios of low or bursty traffic because of unnecessary activations of the radio transceiver. As an alternative technology, Wake-up Radio (WuR) systems present a promising energy-efficient network operation, where target devices are only activated in an on-demand fashion by means of a special radio signal and a WuR receiver. In this paper, we analyze a novel wake-up radio approach that integrates both data communication and wake-up functionalities into one platform, providing a reconfigurable radio operation. Through physical experiments, we characterize the delay, current consumption and overall operational range performance of this approach under different transmit power levels. We also present an actual single-hop WuR application scenario, as well as demonstrate the first true multi-hop capabilities of a WuR platform and simulate its performance in a multi-hop scenario. Finally, by thorough qualitative comparisons to the most relevant WuR proposals in the literature, we state that the proposed WuR system stands out as a strong candidate for any application requiring energy-efficient wireless sensor node communications."}
{"_id":"25fb5a6abcd88ee52bdb3165b844c941e90eb9bf","title":"Revisiting Distributed Synchronous SGD","text":"Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies."}
{"_id":"2d83f7bca2d6f8aa4d2fc8a95489a1e1dc8884f1","title":"Highway long short-term memory RNNS for distant speech recognition","text":"In this paper, we extend the deep long short-term memory (DL-STM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains 43.9\/47.7% WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with 15.7% and 5.3% relative improvement respectively."}
{"_id":"9aa852632f87ba242988e901def37858a1622e35","title":"A REVIEW OF POINT CLOUDS SEGMENTATION AND CLASSIFICATION ALGORITHMS","text":"Today 3D models and point clouds are very popular being currently used in several fields, shared through the internet and even accessed on mobile phones. Despite their broad availability, there is still a relevant need of methods, preferably automatic, to provide 3D data with meaningful attributes that characterize and provide significance to the objects represented in 3D. Segmentation is the process of grouping point clouds into multiple homogeneous regions with similar properties whereas classification is the step that labels these regions. The main goal of this paper is to analyse the most popular methodologies and algorithms to segment and classify 3D point clouds. Strong and weak points of the different solutions presented in literature or implemented in commercial software will be listed and shortly explained. For some algorithms, the results of the segmentation and classification is shown using real examples at different scale in the Cultural Heritage field. Finally, open issues and research topics will be discussed."}
{"_id":"05fcf7d0f007d5beef631737a96768641b6de517","title":"Inferring user demographics and social strategies in mobile social networks","text":"Demographics are widely used in marketing to characterize different types of customers. However, in practice, demographic information such as age, gender, and location is usually unavailable due to privacy and other reasons. In this paper, we aim to harness the power of big data to automatically infer users' demographics based on their daily mobile communication patterns. Our study is based on a real-world large mobile network of more than 7,000,000 users and over 1,000,000,000 communication records (CALL and SMS). We discover several interesting social strategies that mobile users frequently use to maintain their social connections. First, young people are very active in broadening their social circles, while seniors tend to keep close but more stable connections. Second, female users put more attention on cross-generation interactions than male users, though interactions between male and female users are frequent. Third, a persistent same-gender triadic pattern over one's lifetime is discovered for the first time, while more complex opposite-gender triadic patterns are only exhibited among young people.\n We further study to what extent users' demographics can be inferred from their mobile communications. As a special case, we formalize a problem of double dependent-variable prediction-inferring user gender and age simultaneously. We propose the WhoAmI method, a Double Dependent-Variable Factor Graph Model, to address this problem by considering not only the effects of features on gender\/age, but also the interrelation between gender and age. Our experiments show that the proposed WhoAmI method significantly improves the prediction accuracy by up to 10% compared with several alternative methods."}
{"_id":"65cec799ecd6e03119fe61d931a4b83bcdcf4699","title":"An implementation of the earliest deadline first algorithm in Linux","text":"Recently, many projects have been started to introduce some real-time mechanisms into general purpose operating systems (GPOS) in order to make them capable of providing the users with some temporal guarantees. Many of these projects focused especially on Linux for its capillary and widespread adoption throughout many different research and industrial environments.\n By tracking the kernel release cycle, we propose an efficient Earliest Deadline First implementation in the form of a patch-set against the 2.6.27 version, that is the latest released one, as of now. Our implementation provides the user with the possibility to choose SCHED_EDF as one of the possible scheduling policies for a task, with an enhanced version of the standard algorithm. In fact, we propose a new approach to shared resources' access which, differently from many other previous existing works, does not require the user to specify any parameters about the critical sections every task will enter during its execution."}
{"_id":"84205878e0b1702b995a3186563cc996be18600e","title":"Path-space manipulation of physically-based light transport","text":"Industry-quality content creation relies on tools for lighting artists to quickly prototype, iterate, and refine final renders. As industry-leading studios quickly adopt physically-based rendering (PBR) across their art generation pipelines, many existing tools have become unsuitable as they address only simple effects without considering underlying PBR concepts and constraints. We present a novel light transport manipulation technique that operates directly on path-space solutions of the rendering equation. We expose intuitive direct and indirect manipulation approaches to edit complex effects such as (multi-refracted) caustics, diffuse and glossy indirect bounces, and direct\/indirect shadows. With our sketch- and object-space selection, all built atop a parameterized regular expression engine, artists can search and isolate shading effects to inspect and edit. We classify and filter paths on the fly and visualize the selected transport phenomena. We survey artists who used our tool to manipulate complex phenomena on both static and animated scenes."}
{"_id":"23d68bef82c294822147db39ffbfdfcdb860aa88","title":"What's Your Next Move: User Activity Prediction in Location-based Social Networks","text":"Location-based social networks have been gaining increasing popularity in recent years. To increase users\u2019 engagement with location-based services, it is important to provide attractive features, one of which is geo-targeted ads and coupons. To make ads and coupon delivery more effective, it is essential to predict the location that is most likely to be visited by a user at the next step. However, an inherent challenge in location prediction is a huge prediction space, with millions of distinct check-in locations as prediction target. In this paper we exploit the check-in category information to model the underlying user movement pattern. We propose a framework which uses a mixed hidden Markov model to predict the category of user activity at the next step and then predict the most likely location given the estimated category distribution. The advantages of modeling the category level include a significantly reduced prediction space and a precise expression of the semantic meaning of user activities. Extensive experimental results show that, with the predicted category distribution, the number of location candidates for prediction is 5.45 times smaller, while the prediction accuracy is 13.21% higher."}
{"_id":"493765198c1d2c4fc3307e20d3e0df6313d0b88c","title":"Texture Segmentation Based Video Compression Using Convolutional Neural Networks","text":"There has been a growing interest in using different approaches to improve the coding efficiency of modern video codec in recent years as demand for web-based video consumption increases. In this paper, we propose a model-based approach that uses texture analysis\/synthesis to reconstruct blocks in texture regions of a video to achieve potential coding gains using the AV1 codec developed by the Alliance for Open Media (AOM). The proposed method uses convolutional neural networks to extract texture regions in a frame, which are then reconstructed using a global motion model. Our preliminary results show an increase in coding efficiency while maintaining satisfactory visual quality. Introduction With the increasing amount of videos being created and consumed, better video compression tools are needed to provide fast transmission and high visual quality. Modern video coding standards utilize spatial and temporal redundancy in the videos to achieve high coding efficiency and high visual quality with motion compensation techniques and 2-D orthogonal transforms. However, efficient exploitation of statistical dependencies measured by a mean squared error (MSE) does not always produce the best psychovisual result, and may require higher data rate to preserve detail information in the video. Recent advancement in GPU computing has enabled the analysis of large scale data using deep learning method. Deep learning techniques have shown promising performance in many applications such as object detection, natural language process, and synthetic images generation [1, 2, 3, 4]. Several methods have been developed for video applications to improve coding efficiency using deep learning. In [5], sample adaptive offset (SAO) is replaced by a CNN-based in-loop filter (IFCNN) to improve the coding efficiency in HEVC. By learning the predicted residue between the quantized reconstructed frames obtained after deblocking filter (DF) and the original frames, IFCNN is able to reconstruct video frames with higher quality without requiring any bit transmission during coding process. Similar to [5], [6] proposes a Variable-filter-size Residue-learning CNN (VRCNN) to improving coding efficiency by replacing DF and SAO in HEVC. VRCNN is based on the concept of ARCNN [7] which is originally designed for JPEG applications. Instead of only using spatial information to train a CNN to reduce the coding artifacts in HEVC, [8] proposed a spatial temporal residue network (STResNet) as an additional in-loop filter after SAO. A rate-distortion optimization strategy is used to control the on\/off switch of the proposed in-loop filter. There are also some works that have been done in the decoder of HEVC to improve the coding efficiency. In [9], a deep CNN-based auto decoder (DCAD) is implemented in the decoder of HEVC to improve the video quality of decoded video. DCAD is trained to learn the predict residue between decoded video frames and original video frames. By adding the predicted residual generated from DCAD to the compressed video frames, this method enhances the compressed video frames to higher quality. In summary, the above methods improve the coding efficiency by enhancing the quality of reconstructed video frames. However, they require different trained models for video reconstruction at different quantization levels. We are interested in developing deep learning approaches to only encode visually relevant information and use a different coding method for \u201cperceptually insignificant\u201d regions in a frame, which can lead to substantial data rate reductions while maintaining visual quality. In particular, we have developed a model based approach that can be used to improve the coding efficiency by identifying texture areas in a video frame that contain detail irrelevant information, which the viewer does not perceive specific details and can be skipped or encoded at a much lower data rate. The task is then to divide a frame into \u201cperceptually insignificant\u201d texture region and then use a texture model for the pixels in that region. In 1959, Schreiber and colleagues proposed a coding method that divides an image into textures and edges and used it in image coding [10]. This work was later extended by using the human visual system and statistical model to determine the texture region [11, 12, 13]. More recently, several groups have focused on adapting perceptual based approaches to the video coding framework [14]. In our previous work [15], we introduced a texture analyzer before encoding the input sequences to identify detail irrelevant regions in the frame which are classified into different texture classes. At the encoder, no inter-frame prediction is performed for these regions. Instead, displacement of the entire texture region is modeled by just one set of motion parameters. Therefore, only the model parameters are transmitted to the decoder for reconstructing the texture regions using a texture synthesizer. Non-texture regions in the frame are coded conventionally. Since this method uses feature extraction based on texture segmentation technique, a proper set of parameters are required to achieve accurate texture segmentation for different videos. Deep learning methods usually do not require such parameter tuning for inference. As a result, deep learning techniques can be developed to perform texture segmentation and classification for the proposed model-based video coding. A Fisher vector convolution neural networks (FVCNN) that can produce segmentation labels for different texture classes was proposed in [16]. One of the advantage of FV-CNN is that the image input size is flexible and is not limited by the network architecture. Instead of doing pixel-wise classification on texture regions, a texture classification CNN network was described in [17]. To reduce computational expenses, [17] uses a small classification network to classify image patches with size of 227 \u00d7 227. A smaller network is needed to classify smaller image patches in our case. In this paper, we propose a block-based texture segmentation method to extract texture region in a video frame using convolutional neural networks. The block-based segmentation network classifies each 16 \u00d7 16 block in a frame as texture or non-texture. The identified texture region is then synthesized using the temporal correlations among the frames. Our method was implemented using the AOM\/AV1 codec. Preliminary results show significant bitrate savings while maintaining a satisfactory visual quality."}
{"_id":"2862a44d3da8838597ee6d3a89934b9eaf2c2eb3","title":"Visual Descriptors for Dense Tensor Fields in Computational Turbulent Combustion: A Case Study","text":"Simulation and modeling of turbulent flow, and of turbulent reacting flow in particular, involve solving for and analyzing time-dependent and spatially dense tensor quantities, such as turbulent stress tensors. The interactive visual exploration of these tensor quantities can effectively steer the computational modeling of combustion systems. In this article, the authors analyze the challenges in dense symmetric-tensor visualization as applied to turbulent combustion calculation; most notable among these challenges are the dataset size and density. They analyze, together with domain experts, the feasibility of using several established tensor visualization techniques in this application domain. They further examine and propose visual descriptors for volume rendering of the data. Of these novel descriptors, one is a density-gradient descriptor which results in Schlieren-style images, and another one is a classification descriptor inspired by machine-learning techniques. The result is a hybrid visual analysis tool to be utilized in the debugging, benchmarking and verification of models and solutions in turbulent combustion. The authors demonstrate this analysis tool on two example configurations, report feedback from combustion researchers, and summarize the design lessons learned. c \u00a9 2016 Society for Imaging Science and Technology. [DOI: 10.2352\/J.ImagingSci.Technol.2016.60.1.010404] INTRODUCTION Computational simulation of turbulent combustion for gas turbine design has become increasingly important in the last two decades, due in part to environmental concerns and regulations on toxic emissions. Such modern gas turbine designs feature a variety of mixing fuel compositions and possible flow configurations,1,2 which make non-computational simulations difficult. The focus of the computational research effort in this direction is on the development of computational tools for the modeling and prediction of turbulent combustion flows. Received June 30, 2015; accepted for publication Nov. 4, 2015; published online Dec. 10, 2015. Associate Editor: Song Zhang. 1062-3701\/2016\/60(1)\/010404\/11\/$25.00 Tensor quantities are common features in these turbulent combustion models. In particular, stress and strain tensors are often correlated to turbulent quantities\u2014which appear unclosed in the mathematical formulation and thus need to be modeled as part of the computational simulation. Visual identification of the characteristics of such tensor quantities can bring significant insights into the computational modeling process. However, these computational tensor fields are very large and spatially dense\u2014a good example of the Big Data revolution across sciences and engineering. Figure 1 shows an example turbulent combustion configuration, featuring a grid size of 106 and 6\u00d7 106 particles (shown as spheres); this dataset should be considered in contrast to traditional tensor datasets, which feature grid sizes in the 102 range. At such large scales, typical glyph encodings become cluttered and illegible. Furthermore, combustion experts seldom have an intuitive understanding of the tensor quantities. In this respect, froma tensor visualization perspective, workingwith these datasets poses an array of challenges. Are traditional tensor and flow representations useful in this context? Does increasing the level of complexity or expressiveness of such representations help or hinder? Is interaction speed more important than the benefits gained from complex descriptors? In this article, we address a specific application design problem. In the process of exploring the design space, we also investigate some of the larger visualization questions above, through the opportunity of a case study in the computational-combustion domain. In thiswork,motivated by an ongoing collaborationwith domain experts,3 we investigate the challenges associated with the exploratory visualization of tensor quantities in turbulent combustion simulations. We first provide a characterization of the problem domain, including a data analysis. Through a case study involving five senior combustion researchers, we then iteratively explore the space of tensor visual encodings. We implement and evaluate several J. Imaging Sci. Technol. 010404-1 Jan.-Feb. 2016 Marai et al.: Visual descriptors for dense tensor fields in computational turbulent combustion: a case study Figure 1. One timestep in an example turbulent combustion configuration. The grid size is 106. In this image, 6\u00d7 106 particles are shown as spheres. This dataset should be contrasted to traditional tensor datasets, which feature sparse grids in the 102 range. At this scale, typical glyph encodings become cluttered. approaches advocated by the visualization community in an interactive prototype, and we contrast these approaches with the best-of-breed visualization practices in the target domain. Based on domain expert feedback, we then focus our efforts on identifying effective visual descriptors for volume rendering of the combustion tensor data. Our contributions include a novel density-gradient descriptor and the adaptation of a machine-learning classification technique. Next, we evaluate the visual descriptors on two computational-combustion datasets of particular interest, and we show the importance of the proposed approach for debugging the numerical simulation of complex configurations. In an effort to better bridge the gap between the combustion and tensor visualization communities, we describe these tensor field datasets. Last but not least, we contribute a summary of design lessons learned from the study and from the application design process. To the best of our knowledge, this is the first formal, exploratory case study of tensor visualization techniques in the context of very large, high-density turbulent combustion flow. TENSORS IN TURBULENT COMBUSTION MODELING Turbulent Combustion Modeling. A sufficiently accurate, flexible and reliable model can be used for an in silico combustor rig test as a much cheaper alternative to the reallife rig tests employed in combustor design and optimization. In order to achieve such amodel, themethodology should be well tested and proven with lab-scale configurations. Multiple numerical approaches exist for the generation of such computational models of combustion, most notably Direct numerical simulation (DNS), Reynolds-averaged Navier\u2013Stokes (RANS) and Large eddy simulation (LES). DNS, RANS and LES have complementary strengths. However, allmodels begin by describing the compressible reacting flow via a set of partial differential equations (PDEs) that represent the conservation of mass, momentum and energy. These PDEs are a fully coupled set of multi-dimensional non-linear equations and can be posed in a variety of forms depending on the flow conditions (compressibility, scale, flow regime, etc.).4 In this article, we exemplify the visualization of stress\/strain tensors, and therefore restrict the presentation to the pertinent subset of these PDEs, namely the momentum transport equation. Stress, Strain and Turbulent Stress Tensors. A tensor is an extension of the concept of a scalar and a vector to higher orders. For example, while a stress vector is the force acting on a given unit surface, a stress tensor is defined as the components of stress vectors acting on each coordinate surface; thus, stress can be described by a symmetric second-order tensor (a matrix). The velocity stress and strain tensor fields aremanifested in the transport of fluid momentum, which is a vector quantity governed by the following conservation equation:"}
{"_id":"abf8d1b7e2b8e690051ae8e3fc49e24380c712d2","title":"Towards the Semantic Web: Collaborative Tag Suggestions","text":"Content organization over the Internet went through several interesting phases of evolution: from structured di rectories to unstructured Web search engines and more recently, to tagging as a way for aggregating information, a step toward s the semantic web vision. Tagging allows ranking and dat a organization to directly utilize inputs from end us er , enabling machine processing of Web content. Since tags are c reated by individual users in a free form, one important prob lem facing tagging is to identify most appropriate tags, while eliminating noise and spam. For this purpose, we define a set o f general criteria for a good tagging system. These criteria include high coverage of multiple facets to ensure good recall, least effort to reduce the cost involved in browsing, and high popu larity to ensure tag quality. We propose a collaborative tag su gestion algorithm using these criteria to spot high-quality ags. The proposed algorithm employs a goodness measure for t ags derived from collective user authorities to combat spam. Th e goodness measure is iteratively adjusted by a reward-penalty lgorithm, which also incorporates other sources of tags, e.g. , content-based auto-generated tags. Our experiments based on My We b 2.0 show that the algorithm is effective."}
{"_id":"72c99627f3d6bf9a0b00c32d44a0f15ec35ae5c1","title":"The Effect of Focal Loss in Semantic Segmentation of High Resolution Aerial Image","text":"The semantic segmentation of High Resolution Remote Sensing (HRRS) images is the fundamental research area of the earth observation. Convolutional Neural Network (CNN), which has achieved superior performance in computer vision task, is also useful for semantic segmentation of HRRS images. In this work, focal loss is used instead of cross-entropy loss in training of CNN to handle the imbalance in training data. To evaluate the effect of focal loss, we train SegNet and FCN with focal loss and confirm improvement in accuracy in ISPRS 2D Semantic Labeling Contest dataset, especially when $\\gamma$ is 0.5 in SegNet."}
{"_id":"2cdaca30ad5f05703004706a7fcf51060e6a1068","title":"Co-Design for Low Warpage and High Reliability in Advanced Package with TSV-Free Interposer (TFI)","text":"TSV-Free Interposer (TFI) technology eliminates TSV fabrication and reduces manufacturing and material cost. Co-design modelling methodology is established for TFI technology with considering wafer process, package assembly and package\/board level reliability and thermal performance to optimize structure design, wafer process, assembly process and material selection. Experimental results are used for validating warpage modelling results. Through wafer level modelling, suitable carrier wafer and EMC materials are recommended to control wafer warpage less than 2mm. Effects of package substrate coefficient of thermal expansion (CTE) and stiffener on assembly induced package warpage are simulated to reduce package warpage. The recommended materials and geometry design based on reliability are aligned with that from wafer and package warpage simulation results. The final test vehicle (TV) design and material selection are determined based on co-design modelling results for achieving successful TFI wafer process and package assembly process and long term package\/board level reliability."}
{"_id":"b79412cee14e583a5c6816c1124913f560303a95","title":"Learning Fine-grained Features via a CNN Tree for Large-scale Classification","text":"We propose a novel approach to enhance the discriminability of Convolutional Neural Networks (CNN). The key idea is to build a tree structure that could progressively learn fine-grained features to distinguish a subset of classes, by learning features only among these classes. Such features are expected to be more discriminative, compared to features learned for all the classes. We develop a new algorithm to effectively learn the tree structure among a large number of classes. Experiments on large-scale image classification tasks demonstrate that our method could boost the performance of a given basic CNN model. Our method is quite general, hence it can potentially be used in combination with many other deep learning models."}
{"_id":"5ab62d69c9862be4fb80dc1e921752ea8fb625f0","title":"SIMPLIFIED SOLAR TRACKING PROTOTYPE","text":"Solar energy is rapidly advancing as an important means of renewable energy resource. More energy is produced by tracking the solar panel to remain aligned to the sun at a right angle to the rays of light. This paper describes in detail the design and construction of a prototype for solar tracking system with two degrees of freedom, which detects the sunlight using photocells. The control circuit for the solar tracker is based on a PIC16F84A microcontroller (MCU). This is programmed to detect the sunlight through the photocells and then actuate the motor to position the solar panel where it can receive maximum sunlight. Keyword: PIC MCU, tracking, photocell, frame, motion system"}
{"_id":"a7421c8b1dec4401a12f86ee73b7dd105c152691","title":"Real-time automatic tag recommendation","text":"Tags are user-generated labels for entities. Existing research on tag recommendation either focuses on improving its accuracy or on automating the process, while ignoring the efficiency issue. We propose a highly-automated novel framework for real-time tag recommendation. The tagged training documents are treated as triplets of (words, docs, tags), and represented in two bipartite graphs, which are partitioned into clusters by Spectral Recursive Embedding (SRE). Tags in each topical cluster are ranked by our novel ranking algorithm. A two-way Poisson Mixture Model (PMM) is proposed to model the document distribution into mixture components within each cluster and aggregate words into word clusters simultaneously. A new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks. Experiments on large-scale tagging datasets of scientific documents (CiteULike) and web pages del.icio.us) indicate that our framework is capable of making tag recommendation efficiently and effectively. The average tagging time for testing a document is around 1 second, with over 88% test documents correctly labeled with the top nine tags we suggested."}
{"_id":"69bec56035163a2e6da9bf91921c05b9d3ae8d67","title":"Superfamilies of evolved and designed networks.","text":"Complex biological, technological, and sociological networks can be of very different sizes and connectivities, making it difficult to compare their structures. Here we present an approach to systematically study similarity in the local structure of networks, based on the significance profile (SP) of small subgraphs in the network compared to randomized networks. We find several superfamilies of previously unrelated networks with very similar SPs. One superfamily, including transcription networks of microorganisms, represents \"rate-limited\" information-processing networks strongly constrained by the response time of their components. A distinct superfamily includes protein signaling, developmental genetic networks, and neuronal wiring. Additional superfamilies include power grids, protein-structure networks and geometric networks, World Wide Web links and social networks, and word-adjacency networks from different languages."}
{"_id":"bcfc0079c36354dc00cb59018182f3fa0506e9a3","title":"Improving Alzheimer's disease diagnosis with machine learning techniques.","text":"There is not a specific test to diagnose Alzheimer's disease (AD). Its diagnosis should be based upon clinical history, neuropsychological and laboratory tests, neuroimaging and electroencephalography (EEG). Therefore, new approaches are necessary to enable earlier and more accurate diagnosis and to follow treatment results. In this study we used a Machine Learning (ML) technique, named Support Vector Machine (SVM), to search patterns in EEG epochs to differentiate AD patients from controls. As a result, we developed a quantitative EEG (qEEG) processing method for automatic differentiation of patients with AD from normal individuals, as a complement to the diagnosis of probable dementia. We studied EEGs from 19 normal subjects (14 females\/5 males, mean age 71.6 years) and 16 probable mild to moderate symptoms AD patients (14 females\/2 males, mean age 73.4 years. The results obtained from analysis of EEG epochs were accuracy 79.9% and sensitivity 83.2%. The analysis considering the diagnosis of each individual patient reached 87.0% accuracy and 91.7% sensitivity."}
{"_id":"8e63a3e082623804b9850b2bdf38acf647293be6","title":"Cloud Automation: Precomputing Roadmaps for Flexible Manipulation","text":"The goal of this article is to highlight the benefits of cloud automation for industrial adopters and some of the research challenges that must be addressed in this process. The focus is on the use of cloud computing for efficiently planning the motion of new robot manipulators designed for flexible manufacturing floors. In particular, different ways that a robot can interact with a computing cloud are considered, where an architecture that splits computation between the remote cloud and the robot appears advantageous. Given this synergistic robot-cloud architecture, this article describes how solutions from the recent literature can be employed on the cloud during a periodically updated preprocessing phase to efficiently answer manipulation queries on the robot given changes in the workspace. In this setup, interesting tradeoffs arise between path quality and computational efficiency, which are evaluated through simulation. These tradeoffs motivate further research on how motion planning should be executed given access to a computing cloud."}
{"_id":"f414b028e7b8e2b142d32fc18e018f75b114ca96","title":"Soft Robot Arm Inspired by the Octopus","text":"The octopus is a marine animal whose body has no rigid structures. It has eight arms composed of a peculiar muscular structure, named a muscular hydrostat. The octopus arms provide it with both locomotion and grasping capabilities, thanks to the fact that their stiffness can change over a wide range and can be controlled through combined contractions of the muscles. The muscular hydrostat can better be seen as a modifiable skeleton. Furthermore, the morphology the arms and the mechanical characteristics of their tissues are such that the interaction with the environment (i.e., water) is exploited to simplify control. Thanks to this effective mechanism of embodied intelligence, the octopus can control a very high number of degrees of freedom, with relatively limited computing resources. From these considerations, the octopus emerges as a good model for embodied intelligence and for soft robotics. The prototype of a robot arm has been built based on an artificial muscular hydrostat inspired to the muscular hydrostat of the Octopus vulgaris. The prototype presents the morphology of the biological model and the broad arrangement of longitudinal and transverse muscles. Actuation is obtained with cables (longitudinally) and with shape memory alloy springs (transversally). The robot arm combines contractions and it can show the basic movements of the octopus arm, like elongation, shortening and bending, in water. \u00a9 Koninklijke Brill NV, Leiden and The Robotics Society of Japan, 2012"}
{"_id":"f9b6b1cff2d0b3e3987a5ac41ed4c5446ee3eb42","title":"Power Modeling and Optimization for OLED Displays","text":"Emerging organic light-emitting diode (OLED)-based displays obviate external lighting, and consume drastically different power when displaying different colors, due to their emissive nature. This creates a pressing need for OLED display power models for system energy management, optimization as well as energy-efficient GUI design, given the display content or even the graphical-user interface (GUI) code. In this work, we study this opportunity using commercial QVGA OLED displays and user studies. We first present a comprehensive treatment of power modeling of OLED displays, providing models that estimate power consumption based on pixel, image, and code, respectively. These models feature various tradeoffs between computation efficiency and accuracy so that they can be employed in different layers of a mobile system. We validate the proposed models using a commercial QVGA OLED module and a mobile device with a QVGA OLED display. Then, based on the models, we propose techniques that adapt GUIs based on existing mechanisms as well as arbitrarily under usability constraints. Our measurement and user studies show that more than 75 percent display power reduction can be achieved with user acceptance."}
{"_id":"a91de8eb8500eef3eea20fd8f63f460cce354d3b","title":"The 12-Core POWER8\u2122 Processor With 7.6 Tb\/s IO Bandwidth, Integrated Voltage Regulation, and Resonant Clocking","text":"POWER8\u2122 is a 12-core processor fabricated in IBM's 22 nm SOI technology with core and cache improvements driven by big data applications, providing 2.5\u00d7 socket performance over POWER7+\u2122. Core throughput is supported by 7.6 Tb\/s of off-chip I\/O bandwidth which is provided by three primary interfaces, including two new variants of Elastic Interface as well as embedded PCI Gen-3. Power efficiency is improved with several techniques. An on-chip controller based on an embedded PowerPC\u2122 405 processor applies per-core DVFS by adjusting DPLLs and fully integrated voltage regulators. Each voltage regulator is a highly distributed system of digitally controlled microregulators, which achieves a peak power efficiency of 90.5%. A wide frequency range resonant clock design is used in 13 clock meshes and demonstrates a minimum power savings of 4%. Power and delay efficiency is achieved through the use of pulsed-clock latches, which require statistical validation to ensure robust yield."}
{"_id":"ce15e729f003572d459c1301e8322fba86cbdffe","title":"Visceral pain: the ins and outs, the ups and downs.","text":"PURPOSE OF REVIEW\nVisceral pain represents a major clinical problem, yet far less is known about its mechanisms compared with somatic pains, for example, from cutaneous and muscular structures.\n\n\nRECENT FINDINGS\nIn this review, we describe the neuroanatomical bases of visceral pain signalling in the peripheral and central nervous system, comparing to somatic pains and also the channels and receptors involved in these events. We include an overview of potential new targets in the context of mechanisms of visceral pain and hypersensitivity.\n\n\nSUMMARY\nThis review should inform on the recognition of what occurs in patients with visceral pain, why comorbidities are common and how analgesic treatments work."}
{"_id":"c69fecbd7e53f98b7f623056674976dc8d6dce37","title":"On the Use of Logic Trees for Ground-Motion Prediction Equations in Seismic-Hazard Analysis","text":"Logic trees are widely used in probabilistic seismic hazard analysis as a tool to capture the epistemic uncertainty associated with the seismogenic sources and the ground-motion prediction models used in estimating the hazard. Combining two or more ground-motion relations within a logic tree will generally require several conversions to be made, because there are several definitions available for both the predicted ground-motion parameters and the explanatory parameters within the predictive ground-motion relations. Procedures for making conversions for each of these factors are presented, using a suite of predictive equations in current use for illustration. The sensitivity of the resulting ground-motion models to these conversions is shown to be pronounced for some of the parameters, especially the measure of source-to-site distance, highlighting the need to take into account any incompatibilities among the selected equations. Procedures are also presented for assigning weights to the branches in the ground-motion section of the logic tree in a transparent fashion, considering both intrinsic merits of the individual equations and their degree of applicability to the particular application."}
{"_id":"0e3e3c3d8ae5cb7c4636870d69967c197484d3bb","title":"Verb Semantics and Lexical Selection","text":"This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection."}
{"_id":"364c79d2d98819b27641c651cf6553142ef747bf","title":"Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition","text":"As visual recognition scales up to ever larger numbers of categories, maintaining high accuracy is increasingly difficult. In this work, we study the problem of optimizing accuracy-specificity trade-offs in large scale recognition, motivated by the observation that object categories form a semantic hierarchy consisting of many levels of abstraction. A classifier can select the appropriate level, trading off specificity for accuracy in case of uncertainty. By optimizing this trade-off, we obtain classifiers that try to be as specific as possible while guaranteeing an arbitrarily high accuracy. We formulate the problem as maximizing information gain while ensuring a fixed, arbitrarily small error rate with a semantic hierarchy. We propose the Dual Accuracy Reward Trade-off Search (DARTS) algorithm and prove that, under practical conditions, it converges to an optimal solution. Experiments demonstrate the effectiveness of our algorithm on datasets ranging from 65 to over 10,000 categories."}
{"_id":"3a0a839012575ba455f2b84c2d043a35133285f9","title":"Corpus-Guided Sentence Generation of Natural Images","text":"We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone."}
{"_id":"1354476ddeae8354b03e3ab6479fefa58baeb0e1","title":"Understanding user situational relevance in ranking web search results","text":"Relevance plays a key role in determining how ranking features are weighed in rank algorithms. In particular, the Search Engine Result Page (SERP) presents documents from the algorithmic relevance perspective. User relevance, however, is situational and describes the searcher's perception of the contextual document which is interpreted at rank time. Therefore, without it the monotonic decreasing click trend portrayed by rank algorithms will not hold for real time searchers. Instead, users select low ranked documents over high ones or reject the ranking, leading to frustration and several re-query tasks. To understand users situational relevance (USR), we formulate a decreasing probability function to measure each user's personalized probabilistic relevance (PPR). Next, we find an aggregated PPR representing the USR across several query categories. Then, we apply maximum probabilities swapping to modify the rank function and re-rank documents. Finally, we use Normalized Discounted Cumulative Gain (NDCG) and click weight metrics for evaluation. Our results show that a clear interpretation and incorporation of user situational relevance improves click satisfaction by 12.86%, and optimizes click through rate up to 42%."}
{"_id":"0639cc6bcadc6d1a92fcfa89d3beec0d9dfe3102","title":"3D reconstruction based on SIFT and Harris feature points","text":"This paper presents a new 3D reconstruction method using feature points extracted by the SIFT and Harris corner detector. Since the SIFT feature points can be detected stably and relatively accurately, the proposed algorithm first uses the SIFT matching points to calculate the fundamental matrix. On the other hand many of the feature points detected by the SIFT are not what we need for reconstruction, so by combining the SIFT feature points with the Harris corners it is possible to obtain more vivid and detailed 3D information. Experiments have been conducted to validate the proposed method."}
{"_id":"5ae4649147a07e69c590e49e0a05ee9c742467d1","title":"GaN-based high frequency totem-pole bridgeless PFC design with digital implementation","text":"Totem-pole bridgeless PFC is a very promising topology for GaN devices because of very low reverse recovery. However, inherent challenges exist for this topology at zerocrossing point of AC voltage where inductor, current spike is observed which may cause higher harmonics and EMI issue. This paper looks deeply into this phenomena and found two main causes: the transition delay of low frequency leg and the minimum pulse width limit of high frequency leg. A DSP-based digital modulator is designed to address those two issues by applying lead time to the low frequency leg and digital dithering to increase equivalent resolution at the high frequency leg. Some other rules are also summarized to enable robust modulator design, which is the main challenge in this topology. A 500 kHz GaN totem-pole bridgeless PFC converter operating in continuous conduction mode verified the effectiveness of those methods to eliminate the current spike at zero-crossing. All those findings will help the implementation of wide band gap devices in very high frequency PFC circuit."}
{"_id":"0f2e5f0279b015be95fa0e8a0797b453832e1411","title":"Vishleshan: Performance Comparison and Programming Process Mining Algorithms in Graph-Oriented and Relational Database Query Languages","text":"Process-Aware Information System (PAIS) are IT systems that manages, supports business processes and generate large event logs from execution of business processes. Process Mining consists of analyzing event logs generated by PAISs and discover business process models and check for conformance between the discovered and actual models. The large volume of event logs generated are stored in databases. Relational databases perform well for certain class of applications. However, there are certain class of applications for which relational databases are not able to scale. Several NoSQL databases have emerged to encounter the challenges of scalability in traditional databases. Discovering social network from event logs is one of the most challenging and important Process Mining task. Similar-Task algorithm is one of the most widely used Organizational Mining techniques. Our objective is to investigate which of the databases (Relational or Graph) perform better for Organizational Mining under Process Mining. We implement Similar-Task algorithm on relational and NoSQL (graph oriented) databases using only query language constructs. We conduct empirical analysis on a large real world data set to compare the performance of row-oriented database and NoSQL graph-oriented database."}
{"_id":"4ec0c719d348c74ff7e87317db9d3716b664e386","title":"Knowledge management infrastructure and processes on effectiveness of nursing care","text":"The purpose of this study is to investigate the influence of knowledge management infrastructure and process on nursing care effectiveness in selected teaching hospitals in Southwest Nigeria. The organization of nursing knowledge resources is critical to health care organizations for providing safe and high quality care for patients. Therefore, it is necessary to study the role of knowledge management in clinical nursing practice and the outcomes on the effectiveness of nursing care. Despite the critical role of nursing care in defining high-performing health care delivery, knowledge management in this area is still at an early stage of development. Based on organizational capability theory and employing concurrent mixed-method research design, this study seeks to identify the components of knowledge infrastructure and processes that influence the efficiency and effectiveness of nursing care in clinical floor nursing of selected teaching hospitals in Nigeria, a developing country in Africa. The output from the research is expected to contribute to the domain of literature\/knowledge; provide awareness of the knowledge management practices in nursing care in Nigeria; assist nursing administrators, health policy makers and hospital management to exploit and make effective use of knowledge-based resources to enhance nursing care which can improve the productivity of health care organizations."}
{"_id":"e3b915179d070ceb6e7388564c2fa17942750db5","title":"Bidirectional bridgeless PFC with reduced input current distortion and switching loss using gate skipping technique","text":"Bidirectional bridgeless PFC is an advantageous topology for high power application among many bridgeless PFCs due to small EMI filter size and no reverse recovery problem. However due to the large junction capacitance of secondary leg's diodes, the dead angle is shown in the zero-crossing area. Thus, the input current distortion is intensified, and the unnecessary switching loss decreases the efficiency. In this paper, a simple gate skipping technique is implemented to the zero-crossing area of the bidirectional bridgeless PFC. As a result, the unnecessary switching loss of boost switches and the input current distortion are reduced. The improvement of the proposed method is verified by experimental results with the high line input and 800W(400V\/2A) output prototype."}
{"_id":"d4a501fcc263fd5784bb628196c6ab3951e9be75","title":"Patient Monitoring System Using GSM Technology","text":"In this fast pace of life, it is difficult for people to be constantly available for their near ones who might need them while they are suffering from a disease or physical disorder. So also constant monitoring of the patient\u2019s body parameters such as temperature, pulse rate, sugar level etc. becomes difficult. Hence to remove human error and to lessen the burden of monitoring patient\u2019s health from doctor\u2019s head, this paper presents the methodology for monitoring patients remotely using GSM network and Very Large Scale Integration (VLSI) technology. Patient monitoring systems measure physiological characteristics either continuously or at regular intervals of time."}
{"_id":"ee997597395737cf4aa5d66fba620c19d5b5b179","title":"AI Challenges in Human-Robot Cognitive Teaming","text":"Among the many anticipated roles for robots in the future is that of being a human teammate. Aside from all the technological hurdles that have to be overcome with respect to hardware and control to make robots fit to work with humans, the added complication here is that humans have many conscious and subconscious expectations of their teammates \u2013 indeed, we argue that teaming is mostly a cognitive rather than physical coordination activity. This introduces new challenges for the AI and robotics community and requires fundamental changes to the traditional approach to the design of autonomy. With this in mind, we propose an update to the classical view of the intelligent agent architecture, highlighting the requirements for mental modeling of the human in the deliberative process of the autonomous agent. In this article, we outline briefly the recent efforts of ours, and others in the community, towards developing cognitive teammates along these guidelines."}
{"_id":"813c19842d6fe774c490e3076467caced56e79ce","title":"Bayesian policy reuse","text":"A long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain. Acting online requires \u2018fast\u2019 responses, in terms of rapid convergence, especially when the task instance has a short duration such as in applications involving interactions with humans. These requirements can be problematic for many established methods for learning to act. In domains where the agent knows that the task instance is drawn from a family of related tasks, albeit without access to the label of any given instance, it can choose to act through a process of policy reuse from a library in contrast to policy learning. In policy reuse, the agent has prior experience from the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase. We formalise the problem of policy reuse and present an algorithm for efficiently responding to a novel task instance by reusing a policy from this library of existing policies, where the choice is based on observed \u2018signals\u2019 which correlate to policy performance. We achieve this by posing the problem as a Bayesian choice problem with a corresponding notion of an optimal response, but the computation of that response is in many cases intractable. Therefore, to reduce the computation cost of the posterior, we follow a Bayesian optimisation approach and define a set of policy selection functions, which balance exploration in the policy library against exploitation of previously tried policies, together with a model of expected performance of the policy library on their corresponding task instances. We validate our method in several simulated domains of interactive, short-duration episodic tasks, showing rapid convergence in unknown task variations."}
{"_id":"e87e927b9da5f17d4cd710d8b443a8356a1ed5ff","title":"BotGuard: Lightweight real-time botnet detection in software defined networks","text":"The distributed detection of botnets may induce heavy computation and communication costs to network devices. Each device in related scheme only has a regional view of Internet, so it is hard to detect botnet comprehensively. In this paper, we propose a lightweight real-time botnet detection framework called Bot-Guard, which uses the global landscape and flexible configurability of software defined network (SDN) to identify botnets promptly. SDN, as a new network framework, can make centralized control in botnet detection, but there are still some challenges in such detections. We give a convex lens imaging graph (CLI-graph) to depict the topology characteristics of botnet, which allows SDN controller to locate attacks separately and mitigate the burden of network devices. The theoretical and experimental results prove that our scheme is capable of timely botnet detecting in SDNs with the accuracy higher than 90% and the delay less than 56 ms."}
{"_id":"0f5c6113800abd15471a6d3075b0ea09249989bd","title":"A Traceback Attack on Freenet","text":"Freenet is a popular peer to peer anonymous content-sharing network, with the objective to provide the anonymity of both content publishers and retrievers. Despite more than a decade of active development and deployment and the adoption of well-established cryptographic algorithms in Freenet, it remains unanswered how well the anonymity objective of the initial Freenet design has been met. In this paper we develop a traceback attack on Freenet, and show that the originating machine of a content request message in Freenet can be identified; that is, the anonymity of a content retriever can be broken, even if a single request message has been issued by the retriever. We present the design of the traceback attack, and perform both experimental and simulation studies to confirm the feasibility and effectiveness of the attack. For example, with randomly chosen content requesters (and random contents stored in the Freenet testbed), Emulab-based experiments show that, for <inline-formula><tex-math notation=\"LaTeX\">$24$<\/tex-math> <alternatives><inline-graphic xlink:href=\"duan-ieq1-2453983.gif\"\/><\/alternatives><\/inline-formula> to <inline-formula> <tex-math notation=\"LaTeX\">$43$<\/tex-math><alternatives><inline-graphic xlink:href=\"duan-ieq2-2453983.gif\"\/> <\/alternatives><\/inline-formula> percent of the content request messages, we can identify their originating machines. We also briefly discuss potential solutions to address the developed traceback attack. Despite being developed specifically on Freenet, the basic principles of the traceback attack and solutions have important security implications for similar anonymous content-sharing systems."}
{"_id":"124126f1f2688a6acff92e6752a3b0cd33dc50b0","title":"Data summaries for on-demand queries over linked data","text":"Typical approaches for querying structured Web Data collect (crawl) and pre-process (index) large amounts of data in a central data repository before allowing for query answering. However, this time-consuming pre-processing phase however leverages the benefits of Linked Data -- where structured data is accessible live and up-to-date at distributed Web resources that may change constantly -- only to a limited degree, as query results can never be current. An ideal query answering system for Linked Data should return current answers in a reasonable amount of time, even on corpora as large as the Web. Query processors evaluating queries directly on the live sources require knowledge of the contents of data sources. In this paper, we develop and evaluate an approximate index structure summarising graph-structured content of sources adhering to Linked Data principles, provide an algorithm for answering conjunctive queries over Linked Data on theWeb exploiting the source summary, and evaluate the system using synthetically generated queries. The experimental results show that our lightweight index structure enables complete and up-to-date query results over Linked Data, while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost."}
{"_id":"6ad02a3a501ea613688263cacb81027266066ecf","title":"Pure Pursuit Revisited: Field Testing of Autonomous Vehicles in Urban Areas","text":"In this paper, we aim to explore path following. We implement a path following component by referring to the existing Pure Pursuit algorithm. Using the simulation and field operational test, we identified the problem in the path following component. The main problems identified were with respect to vehicles meandering off the path, turning a corner, and the instability of steering control. Therefore, we apply some modifications to the Pure Pursuit[1] algorithm. We have also conducted the simulation and field operational tests again to evaluate these modifications."}
{"_id":"bdb3c0dff85ccac17bd52e5498748084db460944","title":"An ontological analysis of the economic primitives of the extended-REA enterprise information architecture","text":"The Resource-Event-Agent (REA) model for enterprise economic phenomena was first published in The Accounting Review in 1982. Since that time its concepts and use have been extended far beyond its original accountability infrastructure to a framework for enterprise information architectures. The granularity of the model has been extended both up (to enterprise value chains) and down (to workflow or task specification) in the aggregation plane, and additional conceptual type-images and commitment-images have been proposed as well. The REA model actually fits the notion of domain ontology well, a notion that is becoming increasingly important in the era of e-commerce and virtual companies. However, its present and future components have never been analyzed formally from an ontological perspective. This paper intends to do just that, relying primarily on the conceptual terminology of John Sowa. The economic primitives of the original REA model (economic resources, economic events, economic agents, duality relationships, stock-flow relationships, control relationships and responsibility relationships) plus some newer components (commitments, types, custody, reciprocal, etc.) will be analyzed individually and collectively as a specific domain ontology. Such a review can be used to guide further conceptual development of REA extensions."}
{"_id":"77cad1aade0a08387e69b56878f71ec4492a41cb","title":"Point-of-Interest Demand Modeling with Human Mobility Patterns","text":"Point-of-Interest (POI) demand modeling in urban regions is critical for many applications such as business site selection and real estate investment. While some efforts have been made for the demand analysis of some specific POI categories, such as restaurants, it lacks systematic means to support POI demand modeling. To this end, in this paper, we develop a systematic POI demand modeling framework, named Region POI Demand Identification (RPDI), to model POI demands by exploiting the daily needs of people identified from their large-scale mobility data. Specifically, we first partition the urban space into spatially differentiated neighborhood regions formed by many small local communities. Then, the daily activity patterns of people traveling in the city will be extracted from human mobility data. Since the trip activities, even aggregated, are sparse and insufficient to directly identify the POI demands, especially for underdeveloped regions, we develop a latent factor model that integrates human mobility data, POI profiles, and demographic data to robustly model the POI demand of urban regions in a holistic way. In this model, POI preferences and supplies are used together with demographic features to estimate the POI demands simultaneously for all the urban regions interconnected in the city. Moreover, we also design efficient algorithms to optimize the latent model for large-scale data. Finally, experimental results on real-world data in New York City (NYC) show that our method is effective for identifying POI demands for different regions."}
{"_id":"83abfffa02e83dc215ec1dbe8d54f5aedd43ed98","title":"Credit Card Fraud Detection: A case study","text":"In this research, a technique for `Credit Card Fraud Detection' is developed. As fraudsters are increasing day by day. And fallacious transactions are done by the credit card and there are various types of fraud. So to solve this problem combination of technique is used like Genetic Algorithm, Behavior Based Technique and Hidden Markov Model. By this transaction is tested individually and whatever suits the best is further proceeded. And the foremost goal is to detect fraud by filtering the above techniques to get better result."}
{"_id":"948494a85965aebe7dc119f9fd238c5fb51301ef","title":"A Review on the ESD Robustness of Drain-Extended MOS Devices","text":"This paper reviews electrostatic discharge (ESD) investigations on laterally diffused MOS (LDMOS) and drain-extended MOS (DeMOS) devices. The limits of the safe operating area of LDMOS\/DeMOS devices and device physics under ESD stress are discussed under various biasing conditions and layout schemes. Specifically, the root cause of early filament formation is highlighted. Differences in filamentary nature among various LDMOS\/DeMOS devices are shown. Based on the physical understanding, device optimization guidelines are given. Finally, an outlook on technology scaling is presented."}
{"_id":"19138872a2f8ca34e051fdfa03c0dfdb7a77f016","title":"A Survey on Cryptography and Steganography Methods for Information Security","text":"This paper deals with the tidings of cryptography in history, how it has played a vital role in World War -1, World War-2. It also deals with the Substitution, Transposition Cryptographic techniques and Steganography principles which can be used to maintain the confidentiality of computerized and none computerized information files. A number of well known techniques have been adapted for computer usage including the Ceaser cipher, Mono alphabetic cipher, Homophonic substitution, Bale cipher, Play fair cipher, Poly alphabetic cipher, Vigenere Cipher, Onetime pad cipher, Vernam ciphers, Play Color Cipher and usage of rotor machine in Substitutions, Rail fence technique, more complex permutations for more secure transposition and some Steganography principle were briefly discussed with merits and demerits. Finally it gives the broad knowledge on almost all the cryptographic and Steganography principles where a reader or scholar have lot of scope for updating or invention of more secure algorithms to fulfill the global needs in information security."}
{"_id":"408c732c0f06abbc85ac23ab4d3b2353abb4e5dc","title":"Human C-tactile afferents are tuned to the temperature of a skin-stroking caress.","text":"Human C-tactile (CT) afferents respond vigorously to gentle skin stroking and have gained attention for their importance in social touch. Pharmacogenetic activation of the mouse CT equivalent has positively reinforcing, anxiolytic effects, suggesting a role in grooming and affiliative behavior. We recorded from single CT axons in human participants, using the technique of microneurography, and stimulated a unit's receptive field using a novel, computer-controlled moving probe, which stroked the skin of the forearm over five velocities (0.3, 1, 3, 10, and 30 cm s(-1)) at three temperatures (cool, 18 \u00b0C; neutral, 32 \u00b0C; warm, 42 \u00b0C). We show that CTs are unique among mechanoreceptive afferents: they discharged preferentially to slowly moving stimuli at a neutral (typical skin) temperature, rather than at the cooler or warmer stimulus temperatures. In contrast, myelinated hair mechanoreceptive afferents proportionally increased their firing frequency with stroking velocity and showed no temperature modulation. Furthermore, the CT firing frequency correlated with hedonic ratings to the same mechano-thermal stimulus only at the neutral stimulus temperature, where the stimuli were felt as pleasant at higher firing rates. We conclude that CT afferents are tuned to respond to tactile stimuli with the specific characteristics of a gentle caress delivered at typical skin temperature. This provides a peripheral mechanism for signaling pleasant skin-to-skin contact in humans, which promotes interpersonal touch and affiliative behavior."}
{"_id":"a555250adc84829fd42ead8061656a669fb2a29b","title":"Design and fabrication of an autonomous fire fighting robot with multisensor fire detection using PID controller","text":"Recently, Multisensor Fire Detection System (MSFDS) is one of the important research issues. Here, a fire fighter robot is fabricated providing extinguishment platform. The base of the robot is made of the wood of `Rashed tree', locally known as `Kerosene wood'. There is about 1 liter water reserving capacity. An arduino based simple algorithm is used for detection of fire and measurement of distance from fire source while the robot is on its way to extinguish fire. When the fire is detected and the robot is at a distance near to fire, a centrifugal pump is used to throw water for extinguishment purpose. A water spreader is used for effective extinguishing. It is seen that velocity of water is greatly reduced due to the use of water spreader. Two sensors: LM35 and Arduino Flame Sensors are used to detect the fire and distances on its way towards fire. Sensitivity of these sensors at different day times and distances is tested through analog reading of the serial monitor."}
{"_id":"f82d90b5b19db754bec5bf4354eaaf21e5e6d20e","title":"Towards Discipline-Independent Argumentative Zoning : Evidence from Chemistry and Computational Linguistics","text":"Argumentative Zoning (AZ) is an analysis of the argumentative and rhetorical structure of a scientific paper. It has been shown to be reliably used by independent human coders, and has proven useful for various information access tasks. Annotation experiments have however so far been restricted to one discipline, computational linguistics (CL). Here, we present a more informative AZ scheme with 15 categories in place of the original 7, and show that it can be applied to the life sciences as well as to CL. We use a domain expert to encode basic knowledge about the subject (such as terminology and domain specific rules for individual categories) as part of the annotation guidelines. Our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains, chemistry and computational linguistics."}
{"_id":"57aa09e616382c8b111b8e2b6b36ebb9aa0a2d12","title":"Fitting conic sections to \"very scattered\" data: An iterative refinement of the bookstein algorithm","text":"An examination of the geometric interpretation of the error-of-fit measure of the Bookstein algorithm for fitting conic sections shows why it may not be entirely satisfactory when the data are \u201cvery scattered\u201d in the sense that the data points are distributed rather widely about an underlying smooth curve. A simple iterative refinement of the Bookstein algorithm, similar in spirit to iterative weighted least-squares methods in regression analysis, results in a fitted conic section that approximates the conic that would minimize the sum of squared orthogonal distances of data points from the fitted conic. The usefulness and limitations of the refined algorithm are demonstrated on two different types of \u201cvery scattered\u201d data."}
{"_id":"423be52973dab29c31a845ea54c9050aba0d650a","title":"Walking on Minimax Paths for k-NN Search","text":"Link-based dissimilarity measures, such as shortest path or Euclidean commute time distance, base their distance on paths between nodes of a weighted graph. These measures are known to be better suited to data manifold with nonconvex-shaped clusters, compared to Euclidean distance, so that k-nearest neighbor (NN) search is improved in such metric spaces. In this paper we present a new link-based dissimilarity measure based on minimax paths between nodes. Two main benefits of minimax path-based dissimilarity measure are: (1) only a subset of paths is considered to make it scalable, while Euclidean commute time distance considers all possible paths; (2) it better captures nonconvex-shaped cluster structure, compared to shortest path distance. We define the total cost assigned to a path between nodes as Lp norm of intermediate costs of edges involving the path, showing that minimax path emerges from our Lp norm over paths framework. We also define minimax distance as the intermediate cost of the longest edge on the minimax path, then present a greedy algorithm to compute k smallest minimax distances between a query and N data points in O(logN + k log k) time. Numerical experiments demonstrate that our minimax kNN algorithm reduce the search time by several orders of magnitude, compared to existing methods, while the quality of k-NN search is significantly improved over Euclidean distance. Introduction Given a set of N data points X = {x1, . . . ,xN}, k-nearest neighbor (k-NN) search in metric spaces involves finding k closest points in the dataset X to a query xq . Dissimilarity measure defines distance duv between two data points (or nodes of a weighted graph) xu and xv in the corresponding metric space, and the performance of k-NN search depends on distance metric. Euclidean distance \u2016xu \u2212 xv\u20162 is the most popular measure for k-NN search but it does not work well when data points X lie on a curved manifold with nonconvex-shaped clusters (see Fig. 1(a)). Metric learning (Xing et al. 2003; Goldberger et al. 2005; Weinberger and Saul 2009) optimizes parameters involving the Mahalanobis distance using Copyright c \u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. labeled dataset, such that points in the same cluster become close together and points in the different cluster become far apart. Most of metric learning methods are limited to linear embedding, so that the nonconvex-shaped cluster structure is not well captured (see Fig. 1(b)). Link-based (dis)similarity measures (Fouss et al. 2007; Yen, Mantrach, and Shimbo 2008; Yen et al. 2009; Mantrach et al. 2010; Chebotarev 2011) rely on paths between nodes of a weighted graph, on which nodes are associated with data points and intermediate costs (for instance Euclidean distance) are assigned to edge weights. Distance between nodes depends on the total cost that is computed by aggregating edge weights on a path connecting those nodes of interest. Total cost associated with a path is often assumed to be additive (Yen, Mantrach, and Shimbo 2008), so the aggregation reduces to summation. Dissimilarity between two nodes is calculated by integrating the total costs assigned to all possible paths between them. Such integration is often determined by computing the pseudo-inverse of the graph Laplacian, leading to Euclidean commute time distance (ECTD), regularized Laplacian kernel, and Markov diffusion kernel (see (Fouss et al. 2007; Yen, Mantrach, and Shimbo 2008; Fouss et al. 2012) and references therein), which are known to better capture the nonconvex-shaped cluster structure (see Fig. 1(c)). However, all possible paths between nodes need to be considered to compute the distances and the inversion of N \u00d7 N matrix requires O(N) time and O(N) space, so it does not scale well to the problems of interest. Shortest path distance (Dijkstra 1959) is also a popular link-based dissimilarity measure (Tenenbaum, de Silva, and Langford 2000), where only shortest paths are considered to compute the distances between two nodes. Computational cost is reduced, but the cluster structure is not well captured when shortest path distance is used for k-NN search (see Fig. 1(d)). The distance between two nodes is computed along the shortest path only, Randomized shortest path (RSP) dissimilarity measures were proposed as a family of distance measures depending on a single parameter, which has interesting property of reducing, on one end, to the standard shortest path distance when the parameter is large, on the other hand, to the commute time distance when the parameter is near zero (Yen, Mantrach, and Shimbo 2008). In this paper we present a new link-based k-NN search method with minimax paths (Pollack 1960; Gower and Ross (a) Euclidean (b) LMNN (c) ECTD (d) Shortest path (e) Minimax distance Figure 1: Real-world datasets usually contain nonconvex-shaped clusters. For example, two curved clusters (dots) and a query point (star) are given. Heat map shows the closeness to the query at each point, in terms of the distance measure used (best viewed in color; more reddish means closer to the query). (a) Euclidean distance cannot reflect the underlying cluster structure at all. (b) Metric learning (Weinberger and Saul 2009) cannot capture the nonconvex shapes. (c) Link-based distance (Fouss et al. 2007) captures the underlying cluster correctly, but requires a large amount of computation. (d) Shortest path distance is efficient to compute, but it is not reliable near the boundary between different clusters. (e) Our method is as efficient as computing the shortest path distance, while capturing the entire cluster structure correctly. 1969), which is well-known to capture the underlying cluster structure of data (e.g., Fig. 1(e)) (Kim and Choi 2007; Luo et al. 2008; Zadeh and Ben-David 2009). We develop a fast k-NN search algorithm that computes the minimax distance efficiently through the minimax paths between data points. Our method is as efficient as the shortest-path distance computation. More specifically, the overall time for kNN search with N data points is onlyO(logN+k log k). In image retrieval experiments on some public image datasets, our method was several orders of magnitude faster, while achieving the comparable search quality (in terms of precision). The main contributions of this paper are summarized: 1. We develop a novel framework for link-based (dis)similarity measures where we define the total cost (corresponding to dissimilarity) assigned to a path between nodes as Lp norm of intermediate costs of edges involving the path, showing that minimax path emerges from our Lp norm over paths framework. This framework has two main benefits: (1) it can reduce the number of paths considered for the link-based (dis)similarity computation, which improves the scalability greatly; (2) even using a small number of paths, it can capture any nonconvex-shaped cluster structure successfully. 2. We define minimax distance as the intermediate cost of the longest edge on the minimax path, then present a greedy algorithm to compute k smallest minimax distances between a query and N data points in O(logN + k log k) time, whereas the state of the arts, minimax message passing (Kim and Choi 2007) requires O(N) time for k-NN search. Aggregation over Paths: Lp Norm Link-based (dis)similarity is defined on a weighted graph, denoted by G = (X , E): \u2022 The set of nodes, X = {x1, . . . ,xN}, are associated with N data points. E = {(i, j) | i 6= j \u2208 {1, . . . , N}} is a set of edges between the nodes, excluding self-loop edges. \u2022 The graph is usually assumed to be sparse such that the number of edges, |E|, is limited to O(N). A well-known example is the K-NN graph, where an edge (i, j) exists only if xi is one of the K nearest neighbors of xj or xj is one of the K nearest neighbors of xi in the Euclidean space. The value of K is set to a small constant (usually 5-20) for ensuring sparsity. Then a link-based (dis)similarity depends on the total cost associated with paths on the graph. In this section, we describe our Lp norm approach to the cost aggregation over paths to compute the total cost along paths. Let a = (a0, a1, . . . , am) be a path with m hops, connecting the nodes xa0 ,xa1 , . . . ,xam \u2208 X through the consecutive edges {(a0, a1), (a1, a2), . . . , (am\u22121, am)} \u2286 E . We denote a set of paths with m hops between an initial node xu and a destination node xv by Auv = { a \u2223\u2223 a0 = u, am = v, (a`, a`+1) \u2208 E , a` 6= v, \u2200` = 0, . . . ,m\u2212 1 } , (1) and denote a set of all possible paths between xu and xv by Auv = \u222am=1Auv. (2) A weight c(i, j) is assigned to each edge (i, j), representing the intermediate cost of following edge (i, j), usually defined as the Euclidean distance, i.e., c(i, j) = \u2016xi \u2212 xj\u2016. We define the total cost associated with a path a as"}
{"_id":"dfe17bd72e526a55f93511e30d68e68e3d04498b","title":"RCS Reduction of Ridged Waveguide Slot Antenna Array Using EBG Radar Absorbing Material","text":"This letter investigates the application of EBG radar absorbing material (RAM) to asymmetric ridged waveguide slot antenna array to reduce its backward RCS. The EBG RAM is based on the mushroom-like EBG structure loaded with lumped resistances. A ridged waveguide slot antenna array with 4 times 10 slot elements was designed and built, part of the metal ground plane of the array is replaced with this EBG RAM. The measured results show that performance of the antenna array is preserved when EBG RAM is used. At working frequency the backward RCS of antenna array with EBG RAM has dramatically reduced than that of the common antenna array."}
{"_id":"12a68626d06d860a85737b451c2dbbc77b6fa5a6","title":"Curlybot: Designing a New Class of Computational Toys","text":"We introduce an educational toy, called curlybot, as the basis for a new class of toys aimed at children in their early stages of development \u2014 ages four and up. curlybot is an autonomous two-wheeled vehicle with embedded electronics that can record how it has been moved on any flat surface and then play back that motion accurately and repeatedly. Children can use curlybot to develop intuitions for advanced mathematical and computational concepts, like differential geometry, through play away from a traditional computer.\nIn our preliminary studies, we found that children learn to use curlybot quickly. They readily establish an affective and body syntonic connection with curlybot, because of its ability to remember all of the intricacies of their original gesture; every pause, acceleration, and even the shaking in their hand is recorded. Programming by example in this context makes the educational ideas implicit in the design of curlybot accessible to young children."}
{"_id":"d2f328cffab2859b50752175bd3cc26592e9d1b3","title":"BOSCH RARE SOUND EVENTS DETECTION SYSTEMS FOR DCASE 2017 CHALLENGE","text":"In this report, we describe three systems designed at BOSCH for rare audio sound events detection task of DCASE 2017 challenge. The first system is an end-to-end audio event segmentation using embeddings based on deep convolutional neural network (DCNN) and deep recurrent neural network (DRNN) trained on Mel-filter banks and spectogram features. Both system 2 and 3 contain two parts: audio event tagging and audio event segmentation. Audio event tagging selects the positive audio recordings (containing audio events), which are later processed by the audio segmentation part. Feature selection method has been deployed to select a subset of features in both systems. System 2 employs Dilated convolutional neural network on the selected features for audio tagging, and an audio-codebook approach to convert audio features to audio vectors (Audio2vec system) which are then passed to an LSTM network for audio events boundary prediction. System 3 is based on multiple instance learning problem using variational auto encoder (VAE) to perform audio event tagging and segmentation. Similar to system 2, here a LSTM network is used for audio segmentation. Finally, we have utilized models based on score-fusion among different systems to improve the final results."}
{"_id":"2329a46590b2036d508097143e65c1b77e571e8c","title":"Deep Speech: Scaling up end-to-end speech recognition","text":"We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \u201cphoneme.\u201d Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5\u201900, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems."}
{"_id":"6b2921eb65968fb68974b7701e0f3101fdf92eef","title":"Understanding Loneliness in Social Awareness Streams: Expressions and Responses","text":"We studied the experience of loneliness as communicated by thousands of people on Twitter. Using a data set of public Twitter posts containing explicit expressions of loneliness, we qualitatively developed a categorization scheme for these expressions, showing how the context of loneliness expressed on Twitter relates to existing theories about loneliness. A quantitative analysis of the data exposed categories and patterns in communication practices around loneliness. For example, users expressing more severe, enduring loneliness are more likely to be female, and less likely to include requests for social interaction in their tweets. Further, we studied the responses to expressions of loneliness in Twitter\u2019s social settings. Deriving from the same dataset, we examined factors that correlate with the existence and type of response, showing, for example, that men were more likely to receive responses to lonely tweets, and expressions of enduring loneliness are critically less likely to receive responses."}
{"_id":"88dabd8d295ba9f727baccd73c214e094c6d134f","title":"From Node Embedding To Community Embedding","text":"Most of the existing graph embedding methods focus on nodes, which aim to output a vector representation for each node in the graph such that two nodes being \u201cclose\u201d on the graph are close too in the low-dimensional space. Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing. Embedding communities is useful, not only for supporting various community-level applications, but also to help preserve community structure in graph embedding. In fact, we see community embedding as providing a higher-order proximity to define the node closeness, whereas most of the popular graph embedding methods focus on first-order and\/or second-order proximities. To learn the community embedding, we hinge upon the insight that community embedding and node embedding reinforce with each other. As a result, we propose ComEmbed, the first community embedding method, which jointly optimizes the community embedding and node embedding together. We evaluate ComEmbed on real-world data sets. We show it outperforms the state-of-the-art baselines in both tasks of node classification and community prediction."}
{"_id":"e91202fe2aa1eaec2ea35073c0570a06043c5ed6","title":"Predicting Learning by Analyzing Eye-Gaze Data of Reading Behavior","text":"Researchers have highlighted how tracking learners\u2019 eye-gaze can reveal their reading behaviors and strategies, and this provides a framework for developing personalized feedback to improve learning and problem solving skills. In this paper, we describe analyses of eye-gaze data collected from 16 middle school students who worked with Betty\u2019s Brain, an open-ended learning environment, where students learn science by building causal models to teach a virtual agent. Our goal was to test whether newly available consumer-level eye trackers could provide the data that would allow us to probe further into the relations between students\u2019 reading of hypertext resources and building of graphical causal maps. We collected substantial amounts of gaze data and then constructed classifier models to predict whether students would be successful in constructing correct causal links. These models predicted correct map-building actions with an accuracy of 80% (F1 = 0.82; Cohen\u2019s kappa \u03ba = 0.62). The proportions of correct link additions are in turn directly related to learners\u2019 performance in Betty's Brain. Therefore, students\u2019 gaze patterns when reading the resources may be good indicators of their overall performance. These findings can be used to support the development of a real-time eye gaze analysis system, which can detect students reading patterns, and when necessary provide support to help them become better readers."}
{"_id":"f4b44d2374c8387cfca7670d7c0caef769b9496f","title":"A Wireless Sensor Network-Based Ubiquitous Paprika Growth Management System","text":"Wireless Sensor Network (WSN) technology can facilitate advances in productivity, safety and human quality of life through its applications in various industries. In particular, the application of WSN technology to the agricultural area, which is labor-intensive compared to other industries, and in addition is typically lacking in IT technology applications, adds value and can increase the agricultural productivity. This study attempts to establish a ubiquitous agricultural environment and improve the productivity of farms that grow paprika by suggesting a 'Ubiquitous Paprika Greenhouse Management System' using WSN technology. The proposed system can collect and monitor information related to the growth environment of crops outside and inside paprika greenhouses by installing WSN sensors and monitoring images captured by CCTV cameras. In addition, the system provides a paprika greenhouse environment control facility for manual and automatic control from a distance, improves the convenience and productivity of users, and facilitates an optimized environment to grow paprika based on the growth environment data acquired by operating the system."}
{"_id":"ffb6e1d138f86a74e334e7543dcf7b5907274b0a","title":"A conditional approach to dispositional constructs: the local predictability of social behavior.","text":"A conditional approach to dispositions is developed in which dispositional constructs are viewed as clusters of if-then propositions. These propositions summarize contingencies between categories of conditions and categories of behavior rather than generalized response tendencies. A fundamental unit for investigating dispositions is therefore the conditional frequency of acts that are central to a given behavior category in circumscribed situations, not the overall frequency of behaviors. In an empirical application of the model, we examine how people's dispositional judgments are linked to extensive observations of targets' behavior in a range of natural social situations. We identify categories of these social situations in which targets' behavior may be best predicted from observers' dispositional judgements, focusing on the domains of aggression and withdrawal. One such category consists of subjectively demanding or stressful situations that tax people's performance competencies. As expected, children judged to be aggressive or withdrawn were variable across situations in dispositionally relevant behaviors, but they diverged into relatively predictable aggressive and withdrawn actions in situations that required the social, self-regulatory, and cognitive competencies they lacked. Implications of the conditional approach for personality assessment and person perception research are considered."}
{"_id":"3b29e9ae583b28d3f3270b845371845aff4528c7","title":"System structure for software fault tolerance","text":"The paper presents, and discusses the rationale behind, a method for structuring complex computing systems by the use of what we term \u201crecovery blocks\u201d, \u201cconversations\u201d and \u201cfault-tolerant interfaces\u201d. The aim is to facilitate the provision of dependable error detection and recovery facilities which can cope with errors caused by residual design inadequacies, particularly in the system software, rather than merely the occasional malfunctioning of hardware components."}
{"_id":"73677af6c4d7245261a62ca850054928c02f3919","title":"Explainable PCGML via Game Design Patterns","text":"Procedural content generation via Machine Learning (PCGML) is the umbrella term for approaches that generate content for games via machine learning. One of the benefits of PCGML is that, unlike search or grammar-based PCG, it does not require hand authoring of initial content or rules. Instead, PCGML relies on existing content and black box models, which can be difficult to tune or tweak without expert knowledge. This is especially problematic when a human designer needs to understand how to manipulate their data or models to achieve desired results. We present an approach to Explainable PCGML via Design Patterns in which the design patterns act as a vocabulary and mode of interaction between user and model. We demonstrate that our technique outperforms non-explainable versions of our system in interactions with five expert designers, four of whom lack any machine learning expertise."}
{"_id":"2541012a8da585d9f2dfb3d97fdef88c01c2fb84","title":"Learning grammatical structure with Echo State Networks","text":"Echo State Networks (ESNs) have been shown to be effective for a number of tasks, including motor control, dynamic time series prediction, and memorizing musical sequences. However, their performance on natural language tasks has been largely unexplored until now. Simple Recurrent Networks (SRNs) have a long history in language modeling and show a striking similarity in architecture to ESNs. A comparison of SRNs and ESNs on a natural language task is therefore a natural choice for experimentation. Elman applies SRNs to a standard task in statistical NLP: predicting the next word in a corpus, given the previous words. Using a simple context-free grammar and an SRN with backpropagation through time (BPTT), Elman showed that the network was able to learn internal representations that were sensitive to linguistic processes that were useful for the prediction task. Here, using ESNs, we show that training such internal representations is unnecessary to achieve levels of performance comparable to SRNs. We also compare the processing capabilities of ESNs to bigrams and trigrams. Due to some unexpected regularities of Elman's grammar, these statistical techniques are capable of maintaining dependencies over greater distances than might be initially expected. However, we show that the memory of ESNs in this word-prediction task, although noisy, extends significantly beyond that of bigrams and trigrams, enabling ESNs to make good predictions of verb agreement at distances over which these methods operate at chance. Overall, our results indicate a surprising ability of ESNs to learn a grammar, suggesting that they form useful internal representations without learning them."}
{"_id":"c5ffa88dbb91e5bdf9370378a89e5e827ae0e168","title":"Cooperation of the basal ganglia, cerebellum, sensory cerebrum and hippocampus: possible implications for cognition, consciousness, intelligence and creativity","text":"It is suggested that the anatomical structures which mediate consciousness evolved as decisive embellishments to a (non-conscious) design strategy present even in the simplest unicellular organisms. Consciousness is thus not the pinnacle of a hierarchy whose base is the primitive reflex, because reflexes require a nervous system, which the single-celled creature does not possess. By postulating that consciousness is intimately connected to self-paced probing of the environment, also prominent in prokaryotic behavior, one can make mammalian neuroanatomy amenable to dramatically straightforward rationalization. Muscular contraction is the nervous system's only externally directed product, and the signaling routes which pass through the various brain components must ultimately converge on the motor areas. The function of several components is still debatable, so it might seem premature to analyze the global operation of the circuit these routes constitute. But such analysis produces a remarkably simple picture, and it sheds new light on the roles of the individual components. The underlying principle is conditionally permitted movement, some components being able to veto muscular contraction by denying the motor areas sufficient activation. This is true of the basal ganglia (BG) and the cerebellum (Cb), which act in tandem with the sensory cerebrum, and which can prevent the latter's signals to the motor areas from exceeding the threshold for overt movement. It is also true of the anterior cingulate, which appears to play a major role in directing attention. In mammals, the result can be mere thought, provided that a second lower threshold is exceeded. The veto functions of the BG and the Cb stem from inhibition, but the countermanding disinhibition develops at markedly different rates in those two key components. It develops rapidly in the BG, control being exercised by the amygdala, which itself is governed by various other brain regions. It develops over time in the Cb, thereby permitting previously executed movements that have proved advantageous. If cognition is linked to overt or covert movement, intelligence becomes the ability to consolidate individual motor elements into more complex patterns, and creativity is the outcome of a race-to-threshold process which centers on the motor areas. Amongst the ramifications of these ideas are aspects of cortical oscillations, phantom limb sensations, amyotrophic lateral sclerosis (ALS) the difficulty of self-tickling and mirror neurons."}
{"_id":"2f949431c12e4e0e7e1e8bdc8a9e1ca97ef8832e","title":"Incentive mechanism for peer-to-peer media streaming","text":"We propose a rank-based peer-selection mechanism for peer-to-peer media streaming systems. The mechanism provides incentives for cooperation through service differentiation. Contributors to the system are rewarded with flexibility and choice in peer selection, resulting in high quality streaming sessions. Free-riders are given limited options in peer selection, if any, and hence receive low quality streaming. Through simulation and wide-area measurement studies, we verify that the mechanism can provide near optimal streaming quality to the cooperative users until the bottleneck shifts from the sources to the network."}
{"_id":"40f9a4a7107c8fd11acebb62b95b6c78dbb7b608","title":"Model-based Q-learning for humanoid robots","text":"This paper is proposal with regarding reinforcement learning and robotics. It contains our study so far about reinforcement learning problem and Q-learning \u2014 one of the methods to solve it. The method is tested both by running on a simulation and on NAO robot. Both are written in high programming language. Since the testing are also done on NAO robot. This paper also includes our understanding about NAO robot and Robotic Operating System (ROS), and our approach to apply Q-learning on NAO robot. In the end, we have been successfully tested Q-learning method and apply it to NAO robot in real-world environment."}
{"_id":"65baef5edf7d2e12ff5ef8801e3ba6de4c34a56c","title":"Community detection and visualization in social networks: Integrating structural and semantic information","text":"Due to the explosion of social networking and the information sharing among their users, the interest in analyzing social networks has increased over the recent years. Two general interests in this kind of studies are community detection and visualization. In the first case, most of the classic algorithms for community detection use only the structural information to identify groups, that is, how clusters are formed according to the topology of the relationships. However, these methods do not take into account any semantic information which could guide the clustering process, and which may add elements to conduct further analyses. In the second case most of the layout algorithms for clustered graphs have been designed to differentiate the groups within the graph, but they are not designed to analyze the interactions between such groups. Identifying these interactions gives an insight into the way different communities exchange messages or information, and allows the social network researcher to identify key actors, roles, and paths from one community to another.\n This article presents a novel model to use, in a conjoint way, the semantic information from the social network and its structural information to, first, find structurally and semantically related groups of nodes, and second, a layout algorithm for clustered graphs which divides the nodes into two types, one for nodes with edges connecting other communities and another with nodes connecting nodes only within their own community. With this division the visualization tool focuses on the connections between groups facilitating deep studies of augmented social networks."}
{"_id":"43393a561914f05be312a1dff5a757cbc384d1a1","title":"C4: the continuously concurrent compacting collector","text":"C4, the Continuously Concurrent Compacting Collector, an updated generational form of the Pauseless GC Algorithm [7], is introduced and described, along with details of its implementation on modern X86 hardware. It uses a read barrier to support concur- rent compaction, concurrent remapping, and concurrent incremental update tracing. C4 differentiates itself from other generational garbage collectors by supporting simultaneous-generational concurrency: the different generations are collected using concurrent (non stop-the-world) mechanisms that can be simultaneously and independently active. C4 is able to continuously perform concurrent young generation collections, even during long periods of concurrent full heap collection, allowing C4 to sustain high allocation rates and maintain the efficiency typical to generational collectors, without sacrificing response times or reverting to stop-the-world operation. Azul systems has been shipping a commercial implementation of the Pauseless GC mechanism, since 2005. Three successive generations of Azul's Vega series systems relied on custom multi-core processors and a custom OS kernel to deliver both the scale and features needed to support Pauseless GC. In 2010, Azul released its first software-only commercial implementation of C4 for modern commodity X86 hardware, using Linux kernel enhancements to support the required feature set. We discuss implementa- tion details of C4 on X86, including the Linux virtual and physical memory management enhancements that were used to support the high rate of virtual memory operations required for sustained pauseless operation. We discuss updates to the collector's manage- ment of the heap for efficient generational collection and provide throughput and pause time data while running sustained workloads."}
{"_id":"1beddcd2cee2a18e6875d0a624541f1c378cdde8","title":"A study of normative and informational social influences upon individual judgement.","text":"By NOW, many experimental studies (e.g., 1, 3, 6) have demonstrated that individual psychological processes are subject to social influences. Most investigators, however, have not distinguished among different kinds of social influences; rather, they have carelessly used the term \"group\" influence to characterize the impact of many different kinds of social factors. In fact, a review of the major experiments in this area\u2014e.g., those by Sherif (6), Asch (1), Bovard (3)\u2014would indicate that the subjects (5s) in these experiments as they made their judgments were not functioning as members of a group in any simple or obvious manner. The S, in the usual experiment in this area, made perceptual judgments hi the physical presence of others after hearing their judgments. Typically, the S was not given experimental instructions which made him feel that he was a member of a group faced with a common task requiring cooperative effort for its most effective solution. If \"group\" influences were at work in the foregoing experiments, they were subtly and indirectly created rather than purposefully created by the experimenter."}
{"_id":"e7bd0e7a7ee6b0904d5de6e76e095a6a3b88dd12","title":"T REE-STRUCTURED DECODING WITH DOUBLY-RECURRENT NEURAL NETWORKS","text":"We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs."}
{"_id":"20458d93f8bd7fcbd69e3540e9552f5ae577ab99","title":"Extreme Learning Machine for Multilayer Perceptron","text":"Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images\/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via \u21131 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme."}
{"_id":"62c00686e4b8c87aea6e2cf5bfca435ae2dc77fc","title":"Motivational Leadership: Tips From the Business World.","text":"It is an important task for leadership to identify the motivating factors for employees and motivate them to fulfill their individual and organizational goals. Although there are several motivational factors (extrinsic and intrinsic), intrinsic motivational factors such as autonomy, mastery, and purpose are more important for deeper lasting job satisfaction and higher performance. In this article, the authors discuss how an understanding of these factors that influence motivation has the potential to transform an organization."}
{"_id":"f4c8539bed600c9c652aba76a996b8188761d3fe","title":"Stronger Baselines for Trustable Results in Neural Machine Translation","text":"Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios. New research regularly introduces architectural and algorithmic improvements that lead to significant gains over \u201cvanilla\u201d NMT implementations. However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-theart production and shared-task systems. As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use. In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems. Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed. We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong baseline is crucial for reporting reliable experimental results."}
{"_id":"2dc1f0d89a38489070b34327ccb6e4ee0f9bb502","title":"TMCoI-SIOT: A trust management system based on communities of interest for the social Internet of Things","text":"We propose a trust management system (TMS) for the social Internet of things (SIOT) called TMCoI-SIOT. The proposed system integrates numerous factors such as direct and indirect trust; transaction factors and social modeling of trust. The novelty of our approach can be summed up in three aspects. The first aspect concerns the network architecture which is seen as a set of nodes groups called community of interest (CoI). Each community nodes shares the same interests and is headed by an administrator (admin. The second aspect is the trust management system that prevents On-Off attacks. The third aspect concerns the trust prediction method that uses the Kalman filter technique in order to estimate nodes behaviors and prevent possible attacks. We prove the effectiveness of the proposed system by simulation against TMS attacks."}
{"_id":"9b500fca0f2ad51093ce9cc76bc89b9e6ecbac6a","title":"Two comparison-alternative high temperature PCB-embedded transformer designs for a 2 W gate driver power supply","text":"With fast power semiconductor devices based on GaN and SiC becoming more common, there is a need for improved driving circuits. Transformers with smaller inter-winding capacitance in the isolated gate drive power supply helps in reducing the conducted EMI emission from the power converter to auxiliary sources. This paper presents a transformer with a small volume, a low power loss and a small inter-capacitance in a gate drive power supply to fast switching devices, such as GaN HEMT and SiC MOSFET. The transformer core is embedded into PCB to increase the integration density. Two different transformer designs, the coplanar-winding PCB embedded transformer and the toroidal PCB embedded transformer, are presented and compared. The former has a 0.8 pF inter-capacitance and the latter has 85% efficiency with 73 W\/in3 power density. Both designs are dedicated to a 2 W gate drive power supply for wide-band-gap device, which can operate at 200 \u00b0C ambient temperature."}
{"_id":"5c2c7e802a752868087a57de8f151510d1b76414","title":"Towards Effective Log Summarization","text":"Database access logs are the canonical go-to resource for tasks ranging from performance tuning to security auditing. Unfortunately, they are also large, unwieldy, and it can be difficult for a human analyst to divine the intent behind typical queries in the log. With an eye towards creating tools for ad-hoc exploration of queries by intent, we analyze techniques for clustering queries by intent. Although numerous techniques have already been developed for log summarization, they target specific goals like query recommendation or storage layout optimization rather than the more fuzzy notion of query intent. In this paper, we first survey a variety of log summarization techniques, focusing on a class of approaches that use query similarity metrics. We then propose DCABench, a benchmark that evaluates how well query similarity metrics capture query intent, and use it to evaluate three similarity metrics. DCABench uses student answers to query construction assignments to capture a wide range of distinct SQL queries that all have the same intent. Next, we propose and evaluate a query regularization process that standardizes query representations, significantly improving the effectiveness of the three similarity metrics tested. Finally, we propose an entirely new similarity metric based on the Weisfeiler-Lehman (WL) approximate graph isomorphism algorithm, which identifies salient features of a graph \u2014 or in our case, of the abstract syntax tree of a query. We show experimentally that distances in WL-feature space capture a meaningful notion of similarity, while still retaining competitive performance."}
{"_id":"6f657378907d2e4ad3167f4c5fb7b52426a7d6cb","title":"Journey to Data Quality","text":""}
{"_id":"c5e52a5aa89c137ca9dcaaa39b574ae170317b20","title":"Enhancing well-being and alleviating depressive symptoms with positive psychology interventions: a practice-friendly meta-analysis.","text":"Do positive psychology interventions-that is, treatment methods or intentional activities aimed at cultivating positive feelings, positive behaviors, or positive cognitions-enhance well-being and ameliorate depressive symptoms? A meta-analysis of 51 such interventions with 4,266 individuals was conducted to address this question and to provide practical guidance to clinicians. The results revealed that positive psychology interventions do indeed significantly enhance well-being (mean r=.29) and decrease depressive symptoms (mean r=.31). In addition, several factors were found to impact the effectiveness of positive psychology interventions, including the depression status, self-selection, and age of participants, as well as the format and duration of the interventions. Accordingly, clinicians should be encouraged to incorporate positive psychology techniques into their clinical work, particularly for treating clients who are depressed, relatively older, or highly motivated to improve. Our findings also suggest that clinicians would do well to deliver positive psychology interventions as individual (versus group) therapy and for relatively longer periods of time."}
{"_id":"5b5ddfc2a0bfc5a048461eb11c191831cd226014","title":"Finite Automata and Their Decision Problems","text":"Finite automata are considered in this paper a s instruments for classifying finite tapes. Each onetape automaton defines a set of tapes, a two-tape automaton defines a set of pairs of tapes, et cetera. The structure of the defined sets is studied. Various generalizations of the notion of an automaton are introduced and their relation to the classical automata is determined. Some decision problems concerning automata are shown to be solvable by effective algorithms; others turn out to be unsolvable by algorithms."}
{"_id":"5545cff76f3488208b5b22747c1ed00901627180","title":"Impulsive corporal punishment by mothers and antisocial behavior and impulsiveness of children.","text":"This study tested the hypothesis that corporal punishment (CP), such as spanking or slapping a child for purposes of correcting misbehavior, is associated with antisocial behavior (ASB) and impulsiveness by the child. The data were obtained through interviews with a probability sample of 933 mothers of children age 2-14 in two small American cities. Analyses of variance found that the more CP experienced by the child, the greater the tendency for the child to engage in ASB and to act impulsively. These relationships hold even after controlling for family socioeconomic status, the age and sex of the child, nurturance by the mother, and the level of noncorporal interventions by the mother. There were also significant interaction effects of CP with impulsiveness by the mother. When CP was carried out impulsively, it was most strongly related to child impulsiveness and ASB; when CP was done when the mother was under control, the relationship to child behavior problems was reduced but still present. In view of the fact that there is a high risk of losing control when engaged in CP, even by parents who are not usually impulsive, and the fact that impulsive CP is so strongly associated with child behavior problems, the results of this study suggest that CP is an important risk factor for children developing a pattern of impulsive and antisocial behavior which, in turn, may contribute to the level of violence and other crime in society."}
{"_id":"8d06b2be191f8ea5e5db65ca1138566680e703f0","title":"Demystifying Learning Analytics in Personalised Learning","text":"This paper presents learning analytics as a mean to improve students\u2019 learning. Most learning analytics tools are developed by in-house individual educational institutions to meet the specific needs of their students. Learning analytics is defined as a way to measure, collect, analyse and report data about learners and their context, for the purpose of understanding and optimizing learning. The paper concludes by highlighting framework of learning analytics in order to improve personalised learning. In addition, it is an endeavour to define the characterising features that represents the relationship between learning analytics and personalised learning environment. The paper proposes that learning analytics is dependent on personalised approach for both educators and students. From a learning perspective, students can be supported with specific learning process and reflection visualisation that compares their respective performances to the overall performance of a course. Furthermore, the learners may be provided with personalised recommendations for suitable learning resources, learning paths, or peer students through recommending system. The paper\u2019s contribution to knowledge is in considering personalised learning within the context framework of learning analytics."}
{"_id":"f1613bc4a40a987f67eefe60c617a360c6d0d1e8","title":"Erasure coded storage systems for cloud storage \u2014 challenges and opportunities","text":"Erasure coded storage schemes offer a promising future for cloud storage. Highlights of erasure coded storage systems are that these offer the same level of fault tolerance as that of replication, at lower storage footprints. In the big data era, cloud storage systems based on data replication are of dubious usability due to 200% storage overhead in data replication systems. This has prompted storage service providers to use erasure coded storage as an alternative to replication. Refinements are required in various aspects of erasure coded storage systems to make it a real contender against data replication based storage systems. Streamlining huge bandwidth requirements during the recovery of failed nodes, inefficient update operations, effect of topology in recovery and consistency requirements of erasure coded storage systems, are some areas which need attention. This paper presents an in-depth study on the challenges faced, and research pursued in some of these areas. The survey shows that more research is required to improve erasure coded storage system from being bandwidth crunchers to efficient storage systems. Another challenge that has emerged from the study is the requirement of elaborate research for upgrading the erasure coded storage systems from being mere archival storage systems by providing better update methods. Provision of multiple level consistency in erasure coded storage is yet another research opportunity identified in this work. A brief introduction to open source libraries available for erasure coded storage is also presented in the paper."}
{"_id":"12005218a4b5841dd27d75ab88079e7421a56846","title":"Big data and cloud computing: current state and future opportunities","text":"Scalable database management systems (DBMS)---both for update intensive application workloads as well as decision support systems for descriptive and deep analytics---are a critical part of the cloud infrastructure and play an important role in ensuring the smooth transition of applications from the traditional enterprise infrastructures to next generation cloud infrastructures. Though scalable data management has been a vision for more than three decades and much research has focussed on large scale data management in traditional enterprise setting, cloud computing brings its own set of novel challenges that must be addressed to ensure the success of data management solutions in the cloud environment. This tutorial presents an organized picture of the challenges faced by application developers and DBMS designers in developing and deploying internet scale applications. Our background study encompasses both classes of systems: (i) for supporting update heavy applications, and (ii) for ad-hoc analytics and decision support. We then focus on providing an in-depth analysis of systems for supporting update intensive web-applications and provide a survey of the state-of-the-art in this domain. We crystallize the design choices made by some successful systems large scale database management systems, analyze the application demands and access patterns, and enumerate the desiderata for a cloud-bound DBMS."}
{"_id":"19a74d3c08f4fcdbcc367c80992178e93f1ee3bd","title":"Partial-parallel-repair (PPR): a distributed technique for repairing erasure coded storage","text":"With the explosion of data in applications all around us, erasure coded storage has emerged as an attractive alternative to replication because even with significantly lower storage overhead, they provide better reliability against data loss. Reed-Solomon code is the most widely used erasure code because it provides maximum reliability for a given storage overhead and is flexible in the choice of coding parameters that determine the achievable reliability. However, reconstruction time for unavailable data becomes prohibitively long mainly because of network bottlenecks. Some proposed solutions either use additional storage or limit the coding parameters that can be used. In this paper, we propose a novel distributed reconstruction technique, called Partial Parallel Repair (PPR), which divides the reconstruction operation to small partial operations and schedules them on multiple nodes already involved in the data reconstruction. Then a distributed protocol progressively combines these partial results to reconstruct the unavailable data blocks and this technique reduces the network pressure. Theoretically, our technique can complete the network transfer in \u2308(log2(k + 1))\u2309 time, compared to k time needed for a (k, m) Reed-Solomon code. Our experiments show that PPR reduces repair time and degraded read time significantly. Moreover, our technique is compatible with existing erasure codes and does not require any additional storage overhead. We demonstrate this by overlaying PPR on top of two prior schemes, Local Reconstruction Code and Rotated Reed-Solomon code, to gain additional savings in reconstruction time."}
{"_id":"434eb48cfa6e631c9b04b793ca2e827a2d29dc97","title":"Proliferation, cell cycle and apoptosis in cancer","text":"Beneath the complexity and idiopathy of every cancer lies a limited number of 'mission critical' events that have propelled the tumour cell and its progeny into uncontrolled expansion and invasion. One of these is deregulated cell proliferation, which, together with the obligate compensatory suppression of apoptosis needed to support it, provides a minimal 'platform' necessary to support further neoplastic progression. Adroit targeting of these critical events should have potent and specific therapeutic consequences."}
{"_id":"6f2fe472de38ad0995923668aa8e1abe19084d09","title":"Performance Analysis of Engineering Students for Recruitment Using Classification Data Mining Techniques","text":"-Data Mining is a powerful tool for academic intervention. Mining in education environment is called Educational Data Mining. Educational Data Mining is concerned with developing new methods to discover knowledge from educational database and can used for decision making in educational system. In our work, we collected the student\u2019s data from engineering institute that have different information about their previous and current academics records like students S.No., Name, Branch, 10, 12 , B.Tech passing percentage and final grade & then apply different classification algorithm using Data Mining tools (WEKA) for analysis the students academics performance for Training & placement department or company executives. This paper deals with a comparative study of various classification data mining algorithms for the performance analysis of the student\u2019s academic records and check which algorithm is optimal for classifying students\u2019 based on their final grade. This analysis also classifies the performance of Students into Excellent, Good and Average categories. Keywords\u2013 Data Mining, Discover knowledge, Technical Education, Educational Data, Mining, Classification, WEKA, Classifiers."}
{"_id":"4e7a9af1beee401b89a0b66bf881df0abf611263","title":"A novel integrated dielectric-and-conductive ink 3D printing technique for fabrication of microwave devices","text":"In this work a novel combination of printed electronics using conductive nanoparticle ink together with 3D printing of dielectric material is presented as one integrated process. This allows fabrication of complicated 3D electromagnetic (EM) structures such as wide range of different antennas and microwave devices to simultaneously include printing of conductive nanoparticle ink within 3D dielectric configurations. Characterization of conductive ink and polymer based substrate has been done to ensure proper RF, electric, thermal and mechanical performance of both substrate and the ink. A meander line dipole antenna on a V-shaped substrate is printed and tested to demonstrate the efficiency and accuracy of proposed technique. The goal in this paper is to provide a low cost, environmentally friendly integrated process for the fabrication of geometrically complicated 3D electromagnetic structures."}
{"_id":"4e15a268586023e6e6ba6a49eedd4a33e98d531d","title":"Transcriptome-wide Identification of RNA-Binding Protein and MicroRNA Target Sites by PAR-CLIP","text":"RNA transcripts are subject to posttranscriptional gene regulation involving hundreds of RNA-binding proteins (RBPs) and microRNA-containing ribonucleoprotein complexes (miRNPs) expressed in a cell-type dependent fashion. We developed a cell-based crosslinking approach to determine at high resolution and transcriptome-wide the binding sites of cellular RBPs and miRNPs. The crosslinked sites are revealed by thymidine to cytidine transitions in the cDNAs prepared from immunopurified RNPs of 4-thiouridine-treated cells. We determined the binding sites and regulatory consequences for several intensely studied RBPs and miRNPs, including PUM2, QKI, IGF2BP1-3, AGO\/EIF2C1-4 and TNRC6A-C. Our study revealed that these factors bind thousands of sites containing defined sequence motifs and have distinct preferences for exonic versus intronic or coding versus untranslated transcript regions. The precise mapping of binding sites across the transcriptome will be critical to the interpretation of the rapidly emerging data on genetic variation between individuals and how these variations contribute to complex genetic diseases."}
{"_id":"9a76fd4c97aa984b1b70a0fe99810706a226ab41","title":"Revised Estimates for the Number of Human and Bacteria Cells in the Body","text":"Reported values in the literature on the number of cells in the body differ by orders of magnitude and are very seldom supported by any measurements or calculations. Here, we integrate the most up-to-date information on the number of human and bacterial cells in the body. We estimate the total number of bacteria in the 70 kg \"reference man\" to be 3.8\u00b71013. For human cells, we identify the dominant role of the hematopoietic lineage to the total count (\u224890%) and revise past estimates to 3.0\u00b71013 human cells. Our analysis also updates the widely-cited 10:1 ratio, showing that the number of bacteria in the body is actually of the same order as the number of human cells, and their total mass is about 0.2 kg."}
{"_id":"9ca05b6624798e02f1f4a62494b60d8478937914","title":"A New Technique to Design Circularly Polarized Microstrip Antenna by Fractal Defected Ground Structure","text":"A new technique to design single-feed circularly polarized (CP) microstrip antenna is proposed. The CP radiation is obtained by adjusting the dimension of the etched fractal defected ground structure (FDGS) in the ground plane. Parameter studies of the FDGS are given to illustrate the way to achieve CP radiation. The CP microstrip antennas with the second and third iterative FDGS are fabricated and measured. The measured 10-dB return-loss bandwidth of the CP microstrip antenna is about 30 MHz (1.558 to 1.588 GHz), while its 3-dB axial-ratio bandwidth is 6 MHz (1.572 to 1.578 GHz). The gain across the CP band is between 1.7 and 2.2 dBic."}
{"_id":"64b33b7fa675d2f1f19767e35e9857e941e33c78","title":"The Aneka platform and QoS-driven resource provisioning for elastic applications on hybrid Clouds","text":"Cloud computing alters the way traditional software systems are built and run by introducing a utilitybased model for delivering IT infrastructure, platforms, applications, and services. The consolidation of this new paradigm in both enterprises and academia demanded reconsideration in the way IT resources are used, so Cloud computing can be used together with available resources. A case for the utilization of Clouds for increasing the capacity of computing infrastructures is Desktop Grids: these infrastructures typically provide best effort execution of high throughput jobs and other workloads that fit the model of the platform. By enhancing Desktop Grid infrastructures with Cloud resources, it is possible to offer QoS to users, motivating the adoption of Desktop Grids as a viable platform for application execution. In this paper,we describe howAneka, a platform for developing scalable applications on the Cloud, supports such a vision by provisioning resources fromdifferent sources and supporting different applicationmodels.We highlight the key concepts and features of Aneka that support the integration between Desktop Grids and Clouds and present an experiment showing the performance of this integration. \u00a9 2011 Elsevier B.V. All rights reserved."}
{"_id":"d90a33079a401fcc01d31c7f88ffaa7c51fc5c92","title":"Model predictive path-following control of an A.R. drone quadrotor","text":"This paper addresses the design and implementation of the Extended Prediction Self-Adaptive Control (EPSAC) approach to Model Predictive Control (MPC) for path-following. Special attention is paid to the closed-loop performance in order to achieve a fast response without overshoot, which are the necessary conditions to ensure the desired tracking performance in confined or indoor environments. The performance of the proposed MPC strategy is compared to the one achieved using PD controllers. Experimental results using the low-cost quadrotor AR.Drone 2.0 validate the reliability of the proposed strategy for 2D and 3D movements."}
{"_id":"435e2f8a16bccedc8d6728b5ead441998c889111","title":"Mutual Information, Fisher Information, and Population Coding","text":"In the context of parameter estimation and model selection, it is only quite recently that a direct link between the Fisher information and information-theoretic quantities has been exhibited. We give an interpretation of this link within the standard framework of information theory. We show that in the context of population coding, the mutual information between the activity of a large array of neurons and a stimulus to which the neurons are tuned is naturally related to the Fisher information. In the light of this result, we consider the optimization of the tuning curves parameters in the case of neurons responding to a stimulus represented by an angular variable."}
{"_id":"bb4b1f3b7cc1a9a1753c000bce081f0e909a3f55","title":"Novel Concept of the Three-Dimensional Vertical FG nand Flash Memory Using the Separated-Sidewall Control Gate","text":"Recently, we proposed a novel 3-D vertical floating gate (FG)-type nand Flash memory cell array using the separated-sidewall control gate (CG) (S-SCG). This novel cell consists of one cylindrical FG with line-type CG and S-SCG structures. For simplifying the process flow, we realized the common S-SCG lines by using the prestacked polysilicon layer, through which variable medium voltages are applied not only to control the electrically inverted S\/D region but also to assist the program and erase operations. In this paper, we successfully demonstrate the normal Flash cell operation and show its superior performances in comparison with the recent 3-D FG nand cells by using the cylindrical device simulation. It is shown that the proposed cell can realize the highest CG coupling ratio, low-voltage cell operations of program with 15 V at Vth = 4V and erase with 14 V at Vth = -3V , good retention-mode electric field, and sufficient read-mode on-current margin. Moreover, the proposed S-SCG cell array can fully suppress both the interference effects and the disturbance problems at the same time by removing the direct coupling effects in the same cell string, which are the most critical problems of the recent 3-D vertical stacked cell structures. Above all, the proposed cell array has good potential for terabit 3-D vertical nand Flash cell array with highly reliable multilevel cell operation."}
{"_id":"867c9ff1c4801041b0c069c116294f1d71b98b38","title":"\"Preventative\" vs. \"Reactive\": How Parental Mediation Influences Teens' Social Media Privacy Behaviors","text":"Through an empirical, secondary analysis of 588 teens (ages 12 - 17) and one of their parents living in the USA, we present useful insights into how parental privacy concerns for their teens and different parental mediation strategies (direct intervention versus active mediation) influence teen privacy concerns and privacy risk-taking and risk-coping privacy behaviors in social media. Our results suggest that the use of direct intervention by itself may have a suppressive effect on teens, reducing their exposure to online risks but also their ability to engage with others online and to learn how to effectively cope with online risks. Therefore, it may be beneficial for parents to combine active mediation with direct intervention so that they can protect their teens from severe online risks while empowering teens to engage with others online and learn to make good online privacy choices."}
{"_id":"7bcbe5d58352ffc97123526a8fe7188c6185dc2d","title":"The Adult Sensory Profile: measuring patterns of sensory processing.","text":"OBJECTIVE\nThis article describes a series of studies designed to evaluate the reliability and validity of the Adult Sensory Profile.\n\n\nMETHOD\nExpert judges evaluated the construct validity of the items. Coefficient alpha, factor analysis, and correlations of items with subscales determined item reliability, using data from 615 adult sensory profiles. A subsample of 20 adults furnished skin conductance data. A heterogeneous group of 93 adults completed the revised Adult Sensory Profile, and item reliability was reexamined.\n\n\nRESULTS\nExpert judgment indicated that items could be categorized according to Dunn's Model of Sensory Processing. Results suggested reasonable item reliability for all subscales except for the Sensation Avoiding subscale. Skin conductance measures detected distinct patterns of physiological responses consistent with the four-quadrant model. Revision of the Adult Sensory Profile resulted in improved reliability of the Sensation Avoiding subscale.\n\n\nCONCLUSION\nThe series of studies provides evidence to support the four subscales of the Adult Sensory Profile as distinct constructs of sensory processing preferences."}
{"_id":"dca6b4b7cc03e565481e4d8fb255d079c2e61cbb","title":"The core role of the nurse practitioner: practice, professionalism and clinical leadership.","text":"AIM\nTo draw on empirical evidence to illustrate the core role of nurse practitioners in Australia and New Zealand.\n\n\nBACKGROUND\nEnacted legislation provides for mutual recognition of qualifications, including nursing, between New Zealand and Australia. As the nurse practitioner role is relatively new in both countries, there is no consistency in role expectation and hence mutual recognition has not yet been applied to nurse practitioners. A study jointly commissioned by both countries' Regulatory Boards developed information on the core role of the nurse practitioner, to develop shared competency and educational standards. Reporting on this study's process and outcomes provides insights that are relevant both locally and internationally.\n\n\nMETHOD\nThis interpretive study used multiple data sources, including published and grey literature, policy documents, nurse practitioner program curricula and interviews with 15 nurse practitioners from the two countries. Data were analysed according to the appropriate standard for each data type and included both deductive and inductive methods. The data were aggregated thematically according to patterns within and across the interview and material data.\n\n\nFINDINGS\nThe core role of the nurse practitioner was identified as having three components: dynamic practice, professional efficacy and clinical leadership. Nurse practitioner practice is dynamic and involves the application of high level clinical knowledge and skills in a wide range of contexts. The nurse practitioner demonstrates professional efficacy, enhanced by an extended range of autonomy that includes legislated privileges. The nurse practitioner is a clinical leader with a readiness and an obligation to advocate for their client base and their profession at the systems level of health care.\n\n\nCONCLUSION\nA clearly articulated and research informed description of the core role of the nurse practitioner provides the basis for development of educational and practice competency standards. These research findings provide new perspectives to inform the international debate about this extended level of nursing practice.\n\n\nRELEVANCE TO CLINICAL PRACTICE\nThe findings from this research have the potential to achieve a standardised approach and internationally consistent nomenclature for the nurse practitioner role."}
{"_id":"e151ed990085a41668a88545da12daeeaac11e74","title":"Event-based Camera Pose Tracking using a Generative Event Model","text":"Event-based vision sensors mimic the operation of biological retina and they represent a major paradigm shift from traditional cameras. Instead of providing frames of intensity measurements synchronously, at artificially chosen rates, event-based cameras provide information on brightness changes asynchronously, when they occur. Such non-redundant pieces of information are called \u201cevents\u201d. These sensors overcome some of the limitations of traditional cameras (response time, bandwidth and dynamic range) but require new methods to deal with the data they output. We tackle the problem of event-based camera localization in a known environment, without additional sensing, using a probabilistic generative event model in a Bayesian filtering framework. Our main contribution is the design of the likelihood function used in the filter to process the observed events. Based on the physical characteristics of the sensor and on empirical evidence of the Gaussian-like distribution of spiked events with respect to the brightness change, we propose to use the contrast residual as a measure of how well the estimated pose of the event-based camera and the environment explain the observed events. The filter allows for localization in the general case of six degrees-of-freedom motions."}
{"_id":"2a90dfe4736817866f7f388f5c3750222a9a261f","title":"Zero quiescent current, delay adjustable, power-on-reset circuit","text":"A power-on-reset (POR) circuit is extremely important in digital and mixed-signal ICs. It is used to initialize critical nodes of digital circuitry inside the chip during power-on. A novel POR circuit having zero quiescent current and an adjustable delay is proposed and demonstrated. The circuit has been designed in 0.5\u03bcm CMOS process to work for a supply voltage ranging from 1.8V to 5.5V. The circuit generates a POR signal after a predetermined delay, after the supply voltage crosses a predefined threshold voltage. This delay can be increased or decreased via programmable fuses. The POR threshold does not depend upon the supply ramp rate. The Brown-Out (BO) voltage for the proposed circuit matches the minimum supply voltage required by the digital circuitry inside the IC. The circuit consumes zero steady state current, making it ideal for low power applications."}
{"_id":"26747fa3e6b3f53e6d5bcd40076f46054a4f9334","title":"Spatial Partitioning Techniques in Spatial Hadoop","text":"SpatialHadoop is an extended MapReduce framework that supp orts global indexing that spatial partitions the data across machines providing orders of magnitude speedup, compared to traditi onal Hadoop. In this paper, we describe seven alternative partit ioning techniques and experimentally study their effect on the qua lity of the generated index and the performance of range and spatial join queries. We found that using a 1% sample is enough to produce high quality partitions. Also, we found that the total area o f partitions is a reasonable measure of the quality of indexes when r unning spatial join. This study will assist researchers in cho osing a good spatial partitioning technique in distributed enviro nments. 1. INDEXING IN SPATIALHADOOP SpatialHadoop [2, 3] provides a generic indexing algorithm which was used to implement grid, R-tree, and R+-tree based p artitioning. This paper extends our previous study by introduci ng four new partitioning techniques, Z-curve, Hilbert curve, Quad tree, and K-d tree, and experimentally evaluate all of the seven techn iques. The partitioning phase of the indexing algorithm runs in thr ee steps, where the first step is fixed and the last two steps are customiz ed for each partitioning technique. The first step computes number of desired partitionsn based on file size and HDFS block capacity which are both fixed for all partitioning techniques. The second st ep reads a random sample, with a sampling ratio \u03c1, from the input file and uses this sample to partition the space into cells such that number of sample points in each cell is at most \u230ak\/n\u230b, wherek is the sample size. The third step actually partitions the file by as signing each record to one or more cells. Boundary objects are handle d using either thedistribution or replication methods. Thedistribution method assigns an object to exactly one overlapping cell and the cell has to be expanded to enclose all contained records. The replication method avoids expanding cells by replicating each record to all overlapping cells but the query processor has to employ a duplicate avoidance technique to account for replicated records . \u2217This work is supported in part by the National Science Founda tion, USA, under Grants IIS-0952977 and IIS-1218168 and the University of Minnesota Doctoral Disseration Fellowship. This work is licensed under the Creative Commons Attributio nNonCommercial-NoDerivs 3.0 Unported License. To view a cop y f this license, visit http:\/\/creativecommons.org\/licenses\/by-n c-nd\/3.0\/. Obtain permission prior to any use beyond those covered by the license. Contact copyright holder by emailing info@vldb.org. Articles fromthis volume were invited to present their results at the 41st Internatio l Conference on Very Large Data Bases, August 31st September 4th 2015, Koha la Coast, Hawaii. Proceedings of the VLDB Endowment, Vol. 8, No. 12 Copyright 2015 VLDB Endowment 2150-8097\/15\/08. 2. EXPERIMENTAL SETUP All experiments run on Amazon EC2 \u2018m1.large\u2019 instances which have a dual core processor, 7.5 GB RAM and 840 GB disk storage. We use Hadoop 1.2.1 running on Java 1.6 and CentOS 6. Each machine is configured to run three mappers and two reduce rs. Tables 1 and 2 summarize the datasets and configuration param eters used in our experiments, respectively. Default parame ters (in parentheses) are used unless otherwise mentioned. In the fo llowing part, we describe the partitioning techniques, the quer ies we run, and the performance metrics measured in this paper. 2.1 Partitioning Techniques This paper employsgrid and Quad tree as space partitioning techniques;STR, STR+, and K-d tree as data partitioning techniques; andZ-curve andHilbert curve as space filling curve (SFC) partitioning techniques. These techniques can also be grou ped, according to boundary object handling, into replication-based techniques (i.e., Grid, Quad, STR+, and K-d tree) and distributionbased techniques (i.e., STR, Z-Cruve, and Hilbert). Figure 1 illustrates these techniques, where sample points and partition bou daries are shown as dots and rectangles, respectively. 1. Uniform Grid: This technique does not require a random sample as it divides the input MBR using a uniform grid of \u2308\u221an\u2309 \u00d7 \u2308\u221an\u2309 grid cells and employs the replication method to handle boundary objects. 2. Quad tree: This technique inserts all sample points into a quad tree [6] with node capacity of \u230ak\/n\u230b, wherek is the sample size. The boundaries of all leaf nodes are used as cell boundaries. W use the replication method to assign records to cells. 3. STR: This technique bulk loads the random sample into an Rtree using the STR algorithm [8] and the capacity of each node is set to\u230ak\/n\u230b. The MBRs of leaf nodes are used as cell boundaries. Boundary objects are handled using the distribution method where it assigns a record r to the cell with maximal overlap. 4. STR+: This technique is similar to the STR technique but it uses the replication method to handle boundary objects. 5. K-d tree: This technique uses the K-d tree [1] partitioning method to partition the space into n cells. It starts with the input MBR as one cell and partitions it n \u2212 1 times to producen cells. Records are assigned to cells using the replication method. 6. Z-curve: This technique sorts the sample points by their order on the Z-curve and partitions the curve into n splits, each containing roughly\u230ak\/n\u230b points. It uses the distribution method to assign a recordr to one cell by mapping the center point of its MBR to one of then splits. 7. Hilbert curve: This technique is exactly the same as the Zcurve technique but it uses Hilbert space filling curve which has better spatial properties. (a) Grid (b) Quad Tree (c) STR and STR+ (d) K-d Tree (e) Z-curve (f) Hilbert Curve Figure 1: Partitioning Techniques. Examples of nearly-empty partitions are starred. Name Size Records Average Record Size All Objects 88GB 250M 378 bytes Buildings 25GB 109M 234 bytes Roads 23GB 70M 337 bytes Lakes 8.6GB 9M 1KB Cities 1.4GB 170K 8.4KB"}
{"_id":"059b1bcf315be372c8156e07e941fa6539360cba","title":"Mobile application for guiding tourist activities: tourist assistant - TAIS","text":"The paper presents category classification of mobile travel applications accessible at the moment for tourists in application stores for most popular mobile operation systems (Android and iOS). The most interesting category is \"Travel Guides\" that combines \"Information Resources\" and \"Location-Based Services\" category. Authors propose application \"Tourist assistant - TAIS\" that is related to \"Travel Guides\" category and recommends the tourist attractions around. Information about attractions is extracted from different internet sources."}
{"_id":"530867970f5cf19f5115e49292d91ccaae463ca9","title":"A novel isolated electrolytic capacitor-less single-switch AC-DC offline LED driver with power factor correction","text":"Conventional AC-DC driver circuits for Light-Emitting Diode (LED) lamps require large output capacitance across the LED load to minimize the low frequency current ripple. This large capacitance is usually achieved by using an electrolytic capacitor, which has a lifetime that is at least two times less than that of a LED device. To match the potential lifetime of the LEDs, a new isolated single switch AC-DC high power factor LED driver without any electrolytic capacitors is proposed in this paper. In the proposed circuit, the energy storage capacitor is moved to the rectifier side, with a three-winding transformer used to provide isolation; power factor correction as well as to store and provide the required energy to the output. As a result, the energy storage capacitance is significantly reduced, which allows film capacitor to replace the unreliable electrolytic capacitors. The circuit's operating principles and its characteristics are described in this paper. Simulation and experimental results confirm that a power factor of 0.96 is achieved on a 120Vrms, 12W prototype."}
{"_id":"58ec8f00eb746abb4adb27d514333fee3c6bcf97","title":"Lightweight Cryptography for FPGAs","text":"The advent of new low-power Field Programmable Gate Arrays (FPGA) for battery powered devices opens a host of new applications to FPGAs. In order to provide security on resource constrained devices lightweight cryptographic algorithms have been developed. However, there has not been much research on porting these algorithms to FPGAs. In this paper we propose lightweight cryptography for FPGAs by introducing block cipher independent optimization techniques for Xilinx Spartan3 FPGAs and applying them to the lightweight cryptographic algorithms HIGHT and Present. Our implementations are the first reported of these block ciphers on FPGAs. Furthermore, they are the smallest block cipher implementations on FPGAs using only 117 and 91 slices respectively, which makes them comparable in size to stream cipher implementations. Both are less than half the size of the AES implementation by Chodowiec and Gaj without using block RAMs. Present\u2019s throughput over area ratio of 240 Kbps\/slice is similar to that of AES, however, HIGHT outperforms them by far with 720 Kbps\/slice."}
{"_id":"1ea6b2f67a3a7f044209aae0d0fd1cb14a1e9e06","title":"Pixel Recurrent Neural Networks","text":"Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent."}
{"_id":"5ad0e283c4c2aa7b9985012979835d0131fe73d8","title":"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields","text":"We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency."}
{"_id":"9871aa511ca7e3c61c083c327063442bc2c411bf","title":"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks","text":""}
{"_id":"ba753286b9e2f32c5d5a7df08571262e257d2e53","title":"Conditional Generative Adversarial Nets","text":"Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels."}
{"_id":"99a69a20df596b176bbe5aa748b6864032fa0d17","title":"Efficiency of low power audio amplifiers and loudspeakers","text":"In this paper we look at the load presented to audio amplifiers by real transducers. We consider the power losses in Class-AB and Class-D amplifier topologies, and determine that in order to predict efficiency it is necessary to consider the amplifier\/transducer combination. The ability of the class-D amplifier to recycle quadrature load current offers new ways to improve efficiency. INTRODUCTION Class-D amplifiers are beginning to become viable alternatives to the Class-AB amplifier because of the reducing cost of suitable devices. There is particular interest in developing them for low power applications where their intrinsic efficiency advantages are important. The operation of Class-D amplifiers is very different to Class-AB amplifiers and this has implications for other components in the audio chain. Conventionally the performance of audio amplifiers is considered using a pure resistive load of four or eight ohms. The origin of this lies in the nominal impedance given to electro-magnetic loudspeakers. The true impedance of an electro-magnetic loudspeaker will vary over the operating frequency range, but the variability between individual real transducers makes the definition of a more realistic \u2018bench-mark\u2019 load impractical. Other audio transducers also have a complex impedance. Transducers which utilise piezo electric elements have a input impedance that is highly reactive. The load presented to the amplifier is almost entirely capacitive. The power output and efficiency of an amplifier are dependent load impedance and so the resistive load performance may not resemble to the operation of the amplifier in real situations. In this paper we will look at the true load presented to amplifiers by the transducer and derive a more accurate measure of overall efficiency. GENERAL AMPLIFIER EFFICIENCY The electrical efficiency of an amplifier is defined as the ratio of the power developed in the load to the power drawn from the DC supply. Using simple linear analysis we can determine the efficiency of amplifier output stages and the dependence of the efficiency on load parameters. In this section the following symbols are used; Vo Output voltage Vs Supply rail voltage RL Load resistance Ibias Class AB quiescent bias current \u03c6 Load phase angle Zload Load impedance L Class D filter inductance Rin Resistance of filter inductor RDson \u2018On\u2019 state resistance of switching devices fs Class D Switching frequency Class-AB resistive case A simple Class-AB output stage is shown in figure 1. A complementary pair of output devices operate over their linear region to amplify the signal. When the devices are operated in the linear region there will be current flowing through them whilst there is a voltage across them, this will give rise to power dissipation, and hence reduce efficiency. The devices also need a quiescent bias to reduce crossover distortion as one device takes over from the other. Load"}
{"_id":"5d1f2136c3fc17b5ed0864cedcfcca5173ffeaa2","title":"Impact of a contemplative end-of-life training program: being with dying.","text":"OBJECTIVE\nHealth care professionals report a lack of skills in the psychosocial and spiritual aspects of caring for dying people and high levels of moral distress, grief, and burnout. To address these concerns, the \"Being with Dying: Professional Training Program in Contemplative End-of-Life Care\" (BWD) was created. The premise of BWD, which is based on the development of mindfulness and receptive attention through contemplative practice, is that cultivating stability of mind and emotions enables clinicians to respond to others and themselves with compassion. This article describes the impact of BWD on the participants.\n\n\nMETHODS\nNinety-five BWD participants completed an anonymous online survey; 40 completed a confidential open-ended telephone interview.\n\n\nRESULTS\nFour main themes-the power of presence, cultivating balanced compassion, recognizing grief, and the importance of self-care-emerged in the interviews and were supported in the survey data. The interviewees considered BWD's contemplative and reflective practices meaningful, useful, and valuable and reported that BWD provided skills, attitudes, behaviors, and tools to change how they worked with the dying and bereaved.\n\n\nSIGNIFICANCE OF RESULTS\nThe quality of presence has the potential to transform the care of dying people and the caregivers themselves. Cultivating this quality within themselves and others allows clinicians to explore alternatives to exclusively intellectual, procedural, and task-oriented approaches when caring for dying people. BWD provides a rare opportunity to engage in practices and methods that cultivate the stability of mind and emotions that may facilitate compassionate care of dying patients, families, and caregivers."}
{"_id":"f4bd288ddf6f6830bd760db60e6b991ae8d5d0eb","title":"The influence of geometrical deviations of electrical machine systems on the signal quality of the variable reluctance resolver","text":"Variable reluctance resolvers are needed to regulate electrical machines with inverter control. Due to several issues, e.g. discrete windings in the stator and geometric tolerances in the assembly of the electrical machine, the rotors angular position cannot be acquired properly. The subject of this paper is a variable reluctance resolver model to investigate manufacturing induced geometrical tolerances of the electrical machine in means of rotor position. For this purpose, a block diagram of a stator segment is built, connecting 24 segments to one stator core using PLECS. The rotor can be set freely in shape and position. By varying a permeance block the rotation, i.e. the position dependent airgap, is mirrored. The proposed model allows a sensitivity analysis of the sensor signal according to geometrical changes and the absolute position in the electrical machine system. To evaluate the investigations, the results are verified by measurements on a sensor test bench. The simulation results show a similar behavior as the measurement. Although the values differ by a factor of five, this absolute difference is discussed by means of neglected boundary conditions."}
{"_id":"d4d09d20b869389f7313d05462365a9d77706088","title":"Clinical practice guidelines for diagnosis of autism spectrum disorder in adults and children in the UK: a narrative review","text":"BACKGROUND\nResearch suggests that diagnostic procedures for Autism Spectrum Disorder are not consistent across practice and that diagnostic rates can be affected by contextual and social drivers. The purpose of this review was to consider how the content of clinical practice guidelines shapes diagnoses of Autism Spectrum Disorder in the UK; and investigate where, within those guidelines, social factors and influences are considered.\n\n\nMETHODS\nWe electronically searched multiple databases (NICE Evidence Base; TRIP; Social Policy and Practice; US National Guidelines Clearinghouse; HMIC; The Cochrane Library; Embase; Global health; Ovid; PsychARTICLES; PsychINFO) and relevant web sources (government, professional and regional NHS websites) for clinical practice guidelines. We extracted details of key diagnostic elements such as assessment process and diagnostic tools. A qualitative narrative analysis was conducted to identify social factors and influences.\n\n\nRESULTS\nTwenty-one documents were found and analysed. Guidelines varied in recommendations for use of diagnostic tools and assessment procedures. Although multidisciplinary assessment was identified as the 'ideal' assessment, some guidelines suggested in practice one experienced healthcare professional was sufficient. Social factors in operational, interactional and contextual areas added complexity to guidelines but there were few concrete recommendations as to how these factors should be operationalized for best diagnostic outcomes.\n\n\nCONCLUSION\nAlthough individual guidelines appeared to present a coherent and systematic assessment process, they varied enough in their recommendations to make the choices available to healthcare professionals particularly complex and confusing. We recommend a more explicit acknowledgement of social factors in clinical practice guidelines with advice about how they should be managed and operationalised to enable more consistency of practice and transparency for those coming for diagnosis."}
{"_id":"520da33f43fa61ea317a05b21c0457425209cca4","title":"Navigation of an omni-directional mobile robot with active caster wheels","text":"This work deals with navigation of an omni-directional mobile robot with active caster wheels. Initially, the posture of the omni-directional mobile robot is calculated by using the odometry information. Next, the position accuracy of the mobile robot is measured through comparison of the odometry information and the external sensor measurement. Finally, for successful navigation of the mobile robot, a motion planning algorithm that employs kinematic redundancy resolution method is proposed. Through experiments for multiple obstacles and multiple moving obstacles, the feasibility of the proposed navigation algorithm was verified."}
{"_id":"4a68a6a38d5acf40ed43370591495013a47c6f08","title":"Examining a hate speech corpus for hate speech detection and popularity prediction","text":"As research on hate speech becomes more and more relevant every day, most of it is still focused on hate speech detection. By attempting to replicate a hate speech detection experiment performed on an existing Twitter corpus annotated for hate speech, we highlight some issues that arise from doing research in the field of hate speech, which is essentially still in its infancy. We take a critical look at the training corpus in order to understand its biases, while also using it to venture beyond hate speech detection and investigate whether it can be used to shed light on other facets of research, such as popularity of hate tweets."}
{"_id":"d72235862cbe631d71fb234e345f2df3f65ef09a","title":"Process integration of a 27nm, 16Gb Cu ReRAM","text":"A 27nm 16Gb Cu based NV Re-RAM chip has been demonstrated. Novel process introduction to enable this technology include a Damascene Cell, Line-SAC Digit Lines filled with Cu, exhumed-silicided array contacts, raised epitaxial arrays, and high-drive buried access devices."}
{"_id":"06c04b352c0afa70ed33cd53352108c089116ef4","title":"A Comparison of 10 Sampling Algorithms for Configurable Systems","text":"Almost every software system provides configuration options to tailor the system to the target platform and application scenario. Often, this configurability renders the analysis of every individual system configuration infeasible. To address this problem, researchers have proposed a diverse set of sampling algorithms. We present a comparative study of 10 state-of-the-art sampling algorithms regarding their fault-detection capability and size of sample sets. The former is important to improve software quality and the latter to reduce the time of analysis. In a nutshell, we found that sampling algorithms with larger sample sets are able to detect higher numbers of faults, but simple algorithms with small sample sets, such as most-enabled-disabled, are the most efficient in most contexts. Furthermore, we observed that the limiting assumptions made in previous work influence the number of detected faults, the size of sample sets, and the ranking of algorithms. Finally, we have identified a number of technical challenges when trying to avoid the limiting assumptions, which questions the practicality of certain sampling algorithms."}
{"_id":"636e5f1f0c4df3c6b5d230e3375c6c52129430c4","title":"Gaussian Process Regression Networks","text":"We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the nonparametric flexibility of Gaussian processes. GPRN accommodates input (predictor) dependent signal and noise correlations between multiple output (response) variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both elliptical slice sampling and variational Bayes inference procedures for GPRN. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on real datasets, including a 1000 dimensional gene expression dataset."}
{"_id":"60b3d1bfcb7a2bbd46cde6c6c5c91334cf1d13a3","title":"Microplastics in the marine environment.","text":"This review discusses the mechanisms of generation and potential impacts of microplastics in the ocean environment. Weathering degradation of plastics on the beaches results in their surface embrittlement and microcracking, yielding microparticles that are carried into water by wind or wave action. Unlike inorganic fines present in sea water, microplastics concentrate persistent organic pollutants (POPs) by partition. The relevant distribution coefficients for common POPs are several orders of magnitude in favour of the plastic medium. Consequently, the microparticles laden with high levels of POPs can be ingested by marine biota. Bioavailability and the efficiency of transfer of the ingested POPs across trophic levels are not known and the potential damage posed by these to the marine ecosystem has yet to be quantified and modelled. Given the increasing levels of plastic pollution of the oceans it is important to better understand the impact of microplastics in the ocean food web."}
{"_id":"badbdaacdfa3dc3b36da1a3fe338392a1259c14b","title":"Machine Learning and Social Robotics for Detecting Early Signs of Dementia","text":"This paper presents the EACare project, an ambitious multidisciplinary collaboration with the aim to develop an embodied system, capable of carrying out neuropsychological tests to detect early signs of dementia, e.g., due to Alzheimer\u2019s disease. The system will use methods from Machine Learning and Social Robotics, and be trained with examples of recorded clinician-patient interactions. The interaction will be developed using a participatory design approach. We describe the scope and method of the project, and report on a first Wizard of Oz prototype."}
{"_id":"83923ec80bf89923bcc896768e957a5af6bf8355","title":"The basic building blocks and evolution of CRISPR-CAS systems.","text":"CRISPR (clustered regularly interspaced short palindromic repeats)-Cas (CRISPR-associated) is an adaptive immunity system in bacteria and archaea that functions via a distinct self\/non-self recognition mechanism that involves unique spacers homologous with viral or plasmid DNA and integrated into the CRISPR loci. Most of the Cas proteins evolve under relaxed purifying selection and some underwent dramatic structural rearrangements during evolution. In many cases, CRISPR-Cas system components are replaced either by homologous or by analogous proteins or domains in some bacterial and archaeal lineages. However, recent advances in comparative sequence analysis, structural studies and experimental data suggest that, despite this remarkable evolutionary plasticity, all CRISPR-Cas systems employ the same architectural and functional principles, and given the conservation of the principal building blocks, share a common ancestry. We review recent advances in the understanding of the evolution and organization of CRISPR-Cas systems. Among other developments, we describe for the first time a group of archaeal cas1 gene homologues that are not associated with CRISPR-Cas loci and are predicted to be involved in functions other than adaptive immunity."}
{"_id":"86be6821e65ed895e915d4cd679983c661f36c27","title":"An algorithmic approach to multimodality midfacial rejuvenation using a new classification system for midfacial aging.","text":"Midfacial aging is the result of the complex interplay between the osseous skeleton, facial retaining ligaments, soft tissues envelope, facial fat compartments, and the overlying skin elasticity. As a result of the many anatomic components involved in midfacial aging, the authors proposed a classification system based on distinct anatomic factors to direct surgical treatment. Evidence based data suggest that midface rejuvenation often requires a multimodality approach to obtain desired results, especially in patients with more advanced aging and poor tissue elasticity, or those with hypoplastic midfacial skeletal structure."}
{"_id":"446929dbe6d71f386d13650fcd01b0a83c357ba2","title":"A Robust Mid-Level Representation for Harmonic Content in Music Signals","text":"When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research."}
{"_id":"bf79616a8d4d535b1ffdce3eedca0df4dda454be","title":"Roman-txt: forms and functions of roman urdu texting","text":"In this paper, we present a user study conducted on students of a local university in Pakistan and collected a corpus of Roman Urdu text messages. We were interested in forms and functions of Roman Urdu text messages. To this end, we collected a mobile phone usage dataset. The data consists of 116 users and 346, 455 text messages. Roman Urdu text, is the most widely adopted style of writing text messages in Pakistan. Our user study leads to interesting results, for instance, we were able to quantitatively show that a number of words are written using more than one spelling; most participants of our study were not comfortable in English and hence they write their text messages in Roman Urdu; and the choice of language adopted by the participants sometimes varies according to who the message is being sent. Moreover we found that many young students send text messages(SMS) of intimate nature."}
{"_id":"cb67d3d6c085f1fa325cfe34a132e67953f1d26f","title":"OCR and RFID enabled vehicle identification and parking allocation system","text":"The available parking management systems require human efforts for recording entries of coming and leaving vehicles from parking in sheets. For huge parking it is difficult to keep track of the information. Use of Radio Frequency Identification known as RFID technology reduces human efforts and Optical Character Recognition known as OCR, enabled system will provide an automated system for parking management. And also it will provide the control over the access of parking space by the use of boom barriers. For huge parking it will be an effective system. Parking can be a commercial one or can be a Very Important Person (VIP) parking. Depending on the usage system can be used. Vehicle cannot be parked in parking if it is not registered. OCR acts as a solution for vehicles which are not registered. To keep track of the happenings in parking cameras are used. Places where automated systems are required, the proposed system is an answer. System combines the two strong technologies RFID and OCR."}
{"_id":"2621c894885e3f42bca8bd2b11dab1637e697814","title":"The REVIVE (REal Women's VIews of Treatment Options for Menopausal Vaginal ChangEs) survey in Europe: Country-specific comparisons of postmenopausal women's perceptions, experiences and needs.","text":"OBJECTIVES\nTo achieve a better comprehension of the variability of perceptions, experiences and needs in terms of sexual and vaginal health in postmenopausal women (PMW) from four different European countries.\n\n\nMETHODS\nAn internet-based survey was conducted in Italy, Germany, Spain and the United Kingdom with a total surveyed population of 3768 PMW aged between 45 and 75 years.\n\n\nRESULTS\nThe UK sample was significantly older, with almost a quarter of participants over 65 years of age, and had the highest proportion of women experiencing recent vulvar and vaginal atrophy (52.8%). The majority of Italian and Spanish participants were receiving VVA treatment, whereas in the UK only 28% of PMW were on medication. The most common menopausal symptom was vaginal\/vulvar dryness, with almost 80% of participants reporting it in all the countries except the UK (48%). On the other hand, vaginal\/vulvar irritation was more frequently reported in the UK (41%). The percentage of participants with a partner was lower in the UK (71%), as was the monthly rate of sexual activity (49%). In the UK, the proportion of participants who had seen a healthcare professional for gynaecological reasons in the last year was lower than in other countries (27% vs. \u226550%), as was the proportion who has discussed their VVA symptoms with them (45% vs. \u223c67%). In this sense, UK PMW waited for a longer before asking for help (especially for pain with intercourse and dryness). The main issues relating to VVA treatment difficulties expressed by participants were administration route in the UK, efficacy in Germany, and side-effects in Italy.\n\n\nCONCLUSIONS\nAlthough all European women shared the same expectation of improving the quality of their sex lives, the opportunity for that varied among different countries in relation to the healthcare system and to the effective communication achieved with healthcare professionals when managing VVA."}
{"_id":"d2f5c15df36ef495d176a5683de636eef3089359","title":"Do burnout and work engagement predict depressive symptoms and life satisfaction? A three-wave seven-year prospective study.","text":"BACKGROUND\nBurnout and work engagement have been viewed as opposite, yet distinct states of employee well-being. We investigated whether work-related indicators of well-being (i.e. burnout and work engagement) spill-over and generalize to context-free well-being (i.e. depressive symptoms and life satisfaction). More specifically, we examined the causal direction: does burnout\/work engagement lead to depressive symptoms\/life satisfaction, or the other way around?\n\n\nMETHODS\nThree surveys were conducted. In 2003, 71% of all Finnish dentists were surveyed (n=3255), and the response rate of the 3-year follow-up was 84% (n=2555). The second follow-up was conducted four years later with a response rate of 86% (n=1964). Structural equation modeling was used to investigate the cross-lagged associations between the study variables across time.\n\n\nRESULTS\nBurnout predicted depressive symptoms and life dissatisfaction from T1 to T2 and from T2 to T3. Conversely, work engagement had a negative effect on depressive symptoms and a positive effect on life satisfaction, both from T1 to T2 and from T2 to T3, even after adjusting for the impact of burnout at every occasion.\n\n\nLIMITATIONS\nThe study was conducted among one occupational group, which limits its generalizability.\n\n\nCONCLUSIONS\nWork-related well-being predicts general wellbeing in the long-term. For example, burnout predicts depressive symptoms and not vice versa. In addition, burnout and work engagement are not direct opposites. Instead, both have unique, incremental impacts on life satisfaction and depressive symptoms."}
{"_id":"5728401922a96d0918b041162180873079b44db0","title":"Handwriting difficulties in primary school children: a search for underlying mechanisms.","text":"OBJECTIVE\nThis study investigated the contribution of perceptual-motor dysfunction and cognitive planning problems to the quality or speed of handwriting in children with handwriting problems (HWP).\n\n\nMETHOD\nTwenty-nine children with HWP and 20 classroom peers attending regular schools (grade 2 and grade 3) were tested with regard to visual perception, visual-motor integration, fine motor coordination, and cognitive planning abilities.\n\n\nRESULTS\nThe HWP group scored significantly lower on visual perception, visual-motor integration, fine motor coordination, and cognitive planning in comparison with classroom controls. Regression analyses showed that visual-motor integration was the only significant predictor for quality of handwriting in the HWP group, whereas fine motor coordination (i.e., unimanual dexterity) was the only significant predictor of quality of handwriting in the control group.\n\n\nCONCLUSIONS\nResults suggest that two different mechanisms underlie the quality of handwriting in children with and without handwriting problems. Poor quality of handwriting of children with HWP seems particularly related to a deficiency in visual-motor integration."}
{"_id":"46a26a258bbd087240332376a6ead5f1f7cf497a","title":"INFORMATION TECHNOLOGY AND ORGANIZATIONAL CHANGE : CAUSAL STRUCTURE IN THEORY AND","text":"JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. This article concerns theories about why and how information technology affects organizational life. Good theory guides research, which, when applied, increases the likelihood that information technology will be employed with desirable consequences for users, organizations, and other interested parties. But what is a good theory? Theories are often evaluated in terms of their content-the specific concepts used and the human values served. This article examines theories in terms of their structures-theorists' assumptions about the nature and direction of causal influence. Three dimensions of causal structure are considered-causal agency, logical structure, and level of analysis. Causal agency refers to beliefs about the nature of causality: whether external forces cause change, whether people act purposefully to accomplish intended objectives, or whether changes emerge unpredictably from the interaction of people and events. Logical structure refers to the temporal aspect of theory-static versus dynamic-and to the logical relationships between the \"causes\" and the outcomes. Level of analysis refers to the entities about which the theory poses concepts and relationships-individuals, groups, organizations, and society. While there are many possible structures for good theory about the role of information technology in organizational change, only a few of these structures can be seen in current theorizing. Increased awareness of the options, open discussion of their advantages and disadvantages , and explicit characterization of future theoretical statements in terms of the dimensions and categories discussed here should, we believe, promote the development of better theory."}
{"_id":"35b688b67831ed0498a490820bb36860104d2d5a","title":"Mechanical design of a gravity-balancing wearable exoskeleton for the motion enhancement of human upper limb","text":"Powered exoskeletons can provide motion enhancement for both healthy and physically challenged people. Upper limb exoskeletons are required to have multiple degrees-of-freedom and can still produce sufficient force to augment the upper limb motion. The design using serial mechanisms usually results in a complicated and bulky exoskeleton that prevents itself from being wearable. This paper presents a new exoskeleton design aimed to achieve compactness and wearability. We consider a shoulder exoskeleton that consists of a parallel spherical mechanism with two slider crank mechanisms. The actuators can be placed on a stationary platform and attached closely to human body. Thus a better inertia property can be obtained while maintaining lightweight. Through the use of a gravity-balancing mechanism, the required actuator power becomes smaller and with better efficiency. A static model is developed to analyze and optimize the exoskeleton. Through illustrations of a prototype, the exoskeleton is shown to be wearable and can provide adequate motion enhancement of a human's upper limb."}
{"_id":"a0fd813b9218813e1b020d03a3099de7677dd145","title":"A Virtual Feedback Assistance System for Remote Operation of a 3DOF Micromanipulator in Micro-Nanorobotic Manipulation","text":"Manipulation in micro or nanoscale with robotic manipulators under observation of electron microscopes is a widely used strategy for fabrication of nanodevices and nanoscale material property characterization. These types of manipulation systems can handle the relatively larger scale of objects. However, the complexity of manipulation increases highly for 3D manipulation. Since the manipulation system consists of multiple components including manipulator, microscope, and also some end-effector tools, a proper offline visualization of the system is necessary for operation. Therefore, we propose a web-based virtual interface between the user and the actual manipulator operated under digital microscope initially. It gives the operator 3D positional feedback from the virtual model by mapping data read during remote operation. The same interface is used for remote operation of the manipulator within the SEM chamber and a manipulation task is performed."}
{"_id":"b6741b6ee49d26c23655b4ff3febc3166b1e3644","title":"Dependence on computer games by adolescents.","text":"As computer game playing is a popular activity among adolescents, a questionnaire study was undertaken with 387 adolescents (12-16 years of age) to establish their \"dependence\" using a scale adapted from the DSM-III-R criteria for pathological gambling. Analysis indicated that one in five adolescents were currently \"dependent\" upon computer games. Boys played significantly more regularly than girls and were more likely to be classified as \"dependent.\" The earlier children began playing computer games it appeared the more likely they were to be playing at \"dependent\" levels. These and other results are discussed in relation to research on other gaming dependencies."}
{"_id":"599b7e1b4460c8ad77def2330ec76a2e0dfedb84","title":"Robust Subspace Clustering via Smoothed Rank Approximation","text":"Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this letter, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms."}
{"_id":"5facbc5bd594f4faa0524d871d49ba6a6e956e17","title":"Sparse subspace clustering","text":"We propose a method based on sparse representation (SR) to cluster data drawn from multiple low-dimensional linear or affine subspaces embedded in a high-dimensional space. Our method is based on the fact that each point in a union of subspaces has a SR with respect to a dictionary formed by all other data points. In general, finding such a SR is NP hard. Our key contribution is to show that, under mild assumptions, the SR can be obtained `exactly' by using l1 optimization. The segmentation of the data is obtained by applying spectral clustering to a similarity matrix built from this SR. Our method can handle noise, outliers as well as missing data. We apply our subspace clustering algorithm to the problem of segmenting multiple motions in video. Experiments on 167 video sequences show that our approach significantly outperforms state-of-the-art methods."}
{"_id":"726d2d44a739769e90bb7388ed120efe84ea8147","title":"Clustering and projected clustering with adaptive neighbors","text":"Many clustering methods partition the data groups based on the input data similarity matrix. Thus, the clustering results highly depend on the data similarity learning. Because the similarity measurement and data clustering are often conducted in two separated steps, the learned data similarity may not be the optimal one for data clustering and lead to the suboptimal results. In this paper, we propose a novel clustering model to learn the data similarity matrix and clustering structure simultaneously. Our new model learns the data similarity matrix by assigning the adaptive and optimal neighbors for each data point based on the local distances. Meanwhile, the new rank constraint is imposed to the Laplacian matrix of the data similarity matrix, such that the connected components in the resulted similarity matrix are exactly equal to the cluster number. We derive an efficient algorithm to optimize the proposed challenging problem, and show the theoretical analysis on the connections between our method and the K-means clustering, and spectral clustering. We also further extend the new clustering model for the projected clustering to handle the high-dimensional data. Extensive empirical results on both synthetic data and real-world benchmark data sets show that our new clustering methods consistently outperforms the related clustering approaches."}
{"_id":"7561d26274daed0f7575073cebcebe6ba24d8192","title":"Embedded Robust Visual Obstacle Detection on Autonomous Lawn Mowers","text":"Currently, the only mass-market service robots are floor cleaners and lawn mowers. Although available for more than 20 years, they mostly lack intelligent functions from modern robot research. In particular, the obstacle detection and avoidance is typically a simple physical collision detection. In this work, we discuss a prototype autonomous lawn mower with camera-based non-contact obstacle avoidance. We devised a low-cost compact module consisting of color cameras and an ARM-based processing board, which can be added to an autonomous lawn mower with minimal effort. For testing our system, we conducted a field test with 20 prototype units distributed in eight European countries with a total mowing time of 3,494 hours. The results show that our proposed system is able to work without expert interaction for a full season and strongly reduces collision events while still keeping the good mowing performance. Furthermore, a questionnaire with the testers revealed that most people would favor the camera-based mower over a non-camera-based mower."}
{"_id":"9d32b9bf5f7ec86bc876216c4271f18fe68e7dcd","title":"Online Object Tracking Based on CNN with Metropolis-Hasting Re-Sampling","text":"Tracking-by-learning strategies have been effective in solving many challenging problems in visual tracking, in which the learning sample generation and labeling play important roles for final performance. Since the concern of deep learning based approaches has shown an impressive performance in different vision tasks, how to properly apply the learning model, such as CNN, to an online tracking framework is still challenging. In this paper, to overcome the overfitting problem caused by straight-forward incorporation, we propose an online tracking framework by constructing a CNN based adaptive appearance model to generate more reliable training data over time. With a reformative Metropolis-Hastings re-sampling scheme to reshape particles for a better state posterior representation during online learning, the proposed tracking outperforms most of the state-of-art trackers on challenging benchmark video sequences."}
{"_id":"20a74d8887c9a4caa9174f46f127a381a8083278","title":"Avoiding revascularization with lifestyle changes: The Multicenter Lifestyle Demonstration Project.","text":"The Multicenter Lifestyle Demonstration Project was designed to determine if comprehensive lifestyle changes can be a direct alternative to revascularization for selected patients without increasing cardiac events. A total of 333 patients completed this demonstration project (194 in the experimental group and 139 in the control group). We found that experimental group patients were able to avoid revascularization for at least 3 years by making comprehensive lifestyle changes at substantially lower cost without increasing cardiac morbidity and mortality. These patients reported reductions in angina comparable with what can be achieved with revascularization."}
{"_id":"98438f5238c1171c4c7c60d78109509c7ee9d08b","title":"Security risk assessment framework for cloud computing environments","text":"Cloud computing has become today\u2019s most common technology buzzword. Despite the promises of cloud computing to decrease computing implementation costs and deliver computing as a service, which allows clients to pay only for what they need and use, cloud computing also raises many security concerns. Most popular risk assessment standards, such as ISO27005, NIST SP800-30, and AS\/NZS 4360, assume that an organization\u2019s assets are fully managed by the organization itself and that all security management processes are imposed by the organization. These assumptions, however, do not apply to cloud computing environments. Hence, this paper proposes a security risk assessment framework that can enable cloud service providers to assess security risks in the cloud computing environment and allow cloud clients to contribute in risk assessment. The proposed framework provides a more realistic and accurate risk assessment outcome by considering the cloud clients\u2019 evaluation of security risk factors and avoiding the complexity that can result from the involvement of clients in whole risk assessment process. Copyright \u00a9 2014 John Wiley & Sons, Ltd."}
{"_id":"6d2467d75a21d94a4e172d647564b5e238ddfdbe","title":"A Sentence Interaction Network for Modeling Dependence between Sentences","text":"Modeling interactions between two sentences is crucial for a number of natural language processing tasks including Answer Selection, Dialogue Act Analysis, etc. While deep learning methods like Recurrent Neural Network or Convolutional Neural Network have been proved to be powerful for sentence modeling, prior studies paid less attention on interactions between sentences. In this work, we propose a Sentence Interaction Network (SIN) for modeling the complex interactions between two sentences. By introducing \u201cinteraction states\u201d for word and phrase pairs, SIN is powerful and flexible in capturing sentence interactions for different tasks. We obtain significant improvements on Answer Selection and Dialogue Act Analysis without any feature engineering."}
{"_id":"77acdfa74c7c39cc36dee8da63e30aeffdd19e22","title":"On-road vehicle detection: a review","text":"Developing on-board automotive driver assistance systems aiming to alert drivers about driving environments, and possible collision with other vehicles has attracted a lot of attention lately. In these systems, robust and reliable vehicle detection is a critical step. This paper presents a review of recent vision-based on-road vehicle detection systems. Our focus is on systems where the camera is mounted on the vehicle rather than being fixed such as in traffic\/driveway monitoring systems. First, we discuss the problem of on-road vehicle detection using optical sensors followed by a brief review of intelligent vehicle research worldwide. Then, we discuss active and passive sensors to set the stage for vision-based vehicle detection. Methods aiming to quickly hypothesize the location of vehicles in an image as well as to verify the hypothesized locations are reviewed next. Integrating detection with tracking is also reviewed to illustrate the benefits of exploiting temporal continuity for vehicle detection. Finally, we present a critical overview of the methods discussed, we assess their potential for future deployment, and we present directions for future research."}
{"_id":"ec08284de3ac57f72e3aa931881808c322be5edc","title":"Multi-Player Alpha-Beta Pruning","text":"Korf, R.E., Multi-player alpha-beta pruning (Research Note), Artificial Intelligence 48 (1991) 99-111. We consider the generalization of minimax search with alpha-beta pruning to non-cooperative, perfect-information games with more than two players. The minimax algorithm was generalized in [2] to the maxn algorithm applied to vectors of n-tuples representing the evaluations for each of the players. If we assume an upper bound on the sum of the evaluations for each player, and a lower bound on each individual evaluation, then shallow alpha-beta pruning is possible, but not deep pruning. In the best case, the asymptotic branching factor is reduced to (1 + 4bv'~b-\"s~-3)\/2. In the average case, however, pruning does not reduce the asymptotic branching factor. Thus, alpha-beta pruning is found to be effective only in the special case of two-player games. In addition, we show that it is an optimal directional algorithm for two players."}
{"_id":"0ffaf68f4399998864d6d9835c7bd8240d322b49","title":"LogicBlox, Platform and Language: A Tutorial","text":"The modern enterprise software stack\u2014a collection of applications supporting bookkeeping, analytics, planning, and forecasting for enterprise data\u2014is in danger of collapsing under its own weight. The task of building and maintaining enterprise software is tedious and laborious; applications are cumbersome for end-users; and adapting to new computing hardware and infrastructures is difficult. We believe that much of the complexity in today\u2019s architecture is accidental, rather than inherent. This tutorial provides an overview of the LogicBlox platform, a ambitious redesign of the enterprise software stack centered around a unified declarative programming model, based on an extended version of Datalog. 1 The Enterprise Hairball Modern enterprise applications involve an enormously complex technology stack composed of many disparate systems programmed in a hodgepodge of programming languages. We refer to this stack, depicted in Figure 1, as \u201cthe enterprise hairball.\u201d First, there is an online transaction processing (OLTP) layer that performs bookkeeping of the core business data for an enterprise. Such data could include the current product catalog, recent sales figures, current outstanding invoices, customer account balances, and so forth. This OLTP layer typically includes a relational DBMS\u2014programmed in a combination of a query language (SQL), a stored procedure language (like PL\/SQL or TSQL), and a batch programming language like Pro*C\u2014an application server, such as Oracle WebLogic [35], IBM WebSphere [36], or SAP NetWeaver [26]\u2014programmed in an object-oriented language like Java, C#, or ABAP\u2014and a web browser front-end, programmed using HTML and Javascript. In order to track the performance of the enterprise over time, a second business intelligence (BI) layer typically holds five to ten years of historical information that was originally recorded in the OLTP layer and performs read-only analyses on this information. This layer typically includes another DBMS (or, more commonly, a BI variant like Teradata [33] or IBM Netezza [25]) along with a BI application server such as Microstrategy [23], SAP BusinessObjects [5], or IBM Cognos [7], programmed using a vendor-specific declarative language. Fig. 1: Enterprise software components and technology stack example. Data is moved from the transaction layer to the BI layer via so-called extracttransform-load (ETL) tools that come with their own tool-specific programming language. Finally, in order to plan the future actions of an enterprise, there is a planning layer, which supports a class of read-write use cases for which the BI layer is unsuitable. This layer typically includes a planning application server, like Oracle Hyperion [13] or IBM Cognos TM1 [34], that are programmed using a vendor-specific declarative language or a standard language like MDX [21], and spreadsheets like Microsoft Excel that are programmed in a vendor-specific formula language (e.g. A1 = B17 D12) and optionally a scripting language like VBA. In order to enhance or automate decisions made in the planning layer, statistical predictive models are often prototyped using modeling tools like SAS, Matlab, SPSS, or R, and then rewritten for production in C++ or Java so they can be embedded in the OLTP or OLAP layers. In summary, enterprise software developed according to the hairball model must be programmed using a dozen or so different programming languages, running in almost as many system-scale components. Some of the languages are imperative (both object-oriented and not) and some are declarative (every possible flavor). Most of the languages used are vendor-specific and tied to the component in which they run (e.g. ABAP, Excel, R, OPL, etc.). Even the languages that are based on open standards are not easily ported from one component to another because of significant variations in performance or because of vendor specific extensions (e.g., the same SQL query on Oracle will perform very differently on DB2). Recent innovations in infrastructure technology for supporting much larger numbers of users (Web applications) and big data (predictive analytics), including NoSQL [28], NewSQL [27] and analytic databases, have introduced even more components into the technology stack described above, and have not helped reduce its overall complexity. This complexity makes enterprise software hard to build, hard to implement, and hard to change. Simple changes\u2014like extending a product identifier by 3 characters or an employee identifier by 1 digit\u2014often necessitate modifications to most of the different components in the stack, requiring thousands of days of person effort and costing many millions of dollars. An extreme example of the problems caused by this accidental complexity occurred in the lead-up to the year 2000, where the simple problem of adding two digits to the year field (the Y2K problem) cost humanity over $300 billion dollars [24]. Moreover, as a result of the time required for such changes, individuals within an enterprise often resort to ad hoc, error-prone methods to perform the calculations needed in order to make timely business decisions. For example, they often roll their own extensions using spreadsheets, where they incompletely or incorrectly implement such calculations based on copies of the data that is not kept in sync with the OLTP database and thus may no longer accurately reflect the state of their enterprise."}
{"_id":"10e5e1aaec68a679f88c2bf5ef24b8a128d94f4d","title":"Attitudes toward Vegans and Vegetarians : The Role of Anticipated Moral Reproach and Dissonance","text":"This study attempted to determine if anticipated moral reproach and cognitive dissonance explain negative attitudes toward vegans and vegetarians, and if vegans are subject to more negative attitudes than vegetarians. The sample consisted of 65 participants (women =41, men =24) who were randomly assigned into one of four conditions. Each participant read a vignette about a person who consumed either a vegan, vegetarian, plant-based or omnivore diet. They then completed a series of questionnaires that recorded their attitudes towards the person in the vignette, anticipated moral reproach, and cognitive dissonance. The results indicated that for the observed general attitudes for the vegan, vegetarian and plant-based diet conditions were more negative than for the omnivore condition, with plant-based diet being the most negative. For moral reproach the results indicated that higher levels of moral reproach contributed to more negative attitudes, but only in the omnivore condition. Whereas the liner regression indicated that cognitive dissonance was only significant predictor of attitudes in the vegan condition. The findings of this study indicated that diet can have an impact on attitudes, and that the diet its self may have a larger impact than the labels applied. DIET AND ATTITUDES 3 Attitudes Toward Vegans and Vegetarians: The Role of Anticipated Moral Reproach and Dissonance Although the majority of North Americans eat an omnivore diet, veganism and vegetarianism have in recent years experienced an increase in popularity (Duchene & Jackson, 2017). As a result, omnivores are believed to be more likely to discuss their attitudes towards vegans and vegetarians, due to the increased exposure to the groups, both in public dialogue and interpersonally among acquaintances. Cole (2011) examined all British newspapers published in the year 2007 that mentioned the words vegan, vegans or veganism. The results showed that the majority of papers published expressed a negative attitude towards veganism. Additionally, any positive papers published looked at only food suggesting that there is a limited perspective provided by the media on veganism. Cole (2011) provides evidence in support that there are systematic negative associations against vegans. The purpose of this study will be to look at possible explanatory theories that could explain why veganism is observed to be viewed more negatively then vegetarianism. The theories that will be used are moral reproach (Minson & Monin, 2012) and cognitive dissonance (Loughnan, Haslam, & Bastian, 2010). This is an important topic to research as veganism and vegetarianism are theorized to be concepts that are strongly tied to the individual\u2019s identity and are seen to be as important to the individual as their sexual orientation, religion, and\/ or ethnicity (Wright, 2015). Furthermore, this study may also help to explain why strategies that attempt to encourage people to adopt a vegetarian or vegan diet are often unsuccessful (Duchene & Jackson, 2017). It has been found that personal factors are the major contributor to choosing to become a vegetarian. These personal factors are altered by the perceptions held by the dominant group, omnivores (Hussar & Harris, 2010). By understanding why omnivores have these DIET AND ATTITUDES 4 perceptions about vegans and vegetarians, will lead to insight into why these negative attitudes are occurring. Comparing Veganism and Vegetarianism in Terms of Attitudes and Diet To explore the potential difference in attitudes toward vegetarians and vegans it is important to clarify what the difference is between the two groups. Vegetarianism is defined as a diet that doesn't involve the consumption of meat or by-products of animal slaughter (Wright, 2015). In comparison, veganism consists of a diet without any animal-based products, or any item containing an ingredient derived from an animal (Wright, 2015). Furthermore, vegans place more stress on the ethical tie to animal rights then vegetarians. Animal rights scholar, Carol Adams defines veganism as \"an ethical stance based on compassion for all beings\" (Adams, 2010, pp.113) and argues that being vegan is about more than adhering to a specific diet. Further evidence to support that veganism is distinctly different from vegetarianism can be found by looking at the groups\u2019 origins. Historically, veganism was founded from a group of vegetarians, who no longer wanted to be seen as part of vegetarianism but rather as their own group (Adams, 2010). This supports the notion that veganism is distinctly different from vegetarianism, as it was created with the purpose to be different then vegetarianism. Evidently there are some clear differences between vegetarians and vegans. It is important to translate how these differences are believed to lead to differing attitudes towards vegans and vegetarians. To understand why vegetarianism may be viewed more positivity then veganism, one commonly offered explanation is that because vegetarians still consume dairy products, the group is viewed as having less polarizing views (Adams, 2010). Elaborating on this statement, Rothgerber found that 38% of people who identified as vegetarian reported consuming meat DIET AND ATTITUDES 5 products to some capacity (2014a). It has been theorised that the emergence of semivegetarianism has lead to vegetarianism being perceived as less negative then veganism. Semivegetarianism is a term that refers to an individual who identifies as being a vegetarian but still consumes meat (Adams, 2010; Rothgerber, 2014a; Ruby, 2012). The amount of meat consumed, and the degree of willingness varies among semi-vegetarians. The presence of semivegetarianism has lead to a blurred conceptualization of vegetarianism resulting in an increase in the perceived homogeneity between vegetarianism and omnivores (Adams, 2010). In support of this, Rothgerber found that semi-vegetarians were closer to omnivores than strict vegetarians regarding the level of disgust shown towards meat (2014a). In summary, this difference could alter the perception that the similarity between omnivores and vegetarians is greater than the similarity between omnivores and vegans. Previous research has been conducted looking at attitude differences between vegetarians and omnivores. The reasoning that a vegetarian gives for maintaining the diet has been found to influence omnivore attitudes towards the group. Generally, the reasons for vegetarianism have been grouped into one of two categories. Health vegetarians tend to cite personal reasons for their membership, such as citing the health benefits or a food allergy. In contrast, ethical vegetarians cite external reasons, such as animal welfare. It has been found that when vegetarians reported being morally motivated, it resulted in a harsher judgement then when the reason reported was personal motivation (Hussar & Harris, 2010). In summary, it is likely that the observed differences between health and ethical vegetarians may translate to comparing vegetarians and vegans, as veganism is tied more to morality then vegetarianism. Another area that is important to look at is difference between vegetarianism veganism, and conscious omnivores. As it is important to address if it is the association to DIET AND ATTITUDES 6 environmentalism or if it is the act of not eating meat that effects attitudes. Rothgerber (2015) states that vegetarianism is a specific form of environmentalism. As well, Bashir et. al (2013) found that individuals were less likely to affiliate with someone if they engaged in a stereotypical environmental activity such as engaging in environmental activism. However, research has shown that the diet that a person eats has a larger impact on attitudes then environmentalism. Rothgerber (2015) compared conscious omnivores to vegans and vegetarians to see how much of an effect the diet had on attitudes towards animals. Conscious omnivores are omnivores who limit their animal consumption and attempt to avoid consuming commercial meat and instead choose animals that are grass feed, or raised more humanely (Rothgerber,2015). The findings of the study were that conscious omnivores scored significantly lower than vegetarianism on both animal favourability and disgust towards meat. The findings also suggest that conscious omnivores conceptually aren\u2019t a group of transitioning vegetarians but rather a separate group. Also, conscious omnivores were found not to put as much emphasis on their eating habits as being a part of their social identity, aligning with the attitudes of normal omnivores (Rothgerber, 2015). In conclusion, it is evident that there are some characteristics of vegetarianism and veganism that are different then omnivore environmentalists. These differences likely contribute to the differences in perceptions that are experienced by omnivores. To summarise it is evident that there is significant evidence to support that there is a difference in the perceptions of vegans and vegetarians that could contribute to a difference in attitudes between the two groups. This paper will now move to discuss some theories that have been applied to these groups in the past to potentially explain this phenomenon. Anticipated Moral Reproach Towards Vegans and Vegetarians DIET AND ATTITUDES 7 Anticipated moral reproach is defined as believing that an individual who holds a different moral stance is judging your own stance as being immoral (Minson & Monin, 2012). This perception of being judged leads to the individual developing a negative attitude towards the other group. In terms of group attitudes, anticipated moral reproach has been used to explain why, when a group stresses overly moral behaviour, it is given the derogatory label of a dogooder or goody-two-shoes. Though not inherently negative, terms such as do-gooder are applied to the individual in a derogatory manner to express their dislike of the group (Minson & Monin, 2012"}
{"_id":"b98063ae46f00587a63547ce753c515766db4fbb","title":"Dual Band Microstrip Patch Antenna for Sar Applications","text":"Microstrip patch antennas offer the advantages of thin profile, light weight, low cost, ease of fabrication and compatibility with integrated circuitry, so the antennas are widely used to satisfy demands for polarization diversity and dual-frequency. This paper presents a coaxilly-fed single-layer compact dual band (Cand X-band) microstrip patch antenna for achieving dual-polarized radiation suitable for applications in airborne synthetic aperture radar (SAR) systems. The designed antenna consists of three rectangular patches which are overlapped along their diagonals. Full-wave electromagnetic simulations are performed to accurately predict the frequency response of the antenna. The fabricated antenna achieves an impedance bandwidth of 154 MHz (f0 = 6.83 GHz) and 209 MHz (f0 = 9.73 GHz) for VSWR < 2. Simultaneous use of both frequencies should drastically improve data collection and knowledge of the targets in the SAR system."}
{"_id":"8c8902aefbb004c5b496780f6dfd810a91d30dce","title":"Epidemiological features of chronic low-back pain","text":"Although the literature is filled with information about the prevalence and incidence of back pain in general, there is less information about chronic back pain, partly because of a lack of agreement about definition. Chronic back pain is sometimes defined as back pain that lasts for longer than 7-12 weeks. Others define it as pain that lasts beyond the expected period of healing, and acknowledge that chronic pain may not have well-defined underlying pathological causes. Others classify frequently recurring back pain as chronic pain since it intermittently affects an individual over a long period. Most national insurance and industrial sources of data include only those individuals in whom symptoms result in loss of days at work or other disability. Thus, even less is known about the epidemiology of chronic low-back pain with no associated work disability or compensation. Chronic low-back pain has also become a diagnosis of convenience for many people who are actually disabled for socioeconomic, work-related, or psychological reasons. In fact, some people argue that chronic disability in back pain is primarily related to a psychosocial dysfunction. Because the validity and reliability of some of the existing data are uncertain, caution is needed in an assessment of the information on this type of pain."}
{"_id":"96bcc8e5dec0df136774aca1e5b5e3d565967505","title":"Human-Inspired Neurorobotic System for Classifying Surface Textures by Touch","text":"Giving robots the ability to classify surface textures requires appropriate sensors and algorithms. Inspired by the biology of human tactile perception, we implement a neurorobotic texture classifier with a recurrent spiking neural network, using a novel semisupervised approach for classifying dynamic stimuli. Input to the network is supplied by accelerometers mounted on a robotic arm. The sensor data are encoded by a heterogeneous population of neurons, modeled to match the spiking activity of mechanoreceptor cells. This activity is convolved by a hidden layer using bandpass filters to extract nonlinear frequency information from the spike trains. The resulting high-dimensional feature representation is then continuously classified using a neurally implemented support vector machine. We demonstrate that our system classifies 18 metal surface textures scanned in two opposite directions at a constant velocity. We also demonstrate that our approach significantly improves upon a baseline model that does not use the described feature extraction. This method can be performed in real-time using neuromorphic hardware, and can be extended to other applications that process dynamic stimuli online."}
{"_id":"c002ab1190262167d8072442eb7565d1eb204a6f","title":"Determination of differential leakage factors in electrical machines with non-symmetrical full and dead-coil windings","text":"In this paper G\u00f6rges polygons are used in conjunction with masses geometry to find an easy and affordable way to compute the differential leakage factor of non symmetrical full and dead coil winding. By following the traditional way, the use of the Ossanna's infinite series which has to be obviously truncated under the bound of a predetermined accuracy is mandatory. In the presented method no infinite series is instead required. An example is then shown and discussed to demonstrate practically the effectiveness of the proposed method."}
{"_id":"9176e427fe49d9a65417e1a140dd56178b6e3c1a","title":"RUM Extractor: A Facebook Extractor for Data Analysis","text":"Social Network Analysis (SNA) is a field of study that focuses on analyzing user profiles and participations on social network channels in order to model relationships between people and to predict certain behaviors or knowledge. To achieve their goals, researchers, interested in SNA, have to extract content and structure from the numerous social networks available today. Existing tools, which help in this task, often require substantial pre-processing or good programming skills which may not be available for all SNA researchers. This paper describes RUM, a data extraction tool which allows researchers to easily extract several types of content and structure that are available on Facebook pages. Consequently, the extracted data can be saved and analyzed. RUM Extractor is easy to set up and use, and it gives flexible options to users to specify the type and amount of content and structure they want to retrieve. The paper also demonstrates how RUM can be exploited by collecting and further analyzing data collected from two popular Arabic news pages."}
{"_id":"4f9744358d7446f693d88c8e25828c67610bd333","title":"Decentralized frequency control of a DDG-PV Microgrid in islanded mode","text":"This paper presents an innovative approach to control the frequency in Diesel-Driven Generator (DDG)-Photovoltaic (PV) Microgrid (MG) in islanded mode. The common approach is based on hierarchical control: primary control and secondary control. The conventional primary controller is a P-controller. Compared to this, we propose a novel primary controller for PV to mimic the dynamic behavior of the conventional synchronous generator (SG) so that more inertia in the MG can be obtained. The proposed secondary control is accomplished decentralized, too, so that no communication links for the real-time control are required and the single point of failure can be avoided. Although in this approach every controller works locally, the controllers and their parameters are designed centrally. This means, the controllers can be appropriately adjusted, when the parameters or the topology of the system change. The parameters of the controllers are optimized by minimizing the H-inf-norm using a known non-smooth method."}
{"_id":"358d2a02275109c250f74f8af150d42eb75f7b5f","title":"TinyECC: A Configurable Library for Elliptic Curve Cryptography in Wireless Sensor Networks","text":"Public Key Cryptography (PKC) has been the enabling technology underlying many security services and protocols in traditional networks such as the Internet. In the context of wireless sensor networks, elliptic curve cryptography (ECC), one of the most efficient types of PKC, is being investigated to provide PKC supportin sensor network applications so that the existing PKC-based solutions can be exploited. This paper presents the design, implementation, and evaluation of TinyECC, a configurable library for ECC operations in wireless sensor networks. The primary objective of TinyECC is to provide a ready-to-use, publicly available software package for ECC-based PKC operations that can be flexibly configured and integrated into sensor network applications. TinyECC provides a number of optimization switches, which can turn specific optimizations on or off based on developers' needs. Different combinations of the optimizations have different execution time andresource consumptions, giving developers great flexibility in integrating TinyECC into sensor network applications. This paperalso reports the experimental evaluation of TinyECC on several common sensor platforms, including MICAz, Tmote Sky, and Imote2. The evaluation results show the impacts of individual optimizations on the execution time and resource consumptions, and give the most computationally efficient and the most storage efficient configuration of TinyECC."}
{"_id":"e2ed878b43065d7335bb7007fefe2fd5d0919052","title":"Beyond the 'digital natives' debate: Towards a more nuanced understanding of students' technology experiences","text":"The idea of the \u2018digital natives\u2019, a generation of tech-savvy young people immersed in digital technologies for which current education systems cannot cater, has gained widespread popularity on the basis of claims rather than evidence. Recent research has shown flaws in the argument that there is an identifiable generation or even a single type of highly adept technology user. For educators, the diversity revealed by these studies provides valuable insights into students\u2019 experiences of technology inside and outside formal education. While this body of work provides a preliminary understanding, it also highlights subtleties and complexities that require further investigation. It suggests, for example, that we must go beyond simple dichotomies evident in the digital natives debate to develop a more sophisticated understanding of our students\u2019 experiences of technology. Using a review of recent research findings as a starting point, this paper identifies some key issues for educational researchers, offers new ways of conceptualizing key ideas using theoretical constructs from Castells, Bourdieu and Bernstein, and makes a case for how we need to develop the debate in order to advance our understanding."}
{"_id":"3dcfafa8116ad728b842c07ebf0f1bea9745dead","title":"A Frame of Mind: Using Statistical Models for Detection of Framing and Agenda Setting Campaigns","text":"Framing is a sophisticated form of discourse in which the speaker tries to induce a cognitive bias through consistent linkage between a topic and a specific context (frame). We build on political science and communication theory and use probabilistic topic models combined with time series regression analysis (autoregressive distributed-lag models) to gain insights about the language dynamics in the political processes. Processing four years of public statements issued by members of the U.S. Congress, our results provide a glimpse into the complex dynamic processes of framing, attention shifts and agenda setting, commonly known as \u2018spin\u2019. We further provide new evidence for the divergence in party discipline in U.S. politics."}
{"_id":"4a6af1ca3745941f5520435473f1f90f8157577c","title":"Deterministic compressed-sensing matrices: Where Toeplitz meets Golay","text":"Recently, the statistical restricted isometry property (STRIP) has been formulated to analyze the performance of deterministic sampling matrices for compressed sensing. In this paper, a class of deterministic matrices which satisfy STRIP with overwhelming probability are proposed, by taking advantage of concentration inequalities using Stein's method. These matrices, called orthogonal symmetric Toeplitz matrices (OSTM), guarantee successful recovery of all but an exponentially small fraction of K-sparse signals. Such matrices are deterministic, Toeplitz, and easy to generate. We derive the STRIP performance bound by exploiting the specific properties of OSTM, and obtain the near-optimal bound by setting the underlying sign sequence of OSTM as the Golay sequence. Simulation results show that these deterministic sensing matrices can offer reconstruction performance similar to that of random matrices."}
{"_id":"5b3fe72edd2c02a2ae553d40e70010e7257be0f7","title":"Perceptual Control Theory 1 Perceptual Control Theory A Model for Understanding the Mechanisms and Phenomena of Control","text":"Perceptual Control Theory (PCT) provides a general theory of functioning for organisms. At the conceptual core of the theory is the observation that living things control the perceived environment by means of their behavior. Consequently, the phenomenon of control takes center stage in PCT, with observable behavior playing an important but supporting role. The first part of the paper explains how the PCT model works. This explanation includes a definition of \u201ccontrol\u201d as well as the basic equations from which one can see what is required for control to be possible. The second part of the paper describes demonstrations that the reader can download from the Internet and run, so as to learn the basics of control by experiencing and verifying the phenomenon directly. The third part of the paper shows examples of the application of PCT to different areas of psychological research including learning, developmental psychology, social psychology, and psychotherapy. This summary of the current state of the field celebrates the 50th Anniversary of the first major publication in PCT (Powers, Clark & MacFarland, 1960)."}
{"_id":"d1d044d0a94f6d8ad1a00918abededbd2a41f1bd","title":"Intelligent Wireless Patient Monitoring and Tracking System (Using Sensor Network and Wireless Communication).","text":"* Corresponding author: Ch.Sandeep Kumar Subudhi Abstract Aim of our work is to monitor the human body temperature, blood pressure (BP), Pulse Rate and ECG and tracking the patient location. The human body temperature, BP, Pulse Rate and ECG are detected in the working environment; this can be sensed by using respective sensors. The sensed information is send to the PIC16F877 microcontroller through signal conditioning circuit in the patient unit. A desired amount of sensor value is set and if it is exceeded preliminary steps should be taken by the indicating by buzzer.The sensor information will be transmitted from the patient unit to the main controller unit with the help of Zigbee communication system which is connected with the microcontrollers in the both units. The main controller unit will send those sensed data as well as the location of that patient by the help of GPS Module to the observer\/doctor. The observer\/doctor can receive the SMS sent by GSM module and further decision can be taken. The message is sent to a mobile phone using Global system mobile (GSM) Modem. MAX232 was a driver between microcontroller and modem."}
{"_id":"c0b60d02b2d59123f6b336fe2e287bdb02a2a776","title":"Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study","text":"Visual attention is a process that enables biological and machine vision systems to select the most relevant regions from a scene. Relevance is determined by two components: 1) top-down factors driven by task and 2) bottom-up factors that highlight image regions that are different from their surroundings. The latter are often referred to as \u201cvisual saliency.\u201d Modeling bottom-up visual saliency has been the subject of numerous research efforts during the past 20 years, with many successful applications in computer vision and robotics. Available models have been tested with different datasets (e.g., synthetic psychological search arrays, natural images or videos) using different evaluation scores (e.g., search slopes, comparison to human eye tracking) and parameter settings. This has made direct comparison of models difficult. Here, we perform an exhaustive comparison of 35 state-of-the-art saliency models over 54 challenging synthetic patterns, three natural image datasets, and two video datasets, using three evaluation scores. We find that although model rankings vary, some models consistently perform better. Analysis of datasets reveals that existing datasets are highly center-biased, which influences some of the evaluation scores. Computational complexity analysis shows that some models are very fast, yet yield competitive eye movement prediction accuracy. Different models often have common easy\/difficult stimuli. Furthermore, several concerns in visual saliency modeling, eye movement datasets, and evaluation scores are discussed and insights for future work are provided. Our study allows one to assess the state-of-the-art, helps to organizing this rapidly growing field, and sets a unified comparison framework for gauging future efforts, similar to the PASCAL VOC challenge in the object recognition and detection domains."}
{"_id":"c18a0618ada97eba7e72dd9d4ccfa7b838718ad3","title":"Task and context determine where you look.","text":"The deployment of human gaze has been almost exclusively studied independent of any specific ongoing task and limited to two-dimensional picture viewing. This contrasts with its use in everyday life, which mostly consists of purposeful tasks where gaze is crucially involved. To better understand deployment of gaze under such circumstances, we devised a series of experiments, in which subjects navigated along a walkway in a virtual environment and executed combinations of approach and avoidance tasks. The position of the body and the gaze were monitored during the execution of the task combinations and dependence of gaze on the ongoing tasks as well as the visual features of the scene was analyzed. Gaze distributions were compared to a random gaze allocation strategy as well as a specific \"saliency model.\" Gaze distributions showed high similarity across subjects. Moreover, the precise fixation locations on the objects depended on the ongoing task to the point that the specific tasks could be predicted from the subject's fixation data. By contrast, gaze allocation according to a random or a saliency model did not predict the executed fixations or the observed dependence of fixation locations on the specific task."}
{"_id":"11190a466d1085c09a11e52cc63f112280ddce74","title":"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis","text":"A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail."}
{"_id":"494219f151071a752914c51024a307f0e22e9d2c","title":"An Integrated Model of Top-Down and Bottom-Up Attention for Optimizing Detection Speed","text":"Integration of goal-driven, top-down attention and image-driven, bottom-up attention is crucial for visual search. Yet, previous research has mostly focused on models that are purely top-down or bottom-up. Here, we propose a new model that combines both. The bottom-up component computes the visual salience of scene locations in different feature maps extracted at multiple spatial scales. The topdown component uses accumulated statistical knowledge of the visual features of the desired search target and background clutter, to optimally tune the bottom-up maps such that target detection speed is maximized. Testing on 750 artificial and natural scenes shows that the model\u2019s predictions are consistent with a large body of available literature on human psychophysics of visual search. These results suggest that our model may provide good approximation of how humans combine bottom-up and top-down cues such as to optimize target detection speed."}
{"_id":"f5027eb3ad922bf6903b080329c02dff6b34ce20","title":"Factors affecting the acceptance of information systems supporting emergency operations centres","text":"Despite the recognition that information system acceptance is an important antecedent of effective emergency management, there has been comparatively very little research examining this aspect of technology acceptance. The current research responded to this gap in literature by adapting and integrating existing models of technology acceptance. This was done in order to examine how a range of technology acceptance factors could affect the acceptance of emergency operations centre information systems. Relationships between several of these factors were also examined. Questionnaire data from 383 end-users of four different emergency operations centre information systems were analysed using structural equation modelling. This analysis concluded that technology acceptance factors of performance expectancy, effort expectancy, social influence and information quality explained 65 percent of variance in symbolic adoption, which is a combination of mental acceptance and psychological attachment towards an information system. A number of moderating effects of age, gender, experience of use and domain experience were also identified. A mediating component, of performance expectancy, explained 49 percent of variance between facilitating conditions, information quality, effort expectancy, and resulting symbolic adoption. These findings highlight a need to re-focus technology acceptance research on both mediating and moderating effects and the importance of considering domain specific factors. Applied recommendations are also made, for successfully implementing relevant information"}
{"_id":"88a29a406054826fd8ad68b8c23231828d84f3fc","title":"Interventions to address the academic impairment of children and adolescents with ADHD.","text":"There exists a strong link between ADHD and academic underachievement. Both the core behavioral symptoms of ADHD and associated executive functioning deficits likely contribute to academic impairment. Current evidence-based approaches to the treatment of ADHD (i.e., stimulant medication, clinical behavior therapy and classroom behavioral interventions) have demonstrated a robust impact on behavioral variables such as attention and disruptive behavior within classroom analogue settings; however, their efficacy in improving academic outcomes is much less clear. Although surprisingly few treatment outcome studies of ADHD have attempted to incorporate interventions that specifically target academic outcomes, the studies that are available suggest that these interventions may be beneficial. The state of the treatment literature for addressing academic impairment in children and adolescents with ADHD will be reviewed herein, as well as limitations of current research, and directions for future research."}
{"_id":"e25767429f6f0d934a972864b70c3f21cf50dac2","title":"Intravesical stone formation on intrauterine contraceptive device","text":"Since more than 30\u00a0years, intrauterine contraceptive devices (IUCD) have been used for a contraceptive opportunity. Although they are termed to be a safe and effective method for contraception, they also have some type of complications and uterine perforation, septic abortion, pelvic abscess are the serious complications of these devices. The incidence of uterine perforation is very low, but in the literature nearly 100 cases were reported about the extra uterine localization of IUCD. Migration may occur to the adjacent organs. We here in describe a case of a 31\u00a0year-old woman who had an IUCD with stone formation in the bladder. In the literature all of the cases were reported as IUCD migration, but although it seems technically impossible, IUCD placement into the bladder should also be considered in misplaced IUCDs."}
{"_id":"2d193bcd49dcc9a9cb67acf622d074d969e1057b","title":"Offset calibration technique for capacitive transimpedance amplifier used in uncooled infrared detection","text":"This paper presents a novel readout circuit of uncooled, bolometer-based, focal plane arrays (FPAs). The offset and flicker noise are the design challenges of microbolometer readout circuits (ROCs). The ROC is not only required to apply careful noise cancellation techniques, but also to be insensitive to process and supply voltage variations. The proposed circuit involves a new offset cancellation technique which overcomes process variations, noise, random, and systematic offset. & 2016 Elsevier Ltd. All rights reserved."}
{"_id":"ddf5d0dabf9d7463c306d2969307a9541d96ba66","title":"Big data framework for students' academic performance prediction: A systematic literature review","text":"Big Data is becoming an integral part of education system worldwide, bringing in so much of prediction potential and therefore opportunities to improve learning and teaching methodologies. In fact, it has become the digital policy instrument for policy makers to make strategic decisions supported by big data analytics. This paper uses Systematic Literature Review (SLR) to establish a general overview and background in establishing gaps that need to be addressed in big data analytics for education landscape. A total of 59 research papers from 473 papers were chosen and analyzed. There is an increased trend in incorporating big data in education however, it is not sufficient and need more research in this area particularly predictive models and frameworks for educational settings."}
{"_id":"8080f6bc92f788bd72b25919990d50e069ab96b8","title":"Seismic surveying with drone-mounted geophones","text":"Seismic imaging is the primary technique for subsurface exploration. Traditional seismic imaging techniques rely heavily on manual labor to plant sensors, lay miles of cabling, and then recover the sensors. Often sites of resource or rescue interest may be difficult or hazardous to access. Thus, there is a substantial need for unmanned sensors that can be deployed by air and potentially in large numbers. This paper presents working prototypes of autonomous drones equipped with geophones (vibration sensors) that can fly to a site, land, listen for echoes and vibrations, store the information on-board, and subsequently return to home base. The design uses four geophone sensors (with spikes) in place of the landing gear. This provides a stable landing attitude, redundancy in sensing, and ensures the geophones are oriented perpendicular to the ground. The paper describes hardware experiments demonstrating the efficacy of this technique and a comparison with traditional manual techniques. The performance of the seismic drone was comparable to a well planted geophone, proving the drone mount system is a feasible alternative to traditional seismic sensors."}
{"_id":"eb7ba7940e2fda0dfd0f5989e3fd0b5291218450","title":"Prevalence of diabetes mellitus and impaired fasting glucose levels in the Eastern Province of Saudi Arabia: results of a screening campaign.","text":"INTRODUCTION\nThis study aimed to estimate the prevalence of diagnosed and undiagnosed diabetes mellitus (DM) in the Eastern Province of Saudi Arabia, and to study its relationship with socioeconomic factors.\n\n\nMETHODS\nThe study targeted all Saudi subjects aged 30 years and above who resided in the Eastern Province in 2004. DM screening was conducted by taking the capillary fasting blood glucose (CFBG) after eight hours or more of fasting, or the casual capillary blood glucose (CCBG). A positive screening test for hyperglycaemia was defined as CFBG more than or equal to 100 mg\/dl (5.6 mmol\/l), or CCBG more than or equal to 140 mg\/dl (7.8 mmol\/l). A positive result was confirmed on another day through the measurement of fasting plasma glucose (FPG) levels from a venous sample. A diagnosis of DM was considered if FPG was more than or equal to 126 mg\/dl (7.0 mmol\/l), or when there was a history of a previous diagnosis.\n\n\nRESULTS\nOut of 197,681 participants, 35,929 (18.2 percent) had a positive history of DM or a positive screening test for hyperglycaemia. After confirmation by venous blood testing, the prevalence of DM dropped to 17.2 percent while the prevalence of newly diagnosed DM was 1.8 percent. The prevalence increased with age and was higher in women, widows, divorcees, those who had a low education level and the unemployed.\n\n\nCONCLUSION\nThe prevalence of DM in Saudi Arabia is one of the highest reported in the world, and its yield of screening is high."}
{"_id":"d8a56e64bd74bf19d5a2ad6cdf8132c86eff3767","title":"Articulation points guided redundancy elimination for betweenness centrality","text":"Betweenness centrality (BC) is an important metrics in graph analysis which indicates critical vertices in large-scale networks based on shortest path enumeration. Typically, a BC algorithm constructs a shortest-path DAG for each vertex to calculate its BC score. However, for emerging real-world graphs, even the state-of-the-art BC algorithm will introduce a number of redundancies, as suggested by the existence of articulation points. Articulation points imply some common sub-DAGs in the DAGs for different vertices, but existing algorithms do not leverage such information and miss the optimization opportunity.\n We propose a redundancy elimination approach, which identifies the common sub-DAGs shared between the DAGs for different vertices. Our approach leverages the articulation points and reuses the results of the common sub-DAGs in calculating the BC scores, which eliminates redundant computations. We implemented the approach as an algorithm with two-level parallelism and evaluated it on a multicore platform. Compared to the state-of-the-art implementation using shared memory, our approach achieves an average speedup of 4.6x across a variety of real-world graphs, with the traversal rates up to 45 ~ 2400 MTEPS (Millions of Traversed Edges per Second)."}
{"_id":"95176f1ee03e9f460165abd90402e939b7f49fe1","title":"Development of 24 GHz rectennas for Fixed Wireless Access","text":"We need electricity to use wireless information. If we reduce amount of batteries or electrical wires with a wireless power transmission technology via microwave (MPT), it is a green communication system. We Kyoto University propose a Fixed Wireless Access (FWA) system with the MPT with NTT, Japan. In this paper, we show mainly development results of 24GHz rectennas, rectifying antenna, for FWA. We developed some types of the rectennas. Finally we achieve 65% of RF-DC conversion efficiency with output filter of harmonic balance."}
{"_id":"cfc4af4c0c96e39ed48584d61ee9ba5ebd2aa60a","title":"A Weighted Dictionary Learning Model for Denoising Images Corrupted by Mixed Noise","text":"This paper proposes a general weighted l2-l0 norms energy minimization model to remove mixed noise such as Gaussian-Gaussian mixture, impulse noise, and Gaussian-impulse noise from the images. The approach is built upon maximum likelihood estimation framework and sparse representations over a trained dictionary. Rather than optimizing the likelihood functional derived from a mixture distribution, we present a new weighting data fidelity function, which has the same minimizer as the original likelihood functional but is much easier to optimize. The weighting function in the model can be determined by the algorithm itself, and it plays a role of noise detection in terms of the different estimated noise parameters. By incorporating the sparse regularization of small image patches, the proposed method can efficiently remove a variety of mixed or single noise while preserving the image textures well. In addition, a modified K-SVD algorithm is designed to address the weighted rank-one approximation. The experimental results demonstrate its better performance compared with some existing methods."}
{"_id":"5eaaee68d0a4b01eeed884d00a58d8064c17ff4e","title":"Guido: A Musical Score Recognition System","text":"This paper presents an optical music recognition system Guido that can automatically recognize the main musical symbols of music scores that were scanned or taken by a digital camera. The application is based on object model of musical notation and uses linguistic approach for symbol interpretation and error correction. The system offers musical editor with a partially automatic error correction."}
{"_id":"e3ed44c657eb450a38538cd7433a7390ab5f0c46","title":"Batteries and Ultracapacitors for Electric, Hybrid, and Fuel Cell Vehicles","text":"The application of batteries and ultracapacitors in electric energy storage units for battery powered (EV) and charge sustaining and plug-in hybrid-electric (HEV and PHEV) vehicles have been studied in detail. The use of IC engines and hydrogen fuel cells as the primary energy converters for the hybrid vehicles was considered. The study focused on the use of lithium-ion batteries and carbon\/carbon ultracapacitors as the energy storage technologies most likely to be used in future vehicles. The key findings of the study are as follows. 1) The energy density and power density characteristics of both battery and ultracapacitor technologies are sufficient for the design of attractive EVs, HEVs, and PHEVs. 2) Charge sustaining, engine powered hybrid-electric vehicles (HEVs) can be designed using either batteries or ultracapacitors with fuel economy improvements of 50% and greater. 3) Plug-in hybrids (PHEVs) can be designed with effective all-electric ranges of 30-60 km using lithium-ion batteries that are relatively small. The effective fuel economy of the PHEVs can be very high (greater than 100 mpg) for long daily driving ranges (80-150 km) resulting in a large fraction (greater than 75%) of the energy to power the vehicle being grid electricity. 4) Mild hybrid-electric vehicles (MHEVs) can be designed using ultracapacitors having an energy storage capacity of 75-150 Wh. The fuel economy improvement with the ultracapacitors is 10%-15% higher than with the same weight of batteries due to the higher efficiency of the ultracapacitors and more efficient engine operation. 5) Hybrid-electric vehicles powered by hydrogen fuel cells can use either batteries or ultracapacitors for energy storage. Simulation results indicate the equivalent fuel economy of the fuel cell powered vehicles is 2-3 times higher than that of a gasoline fueled IC vehicle of the same weight and road load. Compared to an engine-powered HEV, the equivalent fuel economy of the hydrogen fuel cell vehicle would be 1.66-2.0 times higher"}
{"_id":"4a1b842dbb0f4b609d436ca82880cc64e96a546d","title":"Multi-label Text Categorization with Model Combination based on F1-score Maximization","text":"Text categorization is a fundamental task in natural language processing, and is generally defined as a multi-label categorization problem, where each text document is assigned to one or more categories. We focus on providing good statistical classifiers with a generalization ability for multi-label categorization and present a classifier design method based on model combination and F1-score maximization. In our formulation, we first design multiple models for binary classification per category. Then, we combine these models to maximize the F1-score of a training dataset. Our experimental results confirmed that our proposed method was useful especially for datasets where there were many combinations of category labels."}
{"_id":"7b4df53307a142765b8a60feb0b558c3c4a31c7a","title":"On accelerating pair-HMM computations in programmable hardware","text":"This paper explores hardware acceleration to significantly improve the runtime of computing the forward algorithm on Pair-HMM models, a crucial step in analyzing mutations in sequenced genomes. We describe 1) the design and evaluation of a novel accelerator architecture that can efficiently process real sequence data without performing wasteful work; and 2) aggressive memoization techniques that can significantly reduce the number of invocations of, and the amount of data transferred to the accelerator. We describe our demonstration of the design on a Xilinx Virtex 7 FPGA in an IBM Power8 system. Our design achieves a 14.85\u00d7 higher throughput than an 8-core CPU baseline (that uses SIMD and multi-threading) and a 147.49 \u00d7 improvement in throughput per unit of energy expended on the NA12878 sample."}
{"_id":"a3a9ac13f2022b98154eb864991e9e6d3d5b39d6","title":"The human connectome: Origins and challenges","text":"The human connectome refers to a map of the brain's structural connections, rendered as a connection matrix or network. This article attempts to trace some of the historical origins of the connectome, in the process clarifying its definition and scope, as well as its putative role in illuminating brain function. Current efforts to map the connectome face a number of significant challenges, including the issue of capturing network connectivity across multiple spatial scales, accounting for individual variability and structural plasticity, as well as clarifying the role of the connectome in shaping brain dynamics. Throughout, the article argues that these challenges require the development of new approaches for the statistical analysis and computational modeling of brain network data, and greater collaboration across disciplinary boundaries, especially with researchers in complex systems and network science."}
{"_id":"a2700a5942208de1ab1966ee60bf38bf30bbbac5","title":"Fully convolutional networks for multi-modality isointense infant brain image segmentation","text":"The segmentation of infant brain tissue images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) plays an important role in studying early brain development. In the isointense phase (approximately 6\u20138 months of age), WM and GM exhibit similar levels of intensity in both T1 and T2 MR images, resulting in extremely low tissue contrast and thus making the tissue segmentation very challenging. The existing methods for tissue segmentation in this isointense phase usually employ patch-based sparse labeling on single T1, T2 or fractional anisotropy (FA) modality or their simply-stacked combinations without fully exploring the multi-modality information. To address the challenge, in this paper, we propose to use fully convolutional networks (FCNs) for the segmentation of isointense phase brain MR images. Instead of simply stacking the three modalities, we train one network for each modality image, and then fuse their high-layer features together for final segmentation. Specifically, we conduct a convolution-pooling stream for multimodality information from T1, T2, and FA images separately, and then combine them in high-layer for finally generating the segmentation maps as the outputs. We compared the performance of our approach with that of the commonly used segmentation methods on a set of manually segmented isointense phase brain images. Results showed that our proposed model significantly outperformed previous methods in terms of accuracy. In addition, our results also indicated a better way of integrating multi-modality images, which leads to performance improvement."}
{"_id":"55d20eabc156f954d161c721af2e176f5991ed1d","title":"A 24-GHz high-gain Yagi-Uda antenna array","text":"A compact 24-GHz Yagi-Uda antenna has been developed using standard design tables and simple scaling to take into account the added capacitance due to the supporting dielectric substrate. The antenna results in a directivity of 9.3 dB, a front-to-back ratio of 11 dB, and a bandwidth of 2.5-3%. The Yagi-Uda antenna has been implemented in an 11-beam system using a planar array and a 2-inch Teflon spherical lens. The measured patterns show a 22 dB gain beam, a cross-polarization level of -24 dB, and a crossover level of -6 dB. The design method presented in this paper is quite straightforward, and can be used to develop low-, medium-, and even high-gain endfire Yagi-Uda antennas."}
{"_id":"484d3f1db7c8f89d6cb5c11c2633a8cc316df009","title":"A Decomposed Dual-Cross Generative Adversarial Network for Image Rain Removal","text":"Rain removal is important for many computer vision applications, such as surveillance, autonomous car, etc. Traditionally, rain removal is regarded as a signal removal problem which usually causes over-smoothing by removing texture details in non-rain background regions. This paper considers the issue of rain removal from a completely different perspective, to treat rain removal as a signal decomposition problem. Specifically, we decompose the rain image into two components, namely non-rain background image and rain streaks image. Then, we introduce an adversarial training mechanism to synthesize non-rain background image and rain streaks image in a Dual-Cross manner, which makes the two adversarial branches interact with each other, archiving a win-win result ultimately. The proposed Decomposed Dual-Cross Generative Adversarial Network (DDC-GAN) shows significantly performance improvement compared with stateof-the-art methods on both synthetic and real-world images in terms of qualitative and quantitative measures (over 3dB gains in PSNR)."}
{"_id":"7812d3528829cd26ab446ca208851db00129f294","title":"Content-based audio segmentation using support vector machines","text":"Audio exists at everywhere, but is often out-of-order. It is necessary to arrange them into regularized classes in order to use them more easily. It is also useful, especially in video content analysis, to segment an audio stream according to audio types. In this paper, we present our work in applying support vector machines (SVMs) in audio segmentation and classification. Five audio classes are considered: silence, music, background sound, pure speech, and non-pure speech which includes speech over music and speech over noise. A SVM learns optimal class boundaries from training data to best separate between two classes. A sound clip is segmented by classifying each sub-clip of one second into one of these five classes. Experiments on a database composed of clips of 14870 seconds in total length show that the average accuracy rate for the SVM method is much better than that of the traditional Euclidean distance based (nearest neighbor) method."}
{"_id":"88a6dbdde3e895f9f226d573770e902df22c0a6f","title":"Building contextual visual vocabulary for large-scale image applications","text":"Not withstanding its great success and wide adoption in Bag-of-visual Words representation, visual vocabulary created from single image local features is often shown to be ineffective largely due to three reasons. First, many detected local features are not stable enough, resulting in many noisy and non-descriptive visual words in images. Second, single visual word discards the rich spatial contextual information among the local features, which has been proven to be valuable for visual matching. Third, the distance metric commonly used for generating visual vocabulary does not take the semantic context into consideration, which renders them to be prone to noise. To address these three confrontations, we propose an effective visual vocabulary generation framework containing three novel contributions: 1) we propose an effective unsupervised local feature refinement strategy; 2) we consider local features in groups to model their spatial contexts; 3) we further learn a discriminant distance metric between local feature groups, which we call discriminant group distance. This group distance is further leveraged to induce visual vocabulary from groups of local features. We name it contextual visual vocabulary, which captures both the spatial and semantic contexts. We evaluate the proposed local feature refinement strategy and the contextual visual vocabulary in two large-scale image applications: large-scale near-duplicate image retrieval on a dataset containing 1.5 million images and image search re-ranking tasks. Our experimental results show that the contextual visual vocabulary shows significant improvement over the classic visual vocabulary. Moreover, it outperforms the state-of-the-art Bundled Feature in the terms of retrieval precision, memory consumption and efficiency."}
{"_id":"d171d1ed68ded30892576c4bfb9c155f7f5f7f48","title":"Paraphrase Identification Based on Weighted URAE, Unit Similarity and Context Correlation Feature","text":"A deep learning model adaptive to both sentence-level and articlelevel paraphrase identification is proposed in this paper. It consists of pairwise unit similarity feature and semantic context correlation feature. In this model, sentences are represented by word and phrase embedding while articles are represented by sentence embedding. Those phrase and sentence embedding are learned from parse trees through Weighted Unfolding Recursive Autoencoders (WURAE), an unsupervised learning algorithm. Then, unit similarity matrix is calculated by matching the pairwise lists of embedding. It is used to extract the pairwise unit similarity feature through CNN and k-max pooling layers. In addition, semantic context correlation feature is taken into account, which is captured by the combination of CNN and LSTM. CNN layers learn collocation information between adjacent units while LSTM extracts the long-term dependency feature of the text based on the output of CNN. This model is experimented on a famous English sentence paraphrase corpus, MSRPC, and a Chinese article paraphrase corpus. The results show that the deep semantic feature of text could be extracted based on WURAE, unit similarity and context correlation feature. We release our code of WURAE, deep learning model for paraphrase identification and pre-trained phrase end sentence embedding data for use by the community."}
{"_id":"0afc9ea9db2d9914b1763ae7b7ca05604de0eb05","title":"The impact of cyberloafing towards Malaysia employees' productivity: A conceptual framework","text":"The increasing use of the internet has changed the way we live and work. Moreover, in countries where there is a use of internet on a large scale, changes in the content and context of work have been observed. The use of Internet has become a tool to achieve competitive advantage by organizations. Also, it has been associated with many negative consequences such as reduced employees' productivity. Despite its importance, there is a lack of cyberloafing studies in Malaysia. The paper proposes a modified version of Triandis's theory of interpersonal behaviour by incorporating a new independent variable, ability to hide cyberloafing that directly predicts employees cyberloafing. This paper also recommends examining the relationship between cyberloafing and employees' productivity. Some studies believe that cyberloafing is beneficial for employees' productivity but other studies suggest that it is harmful to employees' productivity. By understanding what causes cyberloafing and its impacts, organizations can be more effective in managing the issue."}
{"_id":"2276067dcaa5fc9c240041a50a114a06a9636276","title":"Competing Memes Propagation on Networks: A Network Science Perspective","text":"In this paper, we study the intertwined propagation of two competing \"memes\" (or data, rumors, etc.) in a composite network. Within the constraints of this scenario, we ask two key questions: (a) which meme will prevail? and (b) can one influence the outcome of the propagations? Our model is underpinned by two key concepts, a structural graph model (composite network) and a viral propagation model (SI1I2S). Using this framework, we formulate a non-linear dynamic system and perform an eigenvalue analysis to identify the tipping point of the epidemic behavior. Based on insights gained from this analysis, we demonstrate an effective and accurate prediction method to determine viral dominance, which we call the EigenPredictor. Next, using a combination of synthetic and real composite networks, we evaluate the effectiveness of various viral suppression techniques by either a) concurrently suppressing both memes or b) unilaterally suppressing a single meme while leaving the other relatively unaffected."}
{"_id":"5b2bc4aaa63412ff1745a79d2f322b5ff67d0f9c","title":"Sentiment in short strength detection informal text","text":""}
{"_id":"0b65ecacd0025b4a0e31c4774a4925d3181631fa","title":"The dynamics of viral marketing","text":"We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We analyze how user behavior varies within user communities defined by a recommendation network. Product purchases follow a \u2018long tail\u2019 where a significant share of purchases belongs to rarely sold items. We establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies communities, product, and pricing categories for which viral marketing seems to be very effective."}
{"_id":"1be4b2bc0e149981f8814a0d9c130fd442c48e93","title":"Comparing and managing multiple versions of slide presentations","text":"Despite the ubiquity of slide presentations, managing multiple presentations remains a challenge. Understanding how multiple versions of a presentation are related to one another, assembling new presentations from existing presentations, and collaborating to create and edit presentations are difficult tasks. In this paper, we explore techniques for comparing and managing multiple slide presentations. We propose a general comparison framework for computing similarities and differences between slides. Based on this framework we develop an interactive tool for visually comparing multiple presentations. The interactive visualization facilitates understanding how presentations have evolved over time. We show how the interactive tool can be used to assemble new presentations from a collection of older ones and to merge changes from multiple presentation authors."}
{"_id":"1150ade08d97b42632df8355faf7f5b0ede31554","title":"Visual pattern discovery for architecture image classification and product image search","text":"Many objects have repetitive elements, and finding repetitive patterns facilitates object recognition and numerous applications. We devise a representation to describe configurations of repetitive elements. By modeling spatial configurations, visual patterns are more discriminative than local features, and are able to tackle with object scaling, rotation, and deformation. We transfer the pattern discovery problem into finding frequent subgraphs from a graph, and exploit a graph mining algorithm to solve this problem. Visual patterns are then exploited in architecture image classification and product image retrieval, based on the idea that visual pattern can describe elements conveying architecture styles and emblematic motifs of brands. Experimental results show that our pattern discovery approach has promising performance and is superior to the conventional bag-of-words approach."}
{"_id":"f5a275a173824adfb619cd4dc941e57ca1248eb7","title":"Autonomous decision-making: a data mining approach","text":"The researchers and practitioners of today create models, algorithms, functions, and other constructs defined in abstract spaces. The research of the future will likely be data driven. Symbolic and numeric data that are becoming available in large volumes will define the need for new data analysis techniques and tools. Data mining is an emerging area of computational intelligence that offers new theories, techniques, and tools for analysis of large data sets. In this paper, a novel approach for autonomous decision-making is developed based on the rough set theory of data mining. The approach has been tested on a medical data set for patients with lung abnormalities referred to as solitary pulmonary nodules (SPNs). The two independent algorithms developed in this paper either generate an accurate diagnosis or make no decision. The methodology discussed in the paper depart from the developments in data mining as well as current medical literature, thus creating a variable approach for autonomous decision-making."}
{"_id":"2c52492f8c44db08c87d368cc406e268163ed9f3","title":"Modeling Propagation Dynamics and Developing Optimized Countermeasures for Rumor Spreading in Online Social Networks","text":"The spread of rumors in Online Social Networks (OSNs) poses great challenges to the social peace and public order. It is imperative to model propagation dynamics of rumors and develop corresponding countermeasures. Most of the existing works either overlook the heterogeneity of social networks or do not consider the cost of countermeasures. Motivated by these issues, this paper proposes a heterogeneous network based epidemic model that incorporates both the network heterogeneity and various countermeasures. Through analyzing the existence and stability of equilibrium solutions of the proposed ODE (Ordinary Differential Equation) system, the critical conditions that determine whether a rumor continuously propagates or becomes extinct are derived. Moreover, we concern about the cost of the main two types of countermeasures, i.e., Blocking rumors at influential users and spreading truth to clarify rumors. Employing the Pontryagin's maximum principle, we obtain the optimized countermeasures that ensures a rumor can become extinct at the end of an expected time period with lowest cost. Both the critical conditions and the optimized countermeasures provide a real-time decision reference to restrain the rumor spreading. Experiments based on Digg2009 dataset are conducted to evaluate the effectiveness of the proposed dynamic model and the efficiency of the optimized countermeasures."}
{"_id":"d5e01fcb42a4f60f1a3686d1b4c37150b17bac29","title":"Bus tracking and monitoring using RFID","text":"The rapid growth in population in India causes more crowding at public bus stops. People long wait for arriving of buses and suddenly gather near bus when it arrives and travel in overcrowded buses on footboards which leads to accidents. Theft is another risk of overcrowding. All this happens due to lack of proper information of arriving of buses at many of the bus stops. To solve this problem in a cost effective way, we have developed an IoT based bus tracking system to show the current location of the bus and seat availability in the arriving buses. We have used RFID technology for tracking the bus and Thingspeak web server for displaying location of the bus and seat availability in android application in a smart phone. This system enables the commuters to know the exact location of the bus, and occupancy level in the bus, which enable the commuters to take decision, whether to board this bus or next. This reduces the waiting time and overcrowding at bus stops. It also reduces the risk of accidents and thefts."}
{"_id":"0bd9f30ad324a65895c52bbc889d69c2222a4684","title":"On path planning methods for automotive collision avoidance","text":"There is a strong trend for increasingly sophisticated Advanced Driver Assistance Systems (ADAS) such as Autonomous Emergency Braking (AEB) systems, Lane Keeping Aid (LKA) systems, and indeed autonomous driving. This trend generates a need for online maneuver generation, for which numerous approaches can be found in the large body of work related to path planning and obstacle avoidance. In order to ease the challenge of choosing a method, this paper reports quantitative and qualitative insights about three different path planning methods: a state lattice planner, predictive constraint-based planning, and spline-based search tree. Each method is described, implemented and compared on two specific traffic situations. The paper will not provide a final answer about which method is best. This depends on several factors such as computational constraints and the formulation of maneuver optimality that is appropriate for a given assistance or safety function. Instead, the conclusions will highlight qualitative merits and drawbacks for each method, in order to provide guidance for choosing a method for a specific application."}
{"_id":"974728aab2d5005b335896b9297e9a9326700782","title":"Edge detection using constrained discrete particle swarm optimisation in noisy images","text":"Edge detection algorithms often produce broken edges, especially in noisy images. We propose an algorithm based on discrete particle swarm optimisation (PSO) to detect continuous edges in noisy images. A constrained PSO-based algorithm with a new objective function is proposed to address noise and reduce broken edges. The localisation accuracy of the new algorithm is compared with that of a modified version of the Canny algorithm as a Gaussian-based edge detector, the robust rank order (RRO)-based algorithm as a statistical based edge detector, and our previously developed PSO-based algorithm. Pratt's figure of merit is used as a measure of localisation accuracy for these edge detection algorithms. Experimental results show that the performance of the new algorithm is higher than the Canny and RRO algorithms in the images corrupted by two different types of noise (impulsive and Gaussian noise). The new algorithm also detects edges more accurately and smoothly than our previously developed algorithm in noisy images."}
{"_id":"9cec2f71a5c953a72bc4242f20a0b9a91a262775","title":"Geo-parsing Messages from Microtext","text":"Widespread use of social media during crises has become commonplace, as shown by the volume of messages during the Haiti earthquake of 2010 and Japan tsunami of 2011. Location mentions are particularly important in disaster messages as they can show emergency responders where problems have occurred. This article explores the sorts of locations that occur in disaster-related social messages, how well off-theshelf software identifies those locations, and what is needed to improve automated location identification, called geo-parsing. To do this, we have sampled Twitter messages from the February 2011 earthquake in Christchurch, Canterbury, New Zealand. We annotated locations in messages manually to make a gold standard by which to measure locations identified by a Named Entity Recognition software. The Stanford NER software found some locations that were proper nouns, but did not identify locations that were not capitalized, local streets and buildings, or nonstandard place abbreviations and mis-spellings that are plentiful in microtext. We review how these problems might be solved in software research, and model a readable crisis map that shows crisis location clusters via enlarged place labels."}
{"_id":"984e9df38eb93781aabca41c91499317e9351ef6","title":"Combining virtual reality enabled simulation with 3D scanning technologies towards smart manufacturing","text":"Recent introduction of low-cost 3D sensing and affordable immersive virtual reality have lowered the barriers for creating and maintaining 3D virtual worlds. In this paper, we propose a way to combine these technologies with discrete-event simulation to improve the use of simulation in decision making in manufacturing. This work will describe how feedback is possible from real world systems directly into a simulation model to guide smart behaviors. Technologies included in the research include feedback from RGBD images of shop floor motion and human interaction within full immersive virtual reality that includes the latest headset technologies."}
{"_id":"a81d135e8102e0e887e8765140f95c2edc88637e","title":"Hybrid PWM Strategy of SVPWM and VSVPWM for NPC Three-Level Voltage-Source Inverter","text":"Neutral-point (NP) voltage drift is the main technical drawback of NP-clamped (NPC) three-level inverters. Traditional space vector pulsewidth modulation (SVPWM) is incapable of controlling the NP voltage for high modulation indexes and low power factors. Virtual SVPWM (VSVPWM) is capable of controlling the NP voltage under full modulation indexes and full power factors. However, this modulation strategy is more complex than SVPWM, increases the switching frequency, and deteriorates the output waveforms of the inverter. A novel PWM concept that includes NP voltage-balancing conditions is proposed. Based on this concept, a hybrid modulation scheme that uses both SVPWM and VSVPWM is presented for complete control of the NP voltage in NPC three-level inverters. The performance of this new modulation approach and its benefits over SVPWM and VSVPWM are verified by simulation and experiments."}
{"_id":"8ebd1dd54b7eaa1f130c9e1fb6052f112290e1b0","title":"Why-type Question Classification in Question Answering System","text":"The fundamental requisite to acquire information on any topic has become increasingly important. The need for Question Answering Systems (QAS) prevalent nowadays, replacing the traditional search engines stems from the user requirement for the most accurate answer to any question or query. Thus, interpreting the information need of the users is quite crucial for designing and developing a question answering system. Question classification is an important component in question answering systems that helps to determine the type of question and its corresponding type of answer. In this paper, we present a new way of classifying Why-type questions, aimed at understanding a questioner\u2019s intent. Our taxonomy classifies Why-type questions into four separate categories. In addition, to automatically detect the categories of these questions by a parser, we differentiate them at lexical level."}
{"_id":"def7eba25a199388d77fb2bc87a131a23c3f8ec6","title":"Circular Quadruple-Ridged Flared Horn Achieving Near-Constant Beamwidth Over Multioctave Bandwidth: Design and Measurements","text":"A circular quadruple-ridged flared horn achieving almost-constant beamwidth over 6:1 bandwidth is presented. This horn is the first demonstration of a wideband feed for radio telescopes which is capable of accommodating different reflector antenna optics, maintains almost constant gain and has excellent match. Measurements of stand-alone horn performance reveal excellent return loss performance as well as stable radiation patterns over 6:1 frequency range. Physical optics calculations predict an average of 69% aperture efficiency and 13 K antenna noise temperature with the horn installed on a radio telescope."}
{"_id":"61bef8f1b9a60cc78332a2e07cfbc9057fa4ee15","title":"Automatic Classification of Power Quality Events Using Balanced Neural Tree","text":"This paper proposes an empirical-mode decomposition (EMD) and Hilbert transform (HT)-based method for the classification of power quality (PQ) events. Nonstationary power signal disturbance waveforms are considered as the superimposition of various undulating modes, and EMD is used to separate out these intrinsic modes known as intrinsic mode functions (IMFs). The HT is applied on all the IMFs to extract instantaneous amplitude and frequency components. This time-frequency analysis results in the clear visual detection, localization, and classification of the different power signal disturbances. The required feature vectors are extracted from the time-frequency distribution to perform the classification. A balanced neural tree is constructed to classify the power signal patterns. Finally, the proposed method is compared with an S-transform-based classifier to show the efficacy of the proposed technique in classifying the PQ disturbances."}
{"_id":"820db2f7f1245f9dca77a7bfd0c8081078573976","title":"Research on nursing handoffs for medical and surgical settings: an integrative review.","text":"AIMS\nTo synthesize outcomes from research on handoffs to guide future computerization of the process on medical and surgical units.\n\n\nBACKGROUND\nHandoffs can create important information gaps, omissions and errors in patient care. Authors call for the computerization of handoffs; however, a synthesis of the literature is not yet available that might guide computerization.\n\n\nDATA SOURCES\nPubMed, CINAHL, Cochrane, PsycINFO, Scopus and a handoff database from Cohen and Hilligoss.\n\n\nDESIGN\nIntegrative literature review.\n\n\nREVIEW METHODS\nThis integrative review included studies from 1980-March 2011 in peer-reviewed journals. Exclusions were studies outside medical and surgical units, handoff education and nurses' perceptions.\n\n\nRESULTS\nThe search strategy yielded a total of 247 references; 81 were retrieved, read and rated for relevance and research quality. A set of 30 articles met relevance criteria.\n\n\nCONCLUSION\nStudies about handoff functions and rituals are saturated topics. Verbal handoffs serve important functions beyond information transfer and should be retained. Greater consideration is needed on analysing handoffs from a patient-centred perspective. Handoff methods should be highly tailored to nurses and their contextual needs. The current preference for bedside handoffs is not supported by available evidence. The specific handoff structure for all units may be less important than having a structure for contextually based handoffs. Research on pertinent information content for contextually based handoffs is an urgent need. Without it, handoff computerization is not likely to be successful. Researchers need to use more sophisticated experimental research designs, control for individual and unit differences and improve sampling frames."}
{"_id":"1ad823bf77c691f1d2b572799f8a8c572d941118","title":"Towards The Deep Model: Understanding Visual Recognition Through Computational Models","text":"Introduction Vision, due to its significance in surviving and socializing, is one of the most important and  extensively studied sensory functions in the human brain. In order to fully understand visual  information processing, or more specifically, visual recognition, David Marr proposed the  Tri-level Hypothesis [29], in that three levels of the system should be studied: the computational  goal of the system, the internal representation or the algorithm the system uses to achieve the  goal, and the neural substrates that implement the system. It is well-known that visual  recognition in the human brain is implemented by the ventral visual pathway [32], which  receives visual information from the retina and goes through a layered structure including V1  (also known as the primary visual cortex), V2, V4, before reaching the inferior temporal cortex  (IT). The topographic mapping between the retina and the human visual cortex follows a  log-polar transformation, in which the Cartesian coordinates of the retina are transformed to  polar coordinates (polar angle and eccentricity) in the human visual cortex. From V1 to V4, each  of the visual areas contains a complete polar angle and eccentricity map, and have a increasing  receptive field (RF) size and respond to increasing complex visual features [12,22]. In higher  level visual areas such as the ventral temporal cortex (VTC), studies using functional magnetic  resonance imaging (fMRI) and diffusion tensor imaging (DTI) have found there exists visual  category-selective regions, such as the Fusiform Face Area (FFA, [26]) for face processing, the  Parahippocampal Place Area (PPA, [13]) for scene perception, and the Lateral Occipital  Complex (LOC, [19]) for general object recognition. Grill-Spector and Malach [20] proposed that  layer-based hierarchical processing, together with the functional specialization, are the two  organizing principles of the human visual cortex. More recent studies have shown that the  central-biased face recognition pathway (FFA) and the peripheral-biased scene recognition  pathway (PPA) are both functionally and anatomically segregated by mid-fusiform sulcus (MFS),  which enables parallel fast processing of visual recognition tasks in the ventral pathway [18, 43,  21, 44]."}
{"_id":"b27dc92565b6dc50599fad2a9bc7ae0d9be0e7b8","title":"UniqueID: Decentralized Proof-of-Unique-Human","text":"Bitcoin and Ethereum are novel mechanisms for decentralizing the concept of money and computation. Extending decentralization to the human identity concept, we can think of using blockchain for creating a list of verified human identities with a one-person-one-ID property. UniqueID is a Decentralized Autonomous Organization(DAO) for maintaining human identities such that every physical human entity can have no more that one account. One part of this identity is simply the user\u2019s claim on one of his unique, permanent, and measurable characteristics -biometrics. Blockchain has proved its integrity as a platform for storing and performing computations on such claims. The biggest challenge here is to ensure that the user has submitted his own valid biometric data. Human verifiers can check if there is any inconsistency in other users\u2019 data, by peer-to-peer checks. For preventing bad behavior and centralization in the verification process, UniqueID benefits from novel governance mechanisms to choose verifiers and punish unjust ones. Also, there are incentives for honest verifiers and users by newly generated tokens. We show how the users\u2019 privacy can be preserved by using stateof-the-art cryptographic techniques, and so they can use their identity without any concerns for votings, financial and banking purposes, social media accounts, reputation systems etc."}
{"_id":"73ec2a1c77dc3cf1e3164626c81e4e72b15b2821","title":"On the multi-stage influence maximization problem","text":"The influence maximization problem turns up in many online social networks (OSN) in which each participant can potentially influence the decisions made by others in the network. Relationships can be friendships, family relationships or even professional relationships. Such influences can be used to achieve some objective and the influence maximization problem attempts to make decisions so as to maximize the effect of these influences. Past work focused on a static problem whereby one tries to identify the participants who are the most influential. Recently, a multi-stage version of the problem was proposed in which outcomes of influence attempts are observed before additional participants are chosen. For example, in online advertising, one can provide an impression to a particular subject and if that subject clicks on the impression, then their friends are informed in the hope that this information will increase the chances that they also click on the impression and eventually purchase the product. This problem is computationally intensive; in this paper we investigate various optimization methods for finding its solution that yield close to optimal results while taking less computation time. These include greedy and particle swarm optimization algorithms."}
{"_id":"12425a9f9ef67a4d1bc3741cc722be2af05f9649","title":"Conflict in Adult Close Relationships : An Attachment Perspective","text":"Relationship researchers have focused on the frequency of conflict in couples' relationships and the manner in which couples engage in and try to resolve conflicts. conflict, under some conditions, may facilitate the development and maintenance of associated with patterns of behavior (e.g., negative affect reciprocity, demand-withdraw) and thought that tend to escalate conflict and make it more difficult to negotiate a facilitates intimacy or exacerbates distress may depend on individual differences in the way in which people interpret and respond to conflict. framework for understanding different responses to conflict. People are thought to differ in their working models of attachment, which include expectations, beliefs, and goals These working models are likely to shape people's thoughts, feelings, and behavior during conflict. For example, a person who expects close others to be generally responsive and available is likely to interpret and respond to conflict very differently from a person who expects close others to be rejecting and unavailable. Attachment theory may be able to inform the literature on 3 conflict in close relationships by suggesting how individuals might differ in how they construe conflict. At the same time, the study of relationship conflict provides a useful context for testing important aspects of attachment theory. Conflict may be particularly likely to reveal attachment processes because (a) it may act as a stressor on the relationship and thereby activate the attachment system (Simpson, Rholes, & Phillips, 1996), (b) it challenges partners' abilities to regulate their emotions and behavior (Kobak & Duemmler, 1994), which are thought to be connected to attachment processes and (c) it may trigger behaviors (e.g., personal disclosures) that typically promote intimacy, thereby providing evidence relevant to different attachment goals such as achieving intimacy or maintaining self-reliance (Pietromonaco & Feldman Barrett, 1997). In this chapter, we first discuss how conflict can be conceptualized within an attachment framework. Specifically, we propose that conflict may pose a threat to the attachment bond, but that it also may provide an opportunity for perceiving or experiencing greater intimacy. Furthermore, the degree to which people perceive conflict as a threat, opportunity, or both will depend on the content (e.g., expectations, beliefs, goals) of their working models of attachment. Next, we identify a set of predictions that follow from this framework, and evaluate extent to which empirical findings support these predictions; in particular, we attempt to integrate divergent findings in the empirical literature. Finally, we outline several critical \u2026"}
{"_id":"581c012d0d3b7b5c2318413f7ad7640643e9f8ba","title":"Evaluation of cross-language voice conversion based on GMM and straight","text":"Voice conversion is a technique for producing utterances using any target speakers\u2019 voice from a single source speaker\u2019s utterance. In this paper, we apply cross-language voice conversion between Japanese and English to a system based on a Gaussian Mixture Model (GMM) method and STRAIGHT, a high quality vocoder. To investigate the effects of this conversion system across different languages, we recorded two sets of bilingual utterances and performed voice conversion experiments using a mapping function which converts parameters of acoustic features for a source speaker to those of a target speaker. The mapping functions were trained using bilingual databases of both Japanese and English speech. In an objective evaluation using Mel cepstrum distortion (Mel CD), it was confirmed that the system can perform cross-language voice conversion with the same performance as that within a single-language."}
{"_id":"b2db00f73fc6b97ebe12e97cfdaefbb2fefc253b","title":"A Comparison Between the Silhouette Index and the Davies-Bouldin Index in Labelling IDS Clusters","text":"One of the most difficult problems in the design of an anomaly b sed intrusion detection system (IDS) that uses clustering is th at of labelling the obtained clusters, i.e. determining which of them correspond t \u201dgood\u201d behaviour on the network\/host and which to \u201dbad\u201d behaviour. In this pap er, a new clusters\u2019 labelling strategy, which makes use of a clustering quality index is proposed for application in such an IDS. The aim of the new labelling algor ithm is to detect compact clusters containing very similar vectors and these are highly likely to be attack vectors. Two clustering quality indexes have been te sted and compared: the Silhouette index and the Davies-Bouldin index. Experiment al results comparing the effectiveness of a multiple classifier IDS with the two in dexes implemented show that the system using the Silhouette index produces sli ghtly more accurate results than the system that uses the Davies-Bouldin index. However, the computation of the Davies-Bouldin index is much less complex th an the computation of the Silhouette index, which is a very important advantage regarding eventual real-time operation of an IDS that employs clustering."}
{"_id":"32b892031491ceef88db855c343071ca914d18ac","title":"Detecting social media mobile botnets using user activity correlation and artificial immune system","text":"With the rapidly growing development of cellular networks and powerful smartphones, botnets have invaded the mobile domain. Social media, like Twitter, Facebook, and YouTube have created a new communication channel for attackers. Recently, bot masters started to exploit social media for different malicious activity, such as sending spam, recruitment of new bots, and botnet command and control. In this paper we propose a detection technique for social mediabased mobile botnets using Twitter. The proposed method combines the correlation between tweeting and user activity, such as clicks or taps, and an Artificial Immune System detector, to detect tweets caused by bots and differentiate them from tweets generated by user or by user-approved applications. This detector creates a signature of the tweet and compares it with a dynamically updated signature library of bot behavior signatures. The proposed system has been fully implemented on Android platform and tested under several sets of generated tweets. The test results show that the proposed method has a very high accuracy in detecting bot tweets with about 95% detection ratio."}
{"_id":"ee35c52c22fadf92277c308263be6288249a6327","title":"A miniaturized wideband dual-polarized linear array with balanced antipodal Vivaldi antenna","text":"Previous studies on Vivaldi wideband phased arrays are mainly focused on Two-dimensional scanning phased arrays. A miniaturized balanced antipodal Vivaldi antenna (BAVA) is presented in this paper. A novel vertical parasitic metal strip loading is employed in the dual-polarized linear phased arrays, it to make a compact structure. With the arc-shaped slots and metal strip loads, the radiation performance in lower operating band can be greatly enhanced. The proposed antenna is simulated in the infinite condition through periodic boundary with the size of 100mm (length) *100mm (width) *125mm (depth). The antenna achieves an impedance bandwidth when scanning to \u00b150\u00b0 for VSWR\u22643 achieves 4:1 (0.5GHz-2GHz) bandwidth and 5:1 (0.4GHz-2GHz) bandwidth in the vertical polarization and horizontal polarization, respectively, and the isolation is less than -18dB over the operating frequency range."}
{"_id":"7227d6b036febeaf686b058a1e2c75fe92eeb5a1","title":"The effects of stimulant therapy, EEG biofeedback, and parenting style on the primary symptoms of attention-deficit\/hyperactivity disorder.","text":"One hundred children, ages 6-19, who were diagnosed with attention-deficit\/hyperactivity disorder (ADHD), either inattentive or combined types, participated in a study examining the effects of Ritalin, EEG biofeedback, and parenting style on the primary symptoms of ADHD. All of the patients participated in a 1-year, multimodal, outpatient program that included Ritalin, parent counseling, and academic support at school (either a 504 Plan or an IEP). Fifty-one of the participants also received EEG biofeedback therapy. Posttreatment assessments were conducted both with and without stimulant therapy. Significant improvement was noted on the Test of Variables of Attention (TOVA; L. M. Greenberg, 1996) and the Attention Deficit Disorders Evaluation Scale (ADDES; S. B. McCarney, 1995) when participants were tested while using Ritalin. However, only those who had received EEG biofeedback sustained these gains when tested without Ritalin. The results of a Quantitative Electroencephalographic Scanning Process (QEEG-Scan; V. J. Monastra et al., 1999) revealed significant reduction in cortical slowing only in patients who had received EEG biofeedback. Behavioral measures indicated that parenting style exerted a significant moderating effect on the expression of behavioral symptoms at home but not at school."}
{"_id":"1a295aae9d78d54b329f5f83e1e3925988dadabd","title":"Cone Trees: animated 3D visualizations of hierarchical information","text":"The task of managing and accessing large information spaces is a problem in large scale cognition. Emerging technologies for 3D visualization and interactive animation offer potential solutions to this problem, especially when the structure of the information can be visualized. We describe one of these Information Visualization techniques, called the Cone Tree, which is used for visualizing hierarchical information structures. The hierarchy is presented in 3D to maximize effective use of available screen space and enable visualization of the whole structure. Interactive animation is used to shift some of the user's cognitive load to the human perceptual system."}
{"_id":"4619c5b1a37ee8e523f8104aa990ff50ca7ec7d9","title":"Estimating alertness from the EEG power spectrum","text":"In tasks requiring sustained attention, human alertness varies on a minute time scale. This can have serious consequences in occupations ranging from air traffic control to monitoring of nuclear power plants. Changes in the electroencephalographic (EEG) power spectrum accompany these fluctuations in the level of alertness, as assessed by measuring simultaneous changes in EEG and performance on an auditory monitoring task. By combining power spectrum estimation, principal component analysis and artificial neural networks, the authors show that continuous, accurate, noninvasive, and near real-time estimation of an operator's global level of alertness is feasible using EEC; measures recorded from as few as two central scalp sites. This demonstration could lead to a practical system for noninvasive monitoring of the cognitive state of human operators in attention-critical settings."}
{"_id":"c285e3f31cde88a3f80c8483edea712a8f02d334","title":"Pregnancy-related gigantomastia. Case report","text":"The term gigantomastia has been used to describe breast enlargement to extreme size, with sloughing, haemorrhage and infection. The condition is rare and a case of pregnancy-related gigantomastia is reported."}
{"_id":"a43b3644116e3289368e277ff4d9c63266991bb7","title":"High-Conversion-Ratio Bidirectional DC\u2013DC Converter With Coupled Inductor","text":"In this paper, a high-conversion-ratio bidirectional dc-dc converter with coupled inductor is proposed. In the boost mode, two capacitors are parallel charged and series discharged by the coupled inductor. Thus, high step-up voltage gain can be achieved with an appropriate duty ratio. The voltage stress on the main switch is reduced by a passive clamp circuit. Therefore, the low resistance RDS(ON) of the main switch can be adopted to reduce conduction loss. In the buck mode, two capacitors are series charged and parallel discharged by the coupled inductor. The bidirectional converter can have high step-down gain. Aside from that, all of the switches achieve zero voltage-switching turn-on, and the switching loss can be improved. Due to two active clamp circuits, the energy of the leakage inductor of the coupled inductor is recycled. The efficiency can be further improved. The operating principle and the steady-state analyses of the voltage gain are discussed. Finally, a 24-V-input-voltage, 400-V-output-voltage, and 200-W-output-power prototype circuit is implemented in the laboratory to verify the performance."}
{"_id":"a4f38719e4e20c25ac1f6cd51f31b38d50264590","title":"Cloud-assisted Industrial Internet of Things (IIoT) - Enabled framework for health monitoring","text":"The promising potential of the emerging Internet of Things (IoT) technologies for interconnected medical devices and sensors has played an important role in the next-generation healthcare industry for quality patient care. Because of the increasing number of elderly and disabled people, there is an urgent need for a real-time health monitoring infrastructure for analyzing patients\u2019 healthcare data to avoid preventable deaths. Healthcare Industrial IoT (HealthIIoT) has significant potential for the realization of such monitoring. HealthIIoT is a combination of communication technologies, interconnected apps, Things (devices and sensors), and people that would function together as one smart system to monitor, track, and store patients\u2019 healthcare information for ongoing care. This paper presents a HealthIIoT-enabled monitoring framework, where ECG and other healthcare data are collected by mobile devices and sensors and securely sent to the cloud for seamless access by healthcare professionals. Signal enhancement, watermarking, and other related analytics will be used to avoid identity theft or clinical error by healthcare professionals. The suitability of this approach has been validated through both experimental evaluation, and simulation by deploying an IoT-driven ECG-based health monitoring service in the cloud. \u00a9 2016 Elsevier B.V. All rights reserved."}
{"_id":"0f4f5ba66a0b666c512c4f120c521cecc89e013f","title":"RFID Technology for IoT-Based Personal Healthcare in Smart Spaces","text":"The current evolution of the traditional medical model toward the participatory medicine can be boosted by the Internet of Things (IoT) paradigm involving sensors (environmental, wearable, and implanted) spread inside domestic environments with the purpose to monitor the user's health and activate remote assistance. RF identification (RFID) technology is now mature to provide part of the IoT physical layer for the personal healthcare in smart environments through low-cost, energy-autonomous, and disposable sensors. It is here presented a survey on the state-of-the-art of RFID for application to body centric systems and for gathering information (temperature, humidity, and other gases) about the user's living environment. Many available options are described up to the application level with some examples of RFID systems able to collect and process multichannel data about the human behavior in compliance with the power exposure and sanitary regulations. Open challenges and possible new research trends are finally discussed."}
{"_id":"c9bfed5fb8c6c7e57f65568f32a311fd9e6148fd","title":"Cloud-enabled wireless body area networks for pervasive healthcare","text":"With the support of mobile cloud computing, wireless body area networks can be significantly enhanced for massive deployment of pervasive healthcare applications. However, several technical issues and challenges are associated with the integration of WBANs and MCC. In this article, we study a cloud-enabled WBAN architecture and its applications in pervasive healthcare systems. We highlight the methodologies for transmitting vital sign data to the cloud by using energy-efficient routing, cloud resource allocation, semantic interactions, and data security mechanisms."}
{"_id":"ceae612aef2950a5b42009c25302079f891bf7e2","title":"An IoT-Aware Architecture for Smart Healthcare Systems","text":"Over the last few years, the convincing forward steps in the development of Internet of Things (IoT)-enabling solutions are spurring the advent of novel and fascinating applications. Among others, mainly radio frequency identification (RFID), wireless sensor network (WSN), and smart mobile technologies are leading this evolutionary trend. In the wake of this tendency, this paper proposes a novel, IoT-aware, smart architecture for automatic monitoring and tracking of patients, personnel, and biomedical devices within hospitals and nursing institutes. Staying true to the IoT vision, we propose a smart hospital system (SHS), which relies on different, yet complementary, technologies, specifically RFID, WSN, and smart mobile, interoperating with each other through a Constrained Application Protocol (CoAP)\/IPv6 over low-power wireless personal area network (6LoWPAN)\/representational state transfer (REST) network infrastructure. The SHS is able to collect, in real time, both environmental conditions and patients' physiological parameters via an ultra-low-power hybrid sensing network (HSN) composed of 6LoWPAN nodes integrating UHF RFID functionalities. Sensed data are delivered to a control center where an advanced monitoring application (MA) makes them easily accessible by both local and remote users via a REST web service. The simple proof of concept implemented to validate the proposed SHS has highlighted a number of key capabilities and aspects of novelty, which represent a significant step forward compared to the actual state of the art."}
{"_id":"c9824c687887ae34739ad0a8cd94c08bf996e9ca","title":"Selective mutism and comorbidity with developmental disorder\/delay, anxiety disorder, and elimination disorder.","text":"OBJECTIVES\nTo assess the comorbidity of developmental disorder\/delay in children with selective mutism (SM) and to assess other comorbid symptoms such as anxiety, enuresis, and encopresis.\n\n\nMETHOD\nSubjects with SM and their matched controls were evaluated by a comprehensive assessment of the child and by means of a parental structured diagnostic interview with focus on developmental history. Diagnoses were made according to DSM-IV.\n\n\nRESULTS\nA total of 54 children with SM and 108 control children were evaluated. Of the children with SM, 68.5% met the criteria for a diagnosis reflecting developmental disorder\/delay compared with 13.0% in the control group. The criteria for any anxiety diagnosis were met by 74.1% in the SM group and for an elimination disorder by 31.5% versus 7.4% and 9.3%, respectively, in the control group. In the SM group, 46.3% of the children met the criteria for both an anxiety diagnosis and a diagnosis reflecting developmental disorder\/delay versus 0.9% in the controls.\n\n\nCONCLUSIONS\nSM is associated with developmental disorder\/delay nearly as frequently as with anxiety disorders. The mutism may conceal developmental problems in children with SM. Children with SM often meet diagnostic criteria for both a developmental and an anxiety disorder."}
{"_id":"b101dd995c21aefde44fdacec799875d8cd943bc","title":"Modelling memory functions with recurrent neural networks consisting of input compensation units: I. Static situations","text":"Humans are able to form internal representations of the information they process\u2014a capability which enables them to perform many different memory tasks. Therefore, the neural system has to learn somehow to represent aspects of the environmental situation; this process is assumed to be based on synaptic changes. The situations to be represented are various as for example different types of static patterns but also dynamic scenes. How are neural networks consisting of mutually connected neurons capable of performing such tasks? Here we propose a new neuronal structure for artificial neurons. This structure allows one to disentangle the dynamics of the recurrent connectivity from the dynamics induced by synaptic changes due to the learning processes. The error signal is computed locally within the individual neuron. Thus, online learning is possible without any additional structures. Recurrent neural networks equipped with these computational units cope with different memory tasks. Examples illustrate how information is extracted from environmental situations comprising fixed patterns to produce sustained activity and to deal with simple algebraic relations."}
{"_id":"7db9c588206d4286c25a12a75f06cc536a99ea34","title":"Optimal Government Debt Maturity \u2217","text":"This paper develops a model of optimal government debt maturity in which the government cannot issue state-contingent bonds and cannot commit to fiscal policy. If the government can perfectly commit, it fully insulates the economy against government spending shocks by purchasing short-term assets and issuing long-term debt. These positions are quantitatively very large relative to GDP and do not need to be actively managed by the government. Our main result is that these conclusions are not robust to the introduction of lack of commitment. Under lack of commitment, large and tilted positions are very expensive to finance ex-ante since they exacerbate the problem of lack of commitment ex-post. In contrast, a flat maturity structure minimizes the cost of lack of commitment, though it also limits insurance and increases the volatility of fiscal policy distortions. We show that the optimal maturity structure is nearly flat because reducing average borrowing costs is quantitatively more important for welfare than reducing fiscal policy volatility. Thus, under lack of commitment, the government actively manages its debt positions and can approximate optimal policy by confining its debt instruments to consols."}
{"_id":"f9cd7da733c5b5b54a4bbd35f67a913c05df83ea","title":"Direction of Arrival Estimation of GNSS Signals Based on Synthetic Antenna Array","text":"Jammer and interference are sources of errors in positions estimated by GNSS receivers. The interfering signals reduce signal-to-noise ratio and cause receiver failure to correctly detect satellite signals. Because of the robustness of beamforming techniques to jamming and multipath mitigation by placing nulls in direction of interference signals, an antenna array with a set of multi-channel receivers can be used to improve GNSS signal reception. Spatial reference beam forming uses the information in the Direction Of Arrival (DOA) of desired and interference signals for this purpose. However, using a multi-channel receiver is not applicable in many applications for estimating the Angle Of Arrival (AOA) of the signal (hardware limitations or portability issues). This paper proposes a new method for DOA estimation of jammer and interference signals based on a synthetic antenna array. In this case, the motion of a single antenna can be used to estimate the AOA of the interfering signals."}
{"_id":"77f14f3b5f094bf2e785fae772846116da18fa48","title":"FIGURE8: A Novel System for Generating and Evaluating Figurative Language","text":"Similes are easily obtained from web-driven and casebased reasoning approaches. Still, generating thoughtful figurative descriptions with meaningful relation to narrative context and author style has not yet been fully explored. In this paper, the author prepares the foundation for a computational model which can achieve this level of aesthetic complexity. This paper also introduces and evaluates a possible architecture for generating and ranking figurative comparisons on par with humans: the"}
{"_id":"5c5a997753f3c158d45e91abd70ede28ba78b0d4","title":"Detecting and Naming Actors in Movies Using Generative Appearance Models","text":"We introduce a generative model for learning person and costume specific detectors from labeled examples. We demonstrate the model on the task of localizing and naming actors in long video sequences. More specifically, the actor's head and shoulders are each represented as a constellation of optional color regions. Detection can proceed despite changes in view-point and partial occlusions. We explain how to learn the models from a small number of labeled key frames or video tracks, and how to detect novel appearances of the actors in a maximum likelihood framework. We present results on a challenging movie example, with 81% recall in actor detection (coverage) and 89% precision in actor identification (naming)."}
{"_id":"98269bdcadf64235a5b7f874f5b578ddcd5912b1","title":"Patient-centredness: a conceptual framework and review of the empirical literature.","text":"A 'patient-centred' approach is increasingly regarded as crucial for the delivery of high quality care by doctors. However, there is considerable ambiguity concerning the exact meaning of the term and the optimum method of measuring the process and outcomes of patient-centred care. This paper reviews the conceptual and empirical literature in order to develop a model of the various aspects of the doctor-patient relationship encompassed by the concept of 'patient-centredness' and to assess the advantages and disadvantages of alternative methods of measurement. Five conceptual dimensions are identified: biopsychosocial perspective; 'patient-as-person'; sharing power and responsibility; therapeutic alliance; and 'doctor-as-person'. Two main approaches to measurement are evaluated: self-report instruments and external observation methods. A number of recommendations concerning the measurement of patient-centredness are made."}
{"_id":"8759c972d89e1bf6deeab780aa2f8e21140c953b","title":"Excitation Control Method for a Low Sidelobe SIW Series Slot Array Antenna With 45$^{\\circ}$  Linear Polarization","text":"A sidelobe suppression method for a series slot array antenna which radiates 45\u00b0 -inclined linear polarization is proposed. Axial displacements are employed to create arbitrary excitation coefficients for individual centered-inclined radiating slots along the center line of a broad wall. To verify the proposed design method, we design two types of center-fed linear slot array antennas with a Dolph-Chebyshev distribution for -20 dB and -26 dB sidelobe levels (SLLs) in the Ka band. Furthermore, a cross-validation process involving an equivalent circuit model analysis and electromagnetic full-wave simulation using CST MWS is utilized. The entire structure of the proposed series slot array antenna is fabricated on printed circuit boards (PCBs), including drilling and chemical etching, to secure advantages of miniaturization and cost reduction. The measured realized gains are 15.17 and 15.95 dBi and SLLs are -18.7 and -22.5 dB respectively for two types of fabricated antennas. It demonstrates the validity of the proposed sidelobe suppression method."}
{"_id":"9ae641778971fd6e0bdf3755ef4658d40c218def","title":"Flow Cytometry Bioinformatics","text":"Flow cytometry bioinformatics is the application of bioinformatics to flow cytometry data, which involves storing, retrieving, organizing, and analyzing flow cytometry data using extensive computational resources and tools. Flow cytometry bioinformatics requires extensive use of and contributes to the development of techniques from computational statistics and machine learning. Flow cytometry and related methods allow the quantification of multiple independent biomarkers on large numbers of single cells. The rapid growth in the multidimensionality and throughput of flow cytometry data, particularly in the 2000s, has led to the creation of a variety of computational analysis methods, data standards, and public databases for the sharing of results. Computational methods exist to assist in the preprocessing of flow cytometry data, identifying cell populations within it, matching those cell populations across samples, and performing diagnosis and discovery using the results of previous steps. For preprocessing, this includes compensating for spectral overlap, transforming data onto scales conducive to visualization and analysis, assessing data for quality, and normalizing data across samples and experiments. For population identification, tools are available to aid traditional manual identification of populations in two-dimensional scatter plots (gating), to use dimensionality reduction to aid gating, and to find populations automatically in higher dimensional space in a variety of ways. It is also possible to characterize data in more comprehensive ways, such as the density-guided binary space partitioning technique known as probability binning, or by combinatorial gating. Finally, diagnosis using flow cytometry data can be aided by supervised learning techniques, and discovery of new cell types of biological importance by high-throughput statistical methods, as part of pipelines incorporating all of the aforementioned methods. Open standards, data, and software are also key parts of flow cytometry bioinformatics. Data standards include the widely adopted Flow Cytometry Standard (FCS) defining how data from cytometers should be stored, but also several new standards under development by the International Society for Advancement of Cytometry (ISAC) to aid in storing more detailed information about experimental design and analytical steps. Open data is slowly growing with the opening of the CytoBank database in 2010 and FlowRepository in 2012, both of which allow users to freely distribute their data, and the latter of which has been recommended as the preferred repository for MIFlowCyt-compliant data by ISAC. Open software is most widely available in the form of a suite of Bioconductor packages, but is also available for web execution on the GenePattern platform."}
{"_id":"4c58cdeba2201739a303cfe157e5dbd9c47ab577","title":"CREDIT CARD FRAUD DETECTION BASED ON BEHAVIOR MINING","text":"NIMISHA PHILIP, SHERLY K.K Department of Computer Science & Engineering, Toc H Institute of Science & Technology email:nimishavellapally@gmail.com Department of Information Technology, Toc H Institute of Science & Technology, Ernakulam, Kerala, India shrly_shilu@yahoo.com Abstract \u2014 Globalization and increased use of the Internet for online shopping has resulted in a considerable proliferation of credit card transactions throughout the world.Higher acceptability and convenience of credit cards for purchases has not only given personal comfort to customers but also attracted a large number of attackers. As a result, credit card payment systems must be supported by efficient fraud detection capability for minimizing unwanted activities by adversaries. Most of the well known algorithms for fraud detection are based on supervised training Every cardholder has a certain shopping behavior, which establishes an activity profile for him. Existing FDS try to capture behavioral patterns as rules which are static .This becomes ineffective when cardholder develops new patterns of behavior Here, we propose a unsupervised method to dynamically profile behavior pattern of customer Then the incoming transactions are compared against the user profile to indicate the anomalies, based on which the corresponding warnings are outputted. A FP tree based pattern matching algorithm is used to evaluate how unusual the new transactions are."}
{"_id":"6f60b08c9ef3040d0d8eaac32e0d991900b0e390","title":"The relationship between gamma frequency and running speed differs for slow and fast gamma rhythms in freely behaving rats.","text":"In hippocampal area CA1 of rats, the frequency of gamma activity has been shown to increase with running speed (Ahmed and Mehta, 2012). This finding suggests that different gamma frequencies simply allow for different timings of transitions across cell assemblies at varying running speeds, rather than serving unique functions. However, accumulating evidence supports the conclusion that slow (\u223c25-55 Hz) and fast (\u223c60-100 Hz) gamma are distinct network states with different functions. If slow and fast gamma constitute distinct network states, then it is possible that slow and fast gamma frequencies are differentially affected by running speed. In this study, we tested this hypothesis and found that slow and fast gamma frequencies change differently as a function of running speed in hippocampal areas CA1 and CA3, and in the superficial layers of the medial entorhinal cortex (MEC). Fast gamma frequencies increased with increasing running speed in all three areas. Slow gamma frequencies changed significantly less across different speeds. Furthermore, at high running speeds, CA3 firing rates were low, and MEC firing rates were high, suggesting that CA1 transitions from CA3 inputs to MEC inputs as running speed increases. These results support the hypothesis that slow and fast gamma reflect functionally distinct states in the hippocampal network, with fast gamma driven by MEC at high running speeds and slow gamma driven by CA3 at low running speeds."}
{"_id":"2683c31534393493924f967c5d1e4ba0657f0181","title":"Design and Implementation of a TEM Stripline for EMC Testing","text":"In this paper, a stripline for radiated field immunity testing of audio\/video products is designed, constructed, calibrated, and verified. Since the canonical one is only suitable for testing equipments with height less than 70 cm, there is a need of a new device which is also in compliance with EN 55020 standard for testing new large sets, like 47'' thin-film transistor liquid crystal display (TFT LCD) television sets, in the frequency range of 150 kHz-150 MHz. Increasing the height and width of testing area causes important problems in terms of field uniformity regarding the higher order modes, characteristic impedance, and reflections in addition to the room resonances and field interference sources like corners at the edges of stripline. Comprehensive numerical study is performed to overcome these problems and obtain the optimum design before the construction. Measured data show that the new stripline is in very good agreement with the specifications determined by the standards and therefore it can be used for the electromagnetic compatibility (EMC) testing."}
{"_id":"b790ed1ac0b3451ff7522b3b2b9cda6ca3e28670","title":"Giving a new makeover to STEAM: Establishing YouTube beauty gurus as digital literacy educators through messages and effects on viewers","text":""}
{"_id":"2f91a4913bcfaa5ed5db8f9fc4f5b28a272e52e6","title":"De-anonymizing scale-free social networks by percolation graph matching","text":"We address the problem of social network de-anonymization when relationships between people are described by scale-free graphs. In particular, we propose a rigorous, asymptotic mathematical analysis of the network de-anonymization problem while capturing the impact of power-law node degree distribution, which is a fundamental and quite ubiquitous feature of many complex systems such as social networks. By applying bootstrap percolation and a novel graph slicing technique, we prove that large inhomogeneities in the node degree lead to a dramatic reduction of the initial set of nodes that must be known a priori (the seeds) in order to successfully identify all other users. We characterize the size of this set when seeds are selected using different criteria, and we show that their number can be as small as n% for any small \u03b5 > 0. Our results are validated through simulation experiments on real social network graphs."}
{"_id":"c639c3aa374b39ae2e461ac704d90324cdcc3ac8","title":"SpiderMAV: Perching and stabilizing micro aerial vehicles with bio-inspired tensile anchoring systems","text":"Whilst Micro Aerial Vehicles (MAVs) possess a variety of promising capabilities, their high energy consumption severely limits applications where flight endurance is of high importance. Reducing energy usage is one of the main challenges in advancing aerial robot utility. To address this bottleneck in the development of unmanned aerial vehicle applications, this work proposes an bioinspired mechanical approach and develops an aerial robotic system for greater endurance enabled by low power station-keeping. The aerial robotic system consists of an multirotor MAV and anchoring modules capable of launching multiple tensile anchors to fixed structures in its operating envelope. The resulting tensile perch is capable of providing a mechanically stabilized mode for high accuracy operation in 3D workspace. We explore generalised geometric and static modelling of the stabilisation concept using screw theory. Following the analytical modelling of the integrated robotic system, the tensile anchoring modules employing high pressure gas actuation are designed, prototyped and then integrated to a quadrotor platform. The presented design is validated with experimental tests, demonstrating the stabilization capability even in a windy environment."}
{"_id":"1d9e8248ec8b333f86233bb0c4a88060776f51b1","title":"SUSAN\u2014A New Approach to Low Level Image Processing","text":"This paper describes a new approach to low level image processing; in particular, edge and corner detection and structure preserving noise reduction. Non-linear filtering is used to define which parts of the image are closely related to each individual pixel; each pixel has associated with it a local image region which is of similar brightness to that pixel. The new feature detectors are based on the minimization of this local image region, and the noise reduction method uses this region as the smoothing neighbourhood. The resulting methods are accurate, noise resistant and fast. Details of the new feature detectors and of the new noise reduction method are described, along with test results."}
{"_id":"3d31692cac919f0b98f5be6be55ff8945c2a810c","title":"An accident prediction approach based on XGBoost","text":"As an important threat to public security, urban fire accident causes huge economic loss and catastrophic collapse. Predicting and analyzing the interior rule of urban fire accident from its appearance needed to be solved in the field. In this paper, we propose a new urban fire accident prediction approach based on XGBoost. The method determines the predictive indexes in a quantitative and qualitative way from different characteristics in various kinds of fire accidents. For screening the features we need, we adopt the feature selection algorithm based on association rules. For data cleaning, we use a method based on Box-Cox transformation that transforms the continual response variables from the feature space for removing the dependencies on unobservable errors and the predictor variable to some extent. Then we use the data to train the model based on XGBoost to obtain the best prediction accuracy. Experiments show that the method provides a feasible solution to urban fire accident prediction. The method contributes to improving the public security situation, we have added the method and related model to the City in a box\u2122, Shenzhen Aerospace Smart City System Technology Co., Ltd."}
{"_id":"c7b2518e892ccb310d66bb9c1114cb78f5a22e23","title":"Generating spiking time series with Generative Adversarial Networks : an application on banking transactions by Luca Simonetto 11413522 September 2018","text":"The task of data generation using Generative Models has recently gained more and more attention from the scientific community, as the number of applications in which these models work surprisingly well is constantly increasing. Some examples are image and video generation, speech synthesis and style transfer, pose guided image generation, cross-domain transfer and super resolution. Contrarily to such tasks generating data coming from the banking domain poses a different challenge, due to its atypical structure when compared with traditional data and its limited availability due to privacy restrictions. In this work, we analyze the feasibility of generating spiking time series patterns appearing in the banking environment using Generative Adversarial Networks. We develop a novel end-to-end framework for training, testing and comparing different generative models using both quantitative and qualitative metrics. Finally, we propose a novel approach that combines Variational Autoencoders with Generative Adversarial Networks in order to learn a loss function for datasets in which good similarity metrics are difficult to define."}
{"_id":"a0efabbcdfa89f60624114fe7fb35c02093f3cc1","title":"Emotional stability, core self-evaluations, and job outcomes: A review of the evidence and an agenda for future research","text":"In this article we present a review of research on core self-evaluations, a broad personality trait indicated by 4 more narrow traits: self-esteem, generalized self-efficacy, locus of control, and emotional stability. We review evidence suggesting that the 4 core traits are highly related, load on a single unitary factor, and have dubious incremental validity controlling for their common core. We more generally investigate the construct validity of core self-evaluations. We also report on the development and validation of the first direct measure of the concept, the Core Self-Evaluations Scale (CSES). Cross-cultural evidence on the CSES is provided. We conclude by offering an agenda for future research, discussing areas where future core self-evaluations research is most needed."}
{"_id":"47f92fe6854c54c0b0e6d5792becfbd589dac877","title":"Analysis of digital bang-bang clock and data recovery for multi-gigabit\/s serial transceivers","text":"A Harmonic Balance method for analyzing digital bang-bang clock and data recovery (CDR) is proposed in this paper. The jitter tolerance performance of the CDR is predicted by a function with variables that can be easily correlated to design parameters. A 6.25Gb\/s serial transceiver was fabricated in 90nm CMOS technology. Measurements show that the jitter tolerance performance can be accurately predicted by the proposed method."}
{"_id":"2343169d2b0ea72c4160006414aa4c19e81728d1","title":"Combining geometry and combinatorics: A unified approach to sparse signal recovery","text":"There are two main algorithmic approaches to sparse signal recovery: geometric and combinatorial. The geometric approach utilizes geometric properties of the measurement matrix Phi. A notable example is the Restricted Isometry Property, which states that the mapping Phi preserves the Euclidean norm of sparse signals; it is known that random dense matrices satisfy this constraint with high probability. On the other hand, the combinatorial approach utilizes sparse matrices, interpreted as adjacency matrices of sparse (possibly random) graphs, and uses combinatorial techniques to recover an approximation to the signal. In this paper we present a unification of these two approaches. To this end, we extend the notion of Restricted Isometry Property from the Euclidean lscr2 norm to the Manhattan lscr1 norm. Then we show that this new lscr1 -based property is essentially equivalent to the combinatorial notion of expansion of the sparse graph underlying the measurement matrix. At the same time we show that the new property suffices to guarantee correctness of both geometric and combinatorial recovery algorithms. As a result, we obtain new measurement matrix constructions and algorithms for signal recovery which, compared to previous algorithms, are superior in either the number of measurements or computational efficiency of decoders."}
{"_id":"98d5590fb9510199f599012b337d6d0505da7fa2","title":"Chapter 3 Semantic Web Service Description","text":"The convergence of semantic Web with service oriented computing is manifested by Semantic Web Services (SWS) technology. It addresses the major challenge of automated, interoperable and meaningful coordination of Web Services to be carried out by intelligent software agents. In this chapter, we briefly discuss prominent SWS description frameworks, that are the standard SAWSDL, OWL-S and WSML. This is complemented by main critics of Semantic Web Services, and selected references to further readings on the subject."}
{"_id":"0e790522e68e44a5c99515e009049831b15cf29f","title":"Reconstructing Storyline Graphs for Image Recommendation from Web Community Photos","text":"In this paper, we investigate an approach for reconstructing storyline graphs from large-scale collections of Internet images, and optionally other side information such as friendship graphs. The storyline graphs can be an effective summary that visualizes various branching narrative structure of events or activities recurring across the input photo sets of a topic class. In order to explore further the usefulness of the storyline graphs, we leverage them to perform the image sequential prediction tasks, from which photo recommendation applications can benefit. We formulate the storyline reconstruction problem as an inference of sparse time-varying directed graphs, and develop an optimization algorithm that successfully addresses a number of key challenges of Web-scale problems, including global optimality, linear complexity, and easy parallelization. With experiments on more than 3.3 millions of images of 24 classes and user studies via Amazon Mechanical Turk, we show that the proposed algorithm improves other candidate methods for both storyline reconstruction and image prediction tasks."}
{"_id":"8e9a75f08e1e7a59f636091c7ea4839f1161e07c","title":"A note on multi-image denoising","text":"Taking photographs under low light conditions with a hand-held camera is problematic. A long exposure time can cause motion blur due to the camera shaking and a short exposure time gives a noisy image. We consider the new technical possibility offered by cameras that take image bursts. Each image of the burst is sharp but noisy. In this preliminary investigation, we explore a strategy to efficiently denoise multi-images or video. The proposed algorithm is a complex image processing chain involving accurate registration, video equalization, noise estimation and the use of state-of-the-art denoising methods. Yet, we show that this complex chain may become risk free thanks to a key feature: the noise model can be estimated accurately from the image burst. Preliminary tests will be presented. On the technical side, the method can already be used to estimate a non parametric camera noise model from any image burst."}
{"_id":"3d23be971f17ac490ba276de93f587b5203fccfc","title":"Air-ground localization and map augmentation using monocular dense reconstruction","text":"We propose a new method for the localization of a Micro Aerial Vehicle (MAV) with respect to a ground robot. We solve the problem of registering the 3D maps computed by the robots using different sensors: a dense 3D reconstruction from the MAV monocular camera is aligned with the map computed from the depth sensor on the ground robot. Once aligned, the dense reconstruction from the MAV is used to augment the map computed by the ground robot, by extending it with the information conveyed by the aerial views. The overall approach is novel, as it builds on recent developments in live dense reconstruction from moving cameras to address the problem of air-ground localization. The core of our contribution is constituted by a novel algorithm integrating dense reconstructions from monocular views, Monte Carlo localization, and an iterative pose refinement. In spite of the radically different vantage points from which the maps are acquired, the proposed method achieves high accuracy whereas appearance-based, state-of-the-art approaches fail. Experimental validation in indoor and outdoor scenarios reported an accuracy in position estimation of 0.08 meters and real time performance. This demonstrates that our new approach effectively overcomes the limitations imposed by the difference in sensors and vantage points that negatively affect previous techniques relying on matching visual features."}
{"_id":"807014cff599a76a2caeed3c88f9d7c265dbfb14","title":"Radar cross section measurement with 77 GHz automotive FMCW radar","text":"In this paper, radar cross section (RCS) measurement for human subjects and vehicles in 77 GHz automotive frequency modulated continuous wave (FMCW) radar system is presented. In this system, it is impossible to utilize conventional RCS definition due to high frequency band and the modulation technique used. Therefore, we introduce a new parameter called pseudo-RCS that can replace the conventional RCS. Then, we conduct actual experiment in the road to measure the RCS of the human subjects and the vehicles. In the measurement, data of four human subjects and four different kinds of vehicles are recorded with the 77 GHz FMCW radar. From the actual measured data, we find RCS distributions of the human subjects and the vehicles. For the human subjects, the RCS values are distributed following the Nakagami distribution. On the other hand, the log-normal distribution is well fit for the RCS values in the case of the vehicle."}
{"_id":"214621f1d3aef3ebe1b661a205801c4c8e8ccb54","title":"Treatment of molluscum contagiosum with cantharidin: a practical approach.","text":"Molluscum contagiosum is very common. In this article we discuss the use of cantharidin as a treatment option for molluscum contagiosum and give detailed information about distribution sources, how to apply it, and caveats regarding its use.Molluscum contagiosum is a common viral disease of childhood caused by a poxvirus, which presents with small, firm, dome-shaped, umbilicated papules. It is generally benign and self-limited, with spontaneous resolution within 6 months to several years. Watchful waiting can often be an appropriate management strategy; however, some patients either desire or require treatment. Reasons for actively treating molluscum contagiosum may include alleviation of discomfort and itching (particularly in patients where an eczematous eruption - the so-called \"molluscum eczema\" - is seen in association) or in patients with ongoing atopic dermatitis where more lesions are likely to be present. Other reasons for treatment include limitation of spread to other areas and people, prevention of scarring and superinfection, and elimination of the social stigma of visible lesions. No one treatment is uniformly effective.Treatment options include destructive therapies (curettage, cryotherapy, cantharidin, and keratolytics, among others), immunomodulators (imiquimod, cimetidine, and Candida antigen), and antivirals (cidofovir). In this article we discuss and describe our first-line treatment approach for those molluscum needing treatment - cantharidin."}
{"_id":"3a7194dbec56500589a8b059cbc422f4bd6dafb4","title":"Telemanipulation of Snake-Like Robots for Minimally Invasive Surgery of the Upper Airway","text":"Abstract. This research focuses on developing and testing the highlevel control of a novel eight DOF hybrid robot using a DaVinci master manipulator. The teleoperation control is formulated as weighted, multi objective constrained least square (LS) optimization problems one for the master controller and the other for the slave controller. This allows us to incorporate various virtual fixtures in our control algorithm as constraints of the LS problem based on the robot environment. Experimental validation to ensure position tracking and sufficient dexterity to perform suturing in confined spaces such as throat are presented."}
{"_id":"19bd0041f6c11f422675ece7e4fd70867d12f4ef","title":"Start Later, Sleep Later: School Start Times and Adolescent Sleep in Homeschool Versus Public\/Private School Students.","text":"Homeschooled students provide a naturalistic comparison group for later\/flexible school start times. This study compared sleep patterns and sleep hygiene for homeschooled students and public\/private school students (grades 6-12). Public\/private school students (n = 245) and homeschooled students (n = 162) completed a survey about sleep patterns and sleep hygiene. Significant school group differences were found for weekday bedtime, wake time, and total sleep time, with homeschooled students waking later and obtaining more sleep. Homeschooled students had later school start times, waking at the same time that public\/private school students were starting school. Public\/private school students had poorer sleep hygiene practices, reporting more homework and use of technology in the hour before bed. Regardless of school type, technology in the bedroom was associated with shorter sleep duration. Later school start times may be a potential countermeasure for insufficient sleep in adolescents. Future studies should further examine the relationship between school start times and daytime outcomes, including academic performance, mood, and health."}
{"_id":"cc558648d9c76e99fb4f7c7f9ba919c9106fe98a","title":"Robot navigation: Review of techniques and research challenges","text":"Robot navigation has multi-spectral applications across a plethora of industries. Being one of the fastest evolving field of Artificial intelligence, it has garnered a lot of speculation and the problem of efficient Robot navigation has become a talking point in the \u201cArtificial Intelligence research community\u201d. Robot Navigation can be thought of as a collection of sub-problems that when solved efficiently will (most likely) give the complete and the most efficient solution. While the main Navigation problem can be divided into numerous smaller ones some of the prevalent (sub)-tasks associated with Robot Navigation are Path Planning, Collision Prevention, Search Algorithm and an appropriate pictorial (graphical) representation of the given environment. Path Planning gives us the most optimal path finding approach while keeping the dynamism of the environment and the objects in mind. Collision Prevention ensures the Robot reaches its goal without colliding with any of the objects present in the environment. Collision Prevention techniques' vitality increases considerably when the number of objects in the input space is large (>10, approximately). Search Algorithms are the functional units of this problem as they determine the path the Robot follows while traveling to its destination. Since the environment greatly affects the choice of technique for all the other tasks it is vital to have a clear, concise and a non-ambiguous representation of the same. The main aim of the paper is to summarise the development of various techniques in the multiple domains of the Robot Navigation problem in the last fifteen years (2000-2015) while stating the scope and the restriction of each of them."}
{"_id":"92b09fbf854caefdb465885b2ebd85d76331dcbf","title":"Unsupervised Pattern Discovery in Speech","text":"We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream."}
{"_id":"37b602e4097b435a0413fd6f99773b83862f4037","title":"Hypothalamic Orexin Neurons Regulate Arousal According to Energy Balance in Mice","text":"Mammals respond to reduced food availability by becoming more wakeful and active, yet the central pathways regulating arousal and instinctual motor programs (such as food seeking) according to homeostatic need are not well understood. We demonstrate that hypothalamic orexin neurons monitor indicators of energy balance and mediate adaptive augmentation of arousal in response to fasting. Activity of isolated orexin neurons is inhibited by glucose and leptin and stimulated by ghrelin. Orexin expression of normal and ob\/ob mice correlates negatively with changes in blood glucose, leptin, and food intake. Transgenic mice, in which orexin neurons are ablated, fail to respond to fasting with increased wakefulness and activity. These findings indicate that orexin neurons provide a crucial link between energy balance and arousal."}
{"_id":"ddc5b36e1f952303de128e1d35ab20a9f80f9936","title":"Design of wideband printed rectangular monopole patch antenna with band notch","text":"In this paper, a microstrip-line fed printed monopole rectangular microstrip patch antenna with wide bandwidth having frequency band-notch function for wireless communication have been proposed. Bandwidth enhancement is obtained by rounding the corners of rectangular patch and modifying the ground plane. By cutting rectangular slot on the radiating element, the frequency band-notch characteristic is obtained. FR-4 substrate having dielectric constant of 4.4 and thickness of 1.6 mm is used to simulate printed rectangular monopole antenna (PRMA) having an overall size of substrate 55 \u00d7 45 mm2. The designed antenna is simulated using CAD FEKO simulation software. The proposed antenna has impedance bandwidth of 6.61 GHz (2.11 GHz to 8.72 GHz) with a rejection band from 3.03 GHz to 3.48 GHz. The observed gain of proposed antenna is 2.42 dBi. The proposed microstrip patch antenna is fabricated, tested and measured result is presented in this paper."}
{"_id":"4746f430e89262ccf8b59ece3f869c790e79c9ba","title":"Co-Clustering with Side Information for Text mining","text":"Many of the text mining applications contain a huge amount of information from document in the form of text. This text can be very helpful for Text Clustering. This text also includes various kind of other information known as Side Information or Metadata. Examples of this side information include links to other web pages, title of the document, author name or date of Publication which are present in the text document. Such metadata may possess a lot of information for the clustering purposes. But this Side information may be sometimes noisy. Using such Side Information for producing clusters without filtering it, can result to bad quality of Clusters. So we use an efficient Feature Selection method to perform the mining process to select that Side Information which is useful for Clustering so as to maximize the advantages from using it. The proposed technique, CCSI (Co-Clustering with Side Information) system makes use of the process of Co-Clustering or Two-mode clustering which is a data mining technique that allows concurrently clustering of the rows and columns of a matrix."}
{"_id":"21be31ffde44487ce2d69e36cc667edffceb87d0","title":"Gaze tracking by Binocular Vision and LBP features","text":"In this paper, a new method for eye gaze tracking is proposed under natural head movement. In this method, Local-Binary-Pattern Texture Feature (LBP) is adopted to calculate the eye features according to the characteristic of the eye, and a precise Binocular Vision approach is used to detect the space coordinate of the eye. The combined features of space coordinates and LBP features of the eyes are fed into Support Vector Regression (SVR) to match the gaze mapping function, in the hope of tracking gaze direction under natural head movement. The experimental results prove that the proposed method can determine the gaze direction accurately."}
{"_id":"ca20e2b6c65a9099f767a1325bd337a0c4e0605c","title":"Bridging the Gap: From Research to Practical Advice","text":"Software engineers must solve practical problems under deadline pressure. They rely on the best-codified knowledge available, turning to weaker results and their expert judgment when sound science is unavailable. Meanwhile, software engineering researchers seek fully validated results, resulting in a lag to practical guidance. To bridge this gap, research results should be systematically distilled into actionable guidance in a way that respects differences in strength and scope among the results. Starting with the practitioners\u2019 need for actionable guidance, this article reviews the evolution of software engineering research expectations, identifies types of results and their strengths, and draws on evidence-based medicine for a concrete example of deriving pragmatic guidance from mixed-strength research results. It advances a levels-of-evidence framework to allow researchers to clearly identify the strengths of their claims and the supporting evidence for their results and to work with practitioners to synthesize actionable recommendations from diverse types of evidence. This article is part of a special issue on software engineering\u2019s 50th anniversary."}
{"_id":"1f26c41f9d637f1e056355341d06472ad65a9a44","title":"Why did TD-Gammon Work?","text":"Although TD-Gammon is one of the major successes in machine learning, it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games. We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neural network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro\u2019s program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself."}
{"_id":"dce23f9f27f1237e1c39226e7105b45bad772818","title":"Subdivision templates for converting a non-conformal hex-dominant mesh to a conformal hex-dominant mesh without pyramid elements","text":"This paper presents a computational method for converting a non-conformal hex-dominant mesh to a conformal hex-dominant mesh without help of pyramid elements. During the conversion, the proposed method subdivides a non-conformal element by applying a subdivision template and conformal elements by a conventional subdivision scheme. Although many finite element solvers accept mixed elements, some of them require a mesh to be conformal without a pyramid element. None of the published automated methods could create a conformal hex-dominant mesh without help of pyramid elements, and therefore the applicability of the hex-dominant mesh has been significantly limited. The proposed method takes a non-conformal hex-dominant mesh as an input and converts it to a conformal hex-dominant mesh that consists only of hex, tet, and prism elements. No pyramid element will be introduced. The conversion thus considerably increases the applicability of the hex-dominant mesh in many finite element solvers."}
{"_id":"32e3fcd0b0fc569a8327ddf25f40e1975a32e636","title":"Total parser combinators","text":"A monadic parser combinator library which guarantees termination of parsing, while still allowing many forms of left recursion, is described. The library's interface is similar to those of many other parser combinator libraries, with two important differences: one is that the interface clearly specifies which parts of the constructed parsers may be infinite, and which parts have to be finite, using dependent types and a combination of induction and coinduction; and the other is that the parser type is unusually informative.\n The library comes with a formal semantics, using which it is proved that the parser combinators are as expressive as possible. The implementation is supported by a machine-checked correctness proof."}
{"_id":"0eac1d94c1aa7c3bd8a91b21d7df1b0e15a27a02","title":"An Inside Look at Botnets","text":"The continued growth and diversification of the Internet has been accompanied by an increasing prevalence of attacks and intrusions [40]. It can be argued, however, that a significant change in motivation for malicious activi ty has taken place over the past several years: from vandalism and recognition in th e hacker community, to attacks and intrusions for financial gain. This shift has bee n marked by a growing sophistication in the tools and methods used to conduct atta cks, thereby escalating the network security arms race. Our thesis is that the reactivemethods for network security that are predominant today are ultimately insufficient and that more proactivemethods are required. One such approach is to develop a foundational understanding of the mechanisms employed by malicious software (malware) which is often readi ly available in source form on the Internet. While it is well known that large IT secu rity companies maintain detailed databases of this information, these are not o penly available and we are not aware of any such open repository. In this paper we begin t he process of codifying the capabilities of malware by dissecting four widely-u sed Internet Relay Chat (IRC) botnet codebases. Each codebase is classified along se ven k y dimensions including botnet control mechanisms, host control mechani sms, propagation mechanisms, exploits, delivery mechanisms, obfuscation and de ception mechanisms. Our study reveals the complexity of botnet software, and we disc us es implications for defense strategies based on our analysis."}
{"_id":"eb2bfe2b5539b9390a83b8d593449dbc6540168f","title":"Lane-Level Integrity Provision for Navigation and Map Matching With GNSS, Dead Reckoning, and Enhanced Maps","text":"Lane-level positioning and map matching are some of the biggest challenges for navigation systems. Additionally, in safety applications or in those with critical performance requirements (such as satellite-based electronic fee collection), integrity becomes a key word for the navigation community. In this scenario, it is clear that a navigation system that can operate at the lane level while providing integrity parameters that are capable of monitoring the quality of the solution can bring important benefits to these applications. This paper presents a pioneering novel solution to the problem of combined positioning and map matching with integrity provision at the lane level. The system under consideration hybridizes measurements from a global navigation satellite system (GNSS) receiver, an odometer, and a gyroscope, along with the road information stored in enhanced digital maps, by means of a multiple-hypothesis particle-filter-based algorithm. A set of experiments in real environments in France and Germany shows the very good results obtained in terms of positioning, map matching, and integrity consistency, proving the feasibility of our proposal."}
{"_id":"c5e5d1bb3b124dbabe86eb3fbc95a94162a07b31","title":"Low voltage low power analog circuit design OTA using signal attenuation technique in universal filter application","text":"This paper presents a new configuration for a linear operational transconductance amplifier (OTA) using a signal attenuation technique. The OTA is designed to operate with a \u00b10.8V supply voltage and consumes 0.45mW power. All simulations are performed by ELDO model BSIM3v3 technology CMOS TSMC 0.18\u03bcm. The simulation results of this circuit showed a high DC gain of 73.6dB with a unity frequency of 50.19MHz and a total harmonic distortion of -60.81dB at 100 kHz for an input voltage of 1Vpp. Based on this circuit, a voltage mode universal filter has been implemented. The simulation results are in a good agreement with the theoretical calculations."}
{"_id":"21c2bd08b2111dcf957567b98e1c8dcad652e3dd","title":"Sample Size in Factor Analysis","text":"The factor analysis literature includes a range of recommendations regarding the minimum sample size necessary to obtain factor solutions that are adequately stable and that correspond closely to population factors. A fundamental misconception about this issue is that the minimum sample size, or the minimum ratio of sample size to the number of variables, is invariant across studies. In fact, necessary sample size is dependent on several aspects of any given study, including the level of communality of the variables and the level of overdetermination of the factors. The authors present a theoretical and mathematical framework that provides a basis for understanding and predicting these effects. The hypothesized effects are verified by a sampling study using artificial data. Results demonstrate the lack of validity of common rules of thumb and provide a basis for establishing guidelines for sample size in factor analysis."}
{"_id":"e99f72bc1d61bc7c8acd6af66880d9a815846653","title":"Automated Irrigation System-IoT Based Approach","text":"Agriculture is a major source of earning of Indians and agriculture has made a big impact on India's economy. The development of crops for a better yield and quality deliver is exceptionally required. So suitable conditions and suitable moisture in beds of crop can play a major role for production.. Mostly irrigation is done by tradition methods of stream flows from one end to other. Such supply may leave varied moisture levels in filed. The administration of the water system can be enhanced utilizing programmed watering framework This paper proposes a programmed water system with framework for the terrains which will reduce manual labour and optimizing water usage increasing productivity of crops. For formulating the setup, Arduino kit is used with moisture sensor with Wi-Fi module. Our experimental setup is connected with cloud framework and data is acquisition is done. Then data is analysed by cloud services and appropriate recommendations are given."}
{"_id":"43c9812beb3c36189c6a2c8299b5b5a7ae80c472","title":"Self-Autonomous Wireless Sensor Nodes With Wind Energy Harvesting for Remote Sensing of Wind-Driven Wildfire Spread","text":"The satellite-based remote sensing technique has been widely used in monitoring wildfire spread. There are two prominent drawbacks with this approach of using satellites located in space: (1) very low sampling rate (temporal resolution problem) and (2) lack of accuracy (spatial and spectral resolution problem). To address these challenges, a wireless sensor network deployed at ground level with high-fidelity and low-altitude atmospheric sensing for wind speed of local wildfire spread has been used. An indirect approach in sensing wind speed has been proposed in this paper as an alternative to the bulky conventional wind anemometer to save cost and space. The wind speed is sensed by measuring the equivalent electrical output voltage of the wind turbine generator (WTG). The percentage error in the wind speed measurement using the proposed indirect method is measured to be well within the \u00b14% limit with respect to wind anemometer accuracy. The same WTG also functions as a wind energy harvesting (WEH) system to convert the available wind energy into electrical energy to sustain the operation of the wireless sensor node. The experimental results show that the designed WEH system is able to harvest an average electrical power of 7.7 mW at an average wind speed of 3.62 m\/s for powering the operation of the wireless sensor node that consumes 3.5 mW for predicting the wildfire spread. Based on the sensed wind speed information, the fire control management system determines the spreading condition of the wildfire, and an adequate fire suppression action can be performed by the fire-fighting experts."}
{"_id":"2e4c3e053452b30b73dfab598bb0862b7c7181b2","title":"Real-parameter optimization with differential evolution","text":"This study reports how the differential evolution (DE) algorithm performed on the test bed developed for the CEC05 contest for real parameter optimization. The test bed includes 25 scalable functions, many of which are both non-separable and highly multi-modal. Results include DE's performance on the 10 and 30-dimensional versions of each function"}
{"_id":"6b5a99dd7a5adb37a7ed99006ed8f080be9380f3","title":"Opposition-based Magnetic Optimization Algorithm with parameter adaptation strategy","text":"Magnetic Optimization Algorithm (MOA) has emerged as a promising optimization algorithm that is inspired by the principles of magnetic field theory. In this paper we improve the performance of the algorithm in two aspects. First an Opposition-Based Learning (OBL) approach is proposed for the algorithm which is applied to the movement operator of the algorithm. Second, by learning from the algorithm\u2019s past experience, an adaptive parameter control strategy which dynamically sets the parameters of the algorithm during the optimization is proposed. To show the significance of the proposed parameter adaptation strategy, we compare the algorithm with two well-known parameter setting techniques on a number of benchmark problems. The results indicate that although the proposed algorithm with the adaptation strategy does not require to set the parameters of the algorithm prior to the optimization process, it outperforms MOA with other parameter setting strategies in most large-scale optimization problems. We also study the algorithm while employing the OBL by comparing it with the original version of MOA. Furthermore, the proposed algorithm is tested and compared with seven traditional population-based algorithms and eight state-of-theart optimization algorithms. The comparisons demonstrate that the proposed algorithm outperforms the traditional algorithms in most benchmark problems, and its results is comparative to those obtained by the state-of-the-art algorithms."}
{"_id":"eaad37c86a2bd3f5aa6e802cf3a9b5bcec6d7658","title":"Lightweight IPS for port scan in OpenFlow SDN networks","text":"Security has been one of the major concerns for the computer network community due to resource abuse and malicious flows intrusion. Before a network or a system is attacked, a port scan is typically performed to discover vulnerabilities, like open ports, which may be used to access and control them. Several studies have addressed Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) methods for detecting malicious activities, based on received flows or packet data analysis. However, those methods lead to an increase in switching latency, due to the need to analyze flows or packets before routing them. This may also increase network overhead when flows or packets are duplicated to be parsed by an external IDS. On the one hand, an IDS\/IPS may be a bottleneck on the network and may not be useful. On the other hand, the new paradigm called Software Defined Networking (SDN) and the OpenFlow protocol provide some statistical information about the network that may be used for detecting malicious activities. Hence, this work presents a new port scan IPS for SDN based on the OpenFlow switch counters data. A non-intrusive and lightweight method was developed and implemented, with low network overhead, and low memory and processing power consumption. The results showed that our method is effective on detecting and preventing port scan attacks."}
{"_id":"fa4377bc57379cdb7284eeeefa96617aa594f846","title":"Improved Option Pricing Using Artificial Neural Networks and Bootstrap Methods","text":"A hybrid neural network is used to predict the difference between the conventional option-pricing model and observed intraday option prices for stock index option futures. Confidence intervals derived with bootstrap methods are used in a trading strategy that only allows trades outside the estimated range of spurious model fits to be executed. Whilst hybrid neural network option pricing models can improve predictions they have bias. The hybrid option-pricing bias can be reduced with bootstrap methods. A modified bootstrap predictor is indexed by a parameter that allows the predictor to range from a pure bootstrap predictor, to a hybrid predictor, and finally the bagging predictor. The modified bootstrap predictor outperforms the hybrid and bagging predictors. Greatly improved performance was observed on the boundary of the training set and where only sparse training data exists. Finally, bootstrap bias estimates were studied."}
{"_id":"4bc8b0e0f0a2a370d3f52e227d560031be3b3d87","title":"High quality & low thermal resistance eutectic flip chip LED bonding","text":"Die attach process plays a key role in the thermal management of high power LED packages by governing the thermal resistance of the solder interfaces between the LED chips and the substrate materials. In this paper, comparison was made between the GaN based flip chip LED packages fabricated by (1) heated collet and (2) flux reflow eutectic die attach process. Thermal transient characteristics of the samples had been investigated based on the evaluation of the differential structure function and chip temperature measurement by infrared microscopy. It was shown that LEDs with higher void content would result in higher thermal resistance and chip temperature. Under high power operation at 6W, it was estimated that the junction temperature would exceed 80\u00b0C with void content 40%, which was at least 10\u00b0C higher comparing to that with void content 20%. Based on the result, LEDs samples bonded by heated collet die attach process had an average void content of 8.8% with 0.9% standard deviation, which were smaller comparing to the samples bonded by flux reflow die attach process (40% void content with standard deviation 20.5%). Due to the advantage of small void content control, it is possible to fabricate high power LED package with low thermal resistance, consistent and reliable die attach quality by using heated collet die attach process."}
{"_id":"4cb2f596c796fda6d12248824f68460db84100e4","title":"Unconstrained Pose-Invariant Face Recognition Using 3D Generic Elastic Models","text":"Classical face recognition techniques have been successful at operating under well-controlled conditions; however, they have difficulty in robustly performing recognition in uncontrolled real-world scenarios where variations in pose, illumination, and expression are encountered. In this paper, we propose a new method for real-world unconstrained pose-invariant face recognition. We first construct a 3D model for each subject in our database using only a single 2D image by applying the 3D Generic Elastic Model (3D GEM) approach. These 3D models comprise an intermediate gallery database from which novel 2D pose views are synthesized for matching. Before matching, an initial estimate of the pose of the test query is obtained using a linear regression approach based on automatic facial landmark annotation. Each 3D model is subsequently rendered at different poses within a limited search space about the estimated pose, and the resulting images are matched against the test query. Finally, we compute the distances between the synthesized images and test query by using a simple normalized correlation matcher to show the effectiveness of our pose synthesis method to real-world data. We present convincing results on challenging data sets and video sequences demonstrating high recognition accuracy under controlled as well as unseen, uncontrolled real-world scenarios using a fast implementation."}
{"_id":"1733583ca8de32ec5ff0526443b46db44f677b3e","title":"Deep Learning for Lung Cancer Detection: Tackling the Kaggle Data Science Bowl 2017 Challenge","text":"We present a deep learning framework for computer-aided lung cancer diagnosis. Our multi-stage framework detects nodules in 3D lung CAT scans, determines if each nodule is malignant, and \u0080nally assigns a cancer probability based on these results. We discuss the challenges and advantages of our framework. In the Kaggle Data Science Bowl 2017, our framework ranked 41st out of 1972 teams."}
{"_id":"a229a5ebba7275c372473071463bee14f6aa530b","title":"Multiple templates auto exposure control based on luminance histogram for onboard camera","text":"In order to adjust the exposure of high resolution and frequency images from an on-board camera in real time, a simple exposure control approach based on luminance histogram analysis is presented to realize light control precisely and rapidly. The algorithm divides the initial image into nine blocks and calculates the mean brightness of every part. It uses the luminance histogram to analyze the degree of exposure of both: the global image and every part, especially interest areas. According to the area size of the neighborhood on the two edges of the histogram, the different weight matrix can be decided to quickly calculate the optimal brightness for the next frame, and then the optimal exposure time can be determined accurately. The algorithm does not need to determine an expensive quadratic function to decide the weight of every part so as to fast perform auto exposure control. The experimental results show that the algorithm can correctly and fast adjust the exposure time of the on-board camera on an autonomous robot, and has strong adaptability for various shootings outdoor."}
{"_id":"24f4a4dba6888730e77ee909bfca7fbed48ccbc4","title":"What campus-based students think about the quality and benefits of e-learning","text":"There is a trend in Irish universities to utilise the benefits of the e-learning as a mechanism to improve learning performance of campus-based students. Whilst traditional methods, such as face-to-face lectures, tutorials, and mentoring, remain dominant in the educational sector, universities are investing heavily in learning technologies, to facilitate improvements with respect to the quality of learning. The technology to support reuse and sharing of educational resources, or learning objects, is becoming more stable, with interoperability standards maturing. However, debate has raged about what constitutes effective use of learning technology. This research expands upon a study carried out in 2003 examining students\u2019 perceptions of e-learning in a large undergraduate accounting class environment. As a result, improvements were made to the instructional design of the course, to enable students to engage interactively with content. The subsequent study, reported in this paper, adopted a broad range of techniques to understand students\u2019 learning experience in depth. The findings of this research provide an insight into how these students really work and learn using technologies, if at all. It is hoped that our findings will improve the experience for both students and lecturers who engage in teaching and learning through this medium. 502 British Journal of Educational Technology Vol 36 No 3 2005 \u00a9 British Educational Communications and Technology Agency, 2005. Introduction A recent trend in higher education is to create and provide online access to course materials. Over the past two decades academics and institutes of higher education have been diversifying their delivery of instruction through new Internet media such as learning management systems, asynchronous distance learning, and online classrooms amongst a myriad of other burgeoning educational technologies. This combination of traditional face-to-face lectures or tutorials, and web-based course content is better known as \u201cblended learning,\u201d purporting to blend the best aspects of real and virtual environments. Many Irish universities have invested in architectures and platforms to support their teaching staff in delivering material to students in a blended manner. Other institutions have adopted a lecturer-driven approach, whereby teaching staff are left to their own devices to supplement their lectures and tutorials with online material, hosted via their own web servers and typically open source, freeware software, or basic web pages. The reasons behind the drive to incorporate technology into the educational process are threefold. First, pressure to utilise information and communications technology (ICT) at university level comes from changes in the student demography. The rise of \u201cfull time part time students\u201d is a phenomenon of recent years, where school leavers take part-time jobs whilst attending university, leaving less time for evening tutorials or weekend study. In addition, there is a drive for what is known as lifelong learning, whereby adults are increasingly returning to institutions of higher education to take supplementary courses whilst in full-time employment, or during short career breaks. In this regard, the Irish government has made a commitment to have 15% of adults in continuing higher education by 2006. The figure currently stands at less than 5% (Department of Education and Science, 2000). This movement to adult continuing education was also reflected in a statement made by Information Commission Society (Ireland): \u201cThe issue of integration IT to the teaching process is an important part of future improvements, which it will be crucial to pursue\u201d (Boylan, 2000, p. 31). Second, changes in the market for delivery of education is also shifting. Private for-profit higher education institutions are offering a wide range of certificate and degree courses. The Irish Open University Initiative \u201cOscail\u201d and its UK equivalent offer distance learning diploma and degree courses. Many of these courses are delivered through ICT, along with weekend face-to-face tutorials. This distance learning market has also seen the arrival of new entrants, such as the Atlantic University Alliance, a collaboration between National University of Ireland, Galway, University College Cork, and the University of Limerick. Similar undertakings are underway in other institutes of higher education, whereby expertise in delivering blended learning within the institution is being extended towards distance learning courses by using a significant amount of ICT to overcome geographic and time barriers. Third, another pressure on traditional institutions of higher education comes from innovations in new technologies. John Seely Brown and Paul Duiguid claim that these technologies \u201coffer new ways to think of producing, distributing and consuming acaWhat students think about e-learning 503 \u00a9 British Educational Communications and Technology Agency, 2005. demic material. As with so many other institutions, new technologies have caused universities to rethink not simply isolated features but their entire mission and how they go about it\u201d (Seely Brown & Duiguid, 2000). This sentiment is also being echoed by government policy towards higher education where the Irish State is attempting to set certain parameters of performance covering such areas as equity of access, commitment to e-learning and to ICT, commitment to life-long learning, and outreach to other communities. In the subject domain of accountancy, the integration of technology into the classroom is also becoming a central issue for accounting educators for an additional reason. Accountancy professionals in industry require competency in accountancy-related information technology (IT) skills. Higher education institutions, in response, have tried to simulate authentic learning scenarios in the accountancy class and incorporate IT topics into their programmes. However, the efficacy of introducing technology in accounting education remains unclear. Positive learning effects associated with IT in accounting education have been noted (Abdolmohammadi et al , 1998; Fetters, McKenzie & Callaghan, 1986; Friedman, 1981). Equally, evidence has shown that technology has negatively impacted on learning (Dickens & Harper, 1986; Togo & McNamee, 1997). Indeed, some critics harshly criticise its introduction to educational contexts. Selwyn (2002, p. 172) claims in generic terms that \u201c...in the final analysis educational technology remains a diverting pastime and \u2018add-on\u2019 to the curriculum for some teaching and learners, whilst for most in education its impact has been slight.\u201d This paper hopes to look at some of the issues raised by individual students with regard to the use of technology in the teaching and learning process. Rather than focusing on macro issues, such as the political decisions behind the integration of technology, this research aims to examine individuals\u2019 acceptance or rejection of using ICT. Exploring the perspective of the individual student when discussing the use of technology is paramount to beginning to understand the nature of ICT in higher educational settings. This is of importance both to policy makers and education technologists in the debate on the role of ICT in education at an organisational level. The module and the medium This research focused on one module of The Principles of Accounting being taught to approximately 600 first-year undergraduate students at the University of Limerick. The module under investigation was part of a wider course, leading to a degree in Business Studies or Law and Accounting. The majority of students were between 17 and 19 years of age, coming from distributed second-level schools in Ireland. A minority were Erasmus students from various European countries, or mature students over the age of 23. For all, except overseas students, this was their first experience in using an e-learning platform, although most had previously informal used the web to gather information, or prepare coursework in second-level education, prior to entering university. This module was delivered by using a blended learning approach, supplementing weekly lectures, tutorials, and laboratory sessions with online course content, interac504 British Journal of Educational Technology Vol 36 No 3 2005 \u00a9 British Educational Communications and Technology Agency, 2005. tive quizzes, and Excel tasks. A range of different e-learning assessments complemented the traditional weekly meetings. Students were asked to submit a compulsory paperbased accounting case study, and additionally were offered two optional ICT assignments, a stream of online multiple-choice quizzes, and an Excel project. A module web site provided course details, additional readings, and supplementary links. Employing multiple teaching methods simultaneously is a form of blended learning (Saunders & Werner, 2003). Alternative instructional resources can stimulate positive effects on accounting students\u2019 learning experiences, according to (Rebele et al , 1998). Furthermore, instructional innovations are desirable to develop accounting students\u2019 IT competencies (Albrecht & Sack, 2000). Figure 1 illustrates the staged delivery of both the traditional and online elements of the module. The university does not provide institutional support for e-learning on a campus-wide scale, but two other modules in their first year also used a blended learning delivery approach, providing lecture notes online, or sample answers, in the subject area of economics and mathematics. Both of these other modules differ in marking scheme and in the extent to which online supports are provided such as discussion boards, or positive marking schemes for continuous online assessments. As such, blended learning is not yet a norm across modules for first-year students at this current time. Methods The research developed on experience gained in a previous study on a prior"}
{"_id":"fe50efe9e282c63941ec23eb9b8c7510b6283228","title":"A Facial Expression Recognition System Using Convolutional Networks","text":"Facial expression recognition has been an active research area in the past ten years, with a growing application area like avatar animation and neuromarketing. The recognition of facial expressions is not an easy problem for machine learning methods, since different people can vary in the way that they show their expressions. And even an image of the same person in one expression can vary in brightness, background and position. Therefore, facial expression recognition is still a challenging problem in computer vision. In this work, we propose a simple solution for facial expression recognition that uses a combination of standard methods, like Convolutional Network and specific image pre-processing steps. Convolutional networks, and the most machine learning methods, achieve better accuracy depending on a given feature set. Therefore, a study of some image pre-processing operations that extract only expression specific features of a face image is also presented. The experiments were carried out using a largely used public database for this problem. A study of the impact of each image pre-processing operation in the accuracy rate is presented. To the best of our knowledge, our method achieves the best result in the literature, 97.81% of accuracy, and takes less time to train than state-of-the-art methods."}
{"_id":"32af32d3f79d0f8130b5170238e9c9695e980097","title":"Analysis of user keyword similarity in online social networks","text":"How do two people become friends? What role does homophily play in bringing two people closer to help them forge friendship? Is the similarity between two friends different from the similarity between any two people? How does the similarity between a friend of a friend compare to similarity between direct friends? In this work, our goal is to answer these questions. We study the relationship between semantic similarity of user profile entries and the social network topology. A user profile in an on-line social network is characterized by its profile entries. The entries are termed as user keywords. We develop a model to relate keywords based on their semantic relationship and define similarity functions to quantify the similarity between a pair of users. First, we present a \u2018forest model\u2019 to categorize keywords across multiple categorization trees and define the notion of distance between keywords. Second, we use the keyword distance to define similarity functions between a pair of users. Third, we analyze a set of Facebook data according to the model to determine the effect of homophily in on-line social networks. Based on our evaluations, we conclude that direct friends are more similar than any other user pair. However, the more striking observation is that except for direct friends, similarities between users are approximately equal, irrespective of the topological distance between them."}
{"_id":"e4e9e923be7dba92d431cb70db67719160949053","title":"\"Industrie 4.0\" and Smart Manufacturing - A Review of Research Issues and Application Examples","text":""}
{"_id":"d605d6981960d0ba9e50d60de9ba72e5f73010da","title":"Effects of energy drinks on the cardiovascular system","text":"Throughout the last decade, the use of energy drinks has been increasingly looked upon with caution as potentially dangerous due to their perceived strong concentration of caffeine aside from other substances such as taurine, guarana, and L-carnitine that are largely unknown to the general public. In addition, a large number of energy drink intoxications have been reported all over the world including cases of seizures and arrhythmias. In this paper, we focus on the effect of energy drinks on the cardiovascular system and whether the current ongoing call for the products' sales and regulation of their contents should continue."}
{"_id":"73d749e30b41452873003a1286e8fee80bf5b69e","title":"Boosting item keyword search with spreading activation","text":"Most keyword search engines return directly matching keyword phrases. However, publishers cannot anticipate all possible ways in which users would search for the items in their documents. In fact, many times, there may be no direct keyword match between a keyword search phrase and descriptions of relevant items that are perfect matches for the search. We present an automated, high precision-based information retrieval solution to boost item find-ability by bridging the semantic gap between item information and popular keyword search phrases. Our solution achieves an average of 80% F-measure for various boosted matches for keyword search phrases in various categories."}
{"_id":"4e8e71312fea5688df34ca1272e19aea9893ce93","title":"Advanced Cooling for Power Electronics","text":"Power electronics devices such as MOSFET's, GTO's, IGBT's, IGCT's etc. are now widely used to efficiently deliver electrical power in home electronics, industrial drives, telecommunication, transport, electric grid and numerous other applications. This paper discusses cooling technologies that have evolved in step to remove increasing levels of heat dissipation and manage junction temperatures to achieve goals for efficiency, cost, and reliability. Cooling technologies rely on heat spreading and convection. In applications that use natural or forced air cooling, water heat pipes provide efficient heat spreading for size and weight reduction. Previous concepts are reviewed and an improved heat sink concept with staggered fin density is described for more isothermal cooling. Where gravity can drive liquid flow, thermosiphons provide efficient heat transport to remote fin volumes that can be oriented for natural and\/or forced air cooling. Liquid cold plates (LCP's) offer the means to cool high heat loads and heat fluxes including double sided cooling for the highest density packaging. LCP's can be used both in single phase cooling systems with aqueous or oil based coolants and in two-phase cooling systems with dielectric fluids and refrigerants. Previous concepts are reviewed and new concepts including an air cooled heat sink, a thermosiphon heat sink, a vortex flow LCP and a shear flow direct contact cooling concept are described."}
{"_id":"797f359b211c072a5b754e7a8f48a3b1ecf9b8be","title":"Evaluating a PID, pure pursuit, and weighted steering controller for an autonomous land vehicle","text":"Wright Laboratory, at Tyndall AFB, Florida, has contracted the University ofFlorida to develop autonomous navigation systems for a variety ofrobotic vehicles, capable ofperforming tasks associated with the location and removal of bombs and mines. One ofthe tasks involves surveying closed target ranges for unexploded buried munitions. Accuracy in path following is critical to the task. There are hundreds of acres that currently require surveying. The sites are typically divided into regions, where each mission can take up to 4.5 hours. These sites are usually surveyed along parallel rows. By improving the accuracy ofpath following, the distance betweenthe rows can be increased to nearly the detection width ofthe ground penetrating sensors, resulting in increased acreage surveyed per mission. This paper evaluates a high-level PID and a pure pursuit steering controller. The controllers were combined into a weighted solution so that the desirable features of each controller is preserved. This strategy was demonstrated in simulation and implemented on a Navigation Test Vehicle (NTV). For a test path ofvarying curvature, the average lateral control error was 2 cm at a vehicle speed of 1.34 mIs."}
{"_id":"50b6bf5e79a50e5d68f06047d850644554164ddc","title":"Large-signal-modulation of high-efficiency light-emitting diodes for optical communication","text":"The dynamic behavior of high-efficiency light-emitting diodes (LEDs) is investigated theoretically and experimentally. A detailed theoretical description of the switch-on and switch-off transients of LEDs is derived. In the limit of small-signal modulation, the well-established exponential behavior is obtained. However, in the case of high injection, which is easily reached for thin active layer LEDs, the small-signal time constant is found to be up to a factor of two faster than the radiative recombination lifetime. Using such quantum-well LEDs, we have demonstrated optical data transfer with wide open eye diagrams at bit rates up to 2 Gbit\/s. In addition, we have combined the use of thin active layers with the concept of surface-textured thin-film LEDs, which allow a significant improvement in the light extraction efficiency. With LEDs operating at 0.5 Gbit\/s and 1 Gbit\/s, we have achieved external quantum efficiencies of 36% and 29%, respectively."}
{"_id":"078e6aee9d0327a851ea04709452d642df8fbafd","title":"Assessment of exposure to sexually explicit materials and factors associated with exposure among preparatory school youths in Hawassa City, Southern Ethiopia: a cross-sectional institution based survey","text":"BACKGROUND\nAccording to the 2007 Ethiopian census, youths aged 15-24 years were more than 15.2 million which contributes to 20.6% of the whole population. These very large and productive groups of the population are exposed to various sexual and reproductive health risks. The aim of this study was to assess exposure to Sexually Explicit Materials (SEM) and factors associated with exposure among preparatory school students in Hawassa city, Southern Ethiopia.\n\n\nMETHODOLOGY\nA cross-sectional institution based study involving 770 randomly selected youth students of preparatory schools at Hawassa city. Multi stage sampling technique was used to select study subjects. Data was collected using pre-tested and self-administered questionnaire. Data was entered by EPI INFO version 3.5.1 and analyzed using SPSS version 20.0 statistical software packages. The result was displayed using descriptive, bivariate and multivariate analysis. Statistical association was done for independent predictors (at p\u2009<\u20090.05).\n\n\nRESULT AND DISCUSSION\nAbout 750 students were participated in this study with a response rate of 97.4%. Among this, about 77.3% of students knew about the presence of SEM and most of the respondents 566(75.5%) were watched SEM films\/movies and 554(73.9%) were exposed to SE texts. The overall exposure to SEM in school youths was 579(77.2%). Among the total respondents, about 522(70.4%) claimed as having no open discussion on sexual issues with in their family. Furthermore, About 450 (60.0%) respondents complained for having no sexual and reproductive health education at their school. Male students had faced almost two times higher exposure to SEM than female students (95 % CI: AOR 1.84(C.I\u2009=\u20091.22, 2.78). Students who attended private school were more than two times more likely exposed to SEM than public schools (95 % CI: AOR 2.07(C.I\u2009=\u20091.29, 3.30). Students who drink alcohol and labelled as 'sometimes' were two times more likely exposed to SEM than those who never drink alcohol (95 % CI\u2009=\u2009AOR 2.33(C.I\u2009=\u20091.26, 4.30). Khat chewers who labelled \"rarely\", \"sometimes\" and \"often\" had shown higher exposure (95 % CI: AOR 3.02(CI\u2009=\u20091.65, 5.52), (95 % CI: AOR 3.40(CI\u2009=\u20091.93, 6.00) and (95 % CI: AOR 2.67(CI\u2009=\u20091.46, 4.86) than those who never chew khat, respectively. Regarding SEM access, school youths with label 'easy access were exposed in odds of six folds than youths of no access (95 % CI: AOR 5.64(C.I\u2009=\u20093.56, 8.9).\n\n\nCONCLUSION\nHigh number of students was exposed to sexually explicit materials. Sex, school type, substance use and access to SEM were observed independent predictors of exposure to SEM.\n\n\nMOTIVATION\nThe current generation of young people is the healthiest, most educated, and most urbanized in history. However, there still remain some serious concerns. Most people become sexually active during adolescence. Premarital sexual activity is common and is on the rise worldwide. Rates are highest in sub Saharan Africa, where more than half of girls aged 15-19 are sexually experienced. Millions of adolescents are bearing children, in sub-Saharan Africa. More than half of women give birth before age 20. The need for improved health and social services aimed at adolescents, including reproductive health services, is being increasingly recognized throughout the world. Approximately 85 % of world adolescents live in developing countries. Each year, up to 100 million becomes infected with a curable sexually transmitted disease (STI). About 40 % of all new global human immunodeficiency virus (HIV) infections occur among 15-24 year olds; with recent estimates of 7000 infected each day. These health risks are influenced by many interrelated factors, such as expectations concerning early marriage and sexual relationships, access to education and employment, gender inequities, sexual violence, and the influence of mass media and popular culture. Furthermore, many adolescents lack strong stable relationships with parents or other adults whom they can talk to about their reproductive health concerns. Despite these challenges, programs that meet the information and service needs of adolescents can make a real difference. Successful programs help young people develop life-planning skill, respect the needs and concerns of young people, involve communities in their efforts, and provide respectful and confidential clinical services. Accordingly, the government of Ethiopia now works on improving adolescent's health as one part of MDG (Goal VI-halting transmission of HIV\/AIDS, STI, and other communicable diseases) with a focus on adolescents, since they are most affected population. This finding, therefore, will benefit the government to partly evaluate the goal achieving through adolescents exposure status to sexually explicit materials and improvement of sexual issues free talk with in school with class mates and their family at home. For that matter, we authors decided to publish this finding in BMC Reproductive Health Journal so that on line access will be easy to all governing bodies that they use to re-plan their strategies for better product of plan. Moreover, Researchers, Practitioners, policy makers, Students, school leaders and professionals will also benefit from this finding for their future researches references, knowledge gain and practice."}
{"_id":"1fc0fadc26eb6adef3692fb5dfa60cc177542986","title":"Automatic fault tree generation from SysML system models","text":"In this paper, a methodology is proposed to integrate safety analysis within a systems engineering approach. This methodology is based on SysML models and aims at generating (semi-) automatically safety analysis artifacts, mainly FMEA and FTA, from system models. Preliminary functional and component FMEA are automatically generated from the functional and structural models respectively, then completed by safety experts. By representing SysML structural diagram as a directed multi-graph, through a graph traversal algorithm and some identified patterns, generic fault trees are automatically derived with corresponding logic gates and events. The proposed methodology provides the safety expert with assistance during safety analysis. It helps reducing time and error proneness of the safety analysis process. It also helps ensuring consistency since the safety analysis artifacts are automatically generated from the latest system model version. The methodology is applied to a real case study, the electromechanical actuator EMA."}
{"_id":"4adef6d5951172dfce9d49e8672d960d11b6f8de","title":"Towards Text Knowledge Engineering","text":"We introduce a methodology for automating the maintenance of domain-specific taxonomies based on natural language text understanding. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual \u201cquality\u201d of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base."}
{"_id":"ccab8d39a252c43803d1e3273a48db162811ae5c","title":"A Single-Stage Single-Switch LED Driver Based on Class-E Converter","text":"This paper proposes a single-stage single-switch light-emitting diode (LED) driver that integrates a buck-boost circuit with a Class-E resonant converter by sharing single-switch device. The buck-boost circuit, working as a power-factor correction (PFC) stage, operates in the discontinuous conduction mode (DCM) to shape the input current. The Class-E converter steps down the voltage to drive the LED. For specific component parameters, the Class-E converter can achieve soft-switching on the power switch and two rectifier diodes, and reduce switch losses at high frequencies. Electrolytic capacitors are used in the proposed converter to achieve a lower cost, but the system reliability decreases. To overcome this disadvantage, film capacitors can be used, but the current ripple increases. Neglecting the cost, multilayered film capacitors are the best option, if higher reliability and lower current ripple are required. The proposed driver features high efficiency (90.8%), and a high power factor (PF) (0.995). In this paper, analytical results and design considerations at 100 kHz are presented, and a 100-W prototype with 110 VAC input was built to validate the theoretical analysis."}
{"_id":"6059d75e3dba73e43cc93762c677a962f5d903d9","title":"Computing the arc length of parametric curves","text":"Specifying constraints on motion is simpler if the curve is parameterized by arc length, but many parametric curves of practical interest cannot be parameterized by arc length. An approximate numerical reparameterization technique that improves on a previous algorithm by using a different numerical integration procedure that recursively subdivides the curve and creates a table of the subdivision points is presented. The use of the table greatly reduces the computation required for subsequent arc length calculations. After table construction, the algorithm takes nearly constant time for each arc length calculation. A linear increase in the number of control points can result in a more than linear increase in computation. Examples of this type of behavior are shown.<<ETX>>"}
{"_id":"135c89b491f82bd4fd7de175ac778207f598342b","title":"Not All Neural Embeddings are Born Equal","text":"Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and\/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status. It is well known that word representations can be learned from the distributional patterns in corpora. Originally, such representations were constructed by counting word co-occurrences, so that the features in one word\u2019s representation corresponded to other words [11, 17]. Neural language models, an alternative means to learn word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence [4, 14, 8], or simply a nearby word given a single cue word [13, 15]. The representations learned by neural models (sometimes called embeddings) generally outperform those acquired by co-occurrence counting models when applied to NLP tasks [3]. Despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings. Here, we explore this question by considering the embeddings learned by architectures with a very different objective function to monolingual language models: neural machine translation models. We show that translation-based embeddings outperform monolingual embeddings on two types of task: those that require knowledge of conceptual similarity (rather than simply association or relatedness), and those that require knowledge of syntactic role. We discuss what the findings indicate about the information content of different embeddings, and suggest how this content might emerge as a consequence of the translation objective. 1 Learning embeddings from language data Both neural language models and translation models learn real-valued embeddings (of specified dimension) for words in some pre-specified vocabulary, V , covering many or all words in their training corpus. At each training step, a \u2018score\u2019 for the current training example (or batch) is computed based on the embeddings in their current state. This score is compared to the model\u2019s objective function, and the error is backpropagated to update both the model weights (affecting how the score is computed from the embeddings) and the embedding features. At the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective. 1.1 Monolingual models In the original neural language model [4] and subsequent variants [8], each training example consists of n subsequent words, of which the model is trained to predict the n-th word given the first n \u2212 1 ar X iv :1 41 0. 07 18 v2 [ cs .C L ] 1 3 N ov 2 01 4 1 words. The model first represents the input as an ordered sequence of embeddings, which it transforms into a single fixed length \u2018hidden\u2019 representation by, e.g., concatenation and non-linear projection. Based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess at the next word. The model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus. More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer [13, 15]. Given a single word in the corpus, these models simply predict which other words will occur nearby. For each word w in V , a list of training cases (w, c) : c \u2208 V is extracted from the training corpus. For instance, in the skipgram approach [13], for each \u2018cue word\u2019 w the \u2018context words\u2019 c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).1 For each w in V , the model initialises both a cue-embedding, representing the w when it occurs as a cue-word, and a context-embedding, used when w occurs as a context-word. For a cue word w, the model can use the corresponding cueembedding and all context-embeddings to compute a probability distribution over V that reflects the probability of a word occurring in the context of w. When a training example (w, c) is observed, the model updates both the cue-word embedding of w and the context-word embeddings in order to increase the conditional probability of c. 1.2 Translation-based embeddings Neural translation models generate an appropriate sentence in their target language St given a sentence Ss in their source language [see, e.g., 16, 6]. In doing so, they learn distinct sets of embeddings for the vocabularies Vs and Vt in the source and target languages respectively. Observing a training case (Ss, St), such a model represents Ss as an ordered sequence of embeddings of words from Vs. The sequence for Ss is then encoded into a single representation RS .2 Finally, by referencing the embeddings in Vt, RS and a representation of what has been generated thus far, the model decodes a sentence in the target language word by word. If at any stage the decoded word does not match the corresponding word in the training target St, the error is recorded. The weights and embeddings in the model, which together parameterise the encoding and decoding process, are updated based on the accumulated error once the sentence decoding is complete. Although neural translation models can differ in low-level architecture [7, 2], the translation objective exerts similar pressure on the embeddings in all cases. The source language embeddings must be such that the model can combine them to form single representations for ordered sequences of multiple words (which in turn must enable the decoding process). The target language embeddings must facilitate the process of decoding these representations into correct target-language sentences. 2 Comparing Mono-lingual and Translation-based Embeddings To learn translation-based embeddings, we trained both the RNN encoder-decoder [RNNenc, 7] and the RNN Search architectures [2] on a 300m word corpus of English-French sentence pairs. We conducted all experiments with the resulting (English) source embeddings from these models. For comparison, we trained a monolingual skipgram model [13] and its Glove variant [15] for the same number of epochs on the English half of the bilingual corpus. We also extracted embeddings from a full-sentence language model [CW, 8] trained for several months on a larger 1bn word corpus. As in previous studies [1, 5, 3], we evaluate embeddings by calculating pairwise (cosine) distances and correlating these distances with (gold-standard) human judgements. Table 1 shows the correlations of different model embeddings with three such gold-standard resources, WordSim-353 [1], MEN [5] and SimLex-999 [10]. Interestingly, translation embeddings perform best on SimLex-999, while the two sets of monolingual embeddings perform better on modelling the MEN and WordSim353. To interpret these results, it should be noted that SimLex-999 evaluation quantifies conceptual similarity (dog wolf ), whereas MEN and WordSim-353 (despite its name) quantify more general relatedness (dog collar) [10]. The results seem to indicate that translation-based embeddings better capture similarity, while monolingual embeddings better capture relatedness. 1 Subsequent variants use different algorithms for selecting the (w, c) from the training corpus [9, 12] Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence [2]."}
{"_id":"1d2a4018b4fc6a5f498e65d68260615dbc9e7ec6","title":"Entity Linking meets Word Sense Disambiguation: a Unified Approach","text":"Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of-the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at http:\/\/babelfy.org"}
{"_id":"32cf9f4c97d74f8ee0d8a16e01cd4fd62d330650","title":"Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity","text":"Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening."}
{"_id":"e5ffd0d05ba5f0171721de5f770f61f2663afe39","title":"Cycling Down Under : A Comparative Analysis of Bicycling Trends and Policies in Sydney and Melbourne","text":"The purpose of this paper is to document and explain differences in cycling between Australia\u2019s two largest cities. Our comparative case study analysis is based on a wide range of statistical datasets, secondary reports, and interviews with a panel of 22 bicycling policy and planning experts. The main finding is that cycling levels in Melbourne are roughly twice as high as in Sydney and have been growing three times as fast in recent years. The difference is due to Melbourne\u2019s more favorable topography, climate, and road network as well as more supportive public policies. In particular, Melbourne has more and better integrated cycling infrastructure as well as more extensive cycling programs, advocacy, and promotional events. Melbourne also benefits from safer cycling than Sydney, which suffers from a lack of traffic-protected cycling facilities and aggressive motorist behavior toward cyclists on the road. While cycling has been increasing in Australia, it remains at very low levels relative to northern Europe, where both land use and transport policies are far more supportive of bicycling while discouraging car use through numerous restrictions and financial disincentives."}
{"_id":"324b3e5f126c9d6b7438a1336484fd2c07942c4c","title":"An attack scenario and mitigation mechanism for enterprise BYOD environments","text":"The recent proliferation of the Internet of Things (IoT) technology poses major security and privacy concerns. Specifically, the use of personal IoT devices, such as tablets, smartphones, and even smartwatches, as part of the Bring Your Own Device (BYOD) trend, may result in severe network security breaches in enterprise environments. Such devices increase the attack surface by weakening the digital perimeter of the enterprise network and opening new points of entry for malicious activities. In this paper we demonstrate a novel attack scenario in an enterprise environment by exploiting the smartwatch device of an innocent employee. Using a malicious application running on a suitable smartwatch, the device imitates a real Wi-Fi direct printer service in the network. Using this attack scenario, we illustrate how an advanced attacker located outside of the organization can leak\/steal sensitive information from the organization by utilizing the compromised smartwatch as a means of attack. An attack mitigation process and countermeasures are suggested in order to limit the capability of the remote attacker to execute the attack on the network, thus minimizing the data leakage by the smartwatch."}
{"_id":"31181e73befea410e25de462eccd0e74ba8fea0b","title":"Introduction to Algorithms, 3rd Edition","text":""}
{"_id":"0e6f5abd7e4738b765cd48f4c272093ecb5fd0bc","title":"Impact of an augmented reality system on students' motivation for a visual art course","text":""}
{"_id":"b4320a830664e3e80155d943f628c97053c5f1bc","title":"OTPaaS\u2014One Time Password as a Service","text":"Conventional password-based authentication is considered inadequate by users as many online services started to affect each other. Online credentials are used to recover other credentials and complex attacks are directed to the weakest one of many of these online credentials. As researchers are looking for new authentication techniques, one time passwords, which is a two-factor authentication scheme, looks like a natural enhancement over conventional username\/password schemes. The manuscript places the OTP verifier to the cloud to ease adoption of its usage by cloud service providers. When the OTP verifier is placed on the cloud as a service, other cloud service providers could outsource their OTP deployments as well as cloud users could activate their respective account on the OTP provider on several cloud services. This enables them to use several cloud services without the difficulty of managing several OTP accounts for each cloud service. On the other hand, OTP service provision saves inexperienced small to medium enterprises from spending extra costs for OTP provisioning hardware, software, and employers. The paper outlines architecture to build a secure, privacy-friendly, and sound OTP provider in the cloud to outsource the second factor of authentication. Cloud user registration to OTP provider, service provider activation, and authentication phases are inspected. The security and privacy considerations of the proposed architecture are defined and analyzed. Attacks from outsiders, unlinkability properties of user profiles, attacks from curious service providers or OTP verifiers are mitigated within the given assumptions. The proposed solution, which locates the OTP provider in the cloud, is rendered robust and sound as a result of the analysis."}
{"_id":"6b69fd4f192d655f5f0fd94bdae81e85ee3d4c75","title":"POLSAR Image Classification via Wishart-AE Model or Wishart-CAE Model","text":"Neural network such as an autoencoder (AE) and a convolutional autoencoder (CAE) have been successfully applied in image feature extraction. For the statistical distribution of polarimetric synthetic aperture radar (POLSAR) data, we combine the Wishart distance measurement into the training process of the AE and the CAE. In this paper, a new type of AE and CAE is specially defined, which we name them Wishart-AE (WAE) and Wishart-CAE (WCAE). Furthermore, we connect the WAE or the WCAE with a softmax classifier to compose a classification model for the purpose of POLSAR image classification. Compared with AE and CAE models, WAE and WCAE models can achieve higher classification accuracy because they could obtain the classification features, which are more suitable for POLSAR data. What is more, the WCAE model utilizes the local spatial information of a POLSAR image when compared with the WAE model. A convolutional natural network (CNN), which also makes use of the spatial information, has been widely applied in image classification, but our WCAE model is time-saving than the CNN model. Given the above, our methods not only improve the classification performance but also save the experimental time. Experimental results on four POLSAR datasets also demonstrate that our proposed methods are significantly effective."}
{"_id":"e216d9a5d5801f79ce88efa9c5ca7fa503f0cc75","title":"Comparison of database replication techniques based on total order broadcast","text":"In this paper, we present a performance comparison of database replication techniques based on total order broadcast. While the performance of total order broadcast-based replication techniques has been studied in previous papers, this paper presents many new contributions. First, it compares with each other techniques that were presented and evaluated separately, usually by comparing them to a classical replication scheme like distributed locking. Second, the evaluation is done using a finer network model than previous studies. Third, the paper compares techniques that offer the same consistency criterion (one-copy serializability) in the same environment using the same settings. The paper shows that, while networking performance has little influence in a LAN setting, the cost of synchronizing replicas is quite high. Because of this, total order broadcast-based techniques are very promising as they minimize synchronization between replicas."}
{"_id":"1de3667abbed271d2ae933ee9bdf8de3ee010f58","title":"A printed 16 ports massive MIMO antenna system with directive port beams","text":"The design of a massive multiple-input-multiple-output (mMIMO) antenna system based on patch antennas is presented in this paper. The array consists of 16 ports, each port consists of a 2\u00d72 patch antenna array with different phase excitation at each element to tilt the beam toward different directions to provide lower correlation coefficient values. A fixed progressive phase feed network is designed to provide the beam tilts. The proposed antenna system is designed using a 3-layer FR4 substrate with total size of 33.33\u00d733.33\u00d70.16 cm3."}
{"_id":"2787fafe4aa765d8716821df9ef9183dd9372323","title":"Social influence based clustering of heterogeneous information networks","text":"Social networks continue to grow in size and the type of information hosted. We witness a growing interest in clustering a social network of people based on both their social relationships and their participations in activity based information networks. In this paper, we present a social influence based clustering framework for analyzing heterogeneous information networks with three unique features. First, we introduce a novel social influence based vertex similarity metric in terms of both self-influence similarity and co-influence similarity. We compute self-influence and co-influence based similarity based on social graph and its associated activity graphs and influence graphs respectively. Second, we compute the combined social influence based similarity between each pair of vertices by unifying the self-similarity and multiple co-influence similarity scores through a weight function with an iterative update method. Third, we design an iterative learning algorithm, SI-Cluster, to dynamically refine the K clusters by continuously quantifying and adjusting the weights on self-influence similarity and on multiple co-influence similarity scores towards the clustering convergence. To make SI-Cluster converge fast, we transformed a sophisticated nonlinear fractional programming problem of multiple weights into a straightforward nonlinear parametric programming problem of single variable. Our experiment results show that SI-Cluster not only achieves a better balance between self-influence and co-influence similarities but also scales extremely well for large graph clustering."}
{"_id":"4d01dc1a9709f1b491ea2c3ceeafdcdf3875cef3","title":"Unemployment alters the set point for life satisfaction.","text":"According to set-point theories of subjective well-being, people react to events but then return to baseline levels of happiness and satisfaction over time. We tested this idea by examining reaction and adaptation to unemployment in a 15-year longitudinal study of more than 24,000 individuals living in Germany. In accordance with set-point theories, individuals reacted strongly to unemployment and then shifted back toward their baseline levels of life satisfaction. However, on average, individuals did not completely return to their former levels of satisfaction, even after they became reemployed. Furthermore, contrary to expectations from adaptation theories, people who had experienced unemployment in the past did not react any less negatively to a new bout of unemployment than did people who had not been previously unemployed. These results suggest that although life satisfaction is moderately stable over time, life events can have a strong influence on long-term levels of subjective well-being."}
{"_id":"3b3b5184573d7748a4f84fe9c5ccccb6838a203d","title":"Towards an intelligent textbook: eye gaze based attention extraction on materials for learning and instruction in physics","text":"In this paper, we propose the attention extraction method on a textbook of physics by using eye tracking glasses. We prepare a document including text and tasks in physics, and record reading behavior on the document from 6-grade students. The result confirms that students pay attention to a different region depending on situations (using the text as a material to learn the content, or using it as hints to solve tasks) and their comprehension."}
{"_id":"439f282365fb5678f8e891cf92fbc0c5b83e030c","title":"A single-phase grid-connected PV converter with minimal DC-link capacitor and low-frequency ripple-free maximum power point tracking","text":"This paper presents a novel single-phase grid-connected photovoltaic (PV) converter based on current-fed dual active bridge (CF-DAB) dc-dc converter. The proposed converter allows a minimal DC-link capacitor with large voltage variation without low-frequency effect on maximum power point tracking (MPPT). An advanced control system is developed based on the converter operation principles. Minimized transformer peak current is capable to be achieved through a symmetrical dc voltage control. Simulation results are presented to validate the performance of the proposed PV converter. A 5kW laboratorial prototype is also implemented and the results will be presented later."}
{"_id":"743bbf46557ec767f389c1ec5ac901ef64ffab37","title":"DeepV2D: Video to Depth with Differentiable Structure from Motion","text":"We propose DeepV2D, an end-to-end differentiable deep learning architecture for predicting depth from a video sequence. We incorporate elements of classical Structure from Motion into an end-to-end trainable pipeline by designing a set of differentiable geometric modules. Our full system alternates between predicting depth and refining camera pose. We estimate depth by building a cost volume over learned features and apply a multi-scale 3D convolutional network for stereo matching. The predicted depth is then sent to the motion module which performs iterative pose updates by mapping optical flow to a camera motion update. We evaluate our proposed system on NYU, KITTI, and SUN3D datasets and show improved results over monocular baselines and deep and classical stereo reconstruction."}
{"_id":"5f7da8725dd6e79228d156df929bcfa450afdfa8","title":"Verification of Periodically Controlled Hybrid Systems: Application to an Autonomous Vehicle","text":"This article introduces Periodically Controlled Hybrid Automata (PCHA) for modular specification of embedded control systems. In a PCHA, control actions that change the control input to the plant occur roughly periodically, while other actions that update the state of the controller may occur in the interim. Such actions could model, for example, sensor updates and information received from higher-level planning modules that change the set point of the controller. Based on periodicity and subtangential conditions, a new sufficient condition for verifying invariant properties of PCHAs is presented. For PCHAs with polynomial continuous vector fields, it is possible to check these conditions automatically using, for example, quantifier elimination or sum of squares decomposition. We examine the feasibility of this automatic approach on a small example. The proposed technique is also used to manually verify safety and progress properties of a fairly complex planner-controller subsystem of an autonomous ground vehicle. Geometric properties of planner-generated paths are derived which guarantee that such paths can be safely followed by the controller."}
{"_id":"2be92d9137f4af64a059da33a58b148d153fc446","title":"Prediction of Student Academic Performance by an Application of K-Means Clustering Algorithm","text":"Data Clustering is used to extract meaningful information and to develop significant relationships among variables stored in large data set\/data warehouse. In this paper data clustering technique named k-means clustering is applied to analyze student\u2019s learning behavior. The student\u2019s evaluation factor like class quizzes, mid and final exam assignment are studied. It is recommended that all these correlated information should be conveyed to the class advisor before the conduction of final exam. This study will help the teachers to reduce the drop out ratio to a significant level and improve the performance of students."}
{"_id":"fd1432a21eaeb1f231781e3f05fe4bb9ff613cd5","title":"Scalable Realistic Rendering with Many-Light Methods","text":"Recent years have seen increasing attention and significant progress in many-light rendering, a class of methods for the efficient computation of global illumination. The many-light formulation offers a unified mathematical framework for the problem reducing the full lighting transport simulation to the calculation of the direct illumination from many virtual light sources. These methods are unrivaled in their scalability: they are able to produce artifact-free images in a fraction of a second but also converge to the full solution over time. In this state-of-the-art report, we have three goals: give an easy-to-follow, introductory tutorial of many-light theory; provide a comprehensive, unified survey of the topic with a comparison of the main algorithms; and present a vision to motivate and guide future research. We will cover both the fundamental concepts as well as improvements, extensions, and applications of many-light rendering."}
{"_id":"023225d739460b96a32cecab31db777fe353dc64","title":"Accelerated training of conditional random fields with stochastic gradient methods","text":"We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques."}
{"_id":"cb3381448d1e79ab28796d74218a988f203b12ee","title":"HURRICANE TRAJECTORY PREDICTION VIA A SPARSE RECURRENT NEURAL NETWORK","text":"A proposed sparse recurrent neural network with flexible topology is used for trajectory prediction of the Atlantic hurricanes. For prediction of the future trajectories of a target hurricane, the most similar hurricanes to the target hurricane are found by comparing directions of the hurricanes. Then, the first and second differences of their positions over their life time are used for training the proposed network. Comparison of the obtained predictions with actual trajectories of Sandy and Humberto hurricanes show that our approach is quite promising for this aim."}
{"_id":"72d1423fdb2e2587a230356b7c1313a3571d9993","title":"Mindful consumption : a customer-centric approach to sustainability","text":"How effectively business deals with the challenges of sustainability will define its success for decades to come. Current sustainability strategies have three major deficiencies: they do not directly focus on the customer, they do not recognize the looming threats from rising global over-consumption, and they do not take a holistic approach. We present a framework for a customer-centric approach to sustainability. This approach recasts the sustainability metric to emphasize the outcomes of business actions measured holistically in term of environmental, personal and economic well-being of the consumer. We introduce the concept of mindful consumption (MC) as the guiding principle in this approach. MC is premised on a consumer mindset of caring for self, for community, and for nature, that translates behaviorally into tempering the selfdefeating excesses associated with acquisitive, repetitive and aspirational consumption. We also make the business case for fostering mindful consumption, and illustrate how the marketing function can be harnessed to successfully implement the customer-centric approach to sustainability."}
{"_id":"0501336bc04470489529b4928c5b6ba0f1bdf5f2","title":"Quality-biased ranking for queries with commercial intent","text":"Modern search engines are good enough to answer popular commercial queries with mainly highly relevant documents. However, our experiments show that users behavior on such relevant commercial sites may differ from one to another web-site with the same relevance label. Thus search engines face the challenge of ranking results that are equally relevant from the perspective of the traditional relevance grading approach. To solve this problem we propose to consider additional facets of relevance, such as trustability, usability, design quality and the quality of service. In order to let a ranking algorithm take these facets in account, we proposed a number of features, capturing the quality of a web page along the proposed dimensions. We aggregated new facets into the single label, commercial relevance, that represents cumulative quality of the site. We extrapolated commercial relevance labels for the entire learning-to-rank dataset and used weighted sum of commercial and topical relevance instead of default relevance labels. For evaluating our method we created new DCG-like metrics and conducted off-line evaluation as well as on-line interleaving experiments demonstrating that a ranking algorithm taking the proposed facets of relevance into account is better aligned with user preferences."}
{"_id":"4a87972b28143b61942a0eb011b60f76be0ebf2e","title":"Scalable Graph Exploration on Multicore Processors","text":"Many important problems in computational sciences, social network analysis, security, and business analytics, are data-intensive and lend themselves to graph-theoretical analyses. In this paper we investigate the challenges involved in exploring very large graphs by designing a breadth-first search (BFS) algorithm for advanced multi-core processors that are likely to become the building blocks of future exascale systems. Our new methodology for large-scale graph analytics combines a highlevel algorithmic design that captures the machine-independent aspects, to guarantee portability with performance to future processors, with an implementation that embeds processorspecific optimizations. We present an experimental study that uses state-of-the-art Intel Nehalem EP and EX processors and up to 64 threads in a single system. Our performance on several benchmark problems representative of the power-law graphs found in real-world problems reaches processing rates that are competitive with supercomputing results in the recent literature. In the experimental evaluation we prove that our graph exploration algorithm running on a 4-socket Nehalem EX is (1) 2.4 times faster than a Cray XMT with 128 processors when exploring a random graph with 64 million vertices and 512 millions edges, (2) capable of processing 550 million edges per second with an R-MAT graph with 200 million vertices and 1 billion edges, comparable to the performance of a similar graph on a Cray MTA-2 with 40 processors and (3) 5 times faster than 256 BlueGene\/L processors on a graph with average degree 50."}
{"_id":"fc72945cd68fe529222a44846c5fadeb791b2e62","title":"Measuring Negative and Positive Thoughts in Children: An Adaptation of the Children\u2019s Automatic Thoughts Scale (CATS)","text":"The aim of this study is to describe the factor structure and psychometric properties of an extended version of the Children\u2019s Automatic Thoughts Scale (CATS), the CATS-Negative\/Positive (CATS-N\/P). The CATS was originally designed to assess negative self-statements in children and adolescents. However, positive thoughts also play a major role in childhood disorders such as anxiety and depression. Therefore, positive self-statements were added to the CATS. The CATS-N\/P was administered to a community sample of 554 children aged 8\u201318\u00a0years. The results of a confirmatory factor analysis revealed that the positive self-statements formed a separate and psychometrically sound factor. Internal and short-term test\u2013retest reliability was good. Boys reported more hostile and positive thoughts than girls; and younger children reported more negative thoughts concerning physical threat, social threat, and failure than older children. In conclusion, the results of the current study support the use of the CATS-N\/P for the measurement of positive and negative thoughts in children. The application of the CATS-N\/P can facilitate further research on cognitive factors in different childhood disorders."}
{"_id":"9406ee01e3fda0932168f31cd3835a7d7a943fc6","title":"Multiple View Geometry in Computer Vision","text":""}
{"_id":"ec3b99525caeeee92887e4dcae8731377d37ddb2","title":"Design of Organic TFT Pixel Electrode Circuit for Active-Matrix OLED Displays","text":"A new current-programming pixel circuit for active-matrix organic light-emitting diode (AM-OLED) displays, composed of four organic thin-film transistors (OTFTs) and one capacitor, has been designed, simulated and evaluated. The most critical issue in realizing AMOLED displays with OTFTs is the variation and aging of the driving-TFTs and degradation of OLEDs. This can cause image sticking or degradation of image quality. These problems require compensation methods for high-quality display applications, and pixel level approach is considered to be one of the most important factors for improving display image quality. Our design shows that the current OLED and OTFT technology can be implemented for AMOLED displays, compensating the degradation of OTFT device characteristics."}
{"_id":"cb638a997e994e84cfd51a01ef1e45d3ccab0457","title":"Standard high-resolution pelvic MRI vs. low-resolution pelvic MRI in the evaluation of deep infiltrating endometriosis","text":"To compare the capabilities of standard pelvic MRI with low-resolution pelvic MRI using fast breath-hold sequences to evaluate deep infiltrating endometriosis (DIE). Sixty-eight consecutive women with suspected DIE were studied with pelvic MRI. A double-acquisition protocol was carried out in each case. High-resolution (HR)-MRI consisted of axial, sagittal, and coronal TSE T2W images, axial TSE T1W, and axial THRIVE. Low-resolution (LR)-MRI was acquired using fast single shot (SSH) T2 and T1 images. Two radiologists with 10 and 2\u00a0years of experience reviewed HR and LR images in two separate sessions. The presence of endometriotic lesions of the uterosacral ligament (USL), rectovaginal septum (RVS), pouch of Douglas (POD), and rectal wall was noted. The accuracies of LR-MRI and HR-MRI were compared with the laparoscopic and histopathological findings. Average acquisition times were 24\u00a0minutes for HR-MRI and 7\u00a0minutes for LR-MRI. The more experienced radiologist achieved higher accuracy with both HR-MRI and LR-MRI. The values of sensitivity, specificity, PPV, NPV, and accuracy did not significantly change between HR and LR images or interobserver agreement for all of the considered anatomic sites. LR-MRI performs as well as HR-MRI and is a valuable tool for the detection of deep endometriosis extension. \u2022 High- and low-resolution MRI perform similarly in deep endometriosis evaluation \u2022 Low-resolution MRI significantly reduces the duration of the examination \u2022 Radiologist experience is fundamental for evaluating deep pelvic endometriosis"}
{"_id":"138f8fc3e05509eb9d43d0446fcff21a73cf06ae","title":"Statistical Pattern Recognition","text":"This course will provide an introduction to statistical pattern recognition. The lectures will focus on different techniques including methods for feature extraction, dimensionality reduction, data clustering and pattern classification. State-of-art approaches such as ensemble learning and sparse modelling will be introduced. Selected real-world applications will illustrate how the techniques are applied in practice."}
{"_id":"f8b12b0a138271c245097f2c70ed9a0b61873968","title":"Simultaneous algebraic reconstruction technique (SART): a superior implementation of the art algorithm.","text":"In this paper we have discussed what appears to be a superior implementation of the Algebraic Reconstruction Technique (ART). The method is based on 1) simultaneous application of the error correction terms as computed by ART for all rays in a given projection; 2) longitudinal weighting of the correction terms back-distributed along the rays; and 3) using bilinear elements for discrete approximation to the ray integrals of a continuous image. Since this implementation generates a good reconstruction in only one iteration, it also appears to have a computational advantage over the more traditional implementation of ART. Potential applications of this implementation include image reconstruction in conjunction with ray tracing for ultrasound and microwave tomography in which the curved nature of the rays leads to a non-uniform ray density across the image."}
{"_id":"0633015006fd8088c9089a94848ef3f21ac3881c","title":"QUASY: Quantitative Synthesis Tool","text":"We present the tool Q UASY, a quantitative synthesis tool. Q UASY takes qualitative and quantitative specifications and automatic ally onstructs a system that satisfies the qualitative specification and optimizes the qu antitative specification, if such a system exists. The user can choose between a system that satisfies and optimi zes the specifications (a) under all possible environment behaviors or (b) under th most-likely environment behaviors given as a probability distribution on the po ssible input sequences. QUASY solves these two quantitative synthesis problems by reduct ion to instances of 2-player games and Markov Decision Processes (MDPs) with qua ntitative winning objectives. QUASY can also be seen as a game solver for quantitative games. Most notable, it can solve lexicographic mean-payoff games with 2 players, MDPs with meanpayoff objectives, and ergodic MDPs with mean-payoff parit y objectives."}
{"_id":"1199c44a5834cce889e52ba128068b4d2224b067","title":"Gist: A Solver for Probabilistic Games","text":"Gist is a tool that (a) solves the qualitative analysis problem of turn-based probabilistic games with \u03c9-regular objectives; and (b) synthesizes reasonable environment assumptions for synthesis of unrealizable specifications. Our tool provides the first and efficient implementations of several reduction-based techniques to solve turn-based probabilistic games, and uses the analysis of turn-based probabilistic games for synthesizing environment assumptions for unrealizable specifications."}
{"_id":"38c6ff59aadeab427024b62f69c8f41d68e7cb37","title":"Playing Stochastic Games Precisely","text":"We study stochastic two-player games where the goal of one player is to achieve precisely a given expected value of the objective function, while the goal of the opponent is the opposite. Potential applications for such games include controller synthesis problems where the optimisation objective is to maximise or minimise a given payoff function while respecting a strict upper or lower bound, respectively. We consider a number of objective functions including reachability, \u03c9-regular, discounted reward, and total reward. We show that precise value games are not determined, and compare the memory requirements for winning strategies. For stopping games we establish necessary and sufficient conditions for the existence of a winning strategy of the controller for a large class of functions, as well as provide the constructions of compact strategies for the studied objectives."}
{"_id":"5b9631561a89a3e071d8ec386a616a120220bfd9","title":"PRISM 4.0: Verification of Probabilistic Real-Time Systems","text":"This paper describes a major new release of the PRISM probabilistic model checker, adding, in particular, quantitative verification of (priced) probabilistic timed automata. These model systems exhibiting probabilistic, nondeterministic and real-time characteristics. In many application domains, all three aspects are essential; this includes, for example, embedded controllers in automotive or avionic systems, wireless communication protocols such as Bluetooth or Zigbee, and randomised security protocols. PRISM, which is open-source, also contains several new components that are of independent use. These include: an extensible toolkit for building, verifying and refining abstractions of probabilistic models; an explicit-state probabilistic model checking library; a discrete-event simulation engine for statistical model checking; support for generation of optimal adversaries\/strategies; and a benchmark suite."}
{"_id":"8a6818f092710d5b03bd713bf74059346cff1678","title":"The Ins and Outs of the Probabilistic Model Checker MRMC","text":"The Markov Reward Model Checker (MRMC) is a software tool for verifying properties over probabilistic models. It supports PCTL and CSL model checking, and their reward extensions. Distinguishing features of MRMC are its support for computing timeand reward-bounded reachability probabilities, (property-driven) bisimulation minimization, and precise on-the-fly steady-state detection. Recent tool features include time-bounded reachability analysis for uniform CTMDPs and CSL model checking by discrete-event simulation. This paper presents the tool\u2019s current status and its implementation details."}
{"_id":"4c4d6fec361b83b4f3575aa78612010de94d2fc7","title":"Connectivity in a citation network: The development of DNA theory","text":"The study of citation networks for both articles and journals is routine. In general, these analyses proceed by considering the similarity of articles or journals and submitting the set of similarity measures to some clustering or scaling procedure. Two common methods are found in bibliomettic coupling, where two citing articles are similar to the extent they cite the same literature, and co-citation analysis where cited articles are similar to the extent they are cited by the same citing articles. Methods based on structural and regular equivalence also seek to partition the article based on their positional location. Such methods have in common focus on the articles and partitions of them. We propose a quite different approach where the connective threads through a network are preserved and the focus is on the links in the network rather than on the nodes. Variants of the depth first search algorithm are used to detect and represent the mainstream of the literature of a clearly delineated area of scientific research. The specific citation network is one that consists of ties among the key events and papers that lead to the discovery and modeling of DNA together with the final experimental confirmation of its representation."}
{"_id":"a39ad25c7df407ef71d9ea33c750641853d0ead3","title":"Semantic similarity based evaluation for C programs through the use of symbolic execution","text":"Automatic grading of programs has existed in various fields for many years ago. Within this paper, we propose a method for evaluating C programs. Two approaches are distinguished in this context: static and dynamic analysis methods. Unlike the dynamic analysis that requires an executable program to be evaluated, static analysis could evaluate a program even if it is not totally correct. The proposed method is based on static analysis of programs. It consists of comparing the evaluated program with the evaluator-provided program through their Control Flow Graphs. Here, the great challenge is to deal with the multiplicity of solutions that exists for the same programming problem. As a solution to this weakness, we propose an innovative similarity measure that compares two programs according to their semantic executions. In fact, the evaluated program is compared to the evaluator-provided program called model program by using the symbolic execution technique. The experimentations presented in this work are performed by using a basic implementation of the proposed method. The obtained results reveal a promising realization in the field of automated evaluation of programs. They also show that the proposed method guarantees a considerable approximation to the human program evaluation."}
{"_id":"73209d66b1894ab5ce25ba70e1e39ed47dbbc11e","title":"Topic Models for Image Annotation and Text Illustration","text":"Image annotation, the task of automatically generating description words for a picture, is a key component in various image search and retrieval applications. Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections. In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement. We describe a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. We show that this model outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of our dataset."}
{"_id":"843b208402a8c52883e7e96d3a5d218fda39e541","title":"Home telehealth - Current state and future trends","text":"OBJECTIVE\nThe purpose of this paper is to give an overview about the state of the art in research on home telehealth in an international perspective.\n\n\nMETHOD\nThe study is based on a review of the scientific literature published between 1990 and 2003 and retrieved via Medline in January\/February 2004. All together, the abstracts of 578 publications have been analyzed.\n\n\nRESULTS\nThe majority of publications (44%) comes from the United States, followed by UK and Japan. Most publications deal with vital sign parameter (VSP) measurement and audio\/video consultations (\"virtual visits\"). Publications about IT tools for improved information access and communication as well as decision support for staff, patients and relatives are relatively sparse. Clinical application domains are mainly chronic diseases, the elderly population and paediatrics.\n\n\nCONCLUSIONS\nInternationally, we observe a trend towards tools and services not only for professionals but also for patients and citizens. However, their impact on the patient-provider relationship and their design for special user groups, such as elderly and\/or disabled needs to be further explored. In general, evaluation studies are rare and further research is critical to determine the impacts and benefits, and limitations, of potential solutions and to overcome a number of hinders and restrictions, such as - the lack of standards to combine incompatible information systems; - the lack of an evaluation framework considering legal, ethical, organisational, clinical, usability and technical aspects; - the lack of proper guidelines for practical implementation of home telehealth solutions."}
{"_id":"c53a27f77d89badc5457391c297eefb415d49861","title":"Intelligent stock data prediction using predictive data mining techniques","text":"Cloud computing is the one of the admired paradigms of current era, which facilitates the users with on demand services and pay as you use services. It has tremendous applications in almost every sphere such as education, gaming, social networking, transportation, medical, business, stock market, pattern matching, etc. Stock market is such an industry where lots of data is generated and benefits are reaped on the basis of accurate prediction. So prediction is a vital part of stock market. Therefore, an attempt is being made to predict the stock market based on the given data set of stock market along with some features; using the techniques available for predictive data mining. Machine learning is one of the upcoming trends of data mining; hence few machine learning algorithms have been used such as Decision tree, Linear model, Random forest and further their results have been compared using the classification evaluation parameters such as H, AUC, ROC, TPR, FPR, etc. Random forest have been consider as the most effective model as it yield the highest accuracy of 54.12% whereas decision tree and linear model gives the accuracy of 51.87% and 52.83% respectively."}
{"_id":"df0ccc8d04b29fed302eeb11bb82712bfde0cfb5","title":"New VF-power system architecture and evaluation for future aircraft","text":"Conventional aircraft power system is a constant-frequency (CF) supply based on mechanical-regulated constant-speed mechanism that has relatively low efficiency. Replacing the CF system with variable-frequency (VF) power improves overall efficiency, and reduce the system's weight and volume. However, this creates a new tier of requirements and design challenges. Novel VF-power architecture is developed with minimization of the power losses throughout the stages of power conversions. Optimal partitioning and grouping of onboard AC loads has been discussed with specific system data. New VF-input multi-functional power converters are also briefly discussed."}
{"_id":"7d97e323bbcacac3d3b38fb4a945abc5d795d51a","title":"A State-of-the-Art of Semantic Change Computation","text":"This paper reviews state-of-the-art of one emerging field in computational linguistics \u2014 semantic change computation, proposing a framework that summarizes the literature by identifying and expounding five essential components in the field: diachronic corpus, diachronic word sense characterization, change modelling, evaluation data and data visualization. Despite the potential of the field, the review shows that current studies are mainly focused on testifying hypotheses proposed in theoretical linguistics and that several core issues remain to be solved: the need for diachronic corpora of languages other than English, the need for comprehensive evaluation data for evaluation, the comparison and construction of approaches to diachronic word sense characterization and change modelling, and further exploration of data visualization techniques for hypothesis"}
{"_id":"5226b9ea9cb26f13681a7d09bc2c1de9deed848e","title":"A DC-90 GHz 4-Vpp differential linear driver in a 0.13 \u03bcm SiGe:C BiCMOS technology for optical modulators","text":"In this paper, a linear driver for optical modulators in a 0.13 \u03bcm SiGe:C BiCMOS technology with fT\/fmax of 300\/500 GHz is presented. The driver is implemented following a distributed amplifier topology in a differential manner. In a 50-\u03a9 environment, the circuit delivers a maximum differential output amplitude of 4 Vpp, featuring a small-signal gain of 13 dB and 3-dB bandwidth of 90 GHz. Time-domain measurements using OOK (up to 56 Gb\/s) and PAM-4 (at 30Gbaud) are performed, demonstrating the maximum output swing and linearity of the driver. The output power to power dissipation ratio is 3.6%. To the best knowledge of the authors, this is the first time a linear driver for optical modulators demonstrates such bandwidth."}
{"_id":"1753c2dc85cc40e0a2e8b4a405c1690eab066d8d","title":"FENNEL: streaming graph partitioning for massive scale graphs","text":"Balanced graph partitioning in the streaming setting is a key problem to enable scalable and efficient computations on massive graph data such as web graphs, knowledge graphs, and graphs arising in the context of online social networks. Two families of heuristics for graph partitioning in the streaming setting are in wide use: place the newly arrived vertex in the cluster with the largest number of neighbors or in the cluster with the least number of non-neighbors.\n In this work, we introduce a framework which unifies the two seemingly orthogonal heuristics and allows us to quantify the interpolation between them. More generally, the framework enables a well principled design of scalable, streaming graph partitioning algorithms that are amenable to distributed implementations. We derive a novel one-pass, streaming graph partitioning algorithm and show that it yields significant performance improvements over previous approaches using an extensive set of real-world and synthetic graphs.\n Surprisingly, despite the fact that our algorithm is a one-pass streaming algorithm, we found its performance to be in many cases comparable to the de-facto standard offline software METIS and in some cases even superiror. For instance, for the Twitter graph with more than 1.4 billion of edges, our method partitions the graph in about 40 minutes achieving a balanced partition that cuts as few as 6.8% of edges, whereas it took more than 81\/2 hours by METIS to produce a balanced partition that cuts 11.98% of edges. We also demonstrate the performance gains by using our graph partitioner while solving standard PageRank computation in a graph processing platform with respect to the communication cost and runtime."}
{"_id":"0650df86ad901fb9aadd9033a83c328a6f595666","title":"A Language Modeling Approach to Predicting Reading Difficulty","text":"We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial na\u00efve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words)."}
{"_id":"4aa0522f8efc370241a0f1acb7c888787e5ab70b","title":"Dynamic projection mapping onto a deformable object with occlusion based on high-speed tracking of dot marker array","text":"In recent years, projection mapping has attracted much attention in a variety of fields. Generally, however, the objects in projection mapping are limited to rigid and static or quasistatic objects. Dynamic projection mapping onto a deformable object could remarkably expand the possibilities. In order to achieve such a projection mapping, it is necessary to recognize the deformation of the object even when it is occluded. However, it is still a challenging problem to achieve this task in real-time with low latency. In this paper, we propose an efficient, high-speed tracking method utilizing high-frame-rate imaging. Our method is able to track an array of dot markers arranged on a deformable object even when there is external occlusion caused by the user interaction and self-occlusion caused by the deformation of the object itself. Additionally, our method can be applied to a stretchable object. Dynamic projection mapping with our method showed robust and consistent display onto a sheet of paper and cloth with a tracking performance of about 0.2 ms per frame, with the result that the projected pattern appeared to be printed on the deformable object."}
{"_id":"812b169bf22777b30823c46312fa9425237bca35","title":"Novel Online Methods for Time Series Segmentation","text":"To efficiently and effectively mine massive amounts of data in the time series, approximate representation of the data is one of the most commonly used strategies. Piecewise linear approximation is such an approach, which represents a time series by dividing it into segments and approximating each segment with a straight line. In this paper, we first propose a new segmentation criterion that improves computing efficiency. Based on this criterion, two novel online piecewise linear segmentation methods are developed, the feasible space window method and the stepwise feasible space window method. The former usually produces much fewer segments and is faster and more reliable in the running time than other methods. The latter can reduce the representation error with fewer segments. It achieves the best overall performance on the segmentation results compared with other methods. Extensive experiments on a variety of real-world time series have been conducted to demonstrate the advantages of our methods."}
{"_id":"ed51689477c76193c09bf41700d43491c0b26cd6","title":"Broadcast Encryption with Traitor Tracing","text":"In this thesis, we look at definitions and black-box constructions with efficient instantiations for broadcast encryption and traitor tracing. We begin by looking at the security notions for broadcast encryption found in the literature. Since there is no easy way to compare these existing notions, we propose a framework of security notions for which we establish relationships. We then show where existing notions fit within this framework. Second, we present a black-box construction of a decentralized dynamic broadcast encryption scheme. This scheme does not rely on any trusted authorities, and new users can join at any time. It achieves the strongest security notion based on the security of its components and has an efficient instantiation that is fully secure under the DDH assumption in the standard model. Finally, we give a black-box construction of a message-based traitor tracing scheme, which allows tracing not only based on pirate decoders but also based on watermarks contained in a message. Our scheme is the first one to obtain the optimal ciphertext rate of 1 asymptotically. We then show that at today\u2019s data rates, the scheme is already practical for standard choices of values."}
{"_id":"3d67f52a871c0a31ce6fa3f3162ab9affd13ed05","title":"Cross-domain security of cyber-physical systems","text":"The interaction between the cyber domain and the physical domain components and processes can be leveraged to enhance the security of the cyber-physical system. In order to do so, we must first analyze various cyber domain and physical domain information flows, and characterize the relation between them using model functions. In this paper, we present a notion of cross-domain security of cyber-physical systems, whereby we present a security analysis framework that can be used for generating novel cross-domain attack models, attack detection methods, etc. We demonstrate how information flows such as discrete domain signal flows and continuous domain energy flows in the cyber and physical domain can be used to generate model functions using data-driven estimation, and use this model functions for performing various cross-domain security analysis. We also demonstrate the practical applicability of the cross-domain security analysis framework using the cyber-physical manufacturing system as a case study."}
{"_id":"af8a14f2698afffad73095255f680632b3dd25ef","title":"Facial reconstruction: soft tissue thickness values for South African black females.","text":"In forensic science, investigators frequently have to deal with unidentified skeletonised remains. When conventional methods of identification are unsuccessful, forensic facial reconstruction (FFR) may be used, often as a last resort, to assist the process. FFR relies on the relationships between the facial features, subcutaneous soft tissues and underlying bony structure of the skull. The aim of this study was to develop soft tissue thickness (STT) values for South African black females for application to FFR, to compare these values to existing literature or databases and to add these values to existing population data. Computerised tomography scanning was used to determine average population-specific STT values at 28 facial landmarks of 154 black females. Descriptive statistics are provided for these STT values, which were also compared to those reported in three other comparable databases. Many of these STT values are significantly different from those reported for comparable groups, suggesting that individuals from different geographical areas have unique facial features thus requiring population-specific STT values. Repeatability tests indicated that most measurements could be recorded with a high degree of reliability."}
{"_id":"a325d5ea42a0b6aeb0390318e9f65f584bd67edd","title":"Fine-Grained Visual Comparisons with Local Learning","text":"Given two images, we want to predict which exhibits a particular visual attribute more than the other-even when the two images are quite similar. Existing relative attribute methods rely on global ranking functions; yet rarely will the visual cues relevant to a comparison be constant for all data, nor will humans' perception of the attribute necessarily permit a global ordering. To address these issues, we propose a local learning approach for fine-grained visual comparisons. Given a novel pair of images, we learn a local ranking model on the fly, using only analogous training comparisons. We show how to identify these analogous pairs using learned metrics. With results on three challenging datasets-including a large newly curated dataset for fine-grained comparisons-our method outperforms stateof-the-art methods for relative attribute prediction."}
{"_id":"8ef4c64cb734a68f4e3c7c2ffaf7d4d403a5aab3","title":"Quantity versus quality: A new approach to examine the relationship between technology use and student outcomes","text":"The author argues that to examine the relationship between technology use and student outcomes, the quality of technology use\u2014how, and what, technology is used\u2014is a more significant factor than the quantity of technology use\u2014how much technology is used. This argument was exemplified by an empirical study that used both angles to examine the association between technology use and student outcomes. When only the quantity of technology use was examined, no significant association was observed. However, when the quality of technology was examined by investigating the specific types of technology uses, a significant association was identified between technology use and all student outcomes. Furthermore, different types of technology use showed different influences on specific student outcomes. General technology uses were positively associated with student technology proficiency, while subject-specific technology uses were negatively associated with student technology proficiency. Social-communication technology uses were significantly positively associated with developmental outcomes such as self-esteem and positive attitude towards school. Entertainment\/exploration technology use showed significant positive association with student learning habits. None of these technology uses had significant influence on student academic outcome. Specific suggestions for integrating technology into schools and future research were provided. Introduction In the last two decades, generous investments have been made in educational technology around the world. For example, the US had invested more than $66 billion in school technology in just 10 years (Quality Education Data, 2004). By 2004, China had spent 100 billion Yuan (about $13.2 billion) on educational technology (Zhao, 2005), and British Journal of Educational Technology Vol 41 No 3 2010 455\u2013472 doi:10.1111\/j.1467-8535.2009.00961.x \u00a9 2009 The Author. Journal compilation \u00a9 2009 Becta. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main Street, Malden, MA 02148, USA. the annual expense on educational technology was projected to reach 35.5 billion Yuan in 2007 (Okokok Report, 2004). Ireland\u2019s second national educational technology plan proposed to invest 107.92 million pounds in educational technology (Ireland Ministry of Education and Science, 2001). The generous investments were supported by the strongly held premise that technology can help students learn more efficiently and effectively, and as a result increase student academic achievement. This belief in the connection between technology and student achievement is a theme commonly emphasised in mission statements of educational technology projects and arguments to support educational technology investment (Zhao & Conway, 2001). For example, the first US national educational technology plan claims, \u2018Properly used, technology increases students\u2019 learning opportunities, motivation, and achievement\u2019 (U.S. Department of Education, 1996, p. 10). The second plan assures that technology would \u2018enhance learning and improve student achievement for all students\u2019 (U.S. Department of Education, 2000, p. 4). The third plan further states that with new technologies, \u201810 years from now we could be looking at the greatest leap forward in achievement in the history of education. By any measure, the improvements will be dramatic\u2019 (U.S. Department of Education, p. 11). However, this premise on the crucial role of technology in student achievement has not been substantially supported by empirical evidence. In fact, findings from different empirical studies focusing on the effect of technology on learning have been inconsistent and contradictory. On the one hand, some studies have identified significant positive impact of technology use on student outcomes in academic areas such as literacy development (Blasewitz & Taylor, 1999; Tracey & Young, 2006), reading comprehension and vocabulary (Scrase, 1998; Stone, 1996; Woehler, 1994), writing (Nix, 1998), mathematics (Elliott & Hall, 1997; Mac Iver, Balfanz & Plank, 1999) and science (Harmer & Cates, 2007; Lazarowitz & Huppert, 1993; Liu, Hsieh, Cho & Schallert, 2006; Reid-Griffin, 2003). For example, Tienken and Wilson (2007) compared seventhgrade students whose teachers used mathematics websites and presentation software in their classrooms with students whose teachers did not teach with these technology tools. They found that the use of these technology tools had a positive effect on students\u2019 learning of basic mathematic skills. In addition, positive impacts have been identified in student developmental areas, including attitude towards learning and self-esteem (Nguyen, Hsieh & Allen, 2006; Sivin-Kachala & Bialo, 2000), motivation, attendance and discipline (eg, Matthew, 1997). For example, using a mixed-method design, Wighting (2006) reported that using computers in the classroom positively affected students\u2019 sense of learning in a community. Similarly, in the UK, a series of research studies have been conducted to examine the effect of the large-scale Tablet PC programmes, and findings reveal that the use of technology has improved student access to curriculum, communication and motivation (eg, Sheehy et al, 2005; Twining et al, 2006). On the other hand, several other researchers have come to very different conclusions. Some argue that technology use may not have any positive impact on student outcomes. For example, The Organization for Economic Co-operation and Development\u2019s (OECD) Programme for International Student Assessment 2003 study found that stu456 British Journal of Educational Technology Vol 41 No 3 2010 \u00a9 2009 The Author. Journal compilation \u00a9 2009 Becta. dents using computers most frequently at school did not necessarily perform better than students using technology less frequently, and the impact of technology use on student math achievement varied by countries (OECD, 2005). In March 2007, the Institute of Education Sciences released an influential report titled Effectiveness of Reading and Mathematics Software Products: Findings from the First Student Cohort. This study, intended to assess the effects of 16 computer software products designed to teach firstand fourthgrade reading and sixth-grade math, using a rigorous random assignment design, found that \u2018test scores in treatment classrooms that were randomly assigned to use products did not differ from test scores in control classrooms by statistically significant margins\u2019 (Dynarski et al, 2007, p. xiii). Furthermore, some studies suggest that technology use might even harm children and their learning (eg, Healy, 1998; Stoll, 1999). For example, Waight and Abd-El-Khalick (2007) found that the use of computer technology restricted rather than promoted \u2018inquiry\u2019 in a sixth-grade science classroom. Mixed findings have also emerged from large-scale international studies. A study of the Trends in International Mathematics and Science Study (TIMSS) reported that technology use was negatively related to science achievement among eighth graders in Turkey (Aypay, Erdogan & Sozer, 2007). Another TIMSS study found that while medium use of computer technology was related to higher science scores, extensive use was related to lower science scores (Antonijevic, 2007). Similarly, based on data collected from 175 000 15-year-old students in 31 countries, researchers at the University of Munich announced that performance in math and reading had suffered significantly among students who had more than one computer at home (MacDonald, 2004). Schacter (1999) also identified some negative impacts on student achievement through the review of five large-scale studies that employed diverse research methods to examine the impact of educational technology. Discouraged by these findings, some people have come to the conclusion that putting computers in classrooms has been wasteful and pointless (Oppenheimer, 2003). Existing research on the relationship technology on student learning presents a mixed message (Andrews et al, 2007; O\u2019Dwyer, Russell, Bebell & Tucker-Seeley, 2005; Torgerson & Zhu, 2003). Such mixed and often conflicting findings make it difficult to draw conclusions about the effects of technology, to provide meaningful advice to those who make decisions about technology investment in education and to make practical suggestions for integrating technology into schools. There are at least two problems contributing to the controversy over the relationship between technology use and student outcomes. The first is that technology is often examined at a very general level (Zhao, 2003). Many studies \u2018treat technology as an undifferentiated characteristic of schools and classrooms. No distinction is made between different types of technology programs\u2019 (Wenglinsky, 1998, p. 3). We know that technology is a very broad term that includes many kinds of hardware and software. These technologies may have different impacts on student outcomes. Even the same technology can be used differently in various contexts to solve all kinds of problems (Zhao) and thus have \u2018different meanings in different settings\u2019 (Peyton & Bruce, Quantity versus quality on technology use 457 \u00a9 2009 The Author. Journal compilation \u00a9 2009 Becta. 1993, p. 10). Treating technology as if it is a single thing obscures the unique characteristics of different technologies and their uses. The second problem is the focus of the studies. Most studies focus on the impact of the quantity of technology use, in other words, how much or how frequently technology is used, but ignore the quality of technology use, that is, how technology is used. For example, many studies examine the relationship between how much time students spend on using computers or how often they use computers and their achievement (eg, Du, Havard, Yu & Adams, 2004; Mann, Shakeshaft, Becker & Kottkamp, 1999). However, research suggests that the quality of technology use is more critical"}
{"_id":"1c82ffbf057067bee2b637f4b25422684c9a4feb","title":"Spatial Correlation and Mobility-Aware Traffic Modeling for Wireless Sensor Networks","text":"Recently, there has been a great deal of research on using mobility in wireless sensor networks (WSNs) to facilitate surveillance and reconnaissance in a wide deployment area. Besides providing an extended sensing coverage, node mobility along with spatial correlation introduces new network dynamics, which could lead to the traffic patterns fundamentally different from the traditional (Markovian) models. In this paper, a novel traffic modeling scheme for capturing these dynamics is proposed that takes into account the statistical patterns of node mobility and spatial correlation. The contributions made in this paper are twofold. First, it is shown that the joint effects of mobility and spatial correlation can lead to bursty traffic. More specifically, a high mobility variance and small spatial correlation can give rise to pseudo-long-range-dependent (LRD) traffic (high bursty traffic), whose autocorrelation function decays slowly and hyperbolically up to a certain cutoff time lag. Second, due to the ad hoc nature of WSNs, certain relay nodes may have several routes passing through them, necessitating local traffic aggregations. At these relay nodes, our model predicts that the aggregated traffic also exhibits the bursty behavior characterized by a scaled power-law decayed autocovariance function. According to these findings, a novel traffic shaping protocol using movement coordination is proposed to facilitate effective and efficient resource provisioning strategy. Finally, simulation results reveal a close agreement between the traffic pattern predicted by our theoretical model and the simulated transmissions from multiple independent sources, under specific bounds of the observation intervals."}
{"_id":"691d0a287a2515ebe5019cda498dcb6d24dfd5a4","title":"Least-Squares Fitting of Two 3-D Point Sets","text":"Two point sets {pi} and {p'i}; i = 1, 2,..., N are related by p'i = Rpi + T + Ni, where R is a rotation matrix, T a translation vector, and Ni a noise vector. Given {pi} and {p'i}, we present an algorithm for finding the least-squares solution of R and T, which is based on the singular value decomposition (SVD) of a 3 \u00d7 3 matrix. This new algorithm is compared to two earlier algorithms with respect to computer time requirements."}
{"_id":"1d50b77cde6c2c9f6b28b3bbdda1fd912383ce37","title":"Area-Preservation Mapping using Optimal Mass Transport","text":"We present a novel area-preservation mapping\/flattening method using the optimal mass transport technique, based on the Monge-Brenier theory. Our optimal transport map approach is rigorous and solid in theory, efficient and parallel in computation, yet general for various applications. By comparison with the conventional Monge-Kantorovich approach, our method reduces the number of variables from O(n2) to O(n), and converts the optimal mass transport problem to a convex optimization problem, which can now be efficiently carried out by Newton's method. Furthermore, our framework includes the area weighting strategy that enables users to completely control and adjust the size of areas everywhere in an accurate and quantitative way. Our method significantly reduces the complexity of the problem, and improves the efficiency, flexibility and scalability during visualization. Our framework, by combining conformal mapping and optimal mass transport mapping, serves as a powerful tool for a broad range of applications in visualization and graphics, especially for medical imaging. We provide a variety of experimental results to demonstrate the efficiency, robustness and efficacy of our novel framework."}
{"_id":"6fe387e0891c47dffc0793ccfdbe92e41f984056","title":"Hierarchical Reinforcement Learning","text":"A hierarchical representation of the input-output transition function in a learning system is suggested. The choice of either representing the knowledge in a learning system as a discrete set of input-output pairs or as a continuous input-output transition function is discussed. The conclusion that both representations could be e cient, but at di erent levels is made. The di erence between strategies and actions is de ned. An algorithm for using adaptive critic methods in a two-level reinforcement learning system is presented. Two problems that are faced, the hierarchical credit assignment problem and the equalized state problem are described. Simulations of a one dimensional hierarchical reinforcement learning system is presented."}
{"_id":"27f3c2b0bb917091f92e4161863ec3559452280f","title":"Noise-contrastive estimation: A new estimation principle for unnormalized statistical models","text":"We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field."}
{"_id":"a6580c11678bc2a2cfe711348eb9c0edfe484fc0","title":"Social Dominance An Intergroup Theory of Social Hierarchy and Oppression","text":"Social dominance : an intergroup theory of social hierarchy and oppression \/ Jim Sidanius, Felicia Pratto. p. cm. Includes bibliographical references and index."}
{"_id":"ad24460dcb4cde52562d6eab97dce2edaa5ed0d3","title":"D-Sempre: Learning Deep Semantic-Preserving Embeddings for User interests-Social Contents Modeling","text":"Exponential growth of social media consumption demands e\u0082ective user interests-social contents modeling for more personalized recommendation and social media summarization. However, due to the heterogeneous nature of social contents, traditional approaches lack the ability of capturing the hidden semantic correlations across these multi-modal data, which leads to semantic gaps between social content understanding and user interests. To e\u0082ectively bridge the semantic gaps, we propose a novel deep learning framework for user interests-social contents modeling. We \u0080rst mine and parse data, i.e. textual content, visual content, social context and social relation, from heterogeneous social media feeds. \u008cen, we design a two-branch network to map the social contents and users into a same latent space. Particularly, the network is trained by a largemargin objective that combines a cross-instance distance constraint with a within-instance semantic-preserving constraint in an endto-end manner. At last, a Deep Semantic-Preserving Embedding (D-Sempre) is learned, and the ranking results can be given by calculating distances between social contents and users. To demonstrate the e\u0082ectiveness of D-Sempre in user interests-social contents modeling, we construct a Twi\u008aer dataset and conduct extensive experiments on it. As a result, D-Sempre e\u0082ectively integrates the multimodal data from heterogeneous social media feeds and captures the hidden semantic correlations between users\u2019 interests and social contents."}
{"_id":"a9e1059254889fa6d5105dfc458bfd43898a74ac","title":"A fuzzy classification system for prediction of the results of the basketball games","text":"Prediction of the sports game results is an interesting topic that has gained attention lately. Mostly there are used stochastical methods of uncertainty description. In this work it is presented a preliminary approach to build a fuzzy model to basketball game results prediction. Ten fuzzy rule learning algorithms are selected, conducted and compared against standard linear regression with use of the KEEL system. Feature selection algorithms are applied and a majority voting is used in order to select the most representative features."}
{"_id":"b44f36231ec7bfae79803ce40738e7eb2a1264cb","title":"Inpainting by Flexible Haar-Wavelet Shrinkage","text":"We present novel wavelet-based inpainting algorithms. Applying ideas from anisotropic regularization and diffusion, our models can better handle degraded pixels at edges. We interpret our algorithms within the framework of forward-backward splitting methods in convex analysis and prove that the conditions for ensuring their convergence are fulfilled. Numerical examples illustrate the good performance of our algorithms."}
{"_id":"6247c0acba0dd88204a0d8d23b89a293f9d58e20","title":"Tutorial on Machine Learning for Spectrum Sharing in Wireless Networks","text":"As spectrum utilization efficiency is the major bottleneck in current wireless networking, many stakeholders discuss that spectrum should be shared rather than being exclusively allocated. Shared spectrum access raises many challenges which, if not properly addressed, degrades the performance level of the co-existing networks. Coexistence scenarios may involve two or more networks: same or different types; operated by the same or different operators. The complex interactions among the coexisting networks can be addressed by the machine learning tools which by their nature embrace uncertainty and can model the complex interactions. In this tutorial, we start with the basics of coexistence of wireless networks in the unlicensed bands. Then, we focus on WiFi and LTE-U coexistence. After providing a brief overview of machine learning topics such as supervised learning, unsupervised learning, reinforcement learning, we overview five particular examples which exploit learning schemes to enable efficient spectrum sharing entailing a generic cognitive radio setting as well as LTE and WiFi coexistence scenarios. We conclude with a list of challenges and research directions. I. SPECTRUM SHARING IN WIRELESS NETWORKS Spectrum sharing is the situation where at least two users or technologies are authorized to use the same portion of the radio spectrum on a non-exclusive manner1. We overview the current state of spectrum sharing and provide a taxonomy of spectrum sharing scenarios. We can list the main challenges in providing peaceful coexistence as follows: (i) scarcity of the resources, (ii) heterogeneity of the coexisting networks, (iii) power asymmetry, and (iv) lack of coordination, communication, and cooperation among the coexisting networks. II. COEXISTENCE IN THE UNLICENSED BANDS: THE CASE OF WIFI AND LTE-U The success of IEEE 802.11 networks in the unlicensed bands, i.e., 2.4 GHz and 5 GHz, has proved the efficiency and feasibility of using spectrum in a license-exempt manner. Currently, even the cellular providers consider expanding their network\u2019s capacity with unlicensed spectrum to cope with the increasing wireless traffic demand. More particularly, Qualcomm\u2019s 2013 proposal [1] of aggregating 5GHz bands with the licensed carriers of an LTE network has paved the way for LTE unlicensed networks. However, operation in the unlicensed bands has to address the coexistence challenges. For example, WiFi networks at 2.4 GHz bands, e.g., 802.11b\/g\/n, have to find the best channel 1S. Forge, R. Horvitz, and C. Blackman. Perspectives on the value of shared spectrum access. Final Report for the European Commission, 2012. among three non-overlapping channels for operation in a very-dense WLAN deployment. Additionally, 2.4 GHz band accommodates also non-WiFi technologies such as Bluetooth, ZigBee, or microwave ovens which all create interference on WLANs. As for 5GHz which has many more non-overlapping channels compared to 2.4 GHz, the more severe challenge is to coexist with technologies other than 802.11n\/ac\/ax networks, namely unlicensed LTE networks and radars. The main coexistence mechanism of WiFi is listen-beforetalk (LBT) which is also known as carrier sense multiple access with collision avoidance (CSMA\/CA). A station with a traffic to transmit has to first check whether the medium is free or not. To decide on the state of the medium, two approaches exist: carrier sense and energy detection. In carrier sensing, a WiFi node decodes the preamble of a WiFi frame that is received above some energy detection level. The node extracts the information from the PLCP header which carries information about the occupancy duration of the medium by that ongoing flow. This mechanism is also referred to as channel reservation. With this information, a WiFi node knows when to re-start sensing the medium for a transmission opportunity. Energy detection (ED) is a simpler approach in which a candidate transmitter decides that the air is free if the signal level is below a predefined ED threshold. This approach is used for detecting inter-technology signals, where the received signal is not decodable, i.e., it belongs to other technologies (or corrupted WiFi signals). Despite its simplicity, ED requires more effort on a potential transmitter as it must constantly sense the energy level in the air to detect a transmission opportunity. As LTE follows a scheduled medium access on the licensedspectrum, there is no notion or necessity of politeness or LBT in more technical terms. However, it is vital for LTE unlicensed to implement such mechanisms for coexistence with WiFi and other unlicensed LTE networks at 5 GHz bands. Currently, frequency-domain sharing is a first step only. In other words, an LTE small cell first checks the channel activities and selects a clear channel, if any. For time sharing, there are two approaches taken by two variants of LTE unlicensed: duty cycling by LTE-U and LBT by License-Assisted-Access (LAA). LTE-U which is an industry-led effort lets small cells apply duty cycling where during the OFF periods WiFi can access the medium. As this approach does not mandate LBT before turning small cell transmissions on, it may degrade WiFi performance drastically. LAA requires LBT similar to WiFi\u2019s CSMA\/CA. LAA speciCROWNCOM2017 TUTORIAL 2 fication is led by 3GPP and aims to develop a global solution in contrast to LTE-U which is only compliant to countries like US, Korea, China where LBT is not mandatory. We overview basics of these two variants and list the major issues in their peaceful coexistence with WiFi networks. III. BACKGROUND ON MACHINE LEARNING We provide a sparse overview of learning approaches: supervised, unsupervised, and reinforcement learning. IV. THE ROLE OF MACHINE LEARNING IN SPECTRUM SHARING AND COEXISTENCE In this part, we examine the literature using ML approaches to solve the coexistence issues as our case studies. Is the channel idle or busy?This question is at the heart of coexistence of networks in a multi-channel environment, as the first step of coexistence is to choose a channel that is clear. For cognitive radio networks, it is mandatory to detect the channel state to avoid violating the rules of secondary spectrum access. Casting this question into a binary classification problem, authors [2] introduce several (un)supervised learning algorithms to correctly identify the state of a channel. While supervised approaches require the real channel state information from the Primary Users, unsupervised learning such as K-means does not require any input from the PUs which is a desirable property of classification scheme in a practical setting. Which unlicensed channel to select for each LAA SBS for inter-operator coexistence?As we expect multiple LAA operators deploy their small cells independently, there is surely the question of how to select an unlicensed channel to aggregate, particularly in case there are more cells than the number of available channels. One way of channel selection is to let every LAA BS learn from its own observations via trial-and-error, Q-learning [3]. Which unlicensed carrier to aggregate and how long to use this carrier?Q-learning framework can also be applied to an LAA setting where an LAA BS needs to select an unlicensed carrier and the transmission duration on the selected carrier [4]. Can WiFi exploit ML for defending itself against LTEU interference?Different than the literature which develops coexistence solutions to be deployed at the LTE base stations for WiFi\/LTE setting, [5] proposes to also equip the WiFi APs with a tool that estimates the ON-duration of an existing LTE-U network in the neighborhood. Moreover, the developed solution can estimate the remaining airtime for the WiFi AP based on the LTE\u2019s predicted ON duration. Key idea of WiPLUS is to detect the times where LTE-U has an ongoing transmission using the data passively collected from the MAC FSM of the NIC. However, although LTE-U signal may not be detected above the ED level, it may still have a severe impact on WiFi. Thus, PHY-layer analysis solely on signal level is short of detecting the moderate interference regime. WiPLUS overcomes this challenge by combining data from MAC FSM states and ARQ missing acknowledgments. Sampled data from a testbed has a lot of noise due to imperfections of the measuring devices and the complex interactions among the coexisting systems as well as PHY and MAC layers. WiPLUS applies K-means clustering to detect outliers on the estimated LTE-U on-durations. After filtering the data points based on the signal\u2019s frequency harmonics, WiPLUS calculates the LTEU on-time as the average of the data points, each of which corresponds to an estimate of LTE-U on-time. Can we estimate WiFi link performance by learning from real-world link capacity measurements?In a multiAP setting, an AP can select the operation channel based on the expected capacity of the existing links. The traditional way is to take the SNIR-based capacity estimate into account, i.e., Shannon\u2019s capacity formula. However, this capacity model may sometimes fail to represent the complex interactions between PHY and MAC layers, e.g., partially-overlapping channels in case of channel bonding in new 802.11ac\/ax standards. The idea of [6] is to use supervised learning as a tool to model the complex interactions among many factors such as power and PHY rate of a neighboring WiFi link implicitly rather than modelling it explicitly. V. OPEN RESEARCH DIRECTIONS Machine learning-based solutions can embrace the complexity and uncertainty prevalent in the complex scenarios, especially hybrid horizontal spectrum sharing, by learning from the observations. However, a wireless network poses peculiar challenges such as the energy limitations, real-time operation, and sometimes fast changes in the operation environment that render learning less effective. We overview such challenges and conclude with some open questions in this"}
{"_id":"b1f53c9ca221f4772155c7e49cf3e298e8648840","title":"Obfuscation procedure based in dead code insertion into crypter","text":"The threat that attacks cyberspace is known as malware. In order to infect the technologic devices that are attacked, malware needs to evade the different antivirus systems. To avoid detection, an obfuscation technique must be applied so malware is updated and ready to be performed. No obstant, the technique implementation presents difficulties in terms of its required ability, evasion tests and infection functionality that turn outs to be a problem to keep malware updated. Therefore, a procedure is proposed that allows applying AVFUCKER or DSPLIT techniques. The purpose is to optimize the required technical means, reduce the antivirus analysis and malware functionality check times."}
{"_id":"a07c0952053f83ae9f32d955d6bb36e60f7a8dd0","title":"GRAPHITE : Polyhedral Analyses and Optimizations for GCC","text":"We present a plan to add loop nest optimizations in GCC based on polyhedral representations of loop nests. We advocate a static analysis approach based on a hierarchy of interchangeable abstractions with solvers that range from the exact solvers such as OMEGA, to faster but less precise solvers based on more coarse abstractions. The intermediate representation GRAPHITE1 (GIMPLE Represented as Polyhedra with Interchangeable Envelopes), built on GIMPLE and the natural loops, hosts the high level loop transformations. We base this presentation on the WRaP-IT project developed in the Alchemy group at INRIA Futurs and Paris-Sud University, on the PIPS compiler developed at \u00c9cole des mines de Paris, and on a joint work with several members of the static analysis and polyhedral compilation community in France. The main goal of this project is to bring more high level loop optimizations to GCC: loop fusion, tiling, strip mining, etc. Thanks to the 1This work was partially supported by ACI\/APRON. WRaP-IT experience, we know that the polyhedral analyzes and transformations are affordable in a production compiler. A second goal of this project is to experiment with compile time reduction versus attainable precision when replacing operations on polyhedra with faster operations on more abstract domains. However, the use of a too coarse representation for computing might also result in an over approximated solution that cannot be used in subsequent computations. There exists a trade off between speed of the computation and the attainable precision that has not yet been analyzed for real world programs."}
{"_id":"b9f75b2fa01347b1f521935354f6724a418f8fc8","title":"From piecemeal to configurational representation of faces.","text":"Unlike older children and adults, children of less than about 10 years of age remember photographs of faces presented upside down almost as well as those shown upright and are easily fooled by simple disguises. The development at age 10 of the ability to encode orientation-specific configurational aspects of a face may reflect completion of certain maturational changes in the right cerebral hemisphere."}
{"_id":"ecfc3bc6294e34f3f9c9d1c0010d756ad345c6a1","title":"Learning and Recognition of Clothing Genres From Full-Body Images","text":"According to the theory of clothing design, the genres of clothes can be recognized based on a set of visually differentiable style elements, which exhibit salient features of visual appearance and reflect high-level fashion styles for better describing clothing genres. Instead of using less-discriminative low-level features or ambiguous keywords to identify clothing genres, we proposed a novel approach for automatically classifying clothing genres based on the visually differentiable style elements. A set of style elements, that are crucial for recognizing specific visual styles of clothing genres, were identified based on the clothing design theory. In addition, the corresponding salient visual features of each style element were identified and formulated with variables that can be computationally derived with various computer vision algorithms. To evaluate the performance of our algorithm, a dataset containing 3250 full-body shots crawled from popular online stores was built. Recognition results show that our proposed algorithms achieved promising overall precision, recall, and  ${F}$ -score of 88.76%, 88.53%, and 88.64% for recognizing upperwear genres, and 88.21%, 88.17%, and 88.19% for recognizing lowerwear genres, respectively. The effectiveness of each style element and its visual features on recognizing clothing genres was demonstrated through a set of experiments involving different sets of style elements or features. In summary, our experimental results demonstrate the effectiveness of the proposed method in clothing genre recognition."}
{"_id":"448787207f4c35772dafd2270f545761caed57f4","title":"Teaching Introductory Artificial Intelligence with Pac-Man","text":"The projects that we have developed for UC Berkeley\u2019s introductory artificial intelligence (AI) course teach foundational concepts using the classic video game Pac-Man. There are four project topics: state-space search, multi-agent search, probabilistic inference, and reinforcement learning. Each project requires students to implement general-purpose AI algorithms and then to inject domain knowledge about the PacMan environment using search heuristics, evaluation functions, and feature functions. We have found that the Pac-Man theme adds consistency to the course, as well as tapping in to students\u2019 excitement about video games."}
{"_id":"c9dd44cf1ee964d8794a3cc51471ae1d95fd66bf","title":"Considering Social and Emotional Artificial Intelligence","text":"This paper presents the concepts of social and emotional intelligence as elements of human intelligence that are complementary to the intelligence assessed by the Turing Test. We argue that these elements are gaining importance as human users are increasingly conceptualising machines as social entities. We describe an implementation of Sensitive Artificial Listeners which provides a hands-on example of technology with some emotional and social skills, and discuss first elements of test methodologies for such technology."}
{"_id":"02413be569417baf09b1b3e1132bb9b94eb19733","title":"Low numeracy and dyscalculia : identification and intervention","text":"One important factor in the failure to learn arithmetic in the normal way is an endogenous core deficit in the sense of number. This has been associated with low numeracy in general (e.g. Halberda et al. in Nature 455:665\u2013668, 2008) and with dyscalculia more specifically (e.g. Landerl et al. in Cognition 93:99\u2013125, 2004). Here, we describe straightforward ways of identifying this deficit, and offer some new ways of strengthening the sense of number using learning technologies."}
{"_id":"c5dc853ea2c2e00af7e83a47ea12082a8f0a470c","title":"Visualizing LSTM decisions","text":"Long Short-Term Memory (LSTM) recurrent neural networks are renowned for being uninterpretable \"black boxes\". In the medical domain where LSTMs have shown promise, this is specifically concerning because it is imperative to understand the decisions made by machine learning models in such acute situations. This study employs techniques used in the Convolutional Neural Network domain to elucidate the operations that LSTMs perform on time series. The visualization techniques include input saliency by means of occlusion and derivatives, class mode visualization, and temporal outputs. Moreover, we demonstrate that LSTMs appear to extract features similar to those extracted by wavelets. It was found that deriving the inputs for saliency is a poor approximation and occlusion is a better approach. Moreover, analyzing LSTMs on different sets of data provide novel interpretations."}
{"_id":"69cf637170453ed80625559326d417c291dded4a","title":"A survey of performance measures to evaluate ego-lane estimation and a novel sensor-independent measure along with its applications","text":"Lane estimation plays a central role for Driver Assistance Systems, therefore many approaches have been proposed to measure its performance. However, no commonly agreed metric exists. In this work, we first present a detailed survey of the current measures. Most of them apply pixel-level benchmarks on camera images and require a time-consuming and fault-prone labeling process. Moreover, these metrics cannot be used to assess other sources such as the detected guardrails, curbs or other vehicles. Therefore, we introduce an efficient and sensor-independent metric, which provides an objective and intuitive self-assessment for the entire road estimation process at multiple levels: individual detectors, lane estimation itself, and the target applications (e.g., lane keeping system). Our metric does not require a high labeling effort and can be used both online and offline. By selecting the evaluated points in specific distances, it can be applied to any road model representation. By comparing in 2D vehicle coordinate system, two possibilities exist to generate the ground-truth: the human-driven path or the expensive alternative with DGPS and detailed maps. This paper applies both methods and reveals that the human-driven path also qualifies for this task and it is applicable to scenarios without GPS signal, e.g., tunnel. Although the lateral offset between reference and detection is widely used in the majority of works, this paper shows that another criterion, the angle deviation, is more appropriate. Finally, we compare our metric with other state-of-the-art metrics using real data recordings from different scenarios."}
{"_id":"caf31ab71b3b45e9881ec4b036067b5854b6841c","title":"Why and How Investors Use ESG Information : Evidence from a Global Survey","text":"Using survey data from a sample of senior investmen professionals from mainstream (i.e. not SRI funds ) investment organizations we provide insights into w hy and how investors use reported environmental, social and governance (ESG) information. The primar y reason survey respondents consider ESG information in investment decisions is because they consider it financially material to investment performance. ESG information is perceived to provid e information primarily about risk rather than a company\u2019s competitive positioning. There is no one siz fits all, with the financial materiality of di fferent ESG issues varying across sectors. Lack of comparab ility due to the lack of reporting standards is the primary impediment to the use of ESG information. M ost frequently, the information is used to screen companies with the most often used method being neg ative screening. However, negative screening is perceived as the least investment beneficial while ful integration into stock valuation and positive screening considered more beneficial. Respondents expect nega tiv screening to be used less in the future, while positive screening and active ownership to be used more."}
{"_id":"bd483defd23589a08065e9dcc7713e928abacfbd","title":"The trouble with overconfidence.","text":"The authors present a reconciliation of 3 distinct ways in which the research literature has defined overconfidence: (a) overestimation of one's actual performance, (b) overplacement of one's performance relative to others, and (c) excessive precision in one's beliefs. Experimental evidence shows that reversals of the first 2 (apparent underconfidence), when they occur, tend to be on different types of tasks. On difficult tasks, people overestimate their actual performances but also mistakenly believe that they are worse than others; on easy tasks, people underestimate their actual performances but mistakenly believe they are better than others. The authors offer a straightforward theory that can explain these inconsistencies. Overprecision appears to be more persistent than either of the other 2 types of overconfidence, but its presence reduces the magnitude of both overestimation and overplacement."}
{"_id":"24aad39e3c07e39478a85947d674df7248c135e9","title":"Dynamic XML documents with distribution and replication","text":"The advent of XML as a universal exchange format, and of Web services as a basis for distributed computing, has fostered the apparition of a new class of documents: dynamic XML documents. These are XML documents where some data is given explicitly while other parts are given only intensionally by means of embedded calls to web services that can be called to generate the required information. By the sole presence of Web services, dynamic documents already include inherently some form of distributed computation. A higher level of distribution that also allows (fragments of) dynamic documents to be distributed and\/or replicated over several sites is highly desirable in today's Web architecture, and in fact is also relevant for regular (non dynamic) documents.The goal of this paper is to study new issues raised by the distribution and replication of dynamic XML data. Our study has originated in the context of the Active XML system [1, 3, 22] but the results are applicable to many other systems supporting dynamic XML data. Starting from a data model and a query language, we describe a complete framework for distributed and replicated dynamic XML documents. We provide a comprehensive cost model for query evaluation and show how it applies to user queries and service calls. Finally, we describe an algorithm that, for a given peer, chooses data and services that the peer should replicate to improve the efficiency of maintaining and querying its dynamic data."}
{"_id":"738feaca3e125c6c6eae4c160b60fdedd12c89aa","title":"A measurement study of google play","text":"Although millions of users download and use third-party Android applications from the Google Play store, little information is known on an aggregated level about these applications. We have built PlayDrone, the first scalable Google Play store crawler, and used it to index and analyze over 1,100,000 applications in the Google Play store on a daily basis, the largest such index of Android applications. PlayDrone leverages various hacking techniques to circumvent Google's roadblocks for indexing Google Play store content, and makes proprietary application sources available, including source code for over 880,000 free applications. We demonstrate the usefulness of PlayDrone in decompiling and analyzing application content by exploring four previously unaddressed issues: the characterization of Google Play application content at large scale and its evolution over time, library usage in applications and its impact on application portability, duplicative application content in Google Play, and the ineffectiveness of OAuth and related service authentication mechanisms resulting in malicious users being able to easily gain unauthorized access to user data and resources on Amazon Web Services and Facebook."}
{"_id":"8961cbea2ebb3d1609fb1ca60155685a5e34597d","title":"Asymmetrical Duty Cycle-Controlled LLC Resonant Converter With Equivalent Switching Frequency Doubler","text":"In the conventional full-bridge LLC converter, the duty cycle is kept as 0.5 and the phase-shift angle of the half-bridge modules is 0\u00b0 to be a symmetrical operation, which makes the resonant tank operating frequency only equal to the switching frequency of the power devices. By regulating the duty cycles of the upper and lower switches in each half-bridge module to be 0.75 and 0.25 and the phase-shift angle of the half-bridge modules to be 180\u00b0, the asymmetrical duty cycle controlled full-bridge LLC resonant converter is derived. The proposed asymmetrical duty cycle controlled scheme halves the switching frequency of the primary switches. As a result, the driving losses are effectively reduced. Compared with the conventional full-bridge LLC converter, the soft-switching condition of the derived asymmetrical controlled LLC converter becomes easier to reduce the resonant current. Consequently, the conduction losses are decreased and the conversion efficiency is improved. The asymmetrical control scheme can be also extended to the stacked structure for high input voltage applications. Finally, two LLC converter prototypes both with 200-kHz resonant frequency for asymmetrical and symmetrical control schemes are built and compared to validate the effectiveness of the proposed control strategy."}
{"_id":"2a51e6646261ed6dd4adde885f6e7d76c94dc11e","title":"Behavioural Analytics using Process Mining in On-line Advertising","text":"Online behavioural targeting is one of the most popular business strategies on the display advertising today. It is based primarily on analysing web user behavioural data with the usage of machine learning techniques with the aim to optimise web advertising. Being able to identify \u201cunknown\u201d and \u201cfirst time seen\u201d customers is of high importance in online advertising since a successful guess could identify \u201cpossible prospects\u201d who would be more likely to purchase an advertisement\u2019s product. By identifying prospective customers, online advertisers may be able to optimise campaign performance, maximise their revenue as well as deliver advertisements tailored to a variety of user interests. This work presents a hybrid approach benchmarking machine-learning algorithms and attribute preprocessing techniques in the context of behavioural targeting in process oriented environments. The performance of our suggested methodology is evaluated using the key performance metric in online advertising which is the predicted conversion rate. Our experimental results indicate that the presented process mining framework can significantly identify prospect customers in most cases. Our results seem promising, indicating that there is a need for further workflow research in online display"}
{"_id":"1004b8bfcf19eecb957b1de4af328f41aa097042","title":"Translating evidence-based decision making into practice: EBDM concepts and finding the evidence.","text":"This is the first of 2 articles that focuses on strategies that can be used to integrate an evidence-based decision making [EBDM] approach into practice. The articles will focus on EBDM methodology and enhancing skills, including how to find valid evidence to answer clinical questions, critically appraise the evidence found and determine if it applies. In addition, online resources will be identified to supplement information presented in each article. The purpose of this article is to define evidence-based decision making and discuss skills necessary for practitioners to efficiently adopt EBDM. It will provide a guide for finding evidence to answer a clinical question using PubMed's specialized searching tools under Clinical Queries."}
{"_id":"280bb91e1f8f84eec3979c5d592396d3a4a2963c","title":"Analysis and Design of a Single-Stage Parallel AC-to-DC Converter","text":"In this paper, a single-stage (S2) parallel ac-to-dc converter based on single-switch two-output boost-flyback converter is presented. The converter contains two semistages. One is the boost-flyback semistage, which transfers partial input power transferred to load directly through one power flow path and has excellent self-power factor correction property when operating in discontinuous conduction mode even though the boost output is close to the peak value of the line voltage. The other one is the flyback dc-to-dc (dc\/dc) semistage that provides the output regulation on another parallel power flow path. With this design, the power conversion efficiency is improved and the current stress of control switch is reduced. Furthermore, the calculation process of power distribution and bulk capacitor voltage, design equations, and design procedure for key parameters are also presented. By following the procedure, an 80 W prototype converter has been built and tested. The experimental results show that the measured line harmonic current at the worst condition complies with the IEC61000-3-2 class D limits, the maximum bulk capacitor voltage is about 415.4 V, and the maximum efficiency is about 85.8%. Hence, the proposed S2 converter is suitable for universal input usage."}
{"_id":"3d7348c63309ddb68b4e69782bc6bf516bb1ced7","title":"A High Step-Down Transformerless Single-Stage Single-Switch AC\/DC Converter","text":"This paper presents a high step-down tranformerless single-stage single-switch ac\/dc converter suitable for universal line applications (90-270 Vrms) . The topology integrates a buck-type power-factor correction (PFC) cell with a buck-boost dc\/dc cell and part of the input power is coupled to the output directly after the first power processing. With this direct power transfer feature and sharing capacitor voltages, the converter is able to achieve efficient power conversion, high power factor, low voltage stress on intermediate bus (less than 130 V) and low output voltage without a high step-down transformer. The absence of transformer reduces the component counts and cost of the converter. Unlike most of the boost-type PFC cell, the main switch of the proposed converter only handles the peak inductor current of dc\/dc cell rather than the superposition of both inductor currents. Detailed analysis and design procedures of the proposed circuit are given and verified by experimental results."}
{"_id":"43d09c0ebcde0c8d06ab1d6ea50701cb15f437f8","title":"A Novel Single-Stage High-Power-Factor AC-to-DC LED Driving Circuit With Leakage Inductance Energy Recycling","text":"This paper proposes a novel single-stage ac-to-dc light-emitting-diode (LED) driver circuit. A buck-boost power factor (PF) corrector is integrated with a flyback converter. A recycling path is built to recover the inductive leakage energy. In this way, the presented circuit can provide not only high PF and low total harmonic distortion but also high conversion efficiency and low switching voltage spikes. Above 0.95 PF and 90% efficiency are obtained from an 8-W LED lamp driver prototype."}
{"_id":"beb38cf2b03e3c22f5fe4e75999bbea52ed3ceee","title":"Flicker-Free Electrolytic Capacitor-Less Universal Input Offline LED Driver With PFC","text":"Recent developments in improving lighting efficiency and cost reduction of LEDs have made them suitable alternatives to the current lighting systems. In this paper, a novel offline structure is proposed to drive LEDs. The proposed circuit has a high-input power factor, high efficiency, a long lifetime, and it produces no flicker. To increase the lifetime of the converter, the proposed circuit does not include any electrolytic capacitors in the power stage. The proposed circuit consists of a transition mode flyback converter in order to improve power factor. Additionally, a buck converter is added to the third winding of the flyback transformer in order to create two parallel paths for the electrical power to feed the output load. DC power reaches the load through one stage (flyback) and ac power reaches the load through two stages of conversion (flyback + buck). Therefore, in the proposed one-and-a-half stage circuit, the efficiency is improved compared to a regular two-stage circuit. Although the proposed structure has some output current ripple, it is low enough (less than 8%) that the structure can be rendered flicker free, as shall be discussed. Principles of operation and design equations are presented as well as experimental results for a 700 mA\/20 W universal input prototype."}
{"_id":"a3faa972480bb9734eb6bafc54c4cfc190b4595e","title":"Large air gap coupler for inductive charger","text":"A novel magnetic coupler of large r r gap IS presented It IS developed for the eleehlo relucle s aumahc mduchve charger The ncw induchve coupler proposed hqe has sUmcient mung ~oducmnce ven rf I has a large a s gap Calculated exrung inductance 18 40 pH at 2 nuns of w d m g and 5 mm ay gap, whch apees well with measured value In order to assess the gmerahon of heat c w e d by the eddy cment, the magneuc flux densrhes in the lnduchve chaser and also a flat non plate, to wbch the inductive charger IS attached, Bre calculated The conyersion efXciency wth the coupler and a MOSFETs full-bndge inverier of IW lrHq IS 97% at 8 3 kW output"}
{"_id":"5f55f327c7fb90cf1831aa54c789d420628191ee","title":"Implementing Remote Procedure Calls","text":"Remote procedure calls (RPC) appear to be a useful paradig m for providing communication across a network between programs written in a high-level language. This paper describes a package providing a remote procedure call facility, the options that face the designer of such a package, and the decisions ~we made. We describe the overall structure of our RPC mechanism, our facilities for binding RPC clients, the transport level communication protocol, and some performance measurements. We include descriptioro~ of some optimizations used to achieve high performance and to minimize the load on server machines that have many clients."}
{"_id":"791aefc89307138bbda68f0f78c30dbe288cc20f","title":"Somatization vs . Psychologization of Emotional Distress : A Paradigmatic Example for Cultural Psychopathology","text":"This paper describes the developing area of cultural psychopathology, an interdisciplinary field of study focusing on the ways in which cultural factors contribute to the experience and expression of psychological distress. We begin by outlining two approaches, often competing, in order to provide a background to some of the issues that complicate the field. The main section of the paper is devoted to a discussion of depression in Chinese culture as an example of the types of questions that can be studied. Here, we start with a review of the epidemiological literature, suggesting low rates of depression in China, and move to the most commonly cited explanation, namely that Chinese individuals with depression present this distress in a physical way. Different explanations of this phenomenon, known as somatization, are explored and reconceptualized according to an increasingly important model for cross-cultural psychologists: the cultural constitution of the self. We close by discussing some of the contributions, both theoretical and methodological, that can be made by cross-cultural psychologists to researchers in cultural psychopathology. Creative Commons License This work is licensed under a Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 License. This article is available in Online Readings in Psychology and Culture: https:\/\/scholarworks.gvsu.edu\/orpc\/vol10\/iss2\/3"}
{"_id":"e19497cc905346401fc969187ce5c0b8f4656c65","title":"Privacy-Aware Tag Recommendation for Image Sharing","text":"Image tags are very important for indexing, sharing, searching, and surfacing images with private content that needs protection. As the tags are at the sole discretion of users, they tend to be noisy and incomplete. In this paper, we present a privacy-aware approach to automatic image tagging, which aims at improving the quality of user annotations, while also preserving the images' original privacy sharing patterns. Precisely, we recommend potential tags for each target image by mining privacy-aware tags from the most similar images of the target image obtained from a large collection. Experimental results show that privacy-aware approach is able to predict accurate tags that can improve the performance of a downstream application on image privacy prediction. Crowd-sourcing predicted tags exhibit the quality of the recommended tags."}
{"_id":"3f4b312d0bc1c954064eca04f2663d8ee72b5dcf","title":"A study to evaluate the safety, tolerability, and efficacy of brodalumab in subjects with rheumatoid arthritis and an inadequate response to methotrexate.","text":"OBJECTIVE\nTo evaluate the efficacy and safety of brodalumab, a human monoclonal antibody inhibitor of the interleukin 17 receptor, in subjects with rheumatoid arthritis (RA).\n\n\nMETHODS\nPatients (n = 252) with inadequate response to methotrexate (MTX) were randomized to receive subcutaneous injections of brodalumab (70 mg, 140 mg, or 210 mg) or placebo. The primary endpoint was the American College of Rheumatology 50% response (ACR50) at Week 12.\n\n\nRESULTS\nDemographics and baseline characteristics were generally balanced among treatment groups. At Week 12, ACR50 occurred in 16% (70 mg), 16% (140 mg), 10% (210 mg), and 13% (placebo; all nonsignificant vs placebo) of subjects. No significant treatment effects were observed for the secondary endpoints, including ACR20, ACR70, and Disease Activity Score in 28 joints. Incidences of all adverse events (AE), including serious AE (SAE), were similar across treatment groups. A total of 7 subjects reported SAE during the study (2 in the placebo group and 5 in the brodalumab groups), none of which was treatment related. There was 1 death (cardiopulmonary failure) \u223c1 week after the last dose in the 140 mg group.\n\n\nCONCLUSION\nOur study failed to find evidence of meaningful clinical efficacy with brodalumab treatment in subjects with RA who had an inadequate response to MTX. These preliminary results do not support further evaluation of brodalumab as a treatment for RA. Clinicaltrials.gov number: NCT00950989."}
{"_id":"40c555885ac9d1df63ff2c2b75c136fe03a35c16","title":"Alternative seating for young children with Autism Spectrum Disorder: effects on classroom behavior.","text":"A single subject, withdrawal design was used to investigate the effects of therapy balls as seating on engagement and in-seat behavior of young children with Autism Spectrum Disorder (ASD). In addition, social validity was assessed to evaluate teachers' opinions regarding the intervention. During baseline and withdrawal (A phases) participants used their typical classroom seating device (chair, bench or carpet square). During the intervention (B phases) participants sat on therapy balls. Results indicated substantial improvements in engagement and in-seat behavior when participants were seated on therapy balls. Social validity findings indicated that the teachers' preferred the therapy balls. This study suggests therapy balls as classroom seating may facilitate engagement and in-seat behavior and create opportunities to provide effective instruction."}
{"_id":"a5e4c83b816f2f004ae5dfd600145cea9ea15724","title":"Automatic Analysis of Rhythmic Poetry with Applications to Generation and Translation","text":"We employ statistical methods to analyze, generate, and translate rhythmic poetry. We first apply unsupervised learning to reveal word-stress patterns in a corpus of raw poetry. We then use these word-stress patterns, in addition to rhyme and discourse models, to generate English love poetry. Finally, we translate Italian poetry into English, choosing target realizations that conform to desired rhythmic patterns."}
{"_id":"a3c7af4ffc4f04dd2cbb32f108af6d7ba823f35a","title":"In vivo 3-dimensional analysis of scapular kinematics: comparison of dominant and nondominant shoulders.","text":"BACKGROUND\nAlterations in scapular motion frequently are seen in association with various shoulder disorders. It is common clinically to compare the pathological shoulder with the contralateral shoulder, in spite of arm dominance, to characterize the disorder. However, there have been few articles that test the underlying assumption that dominant and nondominant shoulders exhibit comparable dynamic kinematics. The purpose of this study was to compare the 3-dimensional (3-D) scapular kinematics of dominant and nondominant shoulders during dynamic scapular plane elevation using 3-D-2-D (2-dimensional) registration techniques.\n\n\nMATERIALS AND METHODS\nTwelve healthy males with a mean age of 32 years (range, 27-36) were enrolled in this study. Bilateral fluoroscopic images during scapular plane elevation and lowering were taken, and CT-derived 3-D bone models were matched with the silhouette of the bones in the fluoroscopic images using 3-D-2-D registration techniques. Angular values of the scapula and scapulohumeral rhythm were compared between dominant and nondominant shoulders with statistical analysis.\n\n\nRESULTS\nThere was a significant difference in upward rotation angles between paired shoulders (P < .001), while significant differences were not found in the other angular values and scapulohumeral rhythm. The dominant scapulae were 10\u00b0 more downwardly rotated at rest and 4\u00b0 more upwardly rotated during elevation compared to the nondominant scapulae.\n\n\nDISCUSSION\/CONCLUSION\nScapular motion was not the same between dominant and nondominant arms in healthy subjects. The dominant scapula was rotated further downward at rest and reached greater upward rotation with abduction. These differences should be considered in clinical assessment of shoulder pathology."}
{"_id":"c0898ad1ead40c3d863661424decd778db6ed437","title":"A Low-Power Text-Dependent Speaker Verification System with Narrow-Band Feature Pre-Selection and Weighted Dynamic Time Warping","text":"To fully enable voice interaction in wearable devices, a system requires low-power, customizable voice-authenticated wake-up. Existing speaker-verification (SV) methods have shortcomings relating to power consumption and noise susceptibility. To meet the application requirements, we propose a low-power, text-dependent SV system comprising a sparse spectral feature extraction front-end showing improved noise robustness and accuracy at low power, and a back-end running an improved dynamic time warping (DTW) algorithm that preserves signal envelope while reducing misalignments. Without background noise, the proposed system achieves an equal-errorrate (EER) of 1.1%, compared to 1.4% with a conventional Mel-frequency cepstral coefficients (MFCC)+DTW system and 2.6% with a Gaussian mixture universal background (GMMUBM) based system. At 3dB signal-to-noise ratio (SNR), the proposed system achieves an EER of 5.7%, compared to 13% with a conventional MFCC+DTW system and 6.8% with a GMM-UBM based system. The proposed system enables simple, low-power implementation such that the power consumption of the end-to-end system, which includes a voice activity detector, feature extraction front-end, and back-end decision unit, is under 380 \u03bcW."}
{"_id":"a904ec1184cdbef0a2a362032b84885779574858","title":"Fast Method to Optimize RF Bumper Transparency for Wide-Band Automotive Radar","text":"This article describes novel methods to enhance the RF integration of wideband automotive radars. The first part of the paper is introducing the context and the considered scenario. Whereas the second part is focusing on two solutions to improve the radar integration in car chassis environment. The aim is to have good RF performances on wideband operation (76\u201381 GHz) and on a wide range of angle of incidence for the radar having wide field of view (e.g. Blind spot radar, Pedestrian detection, \u2026)."}
{"_id":"59cce25151fe4d70383624dd535f533617dd8025","title":"Unsupervised prediction of citation influences","text":"Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations."}
{"_id":"5a4b3297dbced87babc1b4f9f673191e007a3e03","title":"Force to Rebalance Control of HRG and Suppression of Its Errors on the Basis of FPGA","text":"A novel design of force to rebalance control for a hemispherical resonator gyro (HRG) based on FPGA is demonstrated in this paper. The proposed design takes advantage of the automatic gain control loop and phase lock loop configuration in the drive mode while making full use of the quadrature control loop and rebalance control loop in controlling the oscillating dynamics in the sense mode. First, the math model of HRG with inhomogeneous damping and frequency split is theoretically analyzed. In addition, the major drift mechanisms in the HRG are described and the methods that can suppress the gyro drift are mentioned. Based on the math model and drift mechanisms suppression method, four control loops are employed to realize the manipulation of the HRG by using a FPGA circuit. The reference-phase loop and amplitude control loop are used to maintain the vibration of primary mode at its natural frequency with constant amplitude. The frequency split is readily eliminated by the quadrature loop with a DC voltage feedback from the quadrature component of the node. The secondary mode response to the angle rate input is nullified by the rebalance control loop. In order to validate the effect of the digital control of HRG, experiments are carried out with a turntable. The experimental results show that the design is suitable for the control of HRG which has good linearity scale factor and bias stability."}
{"_id":"3c55c334d34b611a565683ea42a06d4e1f01db47","title":"Topic-Oriented Exploratory Search Based on an Indexing Network","text":"An exploratory search may be driven by a user's curiosity or desire for specific information. When users investigate unfamiliar fields, they may want to learn more about a particular subject area to increase their knowledge rather than solve a specific problem. This work proposes a topic-oriented exploratory search method that provides browse guidance to users. It allows them to discover new associations and knowledge, and helps them find their interested information and knowledge. Since an exploratory search needs to judge the ability to discover new knowledge, the existing commonly used metrics fail to capture it. This paper thus defines a new set of criteria containing clarity, relevance, novelty, and diversity to analyze the effectiveness of an exploratory search. Experiments are designed to compare results from the proposed method and Google's \u201csearch related to ....\u201d The results show that the proposed one is more suitable for learning new associations and discovering new knowledge with highly likely relevance to a query. This work concludes that it is more suitable than Google for an exploratory search."}
{"_id":"0889019b395890f57bfae3ce7d8391649ae68de4","title":"Word Embedding based Correlation Model for Question\/Answer Matching","text":"The large scale of Q&A archives accumulated in community based question answering (CQA) servivces are important information and knowledge resource on the web. Question and answer matching task has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, a Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding. Given a random pair of words, WEC can score their co-occurrence probability in Q&A pairs, while it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method\u2019s promising"}
{"_id":"0a0e7baec6a4d2f3100c856c631bbc50e08fbad5","title":"OpenCL framework for ARM processors with NEON support","text":"The state-of-the-art ARM processors provide multiple cores and SIMD instructions. OpenCL is a promising programming model for utilizing such parallel processing capability because of its SPMD programming model and built-in vector support. Moreover, it provides portability between multicore ARM processors and accelerators in embedded systems. In this paper, we introduce the design and implementation of an efficient OpenCL framework for multicore ARM processors. Computational tasks in a program are implemented as OpenCL kernels and run on all CPU cores in parallel by our OpenCL framework. Vector operations and built-in functions in OpenCL kernels are optimized using the NEON SIMD instruction set. We evaluate our OpenCL framework using 37 benchmark applications. The result shows that our approach is effective and promising."}
{"_id":"f3381a72a5ed288d54a93d92a85e96f7ba2ab36c","title":"LEARNING TO SOLVE CIRCUIT-SAT: AN UNSUPERVISED DIFFERENTIABLE APPROACH","text":"Recent efforts to combine Representation Learning with Formal Methods, commonly known as Neuro-Symbolic Methods, have given rise to a new trend of applying rich neural architectures to solve classical combinatorial optimization problems. In this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem. Our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method."}
{"_id":"0052c9768f873bebf4b1788cc94bb8b0b3526be7","title":"Interaction control of an UAV endowed with a manipulator","text":"In this paper, we present the design, simulation and experimental validation of a control architecture for an unmanned aerial vehicle endowed with a manipulation system and interacting with a remote environment. The goal of this work is to show that the interaction control allows the manipulator to track a desired force, normal to a vertical wall, while still maintaining the possibility of moving on the wall. The control strategy has been implemented and validated in simulations and experiments on the manipulator standalone, i.e., attached to a fixed base, and on the manipulator attached to the aerial vehicle."}
{"_id":"6b3bafa55e812213a0caecc06d9a8c625d75f656","title":"Negative pressure wave based pipeline Leak Detection: Challenges and algorithms","text":"Negative pressure wave is a popular method to detect the occurrence and location of leak incidents in oil\/gas pipeline. Three core technical challenges and related algorithm are discussed in this paper. The first is data quality. The balance between noise level and locating precision is discussed in filter design. The second one is dynamic slope in anomaly detection, whence a bi-SPC (Static Process Control) algorithms is proposed to make the threshold be adaptive. The third one is the false alarm caused normal working condition changes. Multiple-sensor paring algorithms is presented. With these algorithms, the robustness and locating precision of LDS (Leak Detection System) can be ensured."}
{"_id":"ac7d747a78ca2ad42627bd6360064d8c694a92e6","title":"Data mining techniques - for marketing, sales, and customer support","text":"When there are many people who don't need to expect something more than the benefits to take, we will suggest you to have willing to reach all benefits. Be sure and surely do to take this data mining techniques for marketing sales and customer support that gives the best reasons to read. When you really need to get the reason why, this data mining techniques for marketing sales and customer support book will probably make you feel curious."}
{"_id":"819d2e6855992a3efbf97750d9ad0b5379c9cc4e","title":"An On-Chip Temperature Sensor With a Self-Discharging Diode in 32-nm SOI CMOS","text":"We report a 39 \u03bcm \u00d7 27 \u03bcm on-chip temperature sensor which uses the temperature-dependent reverse-bias leakage current of a lateral silicon on insulator (SOI) CMOS p-n diode to monitor the thermal profile of a 32-nm microprocessor core. In this sensor, the diode junction capacitance is first charged to a fixed voltage. Subsequently, the diode capacitance is allowed to self-discharge through its temperature-dependent reverse-bias current. Next, by using a time-to-digital-converter circuit, the discharge voltage is converted to a temperature-dependent time pulse, and finally, its width is measured by using a digital counter. This compact temperature sensor demonstrates a 3\u03c3 measurement inaccuracy of \u00b11.95\u00b0C across the 5\u00b0C-100\u00b0C temperature range while consuming only 100 \u03bcW from a single 1.65-V supply."}
{"_id":"1e2aac6424413ede3380957c289daeaff4b7f2a4","title":"Learning Sparsely Used Overcomplete Dictionaries","text":"We consider the problem of learning sparsely used overcomplete dictionaries, where each observation is a sparse combination of elements from an unknown overcomplete dictionary. We establish exact recovery when the dictionary elements are mutually incoherent. Our method consists of a clustering-based initialization step, which provides an approximate estimate of the true dictionary with guaranteed accuracy. This estimate is then refined via an iterative algorithm with the following alternating steps: 1) estimation of the dictionary coefficients for each observation through l1 minimization, given the dictionary estimate, and 2) estimation of the dictionary elements through least squares, given the coefficient estimates. We establish that, under a set of sufficient conditions, our method converges at a linear rate to the true dictionary as well as the true coefficients for each observation."}
{"_id":"46e678c863fa00b178ed17b594da8cf1dab6d5a0","title":"Road Network Extraction and Intersection Detection From Aerial Images by Tracking Road Footprints","text":"In this paper, a new two-step approach (detecting and pruning) for automatic extraction of road networks from aerial images is presented. The road detection step is based on shape classification of a local homogeneous region around a pixel. The local homogeneous region is enclosed by a polygon, called the footprint of the pixel. This step involves detecting road footprints, tracking roads, and growing a road tree. We use a spoke wheel operator to obtain the road footprint. We propose an automatic road seeding method based on rectangular approximations to road footprints and a toe-finding algorithm to classify footprints for growing a road tree. The road tree pruning step makes use of a Bayes decision model based on the area-to-perimeter ratio (the A\/P ratio) of the footprint to prune the paths that leak into the surroundings. We introduce a lognormal distribution to characterize the conditional probability of A\/P ratios of the footprints in the road tree and present an automatic method to estimate the parameters that are related to the Bayes decision model. Results are presented for various aerial images. Evaluation of the extracted road networks using representative aerial images shows that the completeness of our road tracker ranges from 84% to 94%, correctness is above 81%, and quality is from 82% to 92%."}
{"_id":"d77ddbb81886d07941ec4679ffe83721e3f5ef2b","title":"UAV borne real-time road mapping system","text":"Road information is useful in many fields. In this paper, a real-time mapping system is presented to acquire the image and determine the geometry of the road. These include designs of platform and instruments, data transmission, processing and archiving. Compare with the traditional platforms, the UAV (Unmanned Aerial Vehicle) platforms offer greater flexibility, shorter response time and is able to generate very high resolution data. It is inexpensive to operate, and its operation is not limited by air traffic constraints. With autonomous flight control system, it is easy to navigate the aircraft along the planned lines strictly. In this system, an unmanned fixed-wing aircraft with double engine and double generator is used, which helps to improve the reliability and capability of the platform. A digital three-axis stabilized platform is developed, which performs an important role in camera attitude controlling. To ensure there is no coverage hole, a data manage system is adopted: after an instantaneous inspection of data integrity, the data will be archived as they are received. If any anomaly is detected, the mission planning will be adapted to re-image the concerned area. Vehicle information extraction from image sequence is also described in this paper. The experiment showed the presented system worked well in real-time road mapping and vehicle information extraction."}
{"_id":"e429e4874f4b3a397a754f28a919362d282b7966","title":"Road Extraction Using SVM and Image Segmentation","text":"In this paper, a unique approach for road extraction utilizing pixel spectral information for classification and image segmentation-derived object features was developed. In this approach, road extraction was performed in two steps. In the first step, support vector machine (SVM) was employed merely to classify the image into two groups of categories: a road group and a non-road group. For this classification, support vector machine (SVM) achieved higher accuracy than Gaussian maximum likelihood (GML). In the second step, the road group image was segmented into geometrically homogeneous objects using a region growing technique based on a similarity criterion, with higher weighting on shape factors over spectral criteria. A simple thresholding on the shape index and density features derived from these objects was performed to extract road features, which were further processed by thinning and vectorization to obtain road centerlines. The experiment showed the proposed approach worked well with images comprised by both rural and urban area features. Introduction Road information not only plays a central role in the transportation application, but also is an important data layer in Geographical Information Systems (GIS). Automated road extraction can save time and labor to a great degree in updating a road spatial database. Various road extraction approaches have been developed. Xiong (2001) grouped these methods into five categories: ridge finding, heuristic reasoning, dynamic programming, statistical inference, and map matching. In ridge finding, edge operators are performed on images to derive edge magnitude and direction, followed by a thresholding and thinning process to obtain ridge pixels (Nevatia and Babu, 1980; Treash and Amaratunga, 2000). Alternatively, gradient direction profile analysis can be performed to generate edge pixels (Gong and Wang, 1997). Ridge points are linked to produce the road segments. Heuristic reasoning is a knowledge-based method in which a series of pre-set rules on road characteristics such as shape index, the distance between image primitives, fragments trend, and contextual information are employed to detect and connect image primitives or antiparallel linear edges to road segments (McKeown, et al., 1985; Zhu and Yeh, 1986). In the dynamic programming method, roads are modeled with a set of mathematical equations on the derivatives of gray values and select characteristics of roads, such as smooth curves, homogeneous surface, narrow linear features, and relatively constant width. Dynamic programming is employed to solve the optimization problem Road Extraction Using SVM and Image Segmentation Mingjun Song and Daniel Civco (Gruen and Li, 1995). In the statistical inference method, linear features are modeled as a Markov point process or a geometric-stochastic model on the road width, direction, intensity and background intensity, and maximum a posteriori probability is used to estimate the road network (Barzohar and Cooper, 1996; Stoica, et al., 2000). In a map matching method, existing road maps are used as starting point to update the road network. In general, two steps are involved: first, a mapimage matching algorithm is employed to match the roads on the map to the image; second, new roads are searched based on the assumption that they are connected to existing roads (Stilla, 1995). Xiong\u2019s classification on road extraction methods is only a generalization, and some other methods may combine different techniques. Active contour models, known as snakes, are also used in road extraction (Gruen and Li, 1997; Agouris, et al., 2001). A snake is a spline with minimized energy driven by internal spline and external image forces (Park, et al., 2001). In general, external image forces are represented by the gradient magnitude of an image, which attracts snakes to contours with strong edges. Internal forces are given by a continuity term and a curvature term expressed by the differences of adjacent snaxels, which are vertex nodes of the snake, with weights coming from training data, which control the shape and smoothness of the snakes. Through the optimization, the snake evolves from its initial position to desired position with minimized energy. Park and Kim (2001) used template matching to extract road segments in which a road template was formed around the road seed, and an adaptive least squares matching algorithm was used to detect a target window with similar transformation. This method assumes a small difference in brightness values between template and target windows. Most of these road extraction methods require some road seeds as starting points, which are in general provided by users, and road segments evolve under a certain model. Sometimes control points are needed to correct the evolution of roads (Zhao, et al., 2002). Further, these methods use black-and-white aerial photographs or the panchromatic band of high-resolution satellite images and therefore the geometric characteristics of roads alone play a critical role. Boggess (1993) used a classification method incorporating texture and neural networks to extract roads by classifying roads and other features from Landsat TM imagery, but obtained numerous false-inclusions. Roberts, et al. (2001) developed a spectral mixture library using hyperspectral images to extract roads, but the use of spectral information alone does not capture the spatial properties of these curvilinear features. P H OTO G R A M M E T R I C E N G I N E E R I N G & R E M OT E S E N S I N G December 2004 1 3 6 5 Center for Land use Education and Research, Department of Natural Resources Management and Engineering, The University of Connecticut U-4087, 1376 Storrs Road, Storrs, CT 06269-4087 (mingjun.song@uconn.edu, daniel.civco@ uconn.edu). Photogrammetric Engineering & Remote Sensing Vol. 70, No. 12, December 2004, pp. 1365\u20131371. 0099-1112\/04\/7012\u20131365\/$3.00\/0 \u00a9 2004 American Society for Photogrammetry and Remote Sensing LFX-536.qxd 11\/9\/04 16:13 Page 1365"}
{"_id":"f6e1ce05762144ecceb38be94091da4a23b47769","title":"Effect of temporal envelope smearing on speech reception.","text":"The effect of smearing the temporal envelope on the speech-reception threshold (SRT) for sentences in noise and on phoneme identification was investigated for normal-hearing listeners. For this purpose, the speech signal was split up into a series of frequency bands (width of 1\/4, 1\/2, or 1 oct) and the amplitude envelope for each band was low-pass filtered at cutoff frequencies of 0, 1\/2, 1, 2, 4, 8, 16, 32, or 64 Hz. Results for 36 subjects show (1) a severe reduction in sentence intelligibility for narrow processing bands at low cutoff frequencies (0-2 Hz); and (2) a marginal contribution of modulation frequencies above 16 Hz to the intelligibility of sentences (provided that lower modulation frequencies are completely present). For cutoff frequencies above 4 Hz, the SRT appears to be independent of the frequency bandwidth upon which envelope filtering takes place. Vowel and consonant identification with nonsense syllables were studied for cutoff frequencies of 0, 2, 4, 8, or 16 Hz in 1\/4-oct bands. Results for 24 subjects indicate that consonants are more affected than vowels. Errors in vowel identification mainly consist of reduced recognition of diphthongs and of confusions between long and short vowels. In case of consonant recognition, stops appear to suffer most, with confusion patterns depending on the position in the syllable (initial, medial, or final)."}
{"_id":"8c034d0135ba656340ec5220bc2dc0f294e9dd96","title":"A MapReduce Approach to NoSQL RDF Databases","text":"In recent years, the increased need to house and process large volumes of data has prompted the need for distributed storage and querying systems. The growth of machine-readable RDF triples has prompted both industry and academia to develop new database systems, called \u201cNoSQL,\u201d with characteristics that differ from classical databases. Many of these systems compromise ACID properties for increased horizontal scalability and data availability. This thesis concerns the development and evaluation of a NoSQL triplestore. Triplestores are database management systems central to emerging technologies such as the Semantic Web and linked data. A triplestore comprises data storage using the RDF (resource description framework) data model and the execution of queries written in SPARQL. The triplestore developed here exploits an opensource stack comprising, Hadoop, HBase, and Hive. The evaluation spans several benchmarks, including the two most commonly used in triplestore evaluation, the Berlin SPARQL Benchmark, and the DBpedia benchmark, a query workload that operates an RDF representation of Wikipedia. Results reveal that the join algorithm used by the system plays a critical role in dictating query runtimes. Distributed graph databases must carefully optimize queries before generating MapReduce query plans as network traffic for large datasets can become prohibitive if the query is executed naively."}
{"_id":"c77a84cd5a53343e6977bcf1878c0e4cb9263780","title":"NOBLE \u2013 Flexible concept recognition for large-scale biomedical natural language processing","text":"Natural language processing (NLP) applications are increasingly important in biomedical data analysis, knowledge engineering, and decision support. Concept recognition is an important component task for NLP pipelines, and can be either general-purpose or domain-specific. We describe a novel, flexible, and general-purpose concept recognition component for NLP pipelines, and compare its speed and accuracy against five commonly used alternatives on both a biological and clinical corpus. NOBLE Coder implements a general algorithm for matching terms to concepts from an arbitrary vocabulary set. The system\u2019s matching options can be configured individually or in combination to yield specific system behavior for a variety of NLP tasks. The software is open source, freely available, and easily integrated into UIMA or GATE. We benchmarked speed and accuracy of the system against the CRAFT and ShARe corpora as reference standards and compared it to MMTx, MGrep, Concept Mapper, cTAKES Dictionary Lookup Annotator, and cTAKES Fast Dictionary Lookup Annotator. We describe key advantages of the NOBLE Coder system and associated tools, including its greedy algorithm, configurable matching strategies, and multiple terminology input formats. These features provide unique functionality when compared with existing alternatives, including state-of-the-art systems. On two benchmarking tasks, NOBLE\u2019s performance exceeded commonly used alternatives, performing almost as well as the most advanced systems. Error analysis revealed differences in error profiles among systems. NOBLE Coder is comparable to other widely used concept recognition systems in terms of accuracy and speed. Advantages of NOBLE Coder include its interactive terminology builder tool, ease of configuration, and adaptability to various domains and tasks. NOBLE provides a term-to-concept matching system suitable for general concept recognition in biomedical NLP pipelines."}
{"_id":"c500a6168a4767998b3cd2ed7c03fe6e1d6f352b","title":"Analysis and optimization of software requirements prioritization techniques","text":"Prioritizing requirements helps the project team to understand which requirements are most important and most urgent. Based on this finding a software engineer can decide what to develop\/implement in the first release and what on the coming releases. Prioritization is also a useful activity for decision making in other phases of software engineering like development, testing, and implementation. There are a number of techniques available to prioritize the requirements with their associated strengths and limitations. In this paper we will examine state of the art techniques and analyze their applicability on software requirements domain. At the end we present a framework that will help the software engineer of how to perform prioritization process by combining existing techniques and approaches."}
{"_id":"9d6a786d48a1fd63715f1b9a0df8dcdc8f84708e","title":"The academic social network","text":"By means of their academic publications, authors form a social network. Instead of sharing casual thoughts and photos (as in Facebook), authors select co-authors and reference papers written by other authors. Thanks to various efforts (such as Microsoft Academic Search and DBLP), the data necessary for analyzing the academic social network is becoming more available on the Internet. What type of information and queries would be useful for users to discover, beyond the search queries already available from services such as Google Scholar? In this paper, we explore this question by defining a variety of ranking metrics on different entities\u2014authors, publication venues, and institutions. We go beyond traditional metrics such as paper counts, citations, and h-index. Specifically, we define metrics such as influence, connections, and exposure for authors. An author gains influence by receiving more citations, but also citations from influential authors. An author increases his or her connections by co-authoring with other authors, and especially from other authors with high connections. An author receives exposure by publishing in selective venues where publications have received high citations in the past, and the selectivity of these venues also depends on the influence of the authors who publish there. We discuss the computation aspects of these metrics, and the similarity between different metrics. With additional information of author-institution relationships, we are able to study institution rankings based on the corresponding authors\u2019 rankings for each type of metric as well as different domains. We are prepared to demonstrate these ideas with a web site ( http:\/\/pubstat.org ) built from millions of publications and authors."}
{"_id":"eb6c03696c39fd0f15eb5a2d196af79cef985baf","title":"Beamspace channel estimation for millimeter-wave massive MIMO systems with lens antenna array","text":"By employing the lens antenna array, beamspace MIMO can utilize beam selection to reduce the number of required RF chains in mmWave massive MIMO systems without obvious performance loss. However, to achieve the capacity-approaching performance, beam selection requires the accurate information of beamspace channel of large size, which is challenging, especially when the number of RF chains is limited. To solve this problem, in this paper we propose a reliable support detection (SD)-based channel estimation scheme. Specifically, we propose to decompose the total beamspace channel estimation problem into a series of sub-problems, each of which only considers one sparse channel component. For each channel component, we first reliably detect its support by utilizing the structural characteristics of mmWave beamspace channel. Then, the influence of this channel component is removed from the total beamspace channel estimation problem. After the supports of all channel components have been detected, the nonzero elements of the sparse beamspace channel can be estimated with low pilot overhead. Simulation results show that the proposed SD-based channel estimation outperforms conventional schemes and enjoys satisfying accuracy, even in the low SNR region."}
{"_id":"b6a7e07e14178ef46e764ecab9a71202f8f407bd","title":"The Use of Wearable Inertial Motion Sensors in Human Lower Limb Biomechanics Studies: A Systematic Review","text":"Wearable motion sensors consisting of accelerometers, gyroscopes and magnetic sensors are readily available nowadays. The small size and low production costs of motion sensors make them a very good tool for human motions analysis. However, data processing and accuracy of the collected data are important issues for research purposes. In this paper, we aim to review the literature related to usage of inertial sensors in human lower limb biomechanics studies. A systematic search was done in the following search engines: ISI Web of Knowledge, Medline, SportDiscus and IEEE Xplore. Thirty nine full papers and conference abstracts with related topics were included in this review. The type of sensor involved, data collection methods, study design, validation methods and its applications were reviewed."}
{"_id":"e813708e55be12d41812fc05ff9be3fa21f8fc91","title":"An empirical study on the impact of static typing on software maintainability","text":"Static type systems play an essential role in contemporary programming languages. Despite their importance, whether static type systems impact human software development capabilities remains open. One frequently mentioned argument in favor of static type systems is that they improve the maintainability of software systems\u2014an often-used claim for which there is little empirical evidence. This paper describes an experiment that tests whether static type systems improve the maintainability of software systems, in terms of understanding undocumented code, fixing type errors, and fixing semantic errors. The results show rigorous empirical evidence that static types are indeed beneficial to these activities, except when fixing semantic errors. We further conduct an exploratory analysis of the data in order to understand possible reasons for the effect of type systems on the three kinds of tasks used in this experiment. From the exploratory analysis, we conclude that developers using a dynamic type system tend to look at different files more frequently when doing programming tasks\u2014which is a potential reason for the observed differences in time."}
{"_id":"b6203f82bf276fff9a0082ee1b51d37ac90f4b79","title":"A 12.77-MHz 31 ppm\/\u00b0C On-Chip RC Relaxation Oscillator With Digital Compensation Technique","text":"The design of a 12.77-MHz on-chip RC relaxation oscillator with digital compensation technique is presented. To maintain the frequency stability versus temperature and supply voltage variations, loop delay tuning by a digital feedback loop is developed in this system. In order to generate an on-chip reference for digital calibration, a replica comparator is added. The on-chip relaxation oscillator is fabricated in 0.18-\u03bcm CMOS process. The measured output frequency variation is 31 ppm\/\u00b0C across -30 to 120 \u00b0C temperature range after compensation. The frequency variation over the supply voltage from 0.6 V to 1.1 V is \u00b10.5%\/V. The measured total power consumption is 56.2 \u03bcW at 0.9-V supply voltage when the digital compensation blocks are enabled. After digital compensation, the compensation blocks can be shutdown for power saving, and the main oscillator consumes only 12.8 \u03bcW."}
{"_id":"31ace8c9d0e4550a233b904a0e2aabefcc90b0e3","title":"Learning Deep Face Representation","text":"Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easyto-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computationefficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy (85.8% on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance (97.3%) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization."}
{"_id":"ccaf1eeb48ced2dc35c59356439fc128ca571d25","title":"On the minimum node degree and connectivity of a wireless multihop network","text":"This paper investigates two fundamental characteristics of a wireless multi -hop network: its minimum node degree and its k--connectivity. Both topology attributes depend on the spatial distribution of the nodes and their transmission range. Using typical modeling assumptions :--- :a random uniform distribution of the nodes and a simple link model :--- :we derive an analytical expression that enables the determination of the required range r0 that creates, for a given node density \u03c1, an almost surely k--connected network. Equivalently, if the maximum r0 of the nodes is given, we can find out how many nodes are needed to cover a certain area with a k--connected network. We also investigate these questions by various simulations and thereby verify our analytical expressions. Finally, the impact of mobility is discussed.The results of this paper are of practical value for researchers in this area, e.g., if they set the parameters in a network--level simulation of a mobile ad hoc network or if they design a wireless sensor network."}
{"_id":"ad25484abd3fd91cda4f609a022ab7fcd1e81ab4","title":"Imagination-Based Decision Making with Physical Models in Deep Neural Networks","text":"Decision-making is challenging in continuous settings where complex sequences of events determine rewards, even when these event sequences are largely observable. In particular, traditional trial-and-error learning strategies may have a hard time associating continuous actions with their reward because of the size of the state space and the complexity of the reward function. Given a model of the world, a different strategy is to use imagination to exploit the knowledge embedded in that model. In this regime, the system directly optimizes the decision for each episode based on predictions from the model. We extend deep learning methods that have been previously used for model-free learning and apply them towards a model-based approach in which an expert is consulted multiple times in the agents\u2019 imagination before it takes an action in the world. We show preliminary results on a difficult physical reasoning task where our model-based approach outperforms a model-free baseline, even when using an inaccurate expert."}
{"_id":"b605d2e31eb65c23e1eeaaa8a093a1aa02e974f9","title":"Help seeking behavior and the Internet: A national survey","text":"Health-related websites have the potential to powerfully influence the attitudes and behavior of consumers. Access to reliable disease information online has been linked to reduced anxiety, increased feelings of self-efficacy, and decreases in utilization of ambulatory care. Studies report that Internet health information seekers are more likely to have health concerns; adult seekers are more likely to rate themselves as having poor health status and adolescent seekers are more likely to demonstrate clinical impairment or depressive symptomatology compared to non-seekers. Although more and more Americans are using the Internet for healthcare information, little is known about how this information affects their health behaviors. The current study extends the literature by examining characteristics associated with help seeking, either from a healthcare provider or from peers, as a direct result of health information found online. Medical care seekers appear to be using the Internet to enhance their medical care; they report using the information online to diagnose a problem and feel more comfortable about their health provider's advice given the information found on the Internet. Support seekers tend to be of slightly lower income compared to non-support seekers. They are also significantly more likely to have searched for information about a loved one's medical or health condition, signaling that many of these consumers may be caretakers."}
{"_id":"850de93d6812b5913f0ce63d3b77f3c368f493c7","title":"What is the function of the claustrum?","text":"The claustrum is a thin, irregular, sheet-like neuronal structure hidden beneath the inner surface of the neocortex in the general region of the insula. Its function is enigmatic. Its anatomy is quite remarkable in that it receives input from almost all regions of cortex and projects back to almost all regions of cortex. We here briefly summarize what is known about the claustrum, speculate on its possible relationship to the processes that give rise to integrated conscious percepts, propose mechanisms that enable information to travel widely within the claustrum and discuss experiments to address these questions."}
{"_id":"265f49bf5930329da160cb677b937374eb71574a","title":"Distinct mechanisms regulate slow-muscle development","text":"Vertebrate muscle development begins with the patterning of the paraxial mesoderm by inductive signals from midline tissues [1, 2]. Subsequent myotome growth occurs by the addition of new muscle fibers. We show that in zebrafish new slow-muscle fibers are first added at the end of the segmentation period in growth zones near the dorsal and ventral extremes of the myotome, and this muscle growth continues into larval life. In marine teleosts, this mechanism of growth has been termed stratified hyperplasia [3]. We have tested whether these added fibers require an embryonic architecture of muscle fibers to support their development and whether their fate is regulated by the same mechanisms that regulate embryonic muscle fates. Although Hedgehog signaling is required for the specification of adaxial-derived slow-muscle fibers in the embryo [4, 5], we show that in the absence of Hh signaling, stratified hyperplastic growth of slow muscle occurs at the correct time and place, despite the complete absence of embryonic slow-muscle fibers to serve as a scaffold for addition of these new slow-muscle fibers. We conclude that slow-muscle-stratified hyperplasia begins after the segmentation period during embryonic development and continues during the larval period. Furthermore, the mechanisms specifying the identity of these new slow-muscle fibers are different from those specifying the identity of adaxial-derived embryonic slow-muscle fibers. We propose that the independence of early, embryonic patterning mechanisms from later patterning mechanisms may be necessary for growth."}
{"_id":"152f41f32c495276159e484de760ce6d8002915e","title":"A practical approach for computing the diameter of a point set","text":"We present an approximation algorithm for computing the diameter of a point-set in $d$-dimensions. The new algorithm is sensitive to the \u201chardness\u201d of computing the diameter of the given input, and for most inputs it is able to compute the {\\em exact} diameter extremely fast. The new algorithm is simple, robust, has good empirical performance, and can be implemented quickly. As such, it seems to be the algorithm of choice in practice for computing\/approximating the diameter."}
{"_id":"b1b7d431f04db28e568beda816e2f7b536da0b3b","title":"A Trigraph Based Centrality Approach Towards Text Summarization","text":"As the electronic documents are increasing due to the revolution of information there is an urgent need for summarizing the text documents. From the previous works we observed that there is no generalized graph model for text summarization and low order ngrams could not preserve the contextual meaning. This paper focuses on an extractive based graphical approach for text summarization, based on trigrams and graph based centrality measure. Trigraph is generated and the centrality of the connected trigraph is taken to extract the important trigrams. A mapping is done between the original words and the trigrams to regain the link between the words. And after comparing the centrality from the graph, the summary is extracted. The ROUGE-SU4 F-measure obtained for the proposed approach is 0.036 which is significantly better than the previous approaches."}
{"_id":"3813fade6b111f08636ad220ef32bd95b57d3e03","title":"PERFORMANCE LIMITS OF SWITCHED-CAPACITOR DC-DC CONVERTERS","text":"Theoretical performance limits of twophase switched-capacitor (SC) dc-dc converters are discussed in this paper. For a given number of capacitors k, the complete set of attainable dc conversion ratios is found. The maximum step-up or stepdown ratio is given by the k t h Fibonacci number, while the bound on the number of switches required in any SC circuit is 3k 2. Practical implications, illustrated by several SC converter examples, include savings in the number of components required for a given application, and the ability to construct SC converters that can maintain the output voltage regulation and high conversion efficiency over a wide range of input voltage variations. Limits found for the output resistance and efflciency can be used for selection and comparison of SC converters."}
{"_id":"533ea26a8af1f364b92d856f0aae2fc4e1539952","title":"Analysis and Optimization of Switched-Capacitor DC\u2013DC Converters","text":"Analysis methods are developed that fully determine a switched-capacitor (SC) dc-dc converter's steady-state performance through evaluation of its output impedance. This analysis method has been verified through simulation and experimentation. The simple formulation developed permits optimization of the capacitor sizes to meet a constraint such as a total capacitance or total energy storage limit, and also permits optimization of the switch sizes subject to constraints on total switch conductances or total switch volt-ampere (V-A) products. These optimizations then permit comparison among several switched-capacitor topologies, and comparisons of SC converters with conventional magnetic-based dc-dc converter circuits, in the context of various application settings. Significantly, the performance (based on conduction loss) of a ladder-type converter is found to be superior to that of a conventional magnetic-based converter for medium to high conversion ratios."}
{"_id":"0206bc94b05200094f32a7cf440551e4daebc618","title":"MOS charge pumps for low-voltage operation","text":"New MOS charge pumps utilizing the charge transfer switches (CTS\u2019s) to direct charge flow and generate boosted output voltage are described. Using the internal boosted voltage to backward control the CTS of a previous stage yields charge pumps that are suitable for low-voltage operation. Applying dynamic control to the CTS\u2019s can eliminate the reverse charge sharing phenomenon and further improve the voltage pumping gain. The limitation imposed by the diode-configured output stage can be mitigated by pumping it with a clock of enhanced voltage amplitude. Using the new circuit techniques, a 1.2-V-to-3.5-V charge pump and a 2-V-to-16-V charge pump are demonstrated."}
{"_id":"74e7b8023c09e12205745d9a10b95076a9b0f82a","title":"Analytical and Practical Analysis of Switched-Capacitor DC-DC Converters","text":"Switched-capacitor DC-DC converters are useful alternatives to inductor-based converters in many lowpower and medium-power applications. This work develops a straightforward analysis method to determine a switched-capacitor converter\u2019s output impedance (a measure of performance and power loss). This resistive impedance is a function of frequency and has two asymptotic limits, one corresponding to very high switching frequency where resistive paths dominate the impedance, and one corresponding to very low switching frequency where charge transfers among idealized capacitors dominate the impedance. An optimization method is developed to improve the performance of these converters through component sizing based on practical constraints. Several switched-capacitor converter topologies are compared in the two asymptotic limits. Switched-capacitor converter performance (based on conduction loss) is compared with that of two magnetics-based DC-DC converters. At moderate to high conversion ratios, the switchedcapacitor converter has significantly less conduction loss than an inductor-based buck converter. Some aspects of converter implementation are discussed, including the power loss due to device parasitics and methods for transistor control. Implementation using both integrated and discrete devices is discussed. With the correct analysis methods, switched-capacitor DC-DC converters can provide an attractive alternative to conventional power converters."}
{"_id":"7cde4cf792f2be12deb8d5410170a003375397d5","title":"SWITCHED-CAPACITOR DC-DC CONVERTERS FOR LOW-POWER ON-CHIP APPLICATIONS","text":"The paper describes switched-capacitor dc-dc converters (charge pumps) suitable for on-chip, low-power applications. The proposed configurations are based on connecting two identical but opposite-phase SC converters in parallel, thus eliminating the need for separate bootstrap gate drivers. We focus on emerging very low-power VLSI applications such as batterypowered or self-powered signal processors where high power conversion efficiency is important and where power levels are in the milliwatt range. Conduction and switching losses are considered to allow design optimization in terms of switching frequency and component sizes. Open-loop and closed-loop operation of an experimental, fully integrated, 10MHz voltage doubler is described. The doubler has 2V or 3V input and generates 3.3V or 5V output at up to 5mW load. The converter circuit fabricated in a standard 1.2\u03bc CMOS technology takes 0.7mm of the chip area."}
{"_id":"07baaa9229594ec9215a0b2fba5abbd8238759dd","title":"Iterative multi - view plane fitting","text":"We present a method for the reconstruction of 3D planes from calibrated 2D images. Given a set of pixels \u03a9 in a reference image, our method computes a plane which best approximates that part of the scene which has been projected to \u03a9 by exploiting additional views. Based on classical image alignment techniques we derive linear matching equations minimally parameterized by the three parameters of an object-space plane. The resulting iterative algorithm is highly robust because it is able to integrate over large image regions due to the correct object-space approximation and hence is not limited to comparing small image patches. Our method can be applied to a pair of stereo images but is also able to take advantage of the additional information provided by an arbitrary number of input images. A thorough experimental validation shows that these properties enable robust convergence especially under the influence of image sensor noise and camera calibration errors."}
{"_id":"df7ef1abf27970e3952f68f9130d8dccfcbd6841","title":"Adaptive Convolutional Filter Generation for Natural Language Understanding","text":"Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP are not expressive enough, in the sense that all input sentences share the same learned (and static) set of filters. Motivated by this problem, we propose an adaptive convolutional filter generation framework for natural language understanding, by leveraging a meta network to generate inputaware filters. We further generalize our framework to model question-answer sentence pairs and propose an adaptive question answering (AdaQA) model; a novel two-way feature abstraction mechanism is introduced to encapsulate co-dependent sentence representations. We investigate the effectiveness of our framework on document categorization and answer sentence-selection tasks, achieving state-of-the-art performance on several"}
{"_id":"b2e43d1b66cb0f618d113354dd6c29fa729282a6","title":"Convolutional Neural Networks and Language Embeddings for End-to-End Dialect Recognition","text":"\u2022 Fusion between end-to-end system and language embeddings shows better efficiency than between end-to-end system such as MFCC and FBANK \u2022 Spectrograms achieve slightly better results than MFCCs Motivation \u2022 One of the challenges of processing real-world spoken content, such as media broadcasts, is the potential presence of different dialects of a language in the material. \u2022 Dialect identification (DID) can be a useful capability to identify which dialect is being spoken during a recording. \u2022 The Arabic Multi-Genre Broadcast (MGB) Challenge tasks have provided a valuable resource for researchers interested in processing multi-dialectal Arabic speech. \u2022 Investigation of end-to-end DID approach with dataset augmentation for acoustic feature and language embeddings for linguistic feature"}
{"_id":"ec02fc82ba8e101701f59d1ca80a2cd299b07bad","title":"Optic Disc Boundary and Vessel Origin Segmentation of Fundus Images","text":"This paper presents a novel classification-based optic disc (OD) segmentation algorithm that detects the OD boundary and the location of vessel origin (VO) pixel. First, the green plane of each fundus image is resized and morphologically reconstructed using a circular structuring element. Bright regions are then extracted from the morphologically reconstructed image that lie in close vicinity of the major blood vessels. Next, the bright regions are classified as bright probable OD regions and non-OD regions using six region-based features and a Gaussian mixture model classifier. The classified bright probable OD region with maximum Vessel-Sum and Solidity is detected as the best candidate region for the OD. Other bright probable OD regions within 1-disc diameter from the centroid of the best candidate OD region are then detected as remaining candidate regions for the OD. A convex hull containing all the candidate OD regions is then estimated, and a best-fit ellipse across the convex hull becomes the segmented OD boundary. Finally, the centroid of major blood vessels within the segmented OD boundary is detected as the VO pixel location. The proposed algorithm has low computation time complexity and it is robust to variations in image illumination, imaging angles, and retinal abnormalities. This algorithm achieves 98.8%-100% OD segmentation success and OD segmentation overlap score in the range of 72%-84% on images from the six public datasets of DRIVE, DIARETDB1, DIARETDB0, CHASE_DB1, MESSIDOR, and STARE in less than 2.14 s per image. Thus, the proposed algorithm can be used for automated detection of retinal pathologies, such as glaucoma, diabetic retinopathy, and maculopathy."}
{"_id":"3e9cdb6d2509e6a5d376c7e0ef6c7b40903d3d97","title":"Wireless Body Area Network (WBAN): A Survey on Reliability, Fault Tolerance, and Technologies Coexistence","text":"Wireless Body Area Network (WBAN) has been a key element in e-health to monitor bodies. This technology enables new applications under the umbrella of different domains, including the medical field, the entertainment and ambient intelligence areas. This survey paper places substantial emphasis on the concept and key features of the WBAN technology. First, the WBAN concept is introduced and a review of key applications facilitated by this networking technology is provided. The study then explores a wide variety of communication standards and methods deployed in this technology. Due to the sensitivity and criticality of the data carried and handled by WBAN, fault tolerance is a critical issue and widely discussed in this paper. Hence, this survey investigates thoroughly the reliability and fault tolerance paradigms suggested for WBANs. Open research and challenging issues pertaining to fault tolerance, coexistence and interference management and power consumption are also discussed along with some suggested trends in these aspects."}
{"_id":"687b68910a974eb874e355fd2412f1bb4fbc6860","title":"Insufficient effort responding: examining an insidious confound in survey data.","text":"Insufficient effort responding (IER; Huang, Curran, Keeney, Poposki, & DeShon, 2012) to surveys has largely been assumed to be a source of random measurement error that attenuates associations between substantive measures. The current article, however, illustrates how and when the presence of IER can produce a systematic bias that inflates observed correlations between substantive measures. Noting that inattentive responses as a whole generally congregate around the midpoint of a Likert scale, we propose that Mattentive, defined as the mean score of attentive respondents on a substantive measure, will be negatively related to IER's confounding effect on substantive measures (i.e., correlations between IER and a given substantive measure will become less positive [or more negative] as Mattentive increases). Results from a personality questionnaire (Study 1) and a simulation (Study 2) consistently support the hypothesized confounding influence of IER. Using an employee sample (Study 3), we demonstrated how IER can confound bivariate relationships between substantive measures. Together, these studies indicate that IER can inflate the strength of observed relationships when scale means depart from the scale midpoints, resulting in an inflated Type I error rate. This challenges the traditional view that IER attenuates observed bivariate correlations. These findings highlight situations where IER may be a methodological nuisance, while underscoring the need for survey administrators and researchers to deter and detect IER in surveys. The current article serves as a wake-up call for researchers and practitioners to more closely examine IER in their data."}
{"_id":"76194dd34e3054fadcb4af6246b133f18924f419","title":"Joint Factor Analysis Versus Eigenchannels in Speaker Recognition","text":"We compare two approaches to the problem of session variability in Gaussian mixture model (GMM)-based speaker verification, eigenchannels, and joint factor analysis, on the National Institute of Standards and Technology (NIST) 2005 speaker recognition evaluation data. We show how the two approaches can be implemented using essentially the same software at all stages except for the enrollment of target speakers. We demonstrate the effectiveness of zt-norm score normalization and a new decision criterion for speaker recognition which can handle large numbers of t-norm speakers and large numbers of speaker factors at little computational cost. We found that factor analysis was far more effective than eigenchannel modeling. The best result we obtained was a detection cost of 0.016 on the core condition (all trials) of the evaluation"}
{"_id":"35ade5c894d5c859d72736f4e88124a3946235b6","title":"Composite Behavioral Modeling for Identity Theft Detection in Online Social Networks","text":"In this work, we aim at building a bridge from poor behavioral data to an effective, quick-response, and robust behavior model for online identity theft detection. We concentrate on this issue in online social networks (OSNs) where users usually have composite behavioral records, consisting of multi-dimensional low-quality data, e.g., offline check-ins and online user generated content (UGC). As an insightful result, we find that there is a complementary effect among different dimensions of records for modeling users\u2019 behavioral patterns. To deeply exploit such a complementary effect, we propose a joint model to capture both online and offline features of a user\u2019s composite behavior. We evaluate the proposed joint model by comparing with some typical models on two real-world datasets: Foursquare and Yelp. In the widely-used setting of theft simulation (simulating thefts via behavioral replacement), the experimental results show that our model outperforms the existing ones, with the AUC values 0.956 in Foursquare and 0.947 in Yelp, respectively. Particularly, the recall (True Positive Rate) can reach up to 65.3% in Foursquare and 72.2% in Yelp with the corresponding disturbance rate (False Positive Rate) below 1%. It is worth mentioning that these performances can be achieved by examining only one composite behavior (visiting a place and posting a tip online simultaneously) per authentication, which guarantees the low response latency of our method. This study would give the cybersecurity community new insights into whether and how a real-time online identity authentication can be improved via modeling users\u2019 composite behavioral patterns."}
{"_id":"08ab1c63e36ea9311443613f3bb8bb9b5362fd4f","title":"Coherent Path Tracing","text":"Packet tracing is a popular and efficient method for accelerating ray tracing. However, packet traversal techniques become inefficient when they are applied to path tracing since the secondary rays are incoherent. In this paper, we present a simple technique for improving the coherency of secondary rays. This technique uses the same sequence of random numbers for generating secondary rays for all the pixels in each sample. This improves the efficiency of the packet tracing algorithm, but creates structured noise patterns in the image. We propose an interleaved sampling scheme that reduces the correlation in the noise and makes it virtually imperceptible in the final image. Coherent path tracing is unbiased, simple to implement and outperforms standard path tracing with packet tracing, while producing images with similar RMS error values."}
{"_id":"d6847623d62f390f6843cf27868cfc077f5555e1","title":"\u201cI am active\u201d: effects of a program to promote active aging","text":"BACKGROUND\nActive aging involves a general lifestyle strategy that allows preservation of both physical and mental health during the aging process. \"I am Active\" is a program designed to promote active aging by increased physical activity, healthy nutritional habits, and cognitive functioning. The purpose of this study was to assess the effectiveness of this program.\n\n\nMETHODS\nSixty-four healthy adults aged 60 years or older were recruited from senior centers and randomly allocated to an experimental group (n=31) or a control group (n=33). Baseline, post-test, and 6-month follow-up assessments were performed after the theoretical-practical intervention. Effect sizes were calculated.\n\n\nRESULTS\nAt the conclusion of the program, the experimental group showed significant improvement compared with the control group in the following domains: physical activity (falls risk, balance, flexibility, self-efficacy), nutrition (self-efficacy and nutritional status), cognitive performance (processing speed and self-efficacy), and quality of life (general, health and functionality, social and economic status). Although some declines were reported, improvements at follow-up remained in self-efficacy for physical activity, self-efficacy for nutrition, and processing speed, and participants had better nutritional status and quality of life overall.\n\n\nCONCLUSION\nOur findings show that this program promotes improvements in domains of active aging, mainly in self-efficacy beliefs as well as in quality of life in healthy elders."}
{"_id":"857592f00360eba13d2a147ae38cb911a4047dac","title":"Continuous Signed Distance Functions for 3D Vision","text":"We explore the use of continuous signed distance functions as an object representation for 3D vision. Popularized in procedural computer graphics, this representation defines 3D objects as geometric primitives combined with constructive solid geometry and transformed by nonlinear deformations, scaling, rotation or translation. Unlike its discretized counterpart, that has become important in dense 3D reconstruction, the continuous distance function is not stored as a sampled volume, but as a closed mathematical expression. We argue that this representation can have several benefits for 3D vision, such as being able to describe many classes of indoor and outdoor objects at the order of hundreds of bytes per class, getting parametrized shape variations for free. As a distance function, the representation also has useful computational aspects by defining, at each point in space, the direction and distance to the nearest surface, and whether a point is inside or outside the surface."}
{"_id":"2254ae8c87b64d2b8609e5a860648542982029d0","title":"The brief aggression questionnaire: psychometric and behavioral evidence for an efficient measure of trait aggression.","text":"A key problem facing aggression research is how to measure individual differences in aggression accurately and efficiently without sacrificing reliability or validity. Researchers are increasingly demanding brief measures of aggression for use in applied settings, field studies, pretest screening, longitudinal, and daily diary studies. The authors selected the three highest loading items from each of the Aggression Questionnaire's (Buss & Perry, 1992) four subscales--Physical Aggression, Verbal Aggression, anger, and hostility--and developed an efficient 12-item measure of aggression--the Brief Aggression Questionnaire (BAQ). Across five studies (N\u2009=\u20093,996), the BAQ showed theoretically consistent patterns of convergent and discriminant validity with other self-report measures, consistent four-factor structures using factor analyses, adequate recovery of information using item response theory methods, stable test-retest reliability, and convergent validity with behavioral measures of aggression. The authors discuss the reliability, validity, and efficiency of the BAQ, along with its many potential applications."}
{"_id":"85a1ca73388f4a4d0f92d70ac8bc63af06b2f972","title":"Control of a two-dimensional movement signal by a noninvasive brain-computer interface in humans.","text":"Brain-computer interfaces (BCIs) can provide communication and control to people who are totally paralyzed. BCIs can use noninvasive or invasive methods for recording the brain signals that convey the user's commands. Whereas noninvasive BCIs are already in use for simple applications, it has been widely assumed that only invasive BCIs, which use electrodes implanted in the brain, can provide multidimensional movement control of a robotic arm or a neuroprosthesis. We now show that a noninvasive BCI that uses scalp-recorded electroencephalographic activity and an adaptive algorithm can provide humans, including people with spinal cord injuries, with multidimensional point-to-point movement control that falls within the range of that reported with invasive methods in monkeys. In movement time, precision, and accuracy, the results are comparable to those with invasive BCIs. The adaptive algorithm used in this noninvasive BCI identifies and focuses on the electroencephalographic features that the person is best able to control and encourages further improvement in that control. The results suggest that people with severe motor disabilities could use brain signals to operate a robotic arm or a neuroprosthesis without needing to have electrodes implanted in their brains."}
{"_id":"c19047f9c83a31e6d49dfebe1d59bece774d5eab","title":"Investigating the functions of subregions within anterior hippocampus","text":"Previous functional MRI (fMRI) studies have associated anterior hippocampus with imagining and recalling scenes, imagining the future, recalling autobiographical memories and visual scene perception. We have observed that this typically involves the medial rather than the lateral portion of the anterior hippocampus. Here, we investigated which specific structures of the hippocampus underpin this observation. We had participants imagine novel scenes during fMRI scanning, as well as recall previously learned scenes from two different time periods (one week and 30 min prior to scanning), with analogous single object conditions as baselines. Using an extended segmentation protocol focussing on anterior hippocampus, we first investigated which substructures of the hippocampus respond to scenes, and found both imagination and recall of scenes to be associated with activity in presubiculum\/parasubiculum, a region associated with spatial representation in rodents. Next, we compared imagining novel scenes to recall from one week or 30 min before scanning. We expected a strong response to imagining novel scenes and 1-week recall, as both involve constructing scene representations from elements stored across cortex. By contrast, we expected a weaker response to 30-min recall, as representations of these scenes had already been constructed but not yet consolidated. Both imagination and 1-week recall of scenes engaged anterior hippocampal structures (anterior subiculum and uncus respectively), indicating possible roles in scene construction. By contrast, 30-min recall of scenes elicited significantly less activation of anterior hippocampus but did engage posterior CA3. Together, these results elucidate the functions of different parts of the anterior hippocampus, a key brain area about which little is definitely known."}
{"_id":"1abdc7a0494f103967a82a9471a7af42796a41dc","title":"Modern Microwave Ferrites","text":"Microwave ferrites are ubiquitous in systems that send, receive, and manipulate electromagnetic signals across very high frequency to quasi-optical frequency bands. In this paper, modern microwave ferrites are reviewed including spinel, garnet, and hexaferrite systems as thin and thick films, powders and compacts, and metamaterials. Their fundamental properties and utility are examined in the context of high frequency applications ranging from the VHF to millimeter-wave bands. Perspective and outlook of advances in theory, processing, and devices occurring in the science and engineering communities since the year 2000 are presented and discussed."}
{"_id":"09cca9d37140bae6c5a78b7c9ec112bd29ab0b3d","title":"SmartDroid: an automatic system for revealing UI-based trigger conditions in android applications","text":"User interface (UI) interactions are essential to Android applications, as many Activities require UI interactions to be triggered. This kind of UI interactions could also help malicious apps to hide their sensitive behaviors (e.g., sending SMS or getting the user's device ID) from being detected by dynamic analysis tools such as TaintDroid, because simply running the app, but without proper UI interactions, will not lead to the exposure of sensitive behaviors. In this paper we focus on the challenging task of triggering a certain behavior through automated UI interactions. In particular, we propose a hybrid static and dynamic analysis method to reveal UI-based trigger conditions in Android applications. Our method first uses static analysis to extract expected activity switch paths by analyzing both Activity and Function Call Graphs, and then uses dynamic analysis to traverse each UI elements and explore the UI interaction paths towards the sensitive APIs. We implement a prototype system SmartDroid and show that it can automatically and efficiently detect the UI-based trigger conditions required to expose the sensitive behavior of several Android malwares, which otherwise cannot be detected with existing techniques such as TaintDroid."}
{"_id":"79159935ffcfbd18d76fbdc9b108c896ee296716","title":"Stimuli-responsive nanocarriers for drug delivery.","text":"Spurred by recent progress in materials chemistry and drug delivery, stimuli-responsive devices that deliver a drug in spatial-, temporal- and dosage-controlled fashions have become possible. Implementation of such devices requires the use of biocompatible materials that are susceptible to a specific physical incitement or that, in response to a specific stimulus, undergo a protonation, a hydrolytic cleavage or a (supra)molecular conformational change. In this Review, we discuss recent advances in the design of nanoscale stimuli-responsive systems that are able to control drug biodistribution in response to specific stimuli, either exogenous (variations in temperature, magnetic field, ultrasound intensity, light or electric pulses) or endogenous (changes in pH, enzyme concentration or redox gradients)."}
{"_id":"b6d30456d9a1ee711f91581bf1f1b7f5d1e276e2","title":"SCUC With Hourly Demand Response Considering Intertemporal Load Characteristics","text":"In this paper, the hourly demand response (DR) is incorporated into security-constrained unit commitment (SCUC) for economic and security purposes. SCUC considers fixed and responsive loads. Unlike fixed hourly loads, responsive loads are modeled with their intertemporal characteristics. The responsive loads linked to hourly market prices can be curtailed or shifted to other operating hours. The study results show that DR could shave the peak load, reduce the system operating cost, reduce fuel consumptions and carbon footprints, and reduce the transmission congestion by reshaping the hourly load profile. Numerical simulations in this paper exhibit the effectiveness of the proposed approach."}
{"_id":"60afb1828de4efa1588401f87caa55ac3a0cd820","title":"Early-onset hidradenitis suppurativa.","text":"A 9-year-old girl developed hidradenitis suppurativa 3 months after the first signs of adrenarche. Such a close temporal relationship is consistent with the hypothesis that the disease is androgen dependent. Less than 2% of patients have onset of the disease before the age of 11 years. The exceptionally early age of onset in our patient may be partly explained by the fact that she had an early puberty."}
{"_id":"7f60b70dede16fe4d7b412674929b4805c9b5c95","title":"Prepubertal hidradenitis suppurativa: two case reports and review of the literature.","text":"Hidradenitis suppurativa (HS) is a chronic suppurative scarring disease of apocrine sweat gland-bearing skin in the axillary, anogenital, and, rarely, the breast and scalp regions. Females are more commonly affected than males and it is usually seen at puberty or later. We report two girls with prepubertal hidradenitis suppurativa whose initial presentation predated any signs of puberty. This early onset is very rare and its etiology remains unknown. Severe disease can be seen in prepubertal children and surgical intervention is effective in these cases."}
{"_id":"8b85f287c144aad5ff038ec0b140e0c4e210990b","title":"Finasteride for the treatment of hidradenitis suppurativa in children and adolescents.","text":"IMPORTANCE\nHidradenitis suppurativa (HS) is a chronic debilitating cutaneous disease for which there is no universally effective treatment. Patients typically present at puberty with tender subcutaneous nodules that can progress to dermal abscess formation. Antiandrogens have been used in the treatment of HS, and studies have primarily focused on adult patients.\n\n\nOBSERVATIONS\nWe present a case series of 3 pediatric patients with HS who were successfully treated with oral finasteride, resulting in decreased frequency and severity of disease flares with no significant adverse effects.\n\n\nCONCLUSIONS AND RELEVANCE\nFinasteride is a therapeutic option that provides benefit for pediatric patients with HS. Further prospective data and randomized controlled studies will provide helpful information in the management of this disease."}
{"_id":"e72e10ad6228bd3dcee792f6f571c5ffed37266f","title":"Hidradenitis suppurativa in children and adolescents: a review of treatment options.","text":"Hidradenitis suppurativa (HS) is a burdensome disease and has the potential to affect the life course of patients. It is a rare disease in children, and the recorded literature is correspondingly scarce. This article reviews the therapeutic options for HS in children and adolescents, and highlights particular differences or challenges with treating patients in this age group compared with adults. The work-up of paediatric patients with HS should include considerations of possible endocrine co-morbidities and obesity. Medical therapy of lesions may include topical clindamycin. Systemic therapy may include analgesics, clindamycin and rifampicin, finasteride, corticosteroids or tumour necrosis factor alpha (TNF\u03b1) blockers. Superinfections should be appropriately treated. Scarring lesions generally require surgery."}
{"_id":"f4f6ae0b10f2cc26e5cf75b4c39c70703f036e9b","title":"Morbidity in patients with hidradenitis suppurativa.","text":"BACKGROUND\nAlthough skin diseases are often immediately visible to both patients and society, the morbidity they cause is only poorly defined. It has been suggested that quality-of-life measures may be a relevant surrogate measure of skin disease. Hidradenitis suppurativa (HS) leads to painful eruptions and malodorous discharge and is assumed to cause a significant degree of morbidity. The resulting impairment of life quality has not previously been quantitatively assessed, although such an assessment may form a pertinent measure of disease severity in HS.\n\n\nOBJECTIVES\nTo measure the impairment of life quality in patients with HS.\n\n\nMETHODS\nIn total, 160 patients suffering from HS were approached. The following data were gathered: quality-of-life data (Dermatology Life Quality Index, DLQI questionnaire), basic demographic data, age at onset of the condition and the average number of painful lesions per month.\n\n\nRESULTS\nOne hundred and fourteen patients participated in the study. The mean +\/- SD age of the patients was 40.9 +\/- 11.7 years, the mean +\/- SD age at onset 21.8 +\/- 9.9 years and the mean +\/- SD duration of the disease 18.8 +\/- 11.4 years. Patients had a mean +\/- SD DLQI score of 8.9 +\/- 8.3 points. The highest mean score out of the 10 DLQI questions was recorded for question 1, which measures the level of pain, soreness, stinging or itching (mean 1.55 points, median 2 points). Patients experienced a mean of 5.1 lesions per month.\n\n\nCONCLUSIONS\nHS causes a high degree of morbidity, with the highest scores obtained for the level of pain caused by the disease. The mean DLQI score for HS was higher than for previously studied skin diseases, and correlated with disease intensity as expressed by lesions per month. This suggests that the DLQI may be a relevant outcome measure in future therapeutic trials in HS."}
{"_id":"c94ac1ce8492f39c2efa1eb93475372196fe0520","title":"Real-Time Mental Arithmetic Task Recognition From EEG Signals","text":"Electroencephalography (EEG)-based monitoring the state of the user's brain functioning and giving her\/him the visual\/audio\/tactile feedback is called neurofeedback technique, and it could allow the user to train the corresponding brain functions. It could provide an alternative way of treatment for some psychological disorders such as attention deficit hyperactivity disorder (ADHD), where concentration function deficit exists, autism spectrum disorder (ASD), or dyscalculia where the difficulty in learning and comprehending the arithmetic exists. In this paper, a novel method for multifractal analysis of EEG signals named generalized Higuchi fractal dimension spectrum (GHFDS) was proposed and applied in mental arithmetic task recognition from EEG signals. Other features such as power spectrum density (PSD), autoregressive model (AR), and statistical features were analyzed as well. The usage of the proposed fractal dimension spectrum of EEG signal in combination with other features improved the mental arithmetic task recognition accuracy in both multi-channel and one-channel subject-dependent algorithms up to 97.87% and 84.15% correspondingly. Based on the channel ranking, four channels were chosen which gave the accuracy up to 97.11%. Reliable real-time neurofeedback system could be implemented based on the algorithms proposed in this paper."}
{"_id":"2bf11b00938e73468e3ab02dfe678985212f1aea","title":"Mining User Mobility Features for Next Place Prediction in Location-Based Services","text":"Mobile location-based services are thriving, providing an unprecedented opportunity to collect fine grained spatio-temporal data about the places users visit. This multi-dimensional source of data offers new possibilities to tackle established research problems on human mobility, but it also opens avenues for the development of novel mobile applications and services. In this work we study the problem of predicting the next venue a mobile user will visit, by exploring the predictive power offered by different facets of user behavior. We first analyze about 35 million check-ins made by about 1 million Foursquare users in over 5 million venues across the globe, spanning a period of five months. We then propose a set of features that aim to capture the factors that may drive users' movements. Our features exploit information on transitions between types of places, mobility flows between venues, and spatio-temporal characteristics of user check-in patterns. We further extend our study combining all individual features in two supervised learning models, based on linear regression and M5 model trees, resulting in a higher overall prediction accuracy. We find that the supervised methodology based on the combination of multiple features offers the highest levels of prediction accuracy: M5 model trees are able to rank in the top fifty venues one in two user check-ins, amongst thousands of candidate items in the prediction list."}
{"_id":"b92513dac9d5b6a4683bcc625b94dd1ced98734e","title":"Two\/Too Simple Adaptations of Word2Vec for Syntax Problems","text":"We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models."}
{"_id":"6275549f94dc466921fda4115361688222158be4","title":"Digital Fabrication Techniques for Cultural Heritage: A Survey","text":"Digital fabrication devices exploit basic technologies in order to create tangible reproductions of 3D digital models. Although current 3D printing pipelines still suffer from several restrictions, accuracy in reproduction has reached an excellent level. The manufacturing industry has been the main domain of 3D printing applications over the last decade. Digital fabrication techniques have also been demonstrated to be effective in many other contexts, including the consumer domain. The Cultural Heritage is one of the new application contexts and is an ideal domain to test the flexibility and quality of this new technology. This survey overviews the various fabrication technologies, discussing their strengths, limitations, and costs. Various successful uses of 3D printing in the Cultural Heritage are analysed, which should also be useful for other application contexts. We review works that have attempted to extend fabrication technologies in order to deal with the specific issues in the use of digital fabrication in the Cultural Heritage. Finally, we also propose areas for future research."}
{"_id":"65fed11870160561b19c1be168bf9d6a5873ab20","title":"A Multi-Cloud Framework for Measuring and Describing Performance Aspects of Cloud Services Across Different Application Types","text":"Cloud services have emerged as an innovative IT provisioning model in the recent years. However, after their usage severe considerations have emerged with regard to their varying performance due to multitenancy and resource sharing issues. These issues make it very difficult to provide any kind of performance estimation during application design or deployment time. The aim of this paper is to present a mechanism and process for measuring the performance of various Cloud services and describing this information in machine understandable format. The framework is responsible for organizing the execution and can support multiple Cloud providers. Furthermore we present approaches for measuring service performance with the usage of specialized metrics for ranking the services according to a weighted combination of cost, performance and workload."}
{"_id":"9969ff0b19914bc8487b6abb1d5312ce538a7284","title":"Design of novel capacitive type torque sensor for robotic applications","text":"This paper introduces a novel capacitive type torque sensor. The developed torque sensor measures single axis torque value by using two capacitance transducing cells. To make a linear relation between the capacitance and the deformation, the shape of flexure hinge is considered to be deformed constantly. Also, the nonlinear curve fitting method is used to fit the capacitance linearly. Finally, the performance of the sensor is verified in reference to a commercial torque sensor."}
{"_id":"96c76308dc5aca61355bc4529be9e3a8232fa155","title":"Overview of High-Step-Up Coupled-Inductor Boost Converters","text":"High-step-up, high-efficiency, and cost-effective dc-dc converters, serving as an interfacing cell to boost the low-voltage output of renewable sources to the utility voltage level, are an important part in renewable energy systems. Over the past few years, there has been a substantial amount of studies devoted to high-step-up dc-dc converters. Among them, the category of coupled-inductor boost converters is widely researched and considered to be a promising solution for high-step-up applications. In this paper, these converters are categorized into five groups according to the major topological features. The derivation process, advantages, and disadvantages of these converters are systematically discussed, compared, and scrutinized. This paper aims to provide an introduction, review, and framework for the category of high-step-up coupled-inductor boost converters. General structures for the topologies are proposed to clarify the topological derivation process and to show potential gaps. Furthermore, challenges or directions are presented in this paper for deriving new topologies in this field."}
{"_id":"a621face3570bcd56caff92a80fed494dca49bd8","title":"Design and Analysis of a Two Stage Operational Amplifier for High Gain and High Bandwidth","text":"In this paper a design and comparison between a fully differential RC Miller compensated CMOS op-amp and conventional op-amp is presented. High gain enables the circuit to operate efficiently in a closed loop feedback system, whereas high bandwidth makes it suitable for high speed applications. A novel RC Miller compensation technique is used to optimize the parameters of gain and bandwidth for high speed applications are illustrated in this research work. The design is also able to address any fluctuation in supply or dc input voltages and stabilizes the operation by nullifying. The design is implemented on TSMC 0.18 \uf06dm CMOS process at 3.3 V as supply voltage under room temperature 27\uf0b0 C. The simulated result shows that a unity gain bandwidth of 136.8 MHz with a high gain of 92.27 dB is achieved for the proposed op-amp circuit. The total areas of the layouts are 0.000158 mm and 0.000532 mm for conventional and proposed respectively. Key wrods: Two-Stage Op-amp, fully differential, high gain, high bandwidth"}
{"_id":"24c4c20ccda15b831b199f87491813f876d4db07","title":"Assembler: Efficient Discovery of Spatial Co-evolving Patterns in Massive Geo-sensory Data","text":"Recent years have witnessed the wide proliferation of geo-sensory applications wherein a bundle of sensors are deployed at different locations to cooperatively monitor the target condition. Given massive geo-sensory data, we study the problem of mining spatial co-evolving patterns (SCPs), i.e., groups of sensors that are spatially correlated and co-evolve frequently in their readings. SCP mining is of great importance to various real-world applications, yet it is challenging because (1) the truly interesting evolutions are often flooded by numerous trivial fluctuations in the geo-sensory time series; and (2) the pattern search space is extremely large due to the spatiotemporal combinatorial nature of SCP. In this paper, we propose a two-stage method called Assember. In the first stage, Assember filters trivial fluctuations using wavelet transform and detects frequent evolutions for individual sensors via a segment-and-group approach. In the second stage, Assember generates SCPs by assembling the frequent evolutions of individual sensors. Leveraging the spatial constraint, it conceptually organizes all the SCPs into a novel structure called the SCP search tree, which facilitates the effective pruning of the search space to generate SCPs efficiently. Our experiments on both real and synthetic data sets show that Assember is effective, efficient, and scalable."}
{"_id":"ca876e6b2aa530e3108ad307520bc19eaf95e136","title":"A Markov Model for Headway\/Spacing Distribution of Road Traffic","text":"In this paper, we link two research directions of road traffic-the mesoscopic headway distribution model and the microscopic vehicle interaction model-together to account for the empirical headway\/spacing distributions. A unified car-following model is proposed to simulate different driving scenarios, including traffic on highways and at intersections. Unlike our previous approaches, the parameters of this model are directly estimated from the Next Generation Simulation (NGSIM) Trajectory Data. In this model, empirical headway\/spacing distributions are viewed as the outcomes of stochastic car-following behaviors and the reflections of the unconscious and inaccurate perceptions of space and\/or time intervals that people may have. This explanation can be viewed as a natural extension of the well-known psychological car-following model (the action point model). Furthermore, the fast simulation speed of this model will benefit transportation planning and surrogate testing of traffic signals."}
{"_id":"306d317a1d45be685bd5162942d5472aac33559a","title":"Design of a Marx Generator for HEMP filter evaluation taking account of parasitic effect of components","text":"This paper introduces the design of a Marx Generator that can generate high attitude pulses up to 25 kV and 2.5 kA. The pulses have a wave shape representative of that used to test High-altitude ElectroMagnetic (HEMP) filters against the pulse current injection (PCI) requirements. The rise time is 20 ns and the pulse width is 230 ns on a 10 \u03a9 resistor. In the design, it was found that parasitic effect of circuit components could significantly affect the performance of the generator. This is mainly because the pulse width is very narrow and its frequency spectrum is very wideband. The high-frequency circuit models of the components used in the generator were obtained by calculation and optimization, and then further verified by experiment. The verified circuit models were then used to design the three-stage generator. The measured performance agrees very well with the simulated one using the proposed circuit models."}
{"_id":"e2151994deb9da2b192ec1e281558dc509274822","title":"Brain activation during human male ejaculation.","text":"Brain mechanisms that control human sexual behavior in general, and ejaculation in particular, are poorly understood. We used positron emission tomography to measure increases in regional cerebral blood flow (rCBF) during ejaculation compared with sexual stimulation in heterosexual male volunteers. Manual penile stimulation was performed by the volunteer's female partner. Primary activation was found in the mesodiencephalic transition zone, including the ventral tegmental area, which is involved in a wide variety of rewarding behaviors. Parallels are drawn between ejaculation and heroin rush. Other activated mesodiencephalic structures are the midbrain lateral central tegmental field, zona incerta, subparafascicular nucleus, and the ventroposterior, midline, and intralaminar thalamic nuclei. Increased activation was also present in the lateral putamen and adjoining parts of the claustrum. Neocortical activity was only found in Brodmann areas 7\/40, 18, 21, 23, and 47, exclusively on the right side. On the basis of studies in rodents, the medial preoptic area, bed nucleus of the stria terminalis, and amygdala are thought to be involved in ejaculation, but increased rCBF was not found in any of these regions. Conversely, in the amygdala and adjacent entorhinal cortex, a decrease in activation was observed. Remarkably strong rCBF increases were observed in the cerebellum. These findings corroborate the recent notion that the cerebellum plays an important role in emotional processing. The present study for the first time provides insight into which regions in the human brain play a primary role in ejaculation, and the results might have important implications for our understanding of how human ejaculation is brought about, and for our ability to improve sexual function and satisfaction in men."}
{"_id":"381c7853690a0fee6f00d2608a7779737f1365f9","title":"Data Center Energy Consumption Modeling: A Survey","text":"Data centers are critical, energy-hungry infrastructures that run large-scale Internet-based services. Energy consumption models are pivotal in designing and optimizing energy-efficient operations to curb excessive energy consumption in data centers. In this paper, we survey the state-of-the-art techniques used for energy consumption modeling and prediction for data centers and their components. We conduct an in-depth study of the existing literature on data center power modeling, covering more than 200 models. We organize these models in a hierarchical structure with two main branches focusing on hardware-centric and software-centric power models. Under hardware-centric approaches we start from the digital circuit level and move on to describe higher-level energy consumption models at the hardware component level, server level, data center level, and finally systems of systems level. Under the software-centric approaches we investigate power models developed for operating systems, virtual machines and software applications. This systematic approach allows us to identify multiple issues prevalent in power modeling of different levels of data center systems, including: i) few modeling efforts targeted at power consumption of the entire data center ii) many state-of-the-art power models are based on a few CPU or server metrics, and iii) the effectiveness and accuracy of these power models remain open questions. Based on these observations, we conclude the survey by describing key challenges for future research on constructing effective and accurate data center power models."}
{"_id":"17c20fc0d0808d1c685159a52101422b8c7d6868","title":"Differential quasi self-complimentary (QSC) ultra-wideband (UWB) MIMO antenna","text":"In this paper, a differentially excited ultra-wideband (UWB) MIMO antenna is designed for pattern diversity applications, utilizing quasi self-complimentary (QSC) antenna elements. Four QSC elements are realized using half- octagon shaped monopoles, and their complementary cuts from the ground plane. Due to the QSC property, wide impedance bandwidth covering the UWB frequency range of 3\u201311 GHz is achieved within very less antenna footprint. The QSC elements are placed symmetrically about the four edges of a square substrate. Two oppositely positioned elements are fed with 180o phase difference, in order to form a single differential pair. Two such differential pairs are placed perpendicular to each other for realizing pattern diversity performance with high value of differential isolation."}
{"_id":"052402b356a1938ef213cd07810a2f32e7956c5d","title":"Airway management and smoke inhalation injury in the burn patient.","text":"Smoke inhalation injury, a unique form of acute lung injury, greatly increases the occurrence of postburn morbidity and mortality. In addition to early intubation for upper-airway protection, subsequent critical care of patients who have this injury should be directed at maintaining distal airway patency. High-frequency ventilation, inhaled heparin, and aggressive pulmonary toilet are among the therapies available. Even so, immunosuppression, intubation, and airway damage predispose these patients to pneumonia and other complications."}
{"_id":"76b6b2be912f263db3be8dca2724fb32a70a0077","title":"Tibiofemoral movement 1: the shapes and relative movements of the femur and tibia in the unloaded cadaver knee.","text":"In six unloaded cadaver knees we used MRI to determine the shapes of the articular surfaces and their relative movements. These were confirmed by dissection. Medially, the femoral condyle in sagittal section is composed of the arcs of two circles and that of the tibia of two angled flats. The anterior facets articulate in extension. At about 20 degrees the femur 'rocks' to articulate through the posterior facets. The medial femoral condyle does not move anteroposteriorly with flexion to 110 degrees. Laterally, the femoral condyle is composed entirely, or almost entirely, of a single circular facet similar in radius and arc to the posterior medial facet. The tibia is roughly flat. The femur tends to roll backwards with flexion. The combination during flexion of no anteroposterior movement medially (i.e., sliding) and backward rolling (combined with sliding) laterally equates to internal rotation of the tibia around a medial axis with flexion. About 5 degrees of this rotation may be obligatory from 0 degrees to 10 degrees flexion; thereafter little rotation occurs to at least 45 degrees. Total rotation at 110 degrees is about 20 degrees, most if not all of which can be suppressed by applying external rotation to the tibia at 90 degrees."}
{"_id":"ebecd22710d711565d7276f8301753e302ffd0c2","title":"A Layered Security Approach for Cloud Computing Infrastructure","text":"This paper introduces a practical security model based on key security considerations by looking at a number of infrastructure aspects of Cloud Computing such as SaaS, Utility, Web, Platform and Managed Services, Service commerce platforms and Internet Integration which was introduced with a concise literature review. The purpose of this paper is to offer a macro level solution for identified common infrastructure security requirements. This model with a number of emerged patterns can be applied to infrastructure aspect of Cloud Computing as a proposed shared security approach in system development life cycle focusing on the plan-built-run scope."}
{"_id":"943d8c91ebc8a80034d163f3f6cab611460ec411","title":"Development of a three-dimensional ball rotation sensing system using optical mouse sensors","text":"Robots using ball(s) as spherical wheels have the advantage of omnidirectional motion. Some of these robots use only a single ball as their wheel and dynamically balance on it. However, driving a ball wheel is not straightforward. These robots usually use one or more motor-driven rollers or wheels in frictional contact with the ball wheel. Some slippage can occur with these schemes, so it is important to measure the actual rotation of the ball wheel for good control. This paper proposes one method for measuring the three dimensional rotation of a sphere, which is applicable to the case of a ball wheel. The system measures surface speed by using two or more optical mouse sensors and transforms them into the angular velocity vector of the ball, followed by integration to give the rotational angle. Experiments showed the correctness of this method, yielding an error of approximately 1%, with a 10 ms response."}
{"_id":"d68145adc8699818b1d90ec0907028c0257c9289","title":"IMPLEMNTATION OF SIMULINK BASED MODEL USING SOBEL EDGE DETECTOR FOR DENTAL PROBLEMS Deepika Nagpal","text":"Image Segmentation is the process of partitioning a digital image into multiple regions or sets of pixels.Edge Detection is one of the main Technique used in Segmentation.In this paper we used Sobel edge detector for segmenting the dental X-ray image.Using MATLAB,Image is segmented.Sysytem Test tool is used for the verification of the Simulink Model. The Simulink Model based Image Segmentation is a new function in image processing and offers a model based design for processing. Dental Caries is the main problem occurred in the teeths.Segmentation help to identify the places where the problems of dental caries are present."}
{"_id":"6a10db5956411e8d470ca0d063bb4e91c7f16d23","title":"Predicting conversion from MCI to AD with FDG-PET brain images at different prodromal stages","text":"Early diagnosis of Alzheimer disease (AD), while still at the stage known as mild cognitive impairment (MCI), is important for the development of new treatments. However, brain degeneration in MCI evolves with time and differs from patient to patient, making early diagnosis a very challenging task. Despite these difficulties, many machine learning techniques have already been used for the diagnosis of MCI and for predicting MCI to AD conversion, but the MCI group used in previous works is usually very heterogeneous containing subjects at different stages. The goal of this paper is to investigate how the disease stage impacts on the ability of machine learning methodologies to predict conversion. After identifying the converters and estimating the time of conversion (TC) (using neuropsychological test scores), we devised 5 subgroups of MCI converters (MCI-C) based on their temporal distance to the conversion instant (0, 6, 12, 18 and 24 months before conversion). Next, we used the FDG-PET images of these subgroups and trained classifiers to distinguish between the MCI-C at different stages and stable non-converters (MCI-NC). Our results show that MCI to AD conversion can be predicted as early as 24 months prior to conversion and that the discriminative power of the machine learning methods decreases with the increasing temporal distance to the TC, as expected. These findings were consistent for all the tested classifiers. Our results also show that this decrease arises from a reduction in the information contained in the regions used for classification and by a decrease in the stability of the automatic selection procedure."}
{"_id":"93159d70058f52efd42e0d3cfc18375fddfd5771","title":"Graph Partitioning using Quantum Annealing on the D-Wave System","text":"Graph partitioning (GP) applications are ubiquitous throughout mathematics, computer science, chemistry, physics, bio-science, machine learning, and complex systems. Post Moore's era supercomputing has provided us an opportunity to explore new approaches for traditional graph algorithms on quantum computing architectures. In this work, we explore graph partitioning using quantum annealing on the D-Wave 2X machine. Motivated by a recently proposed graph-based electronic structure theory applied to quantum molecular dynamics (QMD) simulations, graph partitioning is used for reducing the calculation of the density matrix into smaller subsystems rendering the calculation more computationally efficient. Unconstrained graph partitioning as community clustering based on the modularity metric can be naturally mapped into the Hamiltonian of the quantum annealer. On the other hand, when constraints are imposed for partitioning into equal parts and minimizing the number of cut edges between parts, a quadratic unconstrained binary optimization (QUBO) reformulation is required. This reformulation may employ the graph complement to fit the problem in the Chimera graph of the quantum annealer. Partitioning into 2 parts and k parts concurrently for arbitrary k are demonstrated with benchmark graphs, random graphs, and small material system density matrix based graphs. Results for graph partitioning using quantum and hybrid classical-quantum approaches are shown to be comparable to current \"state of the art\" methods and sometimes better."}
{"_id":"7359ef31c8bfccad8bf1f374d73db511ab74eb2e","title":"Typology of Distributed Ledger based Business Models","text":"The potential of distributed ledger technology and its application in various industries is a controversially debated topic. Advocates of the technology emphasize the economic benefits of decentralization and transparency, leading to cost reductions as well as the alleviation of several of today`s economic and technological problems. In contrast, critics assert that the potential of distributed ledgers might be overhyped, possibly leading to the next tech bubble. This paper contributes to the discussion by developing a typology of business models that are based on distributed ledger technology. In particular, this paper is a first step towards a more differentiated discussion on the potential of distributed ledges, by taking the underlying business models into consideration. Despite a characterization of the types, a discussion about special features of distributed ledger based business models is provided in the context of contemporary business model literature and the associated role of IT. It is proposed that future research must evaluate each business model isolated to achieve a comprehensive assessment of the potential of distributed ledgers. This paper can be interpreted as starting point for more fruitful discussions and the repeal of the partially diametrical opposed opinions towards the potentials of the technology."}
{"_id":"a6387dd26dedb81440adec9be776ecc03f76f623","title":"DESH: Database evaluation system with hibernate ORM framework","text":"Relational databases have been the predominant choice for back-ends in enterprise applications for several decades. JDBC - a Java API - that is used for developing such applications and persisting data on the back-end requires enormous time and effort. JDBC makes the application logic to become tightly coupled with the database and consequently is inadequate for building enterprise applications that need to adopt to dynamic requirements. Hence, ORM frameworks such as Hibernate, became prominent. However, even with ORM, the relational back-end often faces a drawback of lack of scalability and flexibility. In this context, NoSQL databases are increasingly gaining popularity. Existing research works have either benchmarked Hibernate with an RDBMS or with one of the NoSQL databases. However, it has not been attempted in the literature to test both an RDBMS and a NoSQL database for their performance within the context of a single application developed using the features of Hibernate. This kind of a study will provide an insight that using Hibernate ORM solution would help us to develop database-independent applications and how much performance gain can be achieved when an application is ported from an RDBMS back-end to a NoSQL database backend. The objective of this work is to develop a business application using Hibernate framework that can individually communicate with an RDBMS as well as a specific NoSQL database and to evaluate the performance of both these databases."}
{"_id":"3978e9f794174c7a2700b20193c071a7b1532b22","title":"Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods","text":"Direct search methods are best known as unconstrained optimization techniques that do not explicitly use derivatives. Direct search methods were formally proposed and widely applied in the 1960s but fell out of favor with the mathematical optimization community by the early 1970s because they lacked coherent mathematical analysis. Nonetheless, users remained loyal to these methods, most of which were easy to program, some of which were reliable. In the past fifteen years, these methods have seen a revival due, in part, to the appearance of mathematical analysis, as well as to interest in parallel and distributed computing. This review begins by briefly summarizing the history of direct search methods and considering the special properties of problems for which they are well suited. Our focus then turns to a broad class of methods for which we provide a unifying framework that lends itself to a variety of convergence results. The underlying principles allow generalization to handle bound constraints and linear constraints. We also discuss extensions to problems with nonlinear constraints."}
{"_id":"2a7a76cddd1d04aed660c62a0a879470bf98ca32","title":"On automatic differentiation","text":"During these last years, the environmental problems have acquired a growing place in our society. It becomes necessary to find a lasting way to the nuclear waste storage. The waste storage is in relation to the characteristics of wastes according to their activity and the life of the radionuclide. After many studies on the topic, led in particular by ANDRA, the storage in deep level is considered as the reference strategy for wastes with high or medium activity and long-lived. We have to make simulations of the radionucleide transport in underground in order to determine the impact of a possible propagation of radioelements. The modelling of the flow in porous media around the storage requires to know the physical parameters of the different geological layers. Those parameters (porosity and diffusion) are not directly accessible by measurements, hence we have to solve an inverse problem to recover them."}
{"_id":"4ac53a9341ae34d4651eff34e729e91806ab0c44","title":"Equation of State Calculations by Fast Computing Machines","text":"1087 instead, only water molecules with different amounts of excitation energy. These may follow any of three paths: (a) The excitation energy is lost without dissociation into radicals (by collision, or possibly radiation, as in aromatic hydrocarbons). (b) The molecules dissociate, but the resulting radicals recombine without escaping from the liquid cage. (c) The molecules dissociate and escape from the cage. In this case we would not expect them to move more than a few molecular diameters through the dense medium before being thermalized. paths (a) and (b) can be designated H 2 0* and those following path (c) can be designated H 2 0t. It seems reasonable to assume for the purpose of these calculations that the ionized H 2 0 molecules will become the H 20 t molecules, but this is not likely to be a complete correspondence. In conclusion we would like to emphasize that the qualitative result of this section is not critically dependent on the exact values of the physical parameters used. However, this treatment is classical, and a correct treatment must be wave mechanical; therefore the result of this section cannot be taken as an a priori theoretical prediction. The success of the radical diffusion model given above lends some plausibility to the occurrence of electron capture as described by this crude calculation. Further work is clearly needed. A general method, suitable for fast computing machines, for investigatiflg such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion."}
{"_id":"f514a883e2cb8901fc368078d8445da5aed18806","title":"A theoretical framework for simulated annealing","text":"Simulated Annealing has been a very successful general algorithm for the solution of large, complex combinatorial optimization problems. Since its introduction, several applications in different fields of engineering, such as integrated circuit placement, optimal encoding, resource allocation, logic synthesis, have been developed. In parallel, theoretical studies have been focusing on the reasons for the excellent behavior of the algorithm. This paper reviews most of the important results on the theory of Simulated Annealing, placing them in a unified framework. New results are reported as well."}
{"_id":"7ebb21adef59cdc0e854d75f1829118de8edec4d","title":"Evidence of Time-Dependent Vertical Breakdown in GaN-on-Si HEMTs","text":"This paper demonstrates and investigates the time-dependent vertical breakdown of GaN-on-Si power transistors. The study is based on electrical characterization, dc stress tests and electroluminescence measurements. We demonstrate the following original results: 1) when submitted to two-terminal (drain-to-substrate) stress, the AlGaN\/GaN transistors show a time-dependent degradation process, which leads to the catastrophic failure of the devices; 2) time-to-failure follows a Weibull distribution and is exponentially dependent on stress voltage; 3) the degradation mechanism is strongly field dependent and weakly thermally activated, with an activation energy of 0.25 eV; and 4) emission microscopy suggests that vertical current flows under the whole drain area, possibly through extended defects. The catastrophic failure occurs at random positions under the drain contact. The time-dependent failure is ascribed to a percolation process activated by the high-electric field that leads to the generation of localized shunt paths between drain and substrate."}
{"_id":"6df975cfe9172e41e5cfb47a52956b78b0c818a1","title":"Document clustering algorithms, representations and evaluation for information retrieval","text":"Digital collections of data continue to grow exponentially as the information age continues to infiltrate every aspect of society. These sources of data take many different forms such as unstructured text on the world wide web, sensor data, images, video, sound, results of scientific experiments and customer profiles for marketing. Clustering is an unsupervised learning approach that groups similar examples together without any human labeling of the data. Due to the very broad nature of this definition, there have been many different approaches to clustering explored in the scientific literature. This thesis addresses the computational efficiency of document clustering in an information retrieval setting. This includes compressed representations, efficient and scalable algorithms, and evaluation with a specific use case for increasing efficiency of a distributed and parallel search engine. Furthermore, it addresses the evaluation of document cluster quality for large scale collections containing millions to billions of documents where complete labeling of a collection is impractical. The cluster hypothesis from information retrieval is also tested using several different approaches throughout the thesis. This research introduces novel approaches to clustering algorithms, document representations, and clustering evaluation. The combination of document clustering algorithms and document representations outlined in this thesis are able to process large scale web search data sets. This has not previously been reported in the literature without resorting to sampling and producing a small number of clusters. Furthermore, the algorithms are able to do this on a single machine. However, they can also function in a distributed and parallel setting to further increase their scalability. This thesis introduces new clustering algorithms that can also be applied to problems outside the information retrieval domain. There are many large data sets being produced from advertising, social networks, videos, images, sound, satellites, sensor data, and a myriad of other sources. Therefore, I anticipate these approaches will become applicable to more domains, as larger data sets become available."}
{"_id":"281c3f3ba4d566775c67a99ce8eed1d68c046b07","title":"A Personalized Company Recommender System for Job Seekers","text":"Our team intends to develop a recommendation system for job seekers based on the information of current employees in big companies. Several models are implemented to achieve over 60% success rate in classifying employees, and we use these models to help job seekers identify their best fitting company."}
{"_id":"ff92f53566fc225e349480439e03e0f34cdf71d2","title":"Spaceborne bi-and multistatic SAR : potential and challenges","text":"Biand multistatic synthetic aperture radar (SAR) operates with distinct transmit and receive antennas that are mounted on separate platforms. Such a spatial separation has several operational advantages, which will increase the capability, reliability and flexibility of future SAR missions. Various spaceborne biand multistatic SAR configurations are introduced, and their potential for different applications such as frequent monitoring, wide-swath imaging, scene classification, single pass cross-track interferometry and resolution enhancement is compared. Furthermore, some major challenges such as phase and time synchronisation, biand multistatic SAR processing, satellite orbit selection and relative position sensing are addressed."}
{"_id":"19db8f6acb84546930c6ba22b6fde1c73cfcc4ba","title":"Joint Multimodal Learning with Deep Generative Models","text":"We investigate deep generative models that can exchange mul tiple modalities bidirectionally, e.g., generating images from correspondin g texts and vice versa. Recently, some studies handle multiple modalities on deep gen erative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we sh ould extract a joint representation that captures high-level concepts among al l modalities and through which we can exchange them bi-directionally. As described h rein, we propose a joint multimodal variational autoencoder (JMVAE), in whicall modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to gene rate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence betwee n JMVAE\u2019s encoder and prepared networks of respective modalities. Our experi ments show that our proposed method can obtain appropriate joint representati o from multiple modalities and that it can generate and reconstruct them more prop erly than conventional VAEs. We further demonstrate that JMVAE can generate multip le modalities bidirectionally."}
{"_id":"6f2cd87c665a034fdc4e51be11b2185ace398d79","title":"Virtual garments : A Fully Geometric Approach for Clothing Design","text":"Modeling dressed characters is known as a very tedious process. It usually requires specifying 2D fabric patterns, positioning and assembling them in 3D, and then performing a physically-based simulation. The latter accounts for gravity and collisions to compute the rest shape of the garment, with the adequate folds and wrinkles. This paper presents a more intuitive way to design virtual clothing. We start with a 2D sketching system in which the user draws the contours and seam-lines of the garment directly on a virtual mannequin. Our system then converts the sketch into an initial 3D surface using an existing method based on a precomputed distance field around the mannequin. The system then splits the created surface into different panels delimited by the seam-lines. The generated panels are typically not developable. However, the panels of a realistic garment must be developable, since each panel must unfold into a 2D sewing pattern. Therefore our system automatically approximates each panel with a developable surface, while keeping them assembled along the seams. This process allows us to output the corresponding sewing patterns. The last step of our method computes a natural rest shape for the 3D garment, including the folds due to the collisions with the body and gravity. The folds are generated using procedural modeling of the buckling phenomena observed in real fabric. The result of our algorithm consists of a realistic looking 3D mannequin dressed in the designed garment and the 2D patterns which can be used for distortion free texture mapping. As we demonstrate, the patterns we create allow us to sew real replicas of the virtual garments."}
{"_id":"60cd946e854e2adf256358d2e5e17b0459ba80c6","title":"Synthetic Population Generation Without a Sample","text":""}
{"_id":"554195d80bd87bbad5856ce5b7d166b147272e26","title":"A Low-energy, Multi-copy Inter-contact Routing Protocol for Disaster Response Networks","text":"This paper presents a novel multi-copy routing protocol for disruption-tolerant networks whose objective is to minimize energy expended on communication. The protocol is designed for disaster-response applications, where power and infrastructure resources are disrupted. Unlike other delay-tolerant networks, energy is a vital resource in post-disaster scenarios to ensure availability of (disruption-tolerant) communication until infrastructure is restored. Our approach exploits naturally recurrent mobility and contact patterns in the network, formed by rescue workers, volunteers, survivors, and their (possibly stranded) vehicles to reduce the number of message copies needed to attain an adequate delivery ratio in the face of disconnection and intermittent connectivity. A new notion of inter-contact routing is proposed that allows estimating route delays and delivery probabilities, identifying more reliable routes and controlling message replication and forwarding accordingly. We simulate the scheme using a mobility model that reflects recurrence inspired by disaster scenarios, and compare our results to previous DTN routing techniques. The evaluation shows that the new approach reduces the resource overhead per message over previous approaches while maintaining a comparable delivery ratio at the expense of a small (bounded) increase in latency."}
{"_id":"9b3d37789cfc45affb6eae5bbc3b13fa2ff8afb8","title":"Emotion detection in suicide notes","text":"0957-4174\/$ see front matter 2013 Elsevier Ltd. All rights reserved. http:\/\/dx.doi.org\/10.1016\/j.eswa.2013.05.050 \u21d1 Corresponding author at: LT3 Language and Translation Technology Team, University College Ghent, Groot-Brittanni\u00eblaan 45, 9000 Ghent, Belgium. Tel.: +32 9 224 97 53. E-mail addresses: bart.desmet@hogent.be (B. Desmet), veronique.hoste@ hogent.be (V. Hoste). Bart Desmet a,b,\u21d1, V\u00e9ronique Hoste a,c"}
{"_id":"2eb8df51ba9ed9daa072b98e5b83a05aa8691f1c","title":"An improved MPPT technique for high gain DC-DC converter using model predictive control for photovoltaic applications","text":"This paper presents an enhanced Maximum Power Point Tracking (MPPT) of Photovoltaic (PV) systems by means of Model Predictive Control (MPC) techniques. The PV array can feed power to the load through a DC\/DC converter boosting the output voltage. Due to stochastic behavior of solar energy, MPPT control technique of PV arrays is required to operate at maximum power point. Extracting the maximum power from PV systems has been widely investigated within the literature. The main contribution of this paper is enhancement of the Incremental Conductance (INC) method through a fixed step predictive control under measured fast solar radiation variation. The proposed predictive control to achieve Maximum Power Point (MPP) speeds up the control loop since it predicts error before the switching signal is applied to the selected high gain multilevel DC-DC converter. Comparing the developed technique to the conventional INC method shows significant improvement in PV system performance. Experimental validation is presented using the dSpace CP 1103 to implement the proposed MPC-MPPT."}
{"_id":"e78e8053d9e0dd76c749f6ab3ca3924071863c1a","title":"Islanding detection of active distribution system with parallel inverters","text":"Average absolute frequency deviation value (AFDVavg) based active islanding detection technique is recently introduced for islanding detection of active distribution system with a single active DG. This paper implements, evaluates and analyze the performance of AFDVavg technique for active distribution system with parallel inverter. The main focus is on islanding detection of an ADN with parallel inverters including the worst case i.e. zero power imbalance condition. The case study is done on an ADN which is energized by two parallel inverter based DGs. Worst case of islanding has been simulated successfully by adjusting the equivalent load of ADN to match the total generation. The resultant quality factor of the equivalent load is kept 1 for testing islanding. Computer simulations are done in MATLAB."}
{"_id":"61d76e585ed1ef137a5f0b86a9869134cbcc8122","title":"Integration of open source platform duckietown and gesture recognition as an interactive interface for the museum robotic guide","text":"In recent years, population aging becomes a serious problem. To decrease the demand for labor when navigating visitors in museums, exhibitions, or libraries, this research designs an automatic museum robotic guide which integrates image and gesture recognition technologies to enhance the guided tour quality of visitors. The robot is a self-propelled vehicle developed by ROS (Robot Operating System), in which we achieve the automatic driving based on the function of lane-following via image recognition. This enables the robot to lead guests to visit artworks following the preplanned route. In conjunction with the vocal service about each artwork, the robot can convey the detailed description of the artwork to the guest. We also design a simple wearable device to perform gesture recognition. As a human machine interface, the guest is allowed to interact with the robot by his or her hand gestures. To improve the accuracy of gesture recognition, we design a two phase hybrid machine learning-based framework. In the first phase (or training phase), k-means algorithm is used to train historical data and filter outlier samples to prevent future interference in the recognition phase. Then, in the second phase (or recognition phase), we apply KNN (k-nearest neighboring) algorithm to recognize the hand gesture of users in real time. Experiments show that our method can work in real time and get better accuracy than other methods."}
{"_id":"27db31b3e29cbdb19a91beedd3cdfdda46ce1fd1","title":"Stochastic policy gradient reinforcement learning on a simple 3D biped","text":"We present a learning system which is able to quickly and reliably acquire a robust feedback control policy for 3D dynamic walking from a blank-slate using only trials implemented on our physical robot. The robot begins walking within a minute and learning converges in approximately 20 minutes. This success can be attributed to the mechanics of our robot, which are modeled after a passive dynamic walker, and to a dramatic reduction in the dimensionality of the learning problem. We reduce the dimensionality by designing a robot with only 6 internal degrees of freedom and 4 actuators, by decomposing the control system in the frontal and sagittal planes, and by formulating the learning problem on the discrete return map dynamics. We apply a stochastic policy gradient algorithm to this reduced problem and decrease the variance of the update using a state-based estimate of the expected cost. This optimized learning system works quickly enough that the robot is able to continually adapt to the terrain as it walks."}
{"_id":"dbe1cb8e323da5bf045b51534e7ed06e69ca53df","title":"Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering","text":"While increasingly complex approaches to question answering (QA) have been proposed, the true gain of these systems, particularly with respect to their expensive training requirements, can be in- flated when they are not compared to adequate baselines. Here we propose an unsupervised, simple, and fast alignment and informa- tion retrieval baseline that incorporates two novel contributions: a one-to-many alignment between query and document terms and negative alignment as a proxy for discriminative information. Our approach not only outperforms all conventional baselines as well as many supervised recurrent neural networks, but also approaches the state of the art for supervised systems on three QA datasets. With only three hyperparameters, we achieve 47% P@1 on an 8th grade Science QA dataset, 32.9% P@1 on a Yahoo! answers QA dataset and 64% MAP on WikiQA."}
{"_id":"ae0ea38bee64d91c62200eb9bd4922c326be6b35","title":"MPPT of photovoltaic systems using extremum - seeking control","text":"A stability analysis for a maximum power point tracking (MPPT) scheme based on extremum-seeking control is developed for a photovoltaic (PV) array supplying a dc-to-dc switching converter. The global stability of the extremum-seeking algorithm is demonstrated by means of Lyapunov's approach. Subsequently, the algorithm is applied to an MPPT system based on the \"perturb and observe\" method. The steady-state behavior of the PV system with MPPT control is characterized by a stable oscillation around the maximum power point. The tracking algorithm leads the array coordinates to the maximum power point by increasing or decreasing linearly with time the array voltage. Off-line measurements are not required by the control law, which is implemented by means of an analog multiplier, standard operational amplifiers, a flip-flop circuit and a pulsewidth modulator. The effectiveness of the proposed MPPT scheme is demonstrated experimentally under different operating conditions."}
{"_id":"f5b6dcf63c721869b513d9ba5995c4508414eade","title":"Systematic Risk in Supply Chain Networks","text":"I production output is generally correlated with the state of the economy. Nonetheless, during times of economic downturn, some industries take the biggest hit, whereas at times of economic boom they reap most benefits. To provide insight into this phenomenon, we map supply networks of industries and firms and investigate how the supply network structure mediates the effect of economy on industry or firm sales. Previous research has shown that retail sales are correlated with the state of the economy. Since retailers source their products from other industries, the sales of their suppliers can also be correlated with the state of the economy. This correlation represents the source of systematic risk for an industry that propagates through a supply chain network. Specifically, we identify the following mechanisms that can affect the correlation between sales and the state of the economy in a supply chain network: propagation of systematic risk into production decisions, aggregation of orders from multiple customers in a supply chain network, and aggregation of orders over time. We find that the first effect does not amplify the correlation; however, the latter two intensify correlation and result in the amplification of correlation upstream in supply networks. We demonstrate three managerial implications of this phenomenon: implications for the cost of capital, for the risk-adjusted valuation of supply chain improvement projects, and for supplier selection and risk. Data, as supplemental material, are available at http:\/\/dx.doi.org\/10.1287\/mnsc.2015.2187."}
{"_id":"36fb553aa996885017afe3489a8377eceddc08ee","title":"Planning-based prediction for pedestrians","text":"We present a novel approach for determining robot movements that efficiently accomplish the robot's tasks while not hindering the movements of people within the environment. Our approach models the goal-directed trajectories of pedestrians using maximum entropy inverse optimal control. The advantage of this modeling approach is the generality of its learned cost function to changes in the environment and to entirely different environments. We employ the predictions of this model of pedestrian trajectories in a novel incremental planner and quantitatively show the improvement in hindrance-sensitive robot trajectory planning provided by our approach."}
{"_id":"35582a30685083c62dca992553eec44123be9d07","title":"The Weighted Majority Algorithm","text":"We study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials with a prediction to be made in each and the goal of the learner is to make few mistakes We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well but the learner does not know which one A simple and e ective method based on weighted voting is introduced for constructing a compound algorithm in such a circumstance We call this method the Weighted Majority Algorithm We show that this algorithm is robust in the presence of errors in the data We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool For example given a sequence of trials if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c log jAj m mistakes on that sequence where c is xed constant"}
{"_id":"7dc20c32ea644047aa8989f407a6a93571d1013f","title":"A Novel Approach for Network Attack Classification Based on Sequential Questions","text":"With the development of incipient technologies, user devices becoming more exposed and ill-used by foes. In upcoming decades, traditional security measures will not be sufficient enough to handle this huge threat towards distributed hardware and software. Lack of standard network attack taxonomy has become an indispensable dispute on developing a clear understanding about the attacks in order to have an operative protection mechanism. Present attack categorization techniques protect a specific group of threat which has either messed the entire taxonomy structure or ambiguous when one network attacks get blended with few others attacks. Hence, this raises concerns about developing a common and general purpose taxonomy. In this study, a sequential question-answer based model of categorization is proposed. In this article, an intrusion detection framework and threat grouping schema are proposed on the basis of four sequential questions (\u201cWho\u201d, \u201cWhere\u201d, \u201cHow\u201d and \u201cWhat\u201d). We have used our method for classifying traditional network attacks in order to identify initiator, source, attack style and seriousness of an attack. Another focus of the paper is to provide a preventive list of actions for network administrator as a guideline to reduce overall attack consequence. Recommended taxonomy is designed to detect common attacks rather than any particular type of attack which can have a practical effect in real life attack classification. From the analysis of the classifications obtained from few infamous attacks, it is obvious that the proposed system holds certain benefits related to the prevailing taxonomies. Future research directions have also been well acknowledged."}
{"_id":"85c3d578718efda398973861438e8ab6dfc13b8f","title":"Community Detection in Temporal Networks","text":"Many complex systems in nature, society and technologyfrom the online social networks to the internet from the nervous system to power gridscan be represented as a graph of vertices interconnected by edges. Small world network, Scale free network, Community detection are fundamental properties of complex networks. Community is a sub graph with densely connected nodes typically reveals topological relations, network structure, organizational and functional characteristics of the underlying network, e.g., friendships on Facebook, Followers of a VIP account on Twitter, and interaction with business professionals on LinkedIn. Online social networks are dynamic in nature, they evolve in size and space rapidly. In this paper we are detecting incremental disjoint communities using dyanamic multi label propagation algorithm. It tackles the temporal event changes, i.e., addition, deletion of an edge or vertex in a sub graph for every timestamp. The experimental results on Enron real network dataset shows that the proposed method is quite well in identifying communities."}
{"_id":"7894683e9f0108245d43c3de91a3426e52e0d27f","title":"GMove: Group-Level Mobility Modeling Using Geo-Tagged Social Media","text":"Understanding human mobility is of great importance to various applications, such as urban planning, traffic scheduling, and location prediction. While there has been fruitful research on modeling human mobility using tracking data (e.g., GPS traces), the recent growth of geo-tagged social media (GeoSM) brings new opportunities to this task because of its sheer size and multi-dimensional nature. Nevertheless, how to obtain quality mobility models from the highly sparse and complex GeoSM data remains a challenge that cannot be readily addressed by existing techniques. We propose GMove, a group-level mobility modeling method using GeoSM data. Our insight is that the GeoSM data usually contains multiple user groups, where the users within the same group share significant movement regularity. Meanwhile, user grouping and mobility modeling are two intertwined tasks: (1) better user grouping offers better within-group data consistency and thus leads to more reliable mobility models; and (2) better mobility models serve as useful guidance that helps infer the group a user belongs to. GMove thus alternates between user grouping and mobility modeling, and generates an ensemble of Hidden Markov Models (HMMs) to characterize group-level movement regularity. Furthermore, to reduce text sparsity of GeoSM data, GMove also features a text augmenter. The augmenter computes keyword correlations by examining their spatiotemporal distributions. With such correlations as auxiliary knowledge, it performs sampling-based augmentation to alleviate text sparsity and produce high-quality HMMs.\n Our extensive experiments on two real-life data sets demonstrate that GMove can effectively generate meaningful group-level mobility models. Moreover, with context-aware location prediction as an example application, we find that GMove significantly outperforms baseline mobility models in terms of prediction accuracy."}
{"_id":"5f173fc75106f8234f41890b61567db851b256c2","title":"RN to BSN Transition: A Concept Analysis.","text":"Over 670,000 ADN- and diploma-prepared nurses will need to complete their BSN degrees to meet the Institute of Medicine's recommendation that at least 80% of registered nurses (RNs) be BSN-prepared by year 2020. Understanding motivators, barriers, and the transition experience for RNs to advance their degree will help educators and nurse leaders understand the importance of a partnership to educate and mentor RNs to pursue a BSN degree."}
{"_id":"51cdd25a38c4daa3d57e57218310410b383add73","title":"Design Considerations of Charge Pump for Antenna Switch Controller With SOI CMOS Technology","text":"An enhanced charge pump for the antenna switch controller using the silicon-on-insulator (SOI) CMOS technology is presented in this brief. The pseudo cross-coupled technique is proposed to reduce parasitic capacitances at charging\/discharging nodes through charge transferring paths, which improves the current drive capability and provides better accuracy of the output voltage. Furthermore, the codesign between the gate control voltages of power MOS transistors and the clock drive signals of pumping capacitors has been investigated to eliminate the reversion loss and reduce the ripple voltage. The pseudo cross-coupled charge pump has been fabricated in the 0.18-  $\\mu\\text{m}$ SOI CMOS technology with an area of 0.065 mm2. According to the comparison results of the conventional and enhanced charge pumps, the start-up time and the recovery time are typically shortened by 71.4% and 21.7%, owing to the improvement of the current drive capability, and the ripple voltage at no-load condition is greatly reduced by 46.1%."}
{"_id":"8da1dda34ecc96263102181448c94ec7d645d085","title":"Approximation by superpositions of a sigmoidal function","text":"Abstr,,ct. In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set ofaffine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single bidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."}
{"_id":"9e4291de6cdce8e6f247effa308d72e2ec3f6122","title":"Estimating the dimension of a linear model","text":""}
{"_id":"20c18a6895130ba6617ce6a59b8a0eb8173981e7","title":"Satellite Image Analysis for Disaster and Crisis-Management Support","text":"This paper describes how multisource satellite data and efficient image analysis may successfully be used to conduct rapid-mapping tasks in the domain of disaster and crisis-management support. The German Aerospace Center (DLR) has set up a dedicated crosscutting service, which is the so-called \"Center for satellite-based Crisis Information\" (ZKI), to facilitate the use of its Earth-observation capacities in the service of national and international response to major disaster situations, humanitarian relief efforts, and civil security issues. This paper describes successful rapid satellite mapping campaigns supporting disaster relief and demonstrates how this technology can be used for civilian crisis-management purposes. During the last years, various international coordination bodies were established, improving the disaster-response-related cooperation within the Earth-observation community worldwide. DLR\/ZKI operates in this context, closely networking with public authorities (civil security), nongovernmental organizations (humanitarian relief organizations), satellite operators, and other space agencies. This paper reflects on several of these international activities, such as the International Charter Space and Major Disasters, describes mapping procedures, and reports on rapid-mapping experiences gained during various disaster-response applications. The example cases presented cover rapid impact assessment after the Indian Ocean Tsunami, forest fires mapping for Portugal, earthquake-damage assessment for Pakistan, and landslide extent mapping for the Philippines"}
{"_id":"2074d749e61c4b1ccbf957fe18efe00fa590a124","title":"Toward a Science of Cyber\u2013Physical System Integration","text":"System integration is the elephant in the china store of large-scale cyber-physical system (CPS) design. It would be hard to find any other technology that is more undervalued scientifically and at the same time has bigger impact on the presence and future of engineered systems. The unique challenges in CPS integration emerge from the heterogeneity of components and interactions. This heterogeneity drives the need for modeling and analyzing cross-domain interactions among physical and computational\/networking domains and demands deep understanding of the effects of heterogeneous abstraction layers in the design flow. To address the challenges of CPS integration, significant progress needs to be made toward a new science and technology foundation that is model based, precise, and predictable. This paper presents a theory of composition for heterogeneous systems focusing on stability. Specifically, the paper presents a passivity-based design approach that decouples stability from timing uncertainties caused by networking and computation. In addition, the paper describes cross-domain abstractions that provide effective solution for model-based fully automated software synthesis and high-fidelity performance analysis. The design objectives demonstrated using the techniques presented in the paper are group coordination for networked unmanned air vehicles (UAVs) and high-confidence embedded control software design for a quadrotor UAV. Open problems in the area are also discussed, including the extension of the theory of compositional design to guarantee properties beyond stability, such as safety and performance."}
{"_id":"2cb46d5cab5590ef9950bd303bdfae41e7a98b1a","title":"An evaluation of alternative architectures for transaction processing in the cloud","text":"Cloud computing promises a number of advantages for the deployment of data-intensive applications. One important promise is reduced cost with a pay-as-you-go business model. Another promise is (virtually) unlimited throughput by adding servers if the workload increases. This paper lists alternative architectures to effect cloud computing for database applications and reports on the results of a comprehensive evaluation of existing commercial cloud services that have adopted these architectures. The focus of this work is on transaction processing (i.e., read and update workloads), rather than analytics or OLAP workloads, which have recently gained a great deal of attention. The results are surprising in several ways. Most importantly, it seems that all major vendors have adopted a different architecture for their cloud services. As a result, the cost and performance of the services vary significantly depending on the workload."}
{"_id":"271c41d1584938f618690151eb391eb3545d182e","title":"Integrated Dual-Channel X-Band Offset-Transmitter for Phase Steering and DDMA Arrays","text":"Multiple-Input and Multiple-Output (MIMO) arrays provide more degrees of freedom than conventional phased arrays. They require that every transmitting element of the array can be identified when received. One way to achieve this is to give every element its own unique frequency. Offset-transmitters may be used to introduce MIMO or Doppler Division Multiple Access (DDMA) into phased-arrays without an excessive increase in waveform-generating hardware. Our dual-channel demonstrator IC can obtain a phase accuracy better than 1 degree and an spurious level of better than -65dBc for a single on-chip channel. This work investigates at X-band the effects of the limited on-chip isolation of 35dB, when multiple offset outputs are generated on a single chip for both beam steering and DDMA. In case of beam steering, the requirements on channel-to-channel isolation are less strict, making it well within reach. In the case of DDMA, we recommend increasing the channel-to-channel isolation by implementing multiple chips, in which case independent signals can be generated."}
{"_id":"480b1a47373a9791947da2b83224d2f2bd833e7d","title":"Semi-supervised Semantic Pattern Discovery with Guidance from Unsupervised Pattern Clusters","text":"We present a simple algorithm for clustering semantic patterns based on distributional similarity and use cluster memberships to guide semi-supervised pattern discovery. We apply this approach to the task of relation extraction. The evaluation results demonstrate that our novel bootstrapping procedure significantly outperforms a standard bootstrapping. Most importantly, our algorithm can effectively prevent semantic drift and provide semi-supervised learning with a natural stopping criterion."}
{"_id":"a5e6c14a9335e7bcd981c3ad67cc73af2474b136","title":"A Theoretical Basis for a Biopharmaceutic Drug Classification: The Correlation of in Vitro Drug Product Dissolution and in Vivo Bioavailability","text":"A biopharmaceutics drug classification scheme for correlating in vitro drug product dissolution and in vivo bioavailability is proposed based on recognizing that drug dissolution and gastrointestinal permeability are the fundamental parameters controlling rate and extent of drug absorption. This analysis uses a transport model and human permeability results for estimating in vivo drug absorption to illustrate the primary importance of solubility and permeability on drug absorption. The fundamental parameters which define oral drug absorption in humans resulting from this analysis are discussed and used as a basis for this classification scheme. These Biopharmaceutic Drug Classes are defined as: Case 1. High solubility-high permeability drugs, Case 2. Low solubility-high permeability drugs, Case 3. High solubility-low permeability drugs, and Case 4. Low solubility-low permeability drugs. Based on this classification scheme, suggestions are made for setting standards for in vitro drug dissolution testing methodology which will correlate with the in vivo process. This methodology must be based on the physiological and physical chemical properties controlling drug absorption. This analysis points out conditions under which no in vitro-in vivo correlation may be expected e.g. rapidly dissolving low permeability drugs. Furthermore, it is suggested for example that for very rapidly dissolving high solubility drugs, e.g. 85% dissolution in less than 15 minutes, a simple one point dissolution test, is all that may be needed to insure bioavailability. For slowly dissolving drugs a dissolution profile is required with multiple time points in systems which would include low pH, physiological pH, and surfactants and the in vitro conditions should mimic the in vivo processes. This classification scheme provides a basis for establishing in vitro-in vivo correlations and for estimating the absorption of drugs based on the fundamental dissolution and permeability properties of physiologic importance."}
{"_id":"cd4235fe314179bb2035af3aff81d29b9150b898","title":"A Survey of Enabling Technologies of Low Power and Long Range Machine-to-Machine Communications","text":"Low power and long range machine-to-machine (M2M) communication techniques are expected to provide ubiquitous connections for the wireless devices. In this paper, three major low power and long range M2M solutions are surveyed. The first type of solutions is referred to as the low power wide area (LPWA) network. The design of the LPWA techniques features low cost, low data rate, long communication range, and low power consumption. The second type of solutions is the IEEE 802.11ah which features higher data rates using a wider bandwidth than the LPWA-based solutions. The third type of solutions is operated under the cellular network infrastructure. Based on the analysis of the pros and cons of the enabling technologies of the surveyed M2M solutions, as well as the corresponding deployment strategies, the gaps in knowledge are identified. The paper also presents a summary of the research directions for improving the performance of the surveyed low power and long range M2M communication technologies."}
{"_id":"c093e12d8c95abb49532d2f0dfd04417864942d5","title":"A New Bit-Serial Architecture for Field Multiplication Using Polynomial Bases","text":"Multiplication is the main finite field arithmetic operation in elliptic curve cryptography and its bit-serial hardware implementation is attractive in resource constrained environments such as smart cards, where the chip area is limited. In this paper, a new serial-output bitserial multiplier using polynomial bases over binary extension fields is proposed. It generates a bit of the multiplication in each clock cycle with the latency of one cycle. To the best of our knowledge, this is the first time that such a serial-output bit-serial multiplier architecture using polynomial bases for general irreducible polynomials is proposed."}
{"_id":"72118216235f3875078a3c8696e2abd4273bc3b8","title":"DBpedia SPARQL Benchmark - Performance Assessment with Real Queries on Real Data","text":"Triple stores are the backbone of increasingly many Data Web applications. It is thus evident that the performance of those stores is mission critical for individual projects as well as for data integration on the Data Web in general. Consequently, it is of central importance during the implementation of any of these applications to have a clear picture of the weaknesses and strengths of current triple store implementations. In this paper, we propose a generic SPARQL benchmark creation procedure, which we apply to the DBpedia knowledge base. Previous approaches often compared relational and triple stores and, thus, settled on measuring performance against a relational database which had been converted to RDF by using SQL-like queries. In contrast to those approaches, our benchmark is based on queries that were actually issued by humans and applications against existing RDF data not resembling a relational schema. Our generic procedure for benchmark creation is based on query-log mining, clustering and SPARQL feature analysis. We argue that a pure SPARQL benchmark is more useful to compare existing triple stores and provide results for the popular triple store implementations Virtuoso, Sesame, Jena-TDB, and BigOWLIM. The subsequent comparison of our results with other benchmark results indicates that the performance of triple stores is by far less homogeneous than suggested by previous benchmarks."}
{"_id":"1b35cbf853dd8becf8777aa224896daf4ee4d076","title":"Bioinspired engineering of thermal materials.","text":"In the development of next-generation materials with enhanced thermal properties, biological systems in nature provide many examples that have exceptional structural designs and unparalleled performance in their thermal or nonthermal functions. Bioinspired engineering thus offers great promise in the synthesis and fabrication of thermal materials that are difficult to engineer through conventional approaches. In this review, recent progress in the emerging area of bioinspired advanced materials for thermal science and technology is summarized. State-of-the-art developments of bioinspired thermal-management materials, including materials for efficient thermal insulation and heat transfer, and bioinspired materials for thermal\/infrared detection, are highlighted. The dynamic balance of bioinspiration and practical engineering, the correlation of inspiration approaches with the targeted applications, and the coexistence of molecule-based inspiration and structure-based inspiration are discussed in the overview of the development. The long-term outlook and short-term focus of this critical area of advanced materials engineering are also presented."}
{"_id":"3f530ad817fb4597a1b755ae98daa839a167f2b9","title":"Empirical Study on the Healing Nature of Mandalas","text":"Mandalas were first used in therapy by Carl Jung, who found that the act of drawing mandalas had a calming effect on patients while at the same time facilitating psychic integration. There is a scarcity of controlled empirical studies of the healing impact of mandalas on mental health. Based on the efficacy of James Pennebaker\u2019s written disclosure paradigm in promoting mental well-being (Pennebaker, 1997a, 1997b), the purpose of our study was to examine the benefits for those suffering from post traumatic stress disorder (PTSD) of processing traumatic events through the creation of mandalas. Benefits to participants were measured in terms of changes in the variables of PTSD symptoms, depressive symptoms, anxiety, spiritual meaning, and the frequency of physical symptoms and illness. Relative to those in the control condition, individuals assigned to the experimental mandala-creation group reported greater decreases in symptoms of trauma at the 1-month follow up. There were no other statistically significant outcome differences. Alternative modes of processing traumatic events (e.g., visually symbolically) may serve individuals who are either reluctant or unable to write about their experiences."}
{"_id":"621479624ad634a7ed86977aacd780339af91ec7","title":"Improving the Performance of the FFT-based Parallel Code-phase Search Acquisition of GNSS Signals by Decomposition of the Circular Correlation","text":"Dr. Cyril Botteron leads the GNSS and UWB groups in the electronics and signal processing laboratory at EPFL. He received his PhD degree from the University of Calgary, Canada, in 2003. His current research interests comprise the development of low power radio frequency (RF) integrated circuits and advanced signal processing techniques for ultra-low power communications and global and local positioning applications."}
{"_id":"060c5f6e70c552bb6cb387713e3776af98f7a69b","title":"ButterflyNet: a mobile capture and access system for field biology research","text":"Through a study of field biology practices, we observed that biology fieldwork generates a wealth of heterogeneous information, requiring substantial labor to coordinate and distill. To manage this data, biologists leverage a diverse set of tools, organizing their effort in paper notebooks. These observations motivated ButterflyNet, a mobile capture and access system that integrates paper notes with digital photographs captured during field research. Through ButterflyNet, the activity of leafing through a notebook expands to browsing all associated digital photos. ButterflyNet also facilitates the transfer of captured content to spreadsheets, enabling biologists to share their work. A first-use study with 14 biologists found this system to offer rich data capture and transformation, in a manner felicitous with current practice."}
{"_id":"72f95251e642eee833396d4271daa2bf0d7e074f","title":"An extra low-power 1Tbit\/s bandwidth PLL\/DLL-less eDRAM PHY using 0.3V low-swing IO for 2.5D CoWoS application","text":"A 1Tbit\/s bandwidth PHY is demonstrated through 2.5D CoWoS platform. Two chips: SOC and eDRAM have been fabricated in TSMC 40nm CMOS technology and stacked on another silicon interposer chip in 65nm technology. Total 1024 DQ bus operating in 1.1Gbit\/s with Vmin=0.3V are proven in experimental results. A novel timing compensation mechanism is presented to achieve a low-power and small area eDRAM PHY that excludes PLL\/DLL but retains good timing margin. Another data sampling alignment training approach is reserved to enhance timing robustness. A compact low-swing IO also achieves great power efficiency of 0.105mW\/Gbps."}
{"_id":"d8f080bb2b888f1c19478ddf7ffa95f0cf59a708","title":"Closed-Loop Control of a Three-Phase Neutral-Point-Clamped Inverter Using an Optimized Virtual-Vector-Based Pulsewidth Modulation","text":"This paper presents a closed-loop control scheme for the three-level three-phase neutral-point-clamped dc-ac converter using the optimized nearest three virtual-space-vector pulsewidth modulation, which is a modulation that produces low output-voltage distortion with a significant reduction of the dc-link capacitance. A new specific loop modifying the modulating waveforms is proposed to rapidly control possible perturbations in the neutral-point voltage balance. An online estimation of the load displacement angle and load linear\/nonlinear nature is introduced at no extra cost. The remaining part of the control is analogous to the control for a two-level converter with an appropriate interfacing to the selected modulation. The closed-loop control is designed for the case of a renewable-energy source connected to the ac mains, and its performance is analyzed through simulation and experiments."}
{"_id":"68ab0f936e22acb28a1fdb617b9960b2d799c93a","title":"From single cells to deep phenotypes in cancer","text":"In recent years, major advances in single-cell measurement systems have included the introduction of high-throughput versions of traditional flow cytometry that are now capable of measuring intracellular network activity, the emergence of isotope labels that can enable the tracking of a greater variety of cell markers and the development of super-resolution microscopy techniques that allow measurement of RNA expression in single living cells. These technologies will facilitate our capacity to catalog and bring order to the inherent diversity present in cancer cell populations. Alongside these developments, new computational approaches that mine deep data sets are facilitating the visualization of the shape of the data and enabling the extraction of meaningful outputs. These applications have the potential to reveal new insights into cancer biology at the intersections of stem cell function, tumor-initiating cells and multilineage tumor development. In the clinic, they may also prove important not only in the development of new diagnostic modalities but also in understanding how the emergence of tumor cell clones harboring different sets of mutations predispose patients to relapse or disease progression."}
{"_id":"9d4b4bb9cb11378e82946c0b80d9eba0c8ca351c","title":"Sequence Covering for Efficient Host-Based Intrusion Detection","text":"This paper introduces a new similarity measure, the covering similarity, which we formally define for evaluating the similarity between a symbolic sequence and a set of symbolic sequences. A pairwise similarity can also be directly derived from the covering similarity to compare two symbolic sequences. An efficient implementation to compute the covering similarity is proposed which uses a suffix-tree data structure, but other implementations, based on suffix array for instance, are possible and are possibly necessary for handling very large-scale problems. We have used this similarity to isolate attack sequences from normal sequences in the scope of host-based intrusion detection. We have assessed the covering similarity on two well-known benchmarks in the field. In view of the results reported on these two datasets for the state-of-the-art methods, according to the comparative study, we have carried out based on three challenging similarity measures commonly used for string processing, or in bioinformatics, we show that the covering similarity is particularly relevant to address the detection of anomalies in sequences of system calls."}
{"_id":"cc63ddfadf41bbcbfe89b3c0ff9f9a4868cd9b0c","title":"Qualitative clinical evaluation of scapular dysfunction: a reliability study.","text":"The purpose of this study was to determine the intrarater and interrater reliability of a clinical evaluation system for scapular dysfunction. No commonly accepted terminology presently exists for describing the abnormal dynamic scapular movement patterns that are commonly associated with shoulder injury. A method of observation was devised for clinical evaluation of scapular dysfunction. Blinded evaluators (2 physicians and 2 physical therapists) were familiarized with the evaluation method of scapular movement patterns before viewing a videotape of 26 subjects with and without scapular dysfunction. Each evaluator was asked to categorize the predominant scapular movement pattern observed during bilateral humeral scaption and abduction motions. Reliability was assessed by a kappa coefficient. Intertester reliability (kappa = 0.4) was found to be slightly lower than intratester reliability (kappa = 0.5). These results indicate that, with refinement, this qualitative evaluation method may allow clinicians to standardize the categorization of dynamic scapular dysfunction patterns."}
{"_id":"9b71b307a2f99fb404c6f6159b146547a0dc1cbc","title":"DART: a Dataset of Arguments and their Relations on Twitter","text":"The problem of understanding the stream of messages exchanged on social media such as Facebook and Twitter is becoming a major challenge for automated systems. The tremendous amount of data exchanged on these platforms as well as the specific form of language adopted by social media users constitute a new challenging context for existing argument mining techniques. In this paper, we describe a resource of natural language arguments called DART (Dataset of Arguments and their Relations on Twitter) where the complete argument mining pipeline over Twitter messages is considered: (i) we identify which tweets can be considered as arguments and which cannot, and (ii) we identify what is the relation, i.e., support or attack, linking such tweets to each other."}
{"_id":"cec6a3cc35d3b08c5decc55987a8cf84f0820af8","title":"I feel your pain: emotional closeness modulates neural responses to empathically experienced rejection.","text":"Empathy is generally thought of as the ability to share the emotional experiences of others. In scientific terms, this is usually operationalized as an ability to vicariously feel others' mental and emotional experiences. Supporting this account, research demonstrates that watching others experience physical pain activates similar brain regions to the actual experience of pain itself. First-hand experience of social rejection also activates this network. The current work extends these findings by examining whether the \"pain\" network is similarly implicated in witnessing rejection, and whether emotional closeness modulates this response. We provide evidence for each of these suppositions, demonstrating: (a) that the pain network is activated when watching a friend suffer social rejection, and (b) that interpersonal closeness with that friend modulates this response. Further, we found that the inferior frontal gyrus, critical for representing others' mental and emotional states, mediates the relationship between emotional closeness and neural responses to watching the rejection of a friend."}
{"_id":"52eb8f4aa2ffcdf01f945fc8c3ad466855fd1567","title":"HPI Question Answering System in BioASQ 2016","text":"Question answering (QA) systems are crucial when searching for exact answers for natural language questions in the biomedical domain. Answers to many of such questions can be extracted from the 26 millions biomedical publications currently included in MEDLINE when relying on appropriate natural language processing (NLP) tools. In this work we describe our participation in the task 4b of the BioASQ challenge using two QA systems that we developed for biomedicine. Preliminary results show that our systems achieved first and second positions in the snippet retrieval sub-task and for the generation of ideal answers."}
{"_id":"5a5a51a911e7c5800a8ba518e4347ad3bc91d8cb","title":"The acceptance and use of customer relationship management (CRM) systems: An empirical study of distribution service industry in Taiwan","text":"With the rapid change of business competitive environment, enterprise resource integration and innovative issues of business operation have gradually become the most important issues for businesses. Furthermore, many enterprises have implemented novel information technology and developing the innovative e-business applications systems such as enterprise resource planning (ERP), customer relationship management (CRM), knowledge management (KM) and supply chain management (SCM) to enhance their competitive advantages. CRM systems can help organizations to gain the potential new customers, promote the existing customers\u2019 purchase, maintain good relationship with customers as well as to enhance the customer value, thus can improve the enterprise images. Moreover, the development and applications of CRM systems have also been considered as important issues for researchers and practitioners in recent years. For Taiwan\u2019s industry, it has been gradually transferred from manufacturing-oriented to a service-oriented. Thus, the service industry has higher percentage in the GDP and in which the distribution service industry is the biggest one and be a core industry in the whole of service industry. The main purpose of this study is to explore the factors affecting the acceptance and use of CRM systems. Furthermore, the proposed research model was built on the basis of unified theory of acceptance and use of technology (UTAUT) and task-technology fit (TTF) framework as well as technological and managerial theories. The implications of findings for practice will be discussed. 2010 Elsevier Ltd. All rights reserved."}
{"_id":"6873e61b9a155b6a81c27b09fb8c11c26f5815ca","title":"Spam filtering email classification (SFECM) using gain and graph mining algorithm","text":"This paper proposes a hybrid solution of spam email classifier using context based email classification model as main algorithm complimented by information gain calculation to increase spam classification accuracy. Proposed solution consists of three stages email pre-processing, feature extraction and email classification. Research has found that LingerIG spam filter is highly effective at separating spam emails from cluster of homogenous work emails. Also experiment result proved the accuracy of spam filtering is 100% as recorded by the team of developers at University of Sydney. The study has shown that implementing the spam filter in the context-based email classification model is feasible. Experiment of the study has confirmed that spam filtering aspect of context-based classification model can be improved."}
{"_id":"dbc677c69b3b55da3a817ca6e83f40a8f438dd88","title":"Efficient parallel translating embedding for knowledge graphs","text":"Knowledge graph embedding aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating embedding methods regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph embedding methods. However, a major limitation of these methods is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great difficulty in practical applications. In this paper, we propose an efficient parallel framework for translating embedding methods, called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating embedding methods, i.e., TransE [3], TransH [19], and a more efficient variant TransE- AdaGrad [11] validate that ParTrans-X can speed up the training process by more than an order of magnitude."}
{"_id":"33ffc82ea3d8708ed9037debc3f1d4f9e9e49269","title":"Characterizing the software process: a maturity framework","text":"A description is given of a software-process maturity framework that has been developed to provide the US Department of Defense with a means to characterize the capabilities of software-development organizations. This software-development process-maturity model reasonably represents the actual ways in which software-development organizations improve. It provides a framework for assessing these organizations and identifying the priority areas for immediate improvement. It also helps identify those places where advanced technology can be most valuable in improving the software-development process. The framework can be used by any software organization to assess its own capabilities and identify the most important areas for improvement.<<ETX>>"}
{"_id":"7588c5f7cc699d7a071e85e08eb98d514e9c73f8","title":"Capability Maturity Model for Software","text":"This paper provides an overview of the latest version of the Capability Maturity Model for Software, CMM v1.1. Based on over six years of experience with software process improvement and the contributions of hundreds of reviewers, CMM v1.1 describes the software engineering and management practices that characterize organizations as they mature their processes for developing and maintaining software. This paper stresses the need for a process maturity framework to prioritize improvement actions, describes the process maturity framework of five maturity levels and the associated structural components, and discusses future directions for the CMM."}
{"_id":"f22a4acdd90388f386acaf0589f17731e0cf5cfa","title":"Climbing the \"Stairway to Heaven\" -- A Mulitiple-Case Study Exploring Barriers in the Transition from Agile Development towards Continuous Deployment of Software","text":"Agile software development is well-known for its focus on close customer collaboration and customer feedback. In emphasizing flexibility, efficiency and speed, agile practices have lead to a paradigm shift in how software is developed. However, while agile practices have succeeded in involving the customer in the development cycle, there is an urgent need to learn from customer usage of software also after delivering and deployment of the software product. The concept of continuous deployment, i.e. the ability to deliver software functionality frequently to customers and subsequently, the ability to continuously learn from real-time customer usage of software, has become attractive to companies realizing the potential in having even shorter feedback loops. However, the transition towards continuous deployment involves a number of barriers. This paper presents a multiple-case study in which we explore barriers associated with the transition towards continuous deployment. Based on interviews at four different software development companies we present key barriers in this transition as well as actions that need to be taken to address these."}
{"_id":"98354bb3d015d684d9248589191367fd7069cbc6","title":"Photogrammetric Multi-View Stereo and Imaging Network Design","text":""}
{"_id":"e2e194b5103e233ebe47cf6d1e5bc31e3981fd73","title":"A Multi-User Surface Visuo-Haptic Display Using Electrostatic Friction Modulation and Capacitive-Type Position Sensing","text":"This paper proposes a visuo-haptic feedback system that can provide haptic feedback to multiple users on an LCD monitor using electrostatic adhesion and built-in capacitive sensors. The prototype system consists of a 40-inch LCD monitor, an ITO electrode sheet arranged on the monitor, and multiple contact pads for electrostatic adhesion. By applying low-frequency haptic voltage and high-frequency sensing voltage to each pad, the system realizes passive haptic feedback, as well as position sensing using the same components. The passive haptic feedback force exceeds 1 N at 300 V<inline-formula><tex-math notation=\"LaTeX\">$_\\mathrm{rms}$<\/tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"nakamura-ieq1-2556660.gif\"\/><\/alternatives><\/inline-formula>. The performance of the sensing system is investigated in terms of interference, which shows the haptic voltage and existence of multiple pads can affect the sensing and can shift the output by a few millimeters. A pilot application, virtual hockey game, successfully demonstrates the sensing and haptic rendering capability of the proposed system. The effect of the interference on the demonstration is discussed."}
{"_id":"8efbc664db0f6d0286daefaf9003d068ec1cbaa9","title":"Music, Intelligence and Artificiality","text":"The discipline of Music-AI is defined as that activity which seeks to program computers to perform musical tasks in an intelligent, which possibly means humanlike way. A brief historical survey of different approaches within the discipline is presented. Two particular issues arise: the explicit representation of knowledge; and symbolic and subsymbolic representation and processing. When attempting to give a precise definition of Music-AI, it is argued that all musical processes must make some reference to human behaviour, and so Music-AI is a central rather than a peripheral discipline for musical computing. However, it turns out that the goals of Music-AI as first expressed, the mimicking of human behaviour, are impossible to achieve in full, and that it is impossible, in principle, for computers to pass a musical version of the Turing test. In practice, however, computers are used for their non-human-like behaviour just as much as their human-like behaviour, so the real goal of Music-AI must be reformulated. Furthermore, it is argued that the non-holistic analysis of human behaviour which this reformulation entails is actually informative for our understanding of human behaviour. Music-AI could also be fruitfully concerned with developing musical intelligences which were explicitly not human. Music-AI is then seen to be as much a creative enterprise as a scientific one."}
{"_id":"d07300357d17631864244dded83e9a8a81fa340b","title":"A 450 fs 65-nm CMOS Millimeter-Wave Time-to-Digital Converter Using Statistical Element Selection for All-Digital PLLs","text":"This paper presents a time-to-digital converter (TDC) that operates with a 20\u201364 GHz input and underpins the phase digitization function in a millimeter-wave all-digital fractional-N frequency synthesizer. A self-calibrated inductor-less frequency divider using dynamic CML latches provides an eight-phase input to a 3-bit \u201ccoarse\u201d TDC, which is interfaced to a 5-bit \u201cfine\u201d TDC through a sub-sampling coarse-fine interface circuit. A wide bandwidth low dropout (LDO) on-chip regulator is used to decrease the effect of supply noise on TDC performance. A synthesized digital engine implements calibration using statistical element selection with mean adaptation to alleviate TDC nonlinearity that results from random mismatches and PVT variations. The TDC is fabricated in 65-nm CMOS along with the divider and calibration circuits, and achieves 450-fs resolution. The measured DNL and INL of the TDC are 0.65 and 1.2 LSB, respectively. The TDC consumes 11 mA from 1-V supply voltage. The TDC features a figure-of-merit of 0.167 (0.47) pJ per conversion step without (with) the frequency divider. A single-shot experiment shows that the on-chip LDO reduces the effect of TDC noise by reducing the standard deviation from 0.856 to 0.167 LSB for constant input. The prototype occupies an active area of  $502\\times 110~\\mu \\text{m}^{\\mathbf {2}}$  excluding pads."}
{"_id":"6a1dd4867f7fd75ac083375b41291dc8b419e82c","title":"Presupposed Content and Entailments in Natural Language Inference","text":"Previous work has presented an accurate natural logic model for natural language inference. Other work has demonstrated the effectiveness of computing presuppositions for solving natural language inference problems. We extend this work to create a system for correctly computing lexical presuppositions and their interactions within the natural logic framework. The combination allows our system to properly handle presupposition projection from the lexical to the sentential level while taking advantage of the accuracy and coverage of the natural logic system. To solve an inference problem, our system computes a sequence of edits from premise to hypothesis. For each edit the system computes an entailment relation and a presupposition entailment relation. The relations are then separately composed according to a syntactic tree and the semantic properties of its nodes. Presuppositions are projected based on the properties of their syntactic and semantic environment. The edits are then composed and the resulting entailment relations are combined with the presupposition relation to yield an answer to the inference problem."}
{"_id":"62a9b8aed78beb34266149d569b63262a5a881ba","title":"Reactive power capability of the wind turbine with Doubly Fed Induction Generator","text":"With the increasing integration into power grids, wind power plants play an important role in the power system. Many requirements for the wind power plants have been proposed in the grid codes. According to these grid codes, wind power plants should have the ability to perform voltage control and reactive power compensation at the point of common coupling (PCC). Besides the shunt flexible alternating current transmission system (FACTS) devices such as the static var compensator (SVC) and the static synchronous compensator (STATCOM), the wind turbine itself can also provide a certain amount of reactive power compensation, depending on the wind speed and the active power control strategy. This paper analyzes the reactive power capability of Doubly Fed Induction Generator (DFIG) based wind turbine, considering the rated stator current limit, the rated rotor current limit, the rated rotor voltage limit, and the reactive power capability of the grid side convertor (GSC). The boundaries of reactive power capability of DFIG based wind turbine are derived. The result was obtained using the software MATLAB."}
{"_id":"2753e8c047fb2e58eb60726cdac1f0bdaa71d4ef","title":"Mycorrhizosphere interactions to improve plant fitness and soil quality","text":"Arbuscular mycorrhizal fungi are key components of soil microbiota and obviously interact with other microorganisms in the rhizosphere, i.e. the zone of influence of plant roots on microbial populations and other soil constituents. Mycorrhiza formation changes several aspects of plant physiology and some nutritional and physical properties of the rhizospheric soil. These effects modify the colonization patterns of the root or mycorrhizas (mycorrhizosphere) by soil microorganisms. The rhizosphere of mycorrhizal plants, in practice a mycorrhizosphere, harbors a great array of microbial activities responsible for several key ecosystem processes. This paper summarizes the main conceptual principles and accepted statements on the microbial interactions between mycorrhizal fungi and other members of rhizosphere microbiota and discusses current developments and future trends concerning the following topics: (i) effect of soil microorganisms on mycorrhiza formation; (ii) mycorrhizosphere establishment; (iii) interactions involved in nutrient cycling and plant growth; (iv) interactions involved in the biological control of plant pathogens; and (v) interactions to improve soil quality. The main conclusion is that microbial interactions in the rhizosphere of mycorrhizal plants improve plant fitness and soil quality, critical issues for a sustainable agricultural development and ecosystem functioning."}
{"_id":"973c113db6805ccfb7c1c3d71008867accf4fc03","title":"A Hidden Markov Model for Vehicle Detection and Counting","text":"To reduce roadway congestion and improve traffic safety, accurate traffic metrics, such as number of vehicles travelling through lane-ways, are required. Unfortunately most existing infrastructure, such as loop-detectors and many video detectors, do not feasibly provide accurate vehicle counts. Consequently, a novel method is proposed which models vehicle motion using hidden Markov models (HMM). The proposed method represents a specified small region of the roadway as 'empty', 'vehicle entering', 'vehicle inside', and 'vehicle exiting', and then applies a modified Viterbi algorithm to the HMM sequential estimation framework to initialize and track vehicles. Vehicle observations are obtained using an Adaboost trained Haar-like feature detector. When tested on 88 hours of video, from three distinct locations, the proposed method proved to be robust to changes in lighting conditions, moving shadows, and camera motion, and consistently out-performed Multiple Target Tracking (MTT) and Virtual Detection Line(VDL) implementations. The median vehicle count error of the proposed method is lower than MTT and VDL by 28%, and 70% respectively. As future work, this algorithm will be implemented to provide the traffic industry with improved automated vehicle counting, with the intent to eventually provide real-time counts."}
{"_id":"b4c909ab72e4c1409cbbd160c052bb34793a29a2","title":"Striving for the moral self: the effects of recalling past moral actions on future moral behavior.","text":"People's desires to see themselves as moral actors can contribute to their striving for and achievement of a sense of self-completeness. The authors use self-completion theory to predict (and show) that recalling one's own (im)moral behavior leads to compensatory rather than consistent moral action as a way of completing the moral self. In three studies, people who recalled their immoral behavior reported greater participation in moral activities (Study 1), reported stronger prosocial intentions (Study 2), and showed less cheating (Study 3) than people who recalled their moral behavior. These compensatory effects were related to the moral magnitude of the recalled event, but they did not emerge when people recalled their own positive or negative nonmoral behavior (Study 2) or others' (im)moral behavior (Study 3). Thus, the authors extend self-completion theory to the moral domain and use it to integrate the research on moral cleansing (remunerative moral strivings) and moral licensing (relaxed moral strivings)."}
{"_id":"deb55cf5286c5a04cd25b3e5a91cc8caf3ca7ba1","title":"Transcriptional reprogramming in yeast using dCas9 and combinatorial gRNA strategies","text":"BACKGROUND\nTranscriptional reprogramming is a fundamental process of living cells in order to adapt to environmental and endogenous cues. In order to allow flexible and timely control over gene expression without the interference of native gene expression machinery, a large number of studies have focused on developing synthetic biology tools for orthogonal control of transcription. Most recently, the nuclease-deficient Cas9 (dCas9) has emerged as a flexible tool for controlling activation and repression of target genes, by the simple RNA-guided positioning of dCas9 in the vicinity of the target gene transcription start site.\n\n\nRESULTS\nIn this study we compared two different systems of dCas9-mediated transcriptional reprogramming, and applied them to genes controlling two biosynthetic pathways for biobased production of isoprenoids and triacylglycerols (TAGs) in baker's yeast Saccharomyces cerevisiae. By testing 101 guide-RNA (gRNA) structures on a total of 14 different yeast promoters, we identified the best-performing combinations based on reporter assays. Though a larger number of gRNA-promoter combinations do not perturb gene expression, some gRNAs support expression perturbations up to ~threefold. The best-performing gRNAs were used for single and multiplex reprogramming strategies for redirecting flux related to isoprenoid production and optimization of TAG profiles. From these studies, we identified both constitutive and inducible multiplex reprogramming strategies enabling significant changes in isoprenoid production and increases in TAG.\n\n\nCONCLUSION\nTaken together, we show similar performance for a constitutive and an inducible dCas9 approach, and identify multiplex gRNA designs that can significantly perturb isoprenoid production and TAG profiles in yeast without editing the genomic context of the target genes. We also identify a large number of gRNA positions in 14 native yeast target pomoters that do not affect expression, suggesting the need for further optimization of gRNA design tools and dCas9 engineering."}
{"_id":"7c5adcf159af8df4d91774b196fb32189efc0368","title":"Expertise-related deactivation of the right temporoparietal junction during musical improvisation","text":"Musical training has been associated with structural changes in the brain as well as functional differences in brain activity when musicians are compared to nonmusicians on both perceptual and motor tasks. Previous neuroimaging comparisons of musicians and nonmusicians in the motor domain have used tasks involving prelearned motor sequences or synchronization with an auditorily presented sequence during the experiment. Here we use functional magnetic resonance imaging (fMRI) to examine expertise-related differences in brain activity between musicians and nonmusicians during improvisation--the generation of novel musical-motor sequences--using a paradigm that we previously used in musicians alone. Despite behaviorally matched performance, the two groups showed significant differences in functional brain activity during improvisation. Specifically, musicians deactivated the right temporoparietal junction (rTPJ) during melodic improvisation, while nonmusicians showed no change in activity in this region. The rTPJ is thought to be part of a ventral attentional network for bottom-up stimulus-driven processing, and it has been postulated that deactivation of this region occurs in order to inhibit attentional shifts toward task-irrelevant stimuli during top-down, goal-driven behavior. We propose that the musicians' deactivation of the rTPJ during melodic improvisation may represent a training-induced shift toward inhibition of stimulus-driven attention, allowing for a more goal-directed performance state that aids in creative thought."}
{"_id":"34b958afff511bdc47d6fed6013da8700659f936","title":"With a flick of the wrist: stretch sensors as lightweight input for mobile devices","text":"With WristFlicker, we detect wrist movement through sets of stretch sensors embedded in clothing. Our system supports wrist rotation (pronation\/supination), and both wrist tilts (flexion\/extension and ulnar\/radial deviation). Each wrist movement is measured by two opposing stretch sensors, mimicking the counteracting movement of muscles. We discuss interaction techniques that allow a user to control a music player through this lightweight input."}
{"_id":"212fc5ddeb4416aa7e1435f4c69391d0ad4fb18d","title":"Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content","text":"We consider the problem of learning distributed representations for documents in data streams. The documents are represented as low-dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models. In particular, we exploit the context of documents in streams and use one of the language models to model the document sequences, and the other to model word sequences within them. The models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space. We discuss extensions to our model, which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy, thus learning user-specific vectors to represent individual preferences. We validated the learned representations on a public movie rating data set from MovieLens, as well as on a large-scale Yahoo News data comprising three months of user activity logs collected on Yahoo servers. The results indicate that the proposed model can learn useful representations of both documents and word tokens, outperforming the current state-of-the-art by a large margin."}
{"_id":"a00ae26413dacb97d01eae007ea8b7bcdbbdf1e6","title":"IEC 61850 substation configuration language as a basis for automated security and SDN configuration","text":"IEC61850 has revolutionized the way substations are configured and maintained. Substation Configuration Language (SCL) defines the parameters needed to configure individual devices and combine them into a working system. Security is implemented by IEC62351 and there are potential vulnerabilities. Best practice recommendations are for defense in depth. SCL contains sufficient information to auto-configure network equipment, firewalls, IDS and SDN based networks."}
{"_id":"472e4265895de65961b70779fdfbecafc24079ed","title":"Learning to Navigate for Fine-Grained Classification","text":"Fine-grained classification is challenging due to the difficulty of finding discriminative features. Finding those subtle traits that fully characterize the object is not straightforward. To handle this circumstance, we propose a novel self-supervision mechanism to effectively localize informative regions without the need of bounding-box\/part annotations. Our model, termed NTS-Net for Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a Teacher agent and a Scrutinizer agent. In consideration of intrinsic consistency between informativeness of the regions and their probability being ground-truth class, we design a novel training paradigm, which enables Navigator to detect most informative regions under the guidance from Teacher. After that, the Scrutinizer scrutinizes the proposed regions from Navigator and makes predictions. Our model can be viewed as a multi-agent cooperation, wherein agents benefit from each other, and make progress together. NTS-Net can be trained end-to-end, while provides accurate fine-grained classification predictions as well as highly informative regions during inference. We achieve state-of-the-art performance in extensive benchmark datasets."}
{"_id":"5989fe24effd265b4a8557e1e55bebc4891fb687","title":"Understanding predictability and exploration in human mobility","text":"Predictive models for human mobility have important applications in many fields including traffic control, ubiquitous computing, and contextual advertisement. The predictive performance of models in literature varies quite broadly, from over 90% to under 40%. In this work we study which underlying factors\u00a0- in terms of modeling approaches and spatio-temporal characteristics of the data sources\u00a0- have resulted in this remarkably broad span of performance reported in the literature. Specifically we investigate which factors influence the accuracy of next-place prediction, using a high-precision location dataset of more than 400 users observed for periods between 3 months and one year. We show that it is much easier to achieve high accuracy when predicting the time-bin location than when predicting the next place. Moreover, we demonstrate how the temporal and spatial resolution of the data have strong influence on the accuracy of prediction. Finally we reveal that the exploration of new locations is an important factor in human mobility, and we measure that on average 20-25% of transitions are to new places, and approx. 70% of locations are visited only once. We discuss how these mechanisms are important factors limiting our ability to predict human mobility."}
{"_id":"cf6891232d0589ba2c768a4ab243471e0a84f7c4","title":"Detection of hiding in the least significant bit","text":"In this paper, we apply the theory of hypothesis testing to the steganalysis, or detection of hidden data, in the least significant bit (LSB) of a host image. The hiding rate (if data is hidden) and host probability mass function (PMF) are unknown. Our main results are as follows. a) Two types of tests are derived: a universal (over choices of host PMF) method that has certain asymptotic optimality properties and methods that are based on knowledge or estimation of the host PMF and, hence, an appropriate likelihood ratio (LR). b) For a known host PMF, it is shown that the composite hypothesis testing problem corresponding to an unknown hiding rate reduces to a worst-case simple hypothesis testing problem. c) Using the results for a known host PMF, practical tests based on the estimation of the host PMF are obtained. These are shown to be superior to the state of the art in terms of receiver operating characteristics as well as self-calibration across different host images. Estimators for the hiding rate are also developed."}
{"_id":"b8e7dfa21aac846cb52848e54a68dd822ced20dd","title":"An Efficient Algorithm of Frequent Itemsets Mining Based on MapReduce","text":"Mainstream parallel algorithms for mining frequent itemsets (patterns) were designed by implementing FP-Growth or Apriori algorithms on MapReduce (MR) framework. Existing MR FP-Growth algorithms can not distribute data equally among nodes, and MR Apriori algorithms utilize multiple map\/reduce procedures and generate too many key-value pairs with value of 1; these disadvantages hinder their performance. This paper proposes an algorithm FIMMR: it firstly mines local frequent itemsets for each data chunk as candidates, applies prune strategies to the candidates, and then identifies global frequent itemsets from candidates. Experimental results show that the time efficiency of FIMMR outperforms PFP and SPC significantly; and under small minimum support threshold, FIMMR can achieve one order of magnitude improvement than the other two algorithms; meanwhile, the speedup of FIMMR is also satisfactory."}
{"_id":"b95d4c85157158c02d509953405ebe9708fe8612","title":"Visual Analysis of Public Utility Service Problems in a Metropolis","text":"Issues about city utility services reported by citizens can provide unprecedented insights into the various aspects of such services. Analysis of these issues can improve living quality through evidence-based decision making. However, these issues are complex, because of the involvement of spatial and temporal components, in addition to having multi-dimensional and multivariate natures. Consequently, exploring utility service problems and creating visual representations are difficult. To analyze these issues, we propose a visual analytics process based on the main tasks of utility service management. We also propose an aggregate method that transforms numerous issues into legible events and provide visualizations for events. In addition, we provide a set of tools and interaction techniques to explore such issues. Our approach enables administrators to make more informed decisions."}
{"_id":"1bf2c4ce84b83b285f76a14dee459fd5353f2121","title":"Survey of semantic annotation platforms","text":"The realization of the Semantic Web requires the widespread availability of semantic annotations for existing and new documents on the Web. Semantic annotations are to tag ontology class instance data and map it into ontology classes. The fully automatic creation of semantic annotations is an unsolved problem. Instead, current systems focus on the semi-automatic creation of annotations. The Semantic Web also requires facilities for the storage of annotations and ontologies, user interfaces, access APIs, and other features to fully support annotation usage. This paper examines current Semantic Web annotation platforms that provide annotation and related services, and reviews their architecture, approaches and performance."}
{"_id":"455e1168304e0eb2909093d5ab9b5ec85cda5028","title":"The String-to-String Correction Problem","text":"The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings."}
{"_id":"3e2fd91786d37293af7f60a0d67b290faf5475f3","title":"Ontology Learning for the Semantic Web","text":"The Semantic Web relies heavily on the formal ontologies that structure underlying data for the purpose of comprehensive and transportable machine understanding. Therefore, the success of the Semantic Web depends strongly on the proliferation of ontologies, which requires fast and easy engineering of ontologies and avoidance of a knowledge acquisition bottleneck. Ontology Learning greatly facilitates the construction of ontologies by the ontology engineer. The vision of ontology learning that we propose here includes a number of complementary disciplines that feed on different types of unstructured, semi-structured and fully structured data in order to support a semi-automatic, cooperative ontology engineering process. Our ontology learning framework proceeds through ontology import, extraction, pruning, refinement, and evaluation giving the ontology engineer a wealth of coordinated tools for ontology modeling. Besides of the general framework and architecture, we show in this paper some exemplary techniques in the ontology learning cycle that we have implemented in our ontology learning environment, Text-ToOnto, such as ontology learning from free text, from dictionaries, or from legacy ontologies, and refer to some others that need to complement the complete architecture, such as reverse engineering of ontologies from database schemata or learning from XML documents. Ontologies for the Semantic Web Conceptual structures that define an underlying ontology are germane to the idea of machine processable data on the Semantic Web. Ontologies are (meta)data schemas, providing a controlled vocabulary of concepts, each with an explicitly defined and machine processable semantics. By defining shared and common domain theories, ontologies help both people and machines to communicate concisely,"}
{"_id":"73c66a35fd02a8c1d488d4d16521205a659b360b","title":"Design and optimization on ESD self-protection schemes for 700V LDMOS in high voltage power IC","text":"This paper presents an ESD self-protection scheme for a 700V high-voltage laterally diffused metal-oxide-semiconductor (LDMOS) field effect transistor. The safe operating area (SOA) and breakdown failure mechanism of 700V LDMOS are analyzed using simulations and experimental results. The scalability of thermal failure current with LDMOS width is also demonstrated."}
{"_id":"e34895325d62aa253f7a9034920ba6f3f1fc0906","title":"Battery Pack Modeling , Simulation , and Deployment on a Multicore Real Time Target","text":"Battery Management System (BMS) design is a complex task requiring sophisticated models that mimic the electrochemical behavior of the battery cell under a variety of operating conditions. Equivalent circuits are well-suited for this task because they offer a balance between fidelity and simulation speed, their parameters reflect direct experimental observations, and they are scalable. Scalability is particularly important at the real time simulation stage, where a model of the battery pack runs on a real-time simulator that is physically connected to the peripheral hardware in charge of monitoring and control. With modern battery systems comprising hundreds of cells, it is important to employ a modeling and simulation approach that is capable of handling numerous simultaneous instances of the basic unit cell while maintaining real time performance. In previous publications we presented a technique for the creation of a battery cell model that contains the electrochemical fingerprints of a battery cell based on equivalent circuit model fitting to experimental data. In this work we extend our previous model to represent a battery pack, featuring cell creation, placement, and connection using automation scripts, thus facilitating the design of packs of arbitrary size and electrical topology. In addition, we present an assessment of model partitioning schemes for real time execution on multicore targets to ensure efficient use of hardware resources, a balanced computational load, and a study of the potential impact of the calculation latencies inherent to distributed systems on solver accuracy. Prior to C code generation for real time execution, a model profiler assesses the model partitioning and helps determine the multicore configuration that results in the lowest average turnaround time, the time elapsed between task start and finish. The resulting model is useful in the generation of multiple operating scenarios of interest in the design of charging, balancing, and safety related procedures. CITATION: Gazzarri, J., Shrivastava, N., Jackey, R., and Borghesani, C., \"Battery Pack Modeling, Simulation, and Deployment on a Multicore Real Time Target,\" SAE Int. J. Aerosp. 7(2):2014, doi:10.4271\/2014-01-2217. 2014-01-2217 Published 09\/16\/2014 Copyright \u00a9 2014 The MathWorks, Inc. doi:10.4271\/2014-01-2217 saeaero.saejournals.org A second goal is to provide a scalable methodology capable of supporting any number of battery cell components configured in series or parallel. A MATLAB script creates, places, and connects each model component, including battery cell blocks and electrical components for the battery pack load, and partitions the model."}
{"_id":"06900e1849e604e9cedc3f4e7bae37932c661349","title":"Deep learning for smart agriculture: Concepts, tools, applications, and opportunities","text":"In recent years, Deep Learning (DL), such as the algorithms of Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and Generative Adversarial Networks (GAN), has been widely studied and applied in various fields including agriculture. Researchers in the fields of agriculture often use software frameworks without sufficiently examining the ideas and mechanisms of a technique. This article provides a concise summary of major DL algorithms, including concepts, limitations, implementation, training processes, and example codes, to help researchers in agriculture to gain a holistic picture of major DL techniques quickly. Research on DL applications in agriculture is summarized and analyzed, and future opportunities are discussed in this paper, which is expected to help researchers in agriculture to better understand DL algorithms and learn major DL techniques quickly, and further to facilitate data analysis, enhance related research in agriculture, and thus promote DL applications effectively."}
{"_id":"6ee29ea56fe068365fbc7aa69b14d3be8ee100ad","title":"Using Object Information for Spotting Text","text":"Text spotting, also called text detection, is a challenging computer vision task because of cluttered backgrounds, diverse imaging environments, various text sizes and similarity between some objects and characters, e.g., tyre and \u2019o\u2019. However, text spotting is a vital step in numerous AI and computer vision systems, such as autonomous robots and systems for visually impaired. Due to its potential applications and commercial values, researchers have proposed various deep architectures and methods for text spotting. These methods and architectures concentrate only on text in images, but neglect other information related to text. There exists a strong relationship between certain objects and the presence of text, such as signboards or the absence of text, such as trees. In this paper, a text spotting algorithm based on text and object dependency is proposed. The proposed algorithm consists of two subconvolutional neural networks and three training stages. For this study, a new NTU-UTOI dataset containing over 22k non-synthetic images with 277k bounding boxes for text and 42 text-related object classes is established. According to our best knowledge, it is the second largest nonsynthetic text image database. Experimental results on three benchmark datasets with clutter backgrounds, COCO-Text, MSRA-TD500 and SVT show that the proposed algorithm provides comparable performance to state-of-the-art text spotting methods. Experiments are also performed on our newly established dataset to investigate the effectiveness of object information for text spotting. The experimental results indicate that the object information contributes significantly on the performance gain."}
{"_id":"d3b493a52e4fda9d40f190ddf9541dbf51ed7658","title":"A Mobile Application for Smart House Remote Control System","text":"At the start of the second decade of 21th century, the time has come to make the Smart Houses a reallity for regular use. The different parts of a Smart House are researched but there are still distances from an applicable system, using the modern technology. In this paper we present an overview of the Smart House subsystems necessary for controlling the house using a mobile application efficiently and securely. The sequence diagram of the mobile application connectiing to the server application and also the usecases possible are presented. The challenges faced in designing the mobile application and illustrating the updated house top plane view in that application, are discussed and soloutions are adapted for it. Finally the designed mobile application was implemented and the important sections of it were described, such as the interactive house top view map which indicates the status of the devices using predefined icons. The facilities to manage the scheduled tasks and defined rules are also implemented in this mobile application that was developed for use in Windows Mobile platform. This application has the capability of connecting to the main server using GPRS mobile internet and SMS. This system is expected to be an important step towards a unified system structure that can be used efficiently in near future regular houses. Keywords\u2014Smart House, Mobile Application, Remote Control, Automated Home, Windows Mobile."}
{"_id":"d85692a61a5476b6dfa4e633ac1143d6e1060c71","title":"Adaptive energy-aware free viewpoint video transmission over wireless networks","text":"As consumer-level cameras are becoming cheaper and cheaper, the scene of interest can now be captured by multiple cameras simultaneously from different viewpoints. By transmitting texture and depth maps of the views captured using two adjacent cameras, users can synthesize any view in between using depth image based rendering (DIBR) technique such as 3D-warping. This provides users with continuous view angles and is key technique for Free Viewpoint TV (FTV), etc. In this scenario, multiple views must be delivered through bandwidth-limited wireless networks, how to adaptive reduce the source coding rate to fit the bandwidth becomes an important issue. The standard multi-view video codec H.264 MVC explores the inter-view and intra-view correlation to reduce source encoding rate. But all views are required at the client side for decoding any single view, which is apparently not transmission efficient. A better solution is using H.264 to differentially encode the two adjacent views separately, but still this requires double-folder bandwidth comparing to the traditional video streaming. Multiple description coding (MDC) encodes one view into multiple descriptions for robust transmission. This inspires another option: sending only part of each view. This can greatly reduce the source encoding rate, but interpolating skipped frames before synthesizing the middle view consumes mobile devices' precious battery life. Whether to send at a lower source encoding rate but with extra energy consumption or send at a high source encoding rate is nontrivial, and has not been studied in any formal way. In this paper, we investigated both source encoding rate reduction and energy consumption with the bandwidth constraint. We formulated the problem into an optimization problem, i.e., maximizing the system utility, which is defined as the received video quality minus the weighted energy consumption. Simulation results showed that our proposed scheme provides a significant advantage over competing schemes in typical network conditions."}
{"_id":"85621cd521a0647b42ad0c449a9d03b1bfb47697","title":"Integral Channel Features \u2013 Addendum","text":"This document is meant to serve as an addendum to [1], published at BMVC 2009. The purpose of this addendum is twofold: (1) to respond to feedback we\u2019ve received since publication and (2) to describe a number of changes, especially to the non-maximal suppression, that further improve performance. The performance of our updated detection increases 5% to over 91% detection rate at 1 false positive per image on the INRIA dataset, and similarly on the Caltech Pedestrian Dataset, while overall system runtime for multiscale detection decreases by 1\/3 to just under 1.5s per 640\u00d7480 image. We begin by rectifying an important omission to the related work. Levi and Weiss had an innovative application of integral images to multiple image channels quite early on, demonstrating good results on face detection from few training examples [4]. This work appears to be the earliest such use of integral images, indeed the authors even describe a precursor to integral histograms. Many thanks to Mark Everingham for sending us this reference."}
{"_id":"5e9fafb0e70130417e04cf942a0a6f4b43ad583e","title":"Decision trees and forests: a probabilistic perspective","text":"Decision trees and ensembles of decision trees are very popular in machine learning and often achieve state-of-the-art performance on black-box prediction tasks. However, popular variants such as C4.5, CART, boosted trees and random forests lack a probabilistic interpretation since they usually just specify an algorithm for training a model. We take a probabilistic approach where we cast the decision tree structures and the parameters associated with the nodes of a decision tree as a probabilistic model; given labeled examples, we can train the probabilistic model using a variety of approaches (Bayesian learning, maximum likelihood, etc). The probabilistic approach allows us to encode prior assumptions about tree structures and share statistical strength between node parameters; furthermore, it offers a principled mechanism to obtain probabilistic predictions which is crucial for applications where uncertainty quantification is important. Existing work on Bayesian decision trees relies on Markov chain Monte Carlo which can be computationally slow and suffer from poor mixing. We propose a novel sequential Monte Carlo algorithm that computes a particle approximation to the posterior over trees in a top-down fashion. We also propose a novel sampler for Bayesian additive regression trees by combining the above top-down particle filtering algorithm with the Particle Gibbs (Andrieu et al., 2010) framework. Finally, we propose Mondrian forests (MFs), a computationally efficient hybrid solution that is competitive with non-probabilistic counterparts in terms of speed and accuracy, but additionally produces well-calibrated uncertainty estimates. MFs use the Mondrian process (Roy and Teh, 2009) as the randomization mechanism and hierarchically smooth the node parameters within each tree (using a hierarchical probabilistic model and approximate Bayesian updates), but combine the trees in a non-Bayesian fashion. MFs can be grown in an incremental\/online fashion and remarkably, the distribution of online MFs is the same as that of batch MFs."}
{"_id":"44ae903e53b1969152b02cdd1d3eb4903b681dbb","title":"Recent advances in LVCSR : A benchmark comparison of performances","text":"Large Vocabulary Continuous Speech Recognition (LVCSR), which is characterized by a high variability of the speech, is the most challenging task in automatic speech recognition (ASR). Believing that the evaluation of ASR systems on relevant and common speech corpora is one of the key factors that help accelerating research, we present, in this paper, a benchmark comparison of the performances of the current state-of-the-art LVCSR systems over different speech recognition tasks. Furthermore, we put objectively into evidence the best performing technologies and the best accuracy achieved so far in each task. The benchmarks have shown that the Deep Neural Networks and Convolutional Neural Networks have proven their efficiency on several LVCSR tasks by outperforming the traditional Hidden Markov Models and Guaussian Mixture Models. They have also shown that despite the satisfying performances in some LVCSR tasks, the problem of large-vocabulary speech recognition is far from being solved in some others, where more research efforts are still needed."}
{"_id":"c419854fdb19ae07af2b7d5a7a6ce02f1de5f0e5","title":"Active Learning in Collaborative Filtering Recommender Systems","text":"In Collaborative Filtering Recommender Systems user\u2019s preferences are expressed in terms of rated items and each rating allows to improve system prediction accuracy. However, not all of the ratings bring the same amount of information about the user\u2019s tastes. Active Learning aims at identifying rating data that better reflects users\u2019 preferences. Active learning Strategies are used to selectively choose the items to present to the user in order to acquire her ratings and ultimately improve the recommendation accuracy. In this survey article, we review recent active learning techniques for collaborative filtering along two dimensions: (a) whether the system requested ratings are personalised or not, and, (b) whether active learning is guided by one criterion (heuristic) or multiple"}
{"_id":"414651bbf1520c560ea34175d98b4d688e08e0d9","title":"Acoustical Sound Database in Real Environments for Sound Scene Understanding and Hands-Free Speech Recognition","text":"This paper reports on a project for collection of the sound scene data. The sound scene data is necessary for studies such as sound source localization, sound retrieval, sound recognition and hands-free speech recognition in real acoustical environments. There are many kinds of sound scenes in real environments. The sound scene is denoted by sound sources and room acoustics. The number of combination of the sound sources, source positions and rooms is huge in real acoustical environments. However, the sound in the environments can be simulated by convolution of the isolated sound sources and impulse responses. As an isolated sound source, a hundred kinds of non-speech sounds and speech sounds are collected. The impulse responses are collected in various acoustical environments. In this paper, progress of our sound scene database project and application to environment sound recognition are described."}
{"_id":"8f64de9c6e3c52222896280c9fb19ff6c0a504ea","title":"Teaching the science of learning","text":"The science of learning has made a considerable contribution to our understanding of effective teaching and learning strategies. However, few instructors outside of the field are privy to this research. In this tutorial review, we focus on six specific cognitive strategies that have received robust support from decades of research: spaced practice, interleaving, retrieval practice, elaboration, concrete examples, and dual coding. We describe the basic research behind each strategy and relevant applied research, present examples of existing and suggested implementation, and make recommendations for further research that would broaden the reach of these strategies."}
{"_id":"dfc5ad31fddd4142ef0e5cf2d85cc62e40af7af9","title":"Snitches, Trolls, and Social Norms: Unpacking Perceptions of Social Media Use for Crime Prevention","text":"In this paper, we describe how people perceive the use of social media to support crime prevention in their communities. Based on survey and interview data from residents in high- and low-crime neighborhoods in Chicago, we found that African Americans, people from high-crime neighborhoods, and people with low levels of trust in local police are less likely to view social media as an appropriate tool to support citizens or the police in local crime prevention efforts. Residents' concerns include information getting into the wrong hands, trolls, and being perceived as a snitch. Despite concerns with usage, citizens also viewed social media as a tool that can supplement in-person crime prevention efforts and facilitate relationship-building and information sharing. We discuss the complexities of hyper-local usage of social media to combat crime by describing the social and historical contexts in which these tools exist."}
{"_id":"b8a0cfa55b3393de4cc600d115cf6adb49bfa4ee","title":"Web Service SWePT: A Hybrid Opinion Mining Approach","text":"The increasing use of social networks and online sites where people can express their opinions has created a growing interest in Opinion Mining. One of the main tasks of Opinion Mining is to determine whether an opinion is positive or negative. Therefore, the role of the feelings expressed on the web has become crucial, mainly due to the concern of businesses and government to automatically identify the semantic orientation of the views of customers or citizens. This is also a concern, in the area of health to identify psychological disorders. This research focuses on the development of a web application called SWePT (Web Service for Polarity detection in Spanish Texts), which implements the Sequential Minimal Optimization (SMO) algorithm, extracting its features from an affective lexicon in Mexican Spanish. For this purpose, a corpus and an affective lexicon in Mexican Spanish were created. The experiments using three (positive, neutral, negative) and five categories (very positive, positive, neutral, negative, and very negative) allow us to demonstrate the effectiveness of the presented method. SWePT has also been implemented in the Emotion-bracelet interface, which shows the opinion of a user graphically."}
{"_id":"664e3e84dee394701241ce31222b985268ea005d","title":"Spatio-Temporal Anomaly Detection for Industrial Robots through Prediction in Unsupervised Feature Space","text":"Spatio-temporal anomaly detection by unsupervised learning have applications in a wide range of practical settings. In this paper we present a surveillance system for industrial robots using a monocular camera. We propose a new unsupervised learning method to train a deep feature extractor from unlabeled images. Without any data augmentation, the algorithm co-learns the network parameters on different pseudo-classes simultaneously to create unbiased feature representation. Combining the learned features with a prediction system, we can detect irregularities in high dimensional data feed (e.g. video of a robot performing pick and place task). The results show how the proposed approach can detect previously unseen anomalies in the robot surveillance video. Although the technique is not designed for classification, we show the use of the learned features in a more traditional classification application for CIFAR-10 dataset."}
{"_id":"c337cadc3cbb9e51e425b12afd7bad6dae5700c0","title":"A Cloud-based Approach for Interoperable Electronic Health Records (EHRs)","text":"We present a cloud-based approach for the design of interoperable electronic health record (EHR) systems. Cloud computing environments provide several benefits to all the stakeholders in the healthcare ecosystem (patients, providers, payers, etc.). Lack of data interoperability standards and solutions has been a major obstacle in the exchange of healthcare data between different stakeholders. We propose an EHR system - cloud health information systems technology architecture (CHISTAR) that achieves semantic interoperability through the use of a generic design methodology which uses a reference model that defines a general purpose set of data structures and an archetype model that defines the clinical data attributes. CHISTAR application components are designed using the cloud component model approach that comprises of loosely coupled components that communicate asynchronously. In this paper, we describe the high-level design of CHISTAR and the approaches for semantic interoperability, data integration, and security."}
{"_id":"a8b3c3d49c09d32cab3b97226b811b2ab0790923","title":"Feature Learning for Detection and Prediction of Freezing of Gait in Parkinson's Disease","text":"Freezing of gait (FoG) is a common gait impairment among patients with advanced Parkinson\u2019s disease. FoG is associated with falls and negatively impact the patient\u2019s quality of life. Wearable systems that detect FoG have been developed to help patients resume walking by means of auditory cueing. However, current methods for automated detection are not yet ideal. In this paper, we first compare feature learning approaches based on time-domain and statistical features to unsupervised ones based on principal components analysis. The latter systematically outperforms the former and also the standard in the field \u2013 Freezing Index by up to 8.1% in terms of F1-measure for FoG detection. We go a step further by analyzing FoG prediction, i.e., identification of patterns (pre-FoG) occurring before FoG episodes, based only on motion data. Until now this was only attempted using electroencephalography. With respect to the three-class problem (FoG vs. pre-FoG vs. normal locomotion), we show that FoG prediction performance is highly patient-dependent, reaching an F1-measure of 56% in the pre-FoG class for patients who exhibit enough gait degradation before FoG."}
{"_id":"a1ae532a9ae1cd11b138d9a83cee2d152b91186e","title":"Reading Between the Lines: Content-Agnostic Detection of Spear-Phishing Emails","text":"Spear-phishing is an effective attack vector for infiltrating companies and organisations. Based on the multitude of personal information available online, an attacker can craft seemingly legit emails and trick his victims into opening malicious attachments and links. Although anti-spoofing techniques exist, their adoption is still limited and alternative protection approaches are needed. In this paper, we show that a sender leaves content-agnostic traits in the structure of an email. Based on these traits, we develop a method capable of learning profiles for a large set of senders and identifying spoofed emails as deviations thereof. We evaluate our approach on over 700,000 emails from 16,000 senders and demonstrate that it can discriminate thousands of senders, identifying spoofed emails with 90% detection rate and less than 1 false positive in 10,000 emails. Moreover, we show that individual traits are hard to guess and spoofing only succeeds if entire emails of the sender are available to the attacker."}
{"_id":"e5646254bff42180ac8f9143d8c96b6f9817e0aa","title":"Deep Neural Network Based Subspace Learning of Robotic Manipulator Workspace Mapping","text":"The manipulator workspace mapping is an important problem in robotics and has attracted significant attention in the community. However, most of the pre-existing algorithms have expensive time complexity due to the reliance on sophisticated kinematic equations. To solve this problem, this paper introduces subspace learning (SL), a variant of subspace embedding, where a set of robot and scope parameters is mapped to the corresponding workspace by a deep neural network (DNN). Trained on a large dataset of around 6\u00d7 10 samples obtained from a MATLAB R \u00a9 implementation of a classical method and sampling of designed uniform distributions, the experiments demonstrate that the embedding significantly reduces run-time from 5.23\u00d7 10 s of traditional discretization method to 0.224 s, with high accuracies (average F-measure is 0.9665 with batch gradient descent and resilient backpropagation)."}
{"_id":"8954290aa454ee4c4b3f1dafbe2b66287899ed83","title":"Grasping the Intentions of Others: The Perceived Intentionality of an Action Influences Activity in the Superior Temporal Sulcus during Social Perception","text":"An explication of the neural substrates for social perception is an important component in the emerging field of social cognitive neuroscience and is relevant to the field of cognitive neuroscience as a whole. Prior studies from our laboratory have demonstrated that passive viewing of biological motion (Pelphrey, Mitchell, et al., 2003; Puce et al., 1998) activates the posterior superior temporal sulcus (STS) region. Furthermore, recent evidence has shown that the perceived context of observed gaze shifts (Pelphrey, Singerman, et al., 2003; Pelphrey et al., 2004) modulates STS activity. Here, using event-related functional magnetic resonance imaging at 4 T, we investigated brain activity in response to passive viewing of goal- and non-goal- directed reaching-to-grasp movements. Participants viewed an animated character making reaching-to-grasp movements either toward (correct) or away (incorrect) from a blinking dial. Both conditions evoked significant posterior STS activity that was strongly right lateralized. By examining the time course of the blood oxygenation level-dependent response from areas of activation, we observed a functional dissociation. Incorrect trials evoked significantly greater activity in the STS than did correct trials, while an area posterior and inferior to the STS (likely corresponding to the MT\/V5 complex) responded equally to correct and incorrect movements. Parietal cortical regions, including the superior parietal lobule and the anterior intraparietal sulcus, also responded equally to correct and incorrect movements, but showed evidence for differential responding based on the hand and arm (left or right) of the animated character used to make the reaching-to-grasp movement. The results of this study further suggest that a region of the right posterior STS is involved in analyzing the intentions of other people's actions and that activity in this region is sensitive to the context of observed biological motions."}
{"_id":"7dee0155d17ca046b69e51fecd62a057329add01","title":"Modeling Avoidance in Mood and Anxiety Disorders Using Reinforcement Learning","text":"BACKGROUND\nSerious and debilitating symptoms of anxiety are the most common mental health problem worldwide, accounting for around 5% of all adult years lived with disability in the developed world. Avoidance behavior-avoiding social situations for fear of embarrassment, for instance-is a core feature of such anxiety. However, as for many other psychiatric symptoms the biological mechanisms underlying avoidance remain unclear.\n\n\nMETHODS\nReinforcement learning models provide formal and testable characterizations of the mechanisms of decision making; here, we examine avoidance in these terms. A total of 101 healthy participants and individuals with mood and anxiety disorders completed an approach-avoidance go\/no-go task under stress induced by threat of unpredictable shock.\n\n\nRESULTS\nWe show an increased reliance in the mood and anxiety group on a parameter of our reinforcement learning model that characterizes a prepotent (pavlovian) bias to withhold responding in the face of negative outcomes. This was particularly the case when the mood and anxiety group was under stress.\n\n\nCONCLUSIONS\nThis formal description of avoidance within the reinforcement learning framework provides a new means of linking clinical symptoms with biophysically plausible models of neural circuitry and, as such, takes us closer to a mechanistic understanding of mood and anxiety disorders."}
{"_id":"152086bea7688c533794c0076bfa210ce1031bfc","title":"Semantic Annotation and Reasoning for Sensor Data","text":"Developments in (wireless) sensor and actuator networks and the capabilities to manufacture low cost and energy efficient networked embedded devices have lead to considerable interest in adding real world sense to the Internet and the Web. Recent work has raised the idea towards combining the Internet of Things (i.e. real world resources) with semantic Web technologies to design future service and applications for the Web. In this paper we focus on the current developments and discussions on designing Semantic Sensor Web, particularly, we advocate the idea of semantic annotation with the existing authoritative data published on the semantic Web. Through illustrative examples, we demonstrate how rule-based reasoning can be performed over the sensor observation and measurement data and linked data to derive additional or approximate knowledge. Furthermore, we discuss the association between sensor data, the semantic Web, and the social Web which enable construction of context-aware applications and services, and contribute to construction of a networked knowledge framework."}
{"_id":"158463840d4beed75e5a821e218526cd4d4d6801","title":"The SSN ontology of the W3C semantic sensor network incubator group","text":"The W3C Semantic Sensor Network Incubator group (the SSN-XG) produced an OWL 2 ontology to describe sensors and observations \u2014 the SSN ontology, available at http:\/\/purl.oclc.org\/NET\/ssnx\/ssn. The SSN ontology can describe sensors in terms of capabilities, measurement processes, observations and deployments. This article describes the SSN ontology. It further gives an example and describes the use of the ontology in recent research projects."}
{"_id":"5b081ed14184ca48da725032b1022c23669cc2be","title":"SemSOS: Semantic sensor Observation Service","text":"Sensor Observation Service (SOS) is a Web service specification defined by the Open Geospatial Consortium (OGC) Sensor Web Enablement (SWE) group in order to standardize the way sensors and sensor data are discovered and accessed on the Web. This standard goes a long way in providing interoperability between repositories of heterogeneous sensor data and applications that use this data. Many of these applications, however, are ill equipped at handling raw sensor data as provided by SOS and require actionable knowledge of the environment in order to be practically useful. There are two approaches to deal with this obstacle, make the applications smarter or make the data smarter. We propose the latter option and accomplish this by leveraging semantic technologies in order to provide and apply more meaningful representation of sensor data. More specifically, we are modeling the domain of sensors and sensor observations in a suite of ontologies, adding semantic annotations to the sensor data, using the ontology models to reason over sensor observations, and extending an open source SOS implementation with our semantic knowledge base. This semantically enabled SOS, or SemSOS, provides the ability to query high-level knowledge of the environment as well as low-level raw sensor data."}
{"_id":"961b8e95e4b360e5d95ef79a21958540d4e551ab","title":"New Generation Sensor Web Enablement","text":"Many sensor networks have been deployed to monitor Earth's environment, and more will follow in the future. Environmental sensors have improved continuously by becoming smaller, cheaper, and more intelligent. Due to the large number of sensor manufacturers and differing accompanying protocols, integrating diverse sensors into observation systems is not straightforward. A coherent infrastructure is needed to treat sensors in an interoperable, platform-independent and uniform way. The concept of the Sensor Web reflects such a kind of infrastructure for sharing, finding, and accessing sensors and their data across different applications. It hides the heterogeneous sensor hardware and communication protocols from the applications built on top of it. The Sensor Web Enablement initiative of the Open Geospatial Consortium standardizes web service interfaces and data encodings which can be used as building blocks for a Sensor Web. This article illustrates and analyzes the recent developments of the new generation of the Sensor Web Enablement specification framework. Further, we relate the Sensor Web to other emerging concepts such as the Web of Things and point out challenges and resulting future work topics for research on Sensor Web Enablement."}
{"_id":"a1cb4845c6cb1fa0ee2661f87ccf1a9ece93cc69","title":"An Internet of Things Platform for Real-World and Digital Objects","text":"The vision of the Internet of Things (IoT) relies on the provisioning of real-world services, which are provided by smart objects that are directly related to the physical world. A structured, machine-processible approach to provision such real-world services is needed to make heterogeneous physical objects accessible on a large scale and to integrate them with the digital world. The incorporation of observation and measurement data obtained from the physical objects with the Web data, using information processing and knowledge engineering methods, enables the construction of \u201dintelligent and interconnected things\u201d. The current research mostly focuses on the communication and networking aspects between the devices that are used for sensing amd measurement of the real world objects. There is, however, relatively less effort concentrated on creating dynamic infrastructures to support integration of the data into the Web and provide unified access to such data on service and application levels. This paper presents a semantic modelling and linked data approach to create an information framework for IoT. The paper describes a platform to publish instances of the IoT related resources and entities and to link them to existing resources on the Web. The developed platform supports publication of extensible and interoperable descriptions in the form of linked data."}
{"_id":"0effabe9c90862b3b26dbc7e023bb0515028da51","title":"metaSEM: an R package for meta-analysis using structural equation modeling","text":"The metaSEM package provides functions to conduct univariate, multivariate, and three-level meta-analyses using a structural equation modeling (SEM) approach via the OpenMx package in the R statistical platform. It also implements the two-stage SEM approach to conducting fixed- and random-effects meta-analytic SEM on correlation or covariance matrices. This paper briefly outlines the theories and their implementations. It provides a summary on how meta-analyses can be formulated as structural equation models. The paper closes with a conclusion on several relevant topics to this SEM-based meta-analysis. Several examples are used to illustrate the procedures in the supplementary material."}
{"_id":"a29feffbb8b410c55c9b24f51e8cb5071911b3a4","title":"User-friendly 3D object manipulation gesture using kinect","text":"With the rapid development and wide spread of the virtual reality technology, we can easily find VR systems in various places such as school, library and home. Translation and rotation, the most frequently used gestures for manipulating objects in the real world, need to be implemented in order to make a user feel comfortable while manipulating 3D objects in the systems. In this paper, we propose a set of user-friendly 3D object manipulation gestures and develop a recognizer using Kinect for the VR systems. The usefulness and suitableness of the proposed system is shown from the user study performed."}
{"_id":"357bc607f40abeb75b0c92119c35b698b8f6cd8f","title":"SmartPhoto: A Resource-Aware Crowdsourcing Approach for Image Sensing with Smartphones","text":"Photos obtained via crowdsourcing can be used in many critical applications. Due to the limitations of communication bandwidth, storage and processing capability, it is a challenge to transfer the huge amount of crowdsourced photos. To address this problem, we propose a framework, called SmartPhoto, to quantify the quality (utility) of crowdsourced photos based on the accessible geographical and geometrical information (called metadata) including the smartphone's orientation, position and all related parameters of the built-in camera. From the metadata, we can infer where and how the photo is taken, and then only transmit the most useful photos. Three optimization problems regarding the tradeoffs between photo utility and resource constraints, namely the Max-Utility problem, the online Max-Utility problem and the Min-Selection problem, are studied. Efficient algorithms are proposed and their performance bounds are theoretically proved. We have implemented SmartPhoto in a testbed using Android based smartphones, and proposed techniques to improve the accuracy of the collected metadata by reducing sensor reading errors and solving object occlusion issues. Results based on real implementations and extensive simulations demonstrate the effectiveness of the proposed algorithms."}
{"_id":"339a6951822855cbf86420978f877d58cd1a7aac","title":"Ambiguity as a resource for design","text":"Ambiguity is usually considered anathema in Human Computer Interaction. We argue, in contrast, that it is a resource for design that can be used to encourage close personal engagement with systems. We illustrate this with examples from contemporary arts and design practice, and distinguish three broad classes of ambiguity according to where uncertainty is located in the interpretative relationship linking person and artefact. Ambiguity of information finds its source in the artefact itself, ambiguity of context in the sociocultural discourses that are used to interpret it, and ambiguity of relationship in the interpretative and evaluative stance of the individual. For each of these categories, we describe tactics for emphasising ambiguity that may help designers and other practitioners understand and craft its use."}
{"_id":"310fe4e6cb6d090f7817de4c1034e35567b56e34","title":"Robust Multi-pose Facial Expression Recognition","text":"Previous research on facial expression recognition mainly focuses on near frontal face images, while in realistic interactive scenarios, the interested subjects may appear in arbitrary non-frontal poses. In this paper, we propose a framework to recognize six prototypical facial expressions, namely, anger, disgust, fear, joy, sadness and surprise, in an arbitrary head pose. We build a multi-pose training set by rendering 3D face scans from the BU-4DFE dynamic facial expression database [17] at 49 different viewpoints. We extract Local Binary Pattern (LBP) descriptors and further utilize multiple instance learning to mitigate the influence of inaccurate alignment in this challenging task. Experimental results demonstrate the power and validate the effectiveness of the proposed multi-pose facial expression recognition framework."}
{"_id":"41034ac4b9defdb50b98eeb6836679f375b6644e","title":"Microservices validation: Mjolnirr platform case study","text":"Microservice architecture is a cloud application design pattern that implies that the application is divided into a number of small independent services, each of which is responsible for implementing of a certain feature. The need for continuous integration of developed and\/or modified microservices in the existing system requires a comprehensive validation of individual microservices and their co-operation as an ensemble with other microservices. In this paper, we would provide an analysis of existing methods of cloud applications testing and identify features that are specific to the microservice architecture. Based on this analysis, we will try to propose a validation methodology of the microservice systems."}
{"_id":"1f09b639227b5ff588f8b885dad5474742affff1","title":"Robust Nonrigid Registration by Convex Optimization","text":"We present an approach to nonrigid registration of 3D surfaces. We cast isometric embedding as MRF optimization and apply efficient global optimization algorithms based on linear programming relaxations. The Markov random field perspective suggests a natural connection with robust statistics and motivates robust forms of the intrinsic distortion functional. Our approach outperforms a large body of prior work by a significant margin, increasing registration precision on real data by a factor of 3."}
{"_id":"f176b7177228c1a18793cf922455545d408a65ae","title":"Active Damping in DC\/DC Power Electronic Converters: A Novel Method to Overcome the Problems of Constant Power Loads","text":"Multi-converter power electronic systems exist in land, sea, air, and space vehicles. In these systems, load converters exhibit constant power load (CPL) behavior for the feeder converters and tend to destabilize the system. In this paper, the implementation of novel active-damping techniques on dc\/dc converters has been shown. Moreover, the proposed active-damping method is used to overcome the negative impedance instability problem caused by the CPLs. The effectiveness of the new proposed approach has been verified by PSpice simulations and experimental results."}
{"_id":"a083ba4f23dd2e5229f6009253e28ef4f81759e7","title":"Extracting Social Structures from Conversations in Twitter: A Case Study on Health-Related Posts","text":"Online Social Networks (e.g., Twitter, Facebook) have dramatically grown in usage and popularity in recent years. In addition to keeping track of friends and acquaintances, these networks provide powerful means of exchanging and sharing information on many different topics of interest (e.g., sports, religion, politics, health concerns, etc.). Moreover, the use of these networks has introduced a completely new way of collaboration among people, virtually creating spheres of friends who generally feel comfortable discussing a variety of subjects and even helping each other to grow in knowledge about certain subjects. In this work, we built and analyzed networks of social groups on Twitter related to the top leading causes of death in the United States. Due to space limitations, we present results for the state of Florida and only for the top four leading causes of death. We show that using a concept of time window in the creation of relations between users, we can reconstruct social networks for these conversations and these networks have characteristics that are similar to typical social networks. The argument is that social network information can be extracted even in cases where users are not directly talking to each other (as it is the case in most of Twitter)."}
{"_id":"5bf4644c104ac6778a0aa07418321b14e0010e81","title":"HCI and Autonomous Vehicles: Contextual Experience Informs Design","text":"The interaction between drivers and their cars will change significantly with the introduction of autonomous vehicles. The driver's role will shift towards a supervisory control of their autonomous vehicle. The eventual relief from the driving task enables a complete new area of research and practice in human-computer interaction and interaction design. In this one-day workshop, participants will explore the opportunities the design space of autonomous driving will bring to HCI researchers and designers. On the day before workshop participants are invited to visit (together with workshop organizers) Google Partnerplex and Stanford University. At Google participants will have the opportunity to explore Google's autonomous car simulator and might have the chance to experience one of the Google Cars (if available). At Stanford participants are invited to ride in a Wizard-of-Oz autonomous vehicle. Based on this first-hand experience we will discuss design approaches and prototype interaction systems during the next day's workshop. The outcome of this workshop will be a set of concepts, interaction sketches, and low-fidelity paper prototypes that address constraints and potentials of driving in an autonomous car."}
{"_id":"8887690f0abd32d2d708995b1c85667eae0de753","title":"Using BabelNet to Improve OOV Coverage in SMT","text":"Out-of-vocabulary words (OOVs) are a ubiquitous and difficult problem in statistical machine translation (SMT). This paper studies different strategies of using BabelNet to alleviate the negative impact brought about by OOVs. BabelNet is a multilingual encyclopedic dictionary and a semantic network, which not only includes lexicographic and encyclopedic terms, but connects concepts and named entities in a very large network of semantic relations. By taking advantage of the knowledge in BabelNet, three different methods \u2013 using direct training data, domain-adaptation techniques and the BabelNet API \u2013 are proposed in this paper to obtain translations for OOVs to improve system performance. Experimental results on English\u2013Polish and English\u2013Chinese language pairs show that domain adaptation can better utilize BabelNet knowledge and performs better than other methods. The results also demonstrate that BabelNet is a really useful tool for improving translation performance of SMT systems."}
{"_id":"e58506ef0f6721729d2f72c61e6bb46565b887de","title":"Path selection based on local terrain feature for unmanned ground vehicle in unknown rough terrain environment","text":"In this paper, we propose an autonomous navigation for Unmanned Ground Vehicles (UGVs) by a path selection method based on local features of terrain in unknown outdoor rough terrain environment. The correlation between a local terrain feature obtained from a path and a value of the path obtained from the global path planning is extracted in advance. When UGV comes to a branch while it is running, the value of path is estimated using the correlation with local terrain feature. Thereby, UGV navigation is performed by path selection under the criterion of shortest time in unknown outdoor environment. We experimented on a simulator and confirmed that the proposed method can select more effective paths in comparison with a simple path selection method."}
{"_id":"cc2fb12eaa4dae74c5de0799b29624b5c585c43b","title":"Behavioral Cloning from Observation","text":"Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a wellstudied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available."}
{"_id":"43c46282c1f4cf2e718387d2c136a66d9c832cd1","title":"Inference Over Distribution of Posterior Class Probabilities for Reliable Bayesian Classification and Object-Level Perception","text":"State of the art Bayesian classification approaches typically maintain a posterior distribution over possible classes given available sensor observations (images). Yet, while these approaches fuse all classifier outputs thus far, they do not provide any indication regarding how reliable the posterior classification is, thus limiting its functionality in terms of autonomous systems and robotics. On the other hand, current deep learning based classifiers provide an uncertainty measure, thereby quantifying model uncertainty. However, they do so on a single frame basis and do not consider a sequential framework. In this letter, we develop a novel approach that infers a distribution over posterior class probabilities, while accounting for model uncertainty. This distribution enables reasoning about uncertainty in the posterior classification and, therefore, is of prime importance for robust classification, object-level perception in uncertain and ambiguous scenarios, and for safe autonomy in general. The distribution of the posterior class probability has no known analytical solution; thus, we propose to approximate this distribution via sampling. We evaluate our approach in simulation and using real images fed into a convolutional neural network classifier."}
{"_id":"564fb39c07fcd91594bada44e803478d52231928","title":"Protecting Your Right: Verifiable Attribute-Based Keyword Search with Fine-Grained Owner-Enforced Search Authorization in the Cloud","text":"Search over encrypted data is a critically important enabling technique in cloud computing, where encryption-before-outsourcing is a fundamental solution to protecting user data privacy in the untrusted cloud server environment. Many secure search schemes have been focusing on the single-contributor scenario, where the outsourced dataset or the secure searchable index of the dataset are encrypted and managed by a single owner, typically based on symmetric cryptography. In this paper, we focus on a different yet more challenging scenario where the outsourced dataset can be contributed from multiple owners and are searchable by multiple users, i.e., multi-user multi-contributor case. Inspired by attribute-based encryption (ABE), we present the first attribute-based keyword search scheme with efficient user revocation (ABKS-UR) that enables scalable fine-grained (i.e., file-level) search authorization. Our scheme allows multiple owners to encrypt and outsource their data to the cloud server independently. Users can generate their own search capabilities without relying on an always online trusted authority. Fine-grained search authorization is also implemented by the owner-enforced access policy on the index of each file. Further, by incorporating proxy re-encryption and lazy re-encryption techniques, we are able to delegate heavy system update workload during user revocation to the resourceful semi-trusted cloud server. We formalize the security definition and prove the proposed ABKS-UR scheme selectively secure against chosen-keyword attack. To build confidence of data user in the proposed secure search system, we also design a search result verification scheme. Finally, performance evaluation shows the efficiency of our scheme."}
{"_id":"8e79e46513e83bad37a029d1c49fca4a1c204738","title":"Learning Structured Natural Language Representations for Semantic Parsing","text":"We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.1"}
{"_id":"a1d326e7710cb9a1464ef52ca557a20ea5aa7e91","title":"A four-band dual-polarized cavity-backed antenna on LTCC technology for 60GHz applications","text":"In this work, we present a 4-band dual-polarized antenna designed for 8-chanel applications. Based on LTCC technology, the antenna is a patch coupled aperture with a backed cavity. Each antenna element of a designated band contains two channels through two orthogonally polarized ports. Combining four dual-polarized antenna elements under different frequency, an 8-channel antenna for 60GHz applications can be achieved. The array antenna contains 8 feeding ports, which correspond to 8 independent channels. The isolation between each port can reach 20dB in most of the frequency band."}
{"_id":"1f376c10b20319121102db78e7790cf47d8fa046","title":"Optimal Reconfiguration for Supply Restoration With Informed A$^{\\ast}$  Search","text":"Reconfiguration of radial distribution networks is the basis of supply restoration after faults and of load balancing and loss minimization. The ability to automatically reconfigure the network quickly and efficiently is a key feature of autonomous and self-healing networks, an important part of the future vision of smart grids. We address the reconfiguration problem for outage recovery, where the cost of the switching actions dominates the overall cost: when the network reverts to its normal configuration relatively quickly, the electricity loss and the load imbalance in a temporary suboptimal configuration are of minor importance. Finding optimal feeder configurations under most optimality criteria is a difficult optimization problem. All known complete optimal algorithms require an exponential time in the network size in the worst case, and cannot be guaranteed to scale up to arbitrarily large networks. Hence most works on reconfiguration use heuristic approaches that can deliver solutions but cannot guarantee optimality. These approaches include local search, such as tabu search, and evolutionary algorithms. We propose using optimal informed search algorithms in the A family, introduce admissible heuristics for reconfiguration, and demonstrate empirically the efficiency of our approach. Combining A with admissible cost lower bounds guarantees that reconfiguration plans are optimal in terms of switching action costs."}
{"_id":"30a1f82da02441d099bf15ca026751fcd76c8dee","title":"Additive Quantization for Extreme Vector Compression","text":"We introduce a new compression scheme for high-dimensional vectors that approximates the vectors using sums of M codewords coming from M different codebooks. We show that the proposed scheme permits efficient distance and scalar product computations between compressed and uncompressed vectors. We further suggest vector encoding and codebook learning algorithms that can minimize the coding error within the proposed scheme. In the experiments, we demonstrate that the proposed compression can be used instead of or together with product quantization. Compared to product quantization and its optimized versions, the proposed compression approach leads to lower coding approximation errors, higher accuracy of approximate nearest neighbor search in the datasets of visual descriptors, and lower image classification error, whenever the classifiers are learned on or applied to compressed vectors."}
{"_id":"f5b8894cf0606b991a913b84a2a3e8b43e4c32de","title":"Toward Efficient Action Recognition: Principal Backpropagation for Training Two-Stream Networks","text":"In this paper, we propose the novel principal backpropagation networks (PBNets) to revisit the backpropagation algorithms commonly used in training two-stream networks for video action recognition. We content that existing approaches always take all the frames\/snippets for the backpropagation not optimal for video recognition since the desired actions only occur in a short period within a video. To remedy these drawbacks, we design a watch-and-choose mechanism. In particular, the watching stage exploits a dense snippet-wise temporal pooling strategy to discover the global characteristic for each input video, while the choosing phase only backpropagates a small number of representative snippets that are selected with two novel strategies, i.e., Max-rule and KL-rule. We prove that with the proposed selection strategies, performing the backpropagation on the selected subset is capable of decreasing the loss of the whole snippets as well. The proposed PBNets are evaluated on two standard video action recognition benchmarks UCF101 and HMDB51, where it surpasses the state of the arts consistently, but requiring less memory and computation to achieve high performance."}
{"_id":"291001586b37fbd38e1887378586e40757bea499","title":"Idea Generation Techniques among Creative Professionals","text":"The creative process has been a key topic research over the past century, but it wasn't until the last decade that creativity became a hot topic of research in the HCI. It is an important commodity to businesses and individuals alike spawning numerous research studies in business, psychology and design. However, it wasn't until recently that researchers became interested in developing technologies to support creative behavior. This article outlines the role of creativity in design from the designer's perspective, provides a model for the creative process and provides a foundation and direction for future creativity support research by identifying nineteen idea generation techniques utilized by creative professionals."}
{"_id":"0aae0d373f652fd39f0373199b0416f518d6eb8b","title":"MusicMiner : Visualizing timbre distances of music as topographical maps","text":"Timbre distances and similarities are an expression of the phenomenon that some music appears similar while other songs sound very different to us. The notion of genre is often used to categorize music, but songs from a single genre do not necessarily sound similar and vice versa. Instead we aim at a visualization of timbre similarities of sound within a music collection. We analyzed and compared a large amount of different audio features and psychoacoustic variants thereof for the purpose of modelling timbre distance of sound. The sound of polyphonic music is commonly described by extracting audio features on short time windows during which the sound is assumed to be stationary. The resulting down sampled time series are aggregated to form a high level feature vector describing the music. We generated high level features by systematically applying static and temporal statistics for aggregation. Especially the temporal structure of features has previously been largely neglected. A novel supervised feature selection method is applied to the huge set of possible features. Distances between vectors of the selected features correspond to timbre differences in music. The features show few redundancies and have high potential for explaining possible clusters. They outperform seven other previously proposed feature sets on several datasets w.r.t. the separation of the known groups of timbrally different music. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map and related methods based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song. Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany"}
{"_id":"6674729287f2482eda9e836846d2a35e63ea401c","title":"Rank Pooling for Action Recognition","text":"We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g., how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features."}
{"_id":"cc611049fa1029230858ec62399b571cb40bfb71","title":"Power Amplifiers and Transmitters for RF and Microwave","text":"The generation of RF\/microwave power is required not only in wireless communications, but also in applications such as jamming, imaging, RF heating, and miniature dc\/dc converters. Each application has its own unique requirements for frequency, bandwidth, load, power, efficiency, linearity, and cost. RF power is generated by a wide variety of techniques, implementations, and active devices. Power amplifiers are incorporated into transmitters in a similarly wide variety of architectures, including linear, Kahn, envelope tracking, outphasing, and Doherty. Linearity can be improved through techniques such as feedback, feedforward, and predistortion."}
{"_id":"e6901330f1b056578cf2584420cecd01168b1761","title":"CMOS Outphasing Class-D Amplifier With Chireix Combiner","text":"This letter presents a CMOS outphasing class-D power amplifier (PA) with a Chireix combiner. Two voltage-mode class-D amplifiers used in the outphasing system were designed and implemented with a 0.18-mum CMOS process. By applying the Chireix combiner technique, drain efficiency of the outphasing PA for CDMA signals was improved from 38.6% to 48% while output power was increased from 14.5 to 15.4 dBm with an adjacent channel power ratio of -45 dBc."}
{"_id":"32d31bdc147ee2ade8166b715bd381227c523f0a","title":"An extended Doherty amplifier with high efficiency over a wide power range","text":"An extension of the Doherty amplifier architecture which maintains high efficiency over a wide range of output power (>6 dB) is presented. This extended Doherty amplifier is demonstrated experimentally with InGaP-GaAs HBTs at a frequency of 950 MHz. P\/sub 1 dB\/ is measured at 27.5 dBm with PAE of 46%. PAE of at least 39% is maintained for over an output power range of 12 dB backed-off from P\/sub 1 dB\/. This is an improvement over the classical Doherty amplifier, where high efficiency is typically obtained up to 5-6 dB backed-off from P\/sub 1 dB\/. Generalized design equations for the Doherty amplifier are derived to show a careful choice of the output matching circuit and device scaling parameters can improve efficiencies at lower output power."}
{"_id":"c413d55baf1ee96df29256a879e96daf4c2cc072","title":"Segmenting Salient Objects from Images and Videos","text":"In this paper we introduce a new salient object segmentation method, which is based on combining a saliency measure with a conditional random field (CRF) model. The proposed saliency measure is formulated using a statistical framework and local feature contrast in illumination, color, and motion information. The resulting saliency map is then used in a CRF model to define an energy minimization based segmentation approach, which aims to recover well-defined salient objects. The method is efficiently implemented by using the integral histogram approach and graph cut solvers. Compared to previous approaches the introduced method is among the few which are applicable to both still images and videos including motion cues. The experiments show that our approach outperforms the current state-of-the-art methods in both qualitative and quantitative terms."}
{"_id":"42ca4c23233a346ee921427eaa53f7c85dd2ccf6","title":"Mapping, Learning, Visualization, Classification, and Understanding of fMRI Data in the NeuCube Evolving Spatiotemporal Data Machine of Spiking Neural Networks","text":"This paper introduces a new methodology for dynamic learning, visualization, and classification of functional magnetic resonance imaging (fMRI) as spatiotemporal brain data. The method is based on an evolving spatiotemporal data machine of evolving spiking neural networks (SNNs) exemplified by the NeuCube architecture [1]. The method consists of several steps: mapping spatial coordinates of fMRI data into a 3-D SNN cube (SNNc) that represents a brain template; input data transformation into trains of spikes; deep, unsupervised learning in the 3-D SNNc of spatiotemporal patterns from data; supervised learning in an evolving SNN classifier; parameter optimization; and 3-D visualization and model interpretation. Two benchmark case study problems and data are used to illustrate the proposed methodology\u2014fMRI data collected from subjects when reading affirmative or negative sentences and another one\u2014on reading a sentence or seeing a picture. The learned connections in the SNNc represent dynamic spatiotemporal relationships derived from the fMRI data. They can reveal new information about the brain functions under different conditions. The proposed methodology allows for the first time to analyze dynamic functional and structural connectivity of a learned SNN model from fMRI data. This can be used for a better understanding of brain activities and also for online generation of appropriate neurofeedback to subjects for improved brain functions. For example, in this paper, tracing the 3-D SNN model connectivity enabled us for the first time to capture prominent brain functional pathways evoked in language comprehension. We found stronger spatiotemporal interaction between left dorsolateral prefrontal cortex and left temporal while reading a negated sentence. This observation is obviously distinguishable from the patterns generated by either reading affirmative sentences or seeing pictures. The proposed NeuCube-based methodology offers also a superior classification accuracy when compared with traditional AI and statistical methods. The created NeuCube-based models of fMRI data are directly and efficiently implementable on high performance and low energy consumption neuromorphic platforms for real-time applications."}
{"_id":"48a1aab13e5b36122bc1c22231fa39ad75175852","title":"Locked-in syndrome : a challenge for embodied cognitive science","text":"Embodied approaches in cognitive science hold that the body is crucial for cognition. What this claim amounts to, however, still remains unclear. This paper contributes to its clarification by confronting three ways of understanding embodiment\u2014the sensorimotor approach, extended cognition and enactivism\u2014with Lockedin syndrome (LIS). LIS is a case of severe global paralysis in which patients are unable to move and yet largely remain cognitively intact. We propose that LIS poses a challenge to embodied approaches to cognition requiring them to make explicit the notion of embodiment they defend and its role for cognition. We argue that the sensorimotor and the extended functionalist approaches either fall short of accounting for cognition in LIS from an embodied perspective or do it too broadly by relegating the body only to a historical role. Enactivism conceives of the body as autonomous system and of cognition as sense-making. From this perspective embodiment is not equated with bodily movement but with forms of agency that do not disappear with body paralysis. Enactivism offers a clarifying perspective on embodiment and thus currently appears to be the framework in embodied cognition best suited to address the challenge posed by LIS."}
{"_id":"f86722f9181cbe0a75a60c12ea987b90adce7437","title":"Challenges of Computational Processing of Code-Switching","text":"This paper addresses challenges of Natural Language Processing (NLP) on non-canonical multilingual data in which two or more languages are mixed. It refers to code-switching which has become more popular in our daily life and therefore obtains an increasing amount of attention from the research community. We report our experience that covers not only core NLP tasks such as normalisation, language identification, language modelling, part-of-speech tagging and dependency parsing but also more downstream ones such as machine translation and automatic speech recognition. We highlight and discuss the key problems for each of the tasks with supporting examples from different language pairs and relevant previous work."}
{"_id":"26b9db33bf020a4d145f248094280ba62ab853f9","title":"SME e-readiness in Malaysia : Implications for Planning and Implementation","text":"This study hoped to answer 2 main objectives. The first objective was to assess the level of e-readiness of SMEs in Northern Malaysia. The second objective was to investigate the factors contributing to the e-readiness of SMEs in Northern Malaysia. Questionnaires were distributed using a simple random sampling method to 300 SMEs in Penang, Kedah and Perlis. The findings of this study show that SMEs in Northern Malaysia are ready to go for e-business, e-commerce and Internet in general. The findings also showed that in general top management commitment and infrastructure and technology have significant impact on SMEs\u2019 e-readiness. However, human capital, resistance to change, and information security do not have significant impact or contribution on e-readiness in SMEs."}
{"_id":"78d3b266e1dd981dd7c5eef6371393ea0d0983b2","title":"Fame for sale: Efficient detection of fake Twitter followers","text":"Fake followers are those Twitter accounts specifically created to inflate the number of followers of a target account. Fake followers are dangerous for the social platform and beyond, since they may alter concepts like popularity and influence in the Twittersphere\u2014hence impacting on economy, politics, and society. In this paper, we contribute along different dimensions. First, we review some of the most relevant existing features and rules (proposed by Academia and Media) for anomalous Twitter accounts detection. Second, we create a baseline dataset of verified human and fake follower accounts. Such baseline dataset is publicly available to the scientific community. Then, we exploit the baseline dataset to train a set of machine-learning classifiers built over the reviewed rules and features. Our results show that most of the rules proposed by Media provide unsatisfactory performance in revealing fake followers, while features proposed in the past by Academia for spam detection provide good results. Building on the most promising features, we revise the classifiers both in terms of reduction of overfitting and cost for gathering the data needed to compute the features. The final result is a novel Class A classifier, general enough to thwart overfitting, lightweight thanks to the usage of the less costly features, and still able to correctly classify more than 95% of the accounts of the original training set. We ultimately perform an information fusion-based sensitivity analysis, to assess the global sensitivity of each of the features employed by the classifier. The findings reported in this paper, other than being supported by a thorough experimental methodology and interesting on their own, also pave the way for further investigation on the novel issue of fake Twitter followers."}
{"_id":"749546a58a1d46335de785c41a3eae977e84a0df","title":"SVM incremental learning, adaptation and optimization","text":"The objective of machine learning is to identify a model that yields good generalization performance. This involves repeatedly selecting a hypothesis class, searching the hypothesis class by minimizing a given objective function over the model\u2019s parameter space, and evaluating the generalization performance of the resulting model. This search can be computationally intensive as training data continuously arrives, or as one needs to tune hyperparameters in the hypothesis class and the objective function. In this paper, we present a framework for exact incremental learning and adaptation of support vector machine (SVM) classifiers. The approach is general and allows one to learn and unlearn individual or multiple examples, adapt the current SVM to changes in regularization and kernel parameters, and evaluate generalization performance through exact leave-one-out error estimation. I. I NTRODUCTION SVM techniques for classification and regression provide powerful tools for learning models that generalize well even in sparse, high dimensional settings. Their success can be attributed to Vapnik\u2019s seminal work in statistical learning theory [15] which provided key insights into the factors affecting generalization performance. SVM learning can be viewed as a practical implementation of Vapnik\u2019s structural risk minimizationinduction principle which involves searching over hypothesis classes of varying capacity to find the model with the best generalization performance. SVM classifiers of the formf(x) = w \u00b7\u03a6(x)+b are learned from the data{(xi, yi) \u2208 R I m \u00d7 {\u22121, 1} \u2200 i \u2208 {1, . . . , N}} by minimizing min w,b,\u03be 1 2 \u2016w\u2016 + C N \u2211"}
{"_id":"d09c99383a9d530943da36e4b56f73f22502e278","title":"Speaker-Independent Silent Speech Recognition From Flesh-Point Articulatory Movements Using an LSTM Neural Network","text":"Silent speech recognition SSR converts nonaudio information such as articulatory movements into text. SSR has the potential to enable persons with laryngectomy to communicate through natural spoken expression. Current SSR systems have largely relied on speaker-dependent recognition models. The high degree of variability in articulatory patterns across different speakers has been a barrier for developing effective speaker-independent SSR approaches. Speaker-independent SSR approaches, however, are critical for reducing the amount of training data required from each speaker. In this paper, we investigate speaker-independent SSR from the movements of flesh points on tongue and lips with articulatory normalization methods that reduce the interspeaker variation. To minimize the across-speaker physiological differences of the articulators, we propose Procrustes matching-based articulatory normalization by removing locational, rotational, and scaling differences. To further normalize the articulatory data, we apply feature-space maximum likelihood linear regression and i-vector. In this paper, we adopt a bidirectional long short-term memory recurrent neural network BLSTM as an articulatory model to effectively model the articulatory movements with long-range articulatory history. A silent speech dataset with flesh-point articulatory movements was collected using an electromagnetic articulograph from 12 healthy and two laryngectomized English speakers. Experimental results showed the effectiveness of our speaker-independent SSR approaches on healthy as well as laryngectomy speakers. In addition, BLSTM outperformed the standard deep neural network. The best performance was obtained by the BLSTM with all the three normalization approaches combined."}
{"_id":"86cf4c859d34c84fefdd8d36e5ea8ab691948512","title":"Parallel Training for Deep Stacking Networks","text":"The Deep Stacking Network (DSN) is a special type of deep architecture developed to enable and benefit from parallel learning of its model parameters on large CPU clusters. As a prospective key component of future speech recognizers, the architectural design of the DSN and its parallel training endow the DSN with scalability over a vast amount of training data. In this paper, we present our first parallel implementation of the DSN training algorithm. Particularly, we show the tradeoff between the time\/memory saving via training parallelism and the associated cost arising from inter-CPU communication. Further, in phone classification experiments, we demonstrate a significantly lowered error rate using parallel full-batch training distributed over a CPU cluster, compared with sequential minibatch training implemented in a single CPU machine under otherwise identical experimental conditions and as exploited prior to the work reported in this paper."}
{"_id":"755908791b97686e9646b773e56f22637d9209ac","title":"Feature based Summarization of Customers' Reviews of Online Products","text":"With the growing availability and popularity of opinion-rich resources such as review forums for the product sold online, choosing the right product from a large number of products have become difficult for the user. For trendy product, the number of customers\u2019 opinions available can be in the thousands. It becomes hard for the customers to read all the reviews and if he reads only a few of those reviews, then he may get a biased view about the product. Makers of the products may also feel difficult to maintain, keep track and understand the customers\u2019 views for the products. Several research works have been proposed in the past to address these issues, but they have certain limitations: The systems implemented are completely opaque, the reviews are not easier to perceive and are time consuming to analyze because of large irrelevant information apart from actual opinions about the features, the feature based summarization system that are implemented are more generic ones and static in nature. In this research, we proposed a dynamic system for feature based summarization of customers\u2019 opinions for online products, which works according to the domain of the product. We are extracting online reviews for a product on periodic bases, each time after extraction, we carry out the following work: Firstly, identification of features of a product from customers' opinions is done. Next, for each feature, its corresponding opinions\u2019 are extracted and their orientation or polarity (positive\/negative) is detected. The final polarity of feature-opinions pairs is calculated. At last, feature based summarizations of the reviews are generated, by extracting the relevant excerpts with respect to each feature-opinions pair and placing it into their respective feature based cluster. These feature based excerpts can easily be digested by the user. \u00a9 2013 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of KES International."}
{"_id":"9f6fe58a9e1b306ad9c89f36698bb8afd78413a9","title":"Trust or consequences? Causal effects of perceived risk and subjective norms on cloud technology adoption","text":"Cloud computing has become a popular alternative information curation solution for organizations. As more corporate proprietary information is stored in the Cloud, concerns about Cloud information security have also increased. This study investigates the causal effect of perceived risk and subjective norms on users\u2019 trust intention to adopt Cloud technology. A partial least squares structural equation modeling (PLS-SEM) analysis was performed to assess latent variables and examine moderating effects to Cloud technology adoption. Our findings suggest that a user\u2019s perceived risk and subjective norms have a significant effect on their trust intention vis-a-vis Cloud adoption, which leads to their decision on whether to adopt the Cloud technology. While a user\u2019s attitudes do influence their intention to trust the Cloud, their attitude is not moderated by either perceived risk or subjective norms. On the other hand, a user\u2019s perceived risk of the Cloud environment predominately moderates their knowledge and perceived behavioral control, which results in their knowledge and perceived behavioral control not having a direct effect on their intention to trust the Cloud. Moreover, a user\u2019s"}
{"_id":"e77edecf23e7296cdec9a916249d7ae310324cba","title":"Managing flood disasters on the built environment in the rural communities of Zimbabwe: Lessons learnt","text":"This article is about managing flood disasters affecting the built environment in the rural communities of Zimbabwe. Using Tsholotsho district in Matabeleland North province as a case study, the authors argue that flooding has adversely impacted the built environment through destroying infrastructure. The principal objectives of this study were to establish the impact of flood disasters on the built environment, to demarcate factors that perpetuate communities' vulnerabilities to flooding and to delineate challenges that negate the management of flood disasters in the built environment. This qualitative study was based on a purposive sample of 40 participants. Data were collected through semi-structured interviews and observation methods. The findings were that floods can damage human shelter, roads, bridges and dams. Locating homesteads near rivers and dams, using poor-quality construction materials, and lack of flood warning were found to perpetuate vulnerability to flooding. Poverty and costs of rebuilding infrastructure, lack of cooperation between the communities and duty-bearers, and failure to use indigenous knowledge were found to be impeding the management of flood disasters. The study concluded that flood disasters can wipe out community development gains accumulated over many years. Further, community vulnerability to flooding in the built environment is socially constructed. The study posits that addressing the root causes, reducing flood vulnerability and avoiding risk creation are viable options to development in the built environment. Lastly, reconstruction following flood disasters is arduous and gruelling, and not an easy exercise."}
{"_id":"692cce8f39622626274bb260346e8f645c5e1310","title":"A Reduced Reference Image Quality assessment for Multiply Distorted Images","text":"In this paper, we propose a new Reduced Reference Image Quality Metric for multiply degraded images based on a features extraction step and its combination. The selected features are extracted from the original image and its degraded version. Some of them aim to quantify the level of the considered degradation types, while the others quantify its sharpness. These features are then combined to obtain a single value, which corresponds to the predicted subjective score. Our method has been evaluated and compared in terms of correlation with subjective judgments to some recent methods by using the LIVE Multiply Distorted Image Quality Database."}
{"_id":"d83792571b14041714d3da73e2ca8f5fa59e638e","title":"From Social Media to GeoSocial Intelligence: Crowdsourcing Civic Co-management for Flood Response in Jakarta, Indonesia","text":"Here we present a review of PetaJakarta.org, a system designed to harness social media use in Jakarta for the purpose of relaying information about flood locations from citizen to citizen and from citizens and the city\u2019s emergency management agency. The project aimed to produce an open, real-time situational overview of flood conditions and provide decision support for the management agency, as well as offering the government a data source for post-event analysis. As such, the platform was designed as a socio-technological system and developed as a civic co-management tool to enable climate adaptation and community resilience in Jakarta, a delta megacity suffering enormous infrastructural instability due to a troubled confluence of environmental factors\u2014the city\u2019s rapid urbanization, its unique geographic limitations, and increasing sea-levels and monsoon rainfalls resulting from climate change. The chapter concludes with a discussion of future research in open source platform and their role in infrastructure and disaster management. From now on there is an interconnection, an intertwining, even a symbiosis of technologies, exchanges, movements, which makes it so that a flood\u2014for instance\u2014wherever it may occur, must necessarily involve relationships with any number of technical, social, economic, political intricacies that keep us from regarding it simply as a misadventure or a misfortune whose consequences can be more or less easily circumscribed. \u2014 Jean-Luc Nancy (2015, 3\u20134) * E. Turpin & T. Holderness SMART Infrastructure Facility, University of Wollongong, Australia e-mail: eturpin@uow.edu.au | tomas@uow.edu.au Chapter 6 in Social Media for Government Services, Springer 2016 (preprint version)"}
{"_id":"284387ee4657d761be893316a0d608c52fb018c5","title":"Physically Based Real-Time Translucency for Leaves","text":"This paper presents a new shading model for real-time rendering of plant leaves that reproduces all important attributes of a leaf and allows for a large number of leaves to be shaded. In particular, we use a physically based model for accurate subsurface scattering on the translucent side of directly lit leaves. For real-time rendering of this model, we formulate it as an image convolution process and express the result in an efficient directional basis that is fast to evaluate. We also propose a data acquisition method for leaves that uses off-the-shelf devices."}
{"_id":"6b8fdca2795732ad591a8c247b2dfc89ce9b4c33","title":"Beagle: Automated Extraction and Interpretation of Visualizations from the Web","text":"\"How common is interactive visualization on the web?\" \"What is the most popular visualization design?\" \"How prevalent are pie charts really?\" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 85% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare for the visualization tools that were studied. Our findings also suggest that the total visualization types supported by a given tool could factor into its ease of use. However this effect appears to be mitigated by providing a variety of diverse expert visualization examples to users."}
{"_id":"ad86e63d5ed30ee4484b5d00c1742e93c9e45b49","title":"A review on plant disease detection using image processing","text":"India is the agriculture based country, since it contributes 7.68 percent of total global agricultural output. In India, agricultural sector contributes about seventeen percentage of total Indian gross domestic product (GDP). Effective growth and improved yield of plants are necessary for increment of farmer's profit and economy of India. For this purpose farmers need domain experts for manual monitoring of plants. But manual monitoring will not give satisfactory result all the time. Moreover, domain experts are not available at all regions and are expensive as farmers have to pay fees including travelling charges. Hence, it requires developing an efficient smart farming technique which will help for better yield and growth with less human efforts. In this paper, we provide a review on methods developed by various researchers for detection of diseases in plants, in the field of image processing. It includes research in disease detection of plants such as apple, grapes, pepper, pomegranate, tomato etc."}
{"_id":"de07b140f738b75f04532ecc149bdcf3bc1ee6af","title":"Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks","text":"A lot of knowledge about procedures and how-tos are described in text. Recently, extracting semantic relations from the procedural text has been actively explored. Prior work mostly has focused on finding relationships among verb-noun pairs or clustering of extracted pairs. In this paper, we investigate the problem of learning individual procedure-specific relationships (e.g. is method of, is alternative of, or is subtask of ) among sentences. To identify the relationships, we propose an end-to-end neural network architecture, which can selectively learn important procedure-specific relationships. Using this approach, we could construct a how-to knowledge base from the largest procedure sharing-community, wikihow.com. The evaluation of our approach shows that it outperforms the existing entity relationship extraction algorithms."}
{"_id":"65dc6382c7e7c8d8a96e19db9235f4884f642bb0","title":"A distributed camera system for multi-resolution surveillance","text":"We describe an architecture for a multi-camera, multi-resolution surveillance system. The aim is to support a set of distributed static and pan-tilt-zoom (PTZ) cameras and visual tracking algorithms, together with a central supervisor unit. Each camera (and possibly pan-tilt device) has a dedicated process and processor. Asynchronous interprocess communications and archiving of data are achieved in a simple and effective way via a central repository, implemented using an SQL database."}
{"_id":"2e813ed09b47aaca10209c00d00d6f65b08d2cdc","title":"Separability of EEG signals recorded during right and left motor imagery using adaptive autoregressive parameters.","text":"Electroencephalogram (EEG) recordings during right and left motor imagery can be used to move a cursor to a target on a computer screen. Such an EEG-based brain-computer interface (BCI) can provide a new communication channel to replace an impaired motor function. It can be used by, e.g., patients with amyotrophic lateral sclerosis (ALS) to develop a simple binary response in order to reply to specific questions. Four subjects participated in a series of on-line sessions with an EEG-based cursor control. The EEG was recorded from electrodes overlying sensory-motor areas during left and right motor imagery. The EEG signals were analyzed in subject-specific frequency bands and classified on-line by a neural network. The network output was used as a feedback signal. The on-line error (100%-perfect classification) was between 10.0 and 38.1%. In addition, the single-trial data were also analyzed off-line by using an adaptive autoregressive (AAR) model of order 6. With a linear discriminant analysis the estimated parameters for left and right motor imagery were separated. The error rate obtained varied between 5.8 and 32.8% and was, on average, better than the on-line results. By using the AAR-model for on-line classification an improvement in the error rate can be expected, however, with a classification delay around 1 s."}
{"_id":"03b18dcde7ba5bb0e87b2bdb68ab7af951daf162","title":"On Using Very Large Target Vocabulary for Neural Machine Translation","text":"Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English\u2192German and English\u2192French translation tasks of WMT\u201914."}
{"_id":"060e380b28be29b7eda509981a50b4406ea6b21b","title":"Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation","text":"The attentional mechanism has proven to be effective in improving end-to-end neural machine translation. However, due to the intricate structural divergence between natural languages, unidirectional attention-based models might only capture partial aspects of attentional regularities. We propose agreement-based joint training for bidirectional attention-based end-to-end neural machine translation. Instead of training source-to-target and target-to-source translation models independently, our approach encourages the two complementary models to agree on word alignment matrices on the same training data. Experiments on ChineseEnglish and English-French translation tasks show that agreement-based joint training significantly improves both alignment and translation quality over independent training."}
{"_id":"1c23e1ad1a538416e8123f128a87c928b09be868","title":"Alignment by Agreement","text":"We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. Disciplines Computer Sciences Comments Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL '06). Association for Computational Linguistics, Stroudsburg, PA, USA, 104-111. DOI=10.3115\/1220835.1220849 http:\/\/dx.doi.org\/10.3115\/1220835.1220849 \u00a9 ACM, 2006. This is the author's version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, {(2006)} http:\/\/doi.acm.org\/10.3115\/1220835.1220849\" Email permissions@acm.org This conference paper is available at ScholarlyCommons: http:\/\/repository.upenn.edu\/cis_papers\/533 Alignment by Agreement Percy Liang UC Berkeley Berkeley, CA 94720 pliang@cs.berkeley.edu Ben Taskar UC Berkeley Berkeley, CA 94720 taskar@cs.berkeley.edu Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu"}
{"_id":"f7b48b0028a9887f85fe857b62441f391560ef6d","title":"Fan-Beam Millimeter-Wave Antenna Design Based on the Cylindrical Luneberg Lens","text":"A new design of two-dimensional cylindrical Luneberg lens is introduced based on TE10 mode propagation between parallel plates, with special focus on ease of manufacturing. The parallel plates are partially filled with low cost polymer material (Rexolite epsivr = 2.54) to match Luneberg's law. A planar linear tapered slot antenna (LTSA) is inserted into the air region between the parallel plates at the edge of the Luneberg lens as a feed antenna, with fine positioning to the focal point of the Luneberg lens to optimize the antenna system performance. A combined ray-optics\/diffraction method is used to obtain the radiation pattern of the system and results are compared with predictions of a time domain numerical solver. Measurements done on a 10-cm Luneberg lens designed for operation at 30 GHz agree very well with predictions. For this prototype, 3-dB E- and if-plane beamwidths of 6.6deg and 54deg respectively were obtained, and the sidelobe level in the E-plane was -17.7-dB. Although the parallel plate configuration should lead to a narrow band design due to the dispersion characteristics of the TE10 mode, the measurement results demonstrate broadband characteristics with radiation efficiencies varying between 43% and 72% over the tested frequency band of 26.5-37 GHz. The designed cylindrical Luneberg lens can be used to launch multiple beams by implementing an arc array of planar LTSA elements at the periphery of the lens, and can be easily extended to higher mm-wave frequencies."}
{"_id":"0b789e34df1af1c23d64ee82dce9aeb6982bbba9","title":"SINGO: A single-end-operative and genderless connector for self-reconfiguration, self-assembly and self-healing","text":"Flexible and reliable connection is critical for self-reconfiguration, self-assembly, or self-healing. However, most existing connection mechanisms suffer from a deficiency that a connection would seize itself if one end malfunctions or is out of service. To mitigate this limitation on self-healing, this paper presents a new SINGO connector that can establish or disengage a connection even if one end of the connection is not operational. We describe the design and the prototype of the connector and demonstrate its performance by both theoretical analysis and physical experimentations."}
{"_id":"7c3d54d7ca0d0960ec5e03db9e75c09db7005a2c","title":"Content matters: A study of hate groups detection based on social networks analysis and web mining","text":"In recent years, with rapid growth of social networking websites, users are very active in these platforms and large amount of data are aggregated. Among those social networking websites, Facebook is the most popular website that has most users. However, in Facebook, the abusing problem is a very critical issue, such as Hate Groups. Therefore, many researchers are devoting on how to detect potential hate groups, such as using the techniques of social networks analysis. However, we believe content is also a very important factors for hate groups detection. Thus, in this paper, we will propose an architecture to for hate groups detection which is based on the technique of Social Networks Analysis and Web Mining (Text Mining; Natural Language Processing). From the experiment result, it shows that content plays an critical role for hate groups detection and the performance is better than the system that just applying social networks analysis."}
{"_id":"1f95617ace456a1d543ccc6de4171904d8aeb1d2","title":"Occupational risks and challenges of seafaring.","text":"UNLABELLED\nSeafarers are exposed to a high diversity of occupational health hazards onboard ships.\n\n\nOBJECTIVE\nThe aim of this article is to present a survey of the current, most important hazards in seafaring including recommendations on measures how to deal with these problems.\n\n\nMETHODS\nThe review is based on maritime expert opinions as well a PubMed analysis related to the occupational risks of seafaring.\n\n\nRESULTS\nDespite recent advances in injury prevention, accidents due to harmful working and living conditions at sea and of non-observance of safety rules remain a main cause of injury and death. Mortality in seafaring from cardiovascular diseases (CVD) is mainly caused by increased risks and impaired treatment options of CVD at sea. Further, shipboard stress and high demand may lead to fatigue and isolation which have an impact on the health of onboard seafarers. Communicable diseases in seafaring remain an occupational problem. Exposures to hazardous substances and UV-light are important health risks onboard ships. Because of harsh working conditions onboard including environmental conditions, sufficient recreational activities are needed for the seafarers' compensation both onboard and ashore. However, in reality there is often a lack of leisure time possibilities.\n\n\nDISCUSSION\nSeafaring is still an occupation with specific work-related risks. Thus, a further reduction of occupational hazards aboard ships is needed and poses a challenge for maritime health specialists and stakeholders. Nowadays, maritime medicine encompasses a broad field of workplaces with different job-related challenges."}
{"_id":"8cab4d4370461767e7193679a308d92f6b89f00a","title":"IoT Security: Ongoing Challenges and Research Opportunities","text":"The Internet of Things (IoT) opens opportunities for wearable devices, home appliances, and software to share and communicate information on the Internet. Given that the shared data contains a large amount of private information, preserving information security on the shared data is an important issue that cannot be neglected. In this paper, we begin with general information security background of IoT and continue on with information security related challenges that IoT will encountered. Finally, we will also point out research directions that could be the future work for the solutions to the security challenges that IoT encounters."}
{"_id":"57d774b8592b4b3f83f1304be43701ad8517e79a","title":"Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization","text":"In multilabel learning, each instance in the training set is associated with a set of labels and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, this problem is addressed in the way that a neural network algorithm named BP-MLL, i.e., backpropagation for multilabel learning, is proposed. It is derived from the popular backpropagation algorithm through employing a novel error function capturing the characteristics of multilabel learning, i.e., the labels belonging to an instance should be ranked higher than those not belonging to that instance. Applications to two real-world multilabel learning problems, i.e., functional genomics and text categorization, show that the performance of BP-MLL is superior to that of some well-established multilabel learning algorithms"}
{"_id":"2881b79ff142496c27d9558361e48f105208dec4","title":"Investigating Information Systems with Action Research","text":"Action research is an established research method in use in the social and medical sciences since the mid-twentieth century, and has increased in importance for information systems toward the end of the 1990s. Its particular philosophic context is couched in strongly post-positivist assumptions such as idiographic and interpretive research ideals. Action research has developed a history within information systems that can be explicitly linked to early work by Lewin and the Tavistock Institute. Action research varies in form, and responds to particular problem domains. The most typical form is a participatory method based on a five-step model, which is exemplified by published IS research."}
{"_id":"e578f65fd5e499a72481da3a288a401f564a779a","title":"Chapter 2 Depression and a Stepped Care Model","text":"Given the public health significance of depression and the limited resources available for providing evidence-based treatment, there is a need to develop effective models of care to reduce the personal and societal costs of the disorder. Within stepped care service provisions, all patients presenting with symptoms of depression generally are first offered the lowest intensity and least intrusive intervention deemed necessary following assessment and triage. Only when patients do not show improvement do they move to higher, more intensive levels of care. However, stepped care models also provide information to aid clinicians in decision making regarding selection of treatment strategies that are most appropriate for an individual patient. For some individuals, lower levels of care would never be appropriate or may not be preferred by the consumer. Thus, stepped interventions offer a variety of treatment options to match the intensity of the patient\u2019s presenting problem as well as potential patient preference. In this chapter, we discuss various strategies for treating depression consistent with a stepped model of care beginning with least intensive treatment and then moving up through the hierarchy of steps of care. While this is not a comprehensive review of all available treatments for depression, the chapter is designed to make clinicians aware of specific strategies for addressing depressive symptoms and to provide guidance about resources available at the various levels of care."}
{"_id":"0562bc5f82b40e2e9c0ae035aa2dd1da6107017c","title":"vSlicer: latency-aware virtual machine scheduling via differentiated-frequency CPU slicing","text":"Recent advances in virtualization technologies have made it feasible to host multiple virtual machines (VMs) in the same physical host and even the same CPU core, with fair share of the physical resources among the VMs. However, as more VMs share the same core\/CPU, the CPU access latency experienced by each VM increases substantially, which translates into longer I\/O processing latency perceived by I\/O-bound applications. To mitigate such impact while retaining the benefit of CPU sharing, we introduce a new class of VMs called latency-sensitive VMs (LSVMs), which achieve better performance for I\/O-bound applications while maintaining the same resource share (and thus cost) as other CPU-sharing VMs. LSVMs are enabled by vSlicer, a hypervisor-level technique that schedules each LSVM more frequently but with a smaller micro time slice. vSlicer enables more timely processing of I\/O events by LSVMs, without violating the CPU share fairness among all sharing VMs. Our evaluation of a vSlicer prototype in Xen shows that vSlicer substantially reduces network packet round-trip times and jitter and improves application-level performance. For example, vSlicer doubles both the connection rate and request processing throughput of an Apache web server; reduces a VoIP server's upstream jitter by 62%; and shortens the execution times of Intel MPI benchmark programs by half or more."}
{"_id":"b5429dd7cce627a1699daa0a6a0edc9d0e3081f1","title":"CENTURION: Incentivizing multi-requester mobile crowd sensing","text":"The recent proliferation of increasingly capable mobile devices has given rise to mobile crowd sensing (MCS) systems that outsource the collection of sensory data to a crowd of participating workers that carry various mobile devices. Aware of the paramount importance of effectively incentivizing participation in such systems, the research community has proposed a wide variety of incentive mechanisms. However, different from most of these existing mechanisms which assume the existence of only one data requester, we consider MCS systems with multiple data requesters, which are actually more common in practice. Specifically, our incentive mechanism is based on double auction, and is able to stimulate the participation of both data requesters and workers. In real practice, the incentive mechanism is typically not an isolated module, but interacts with the data aggregation mechanism that aggregates workers' data. For this reason, we propose CENTURION, a novel integrated framework for multi-requester MCS systems, consisting of the aforementioned incentive and data aggregation mechanism. CENTURION's incentive mechanism satisfies truthfulness, individual rationality, computational efficiency, as well as guaranteeing non-negative social welfare, and its data aggregation mechanism generates highly accurate aggregated results. The desirable properties of CENTURION are validated through both theoretical analysis and extensive simulations."}
{"_id":"463d85ce8348eba0935d155d557634ed7e57bb07","title":"FEATURE EXTRACTION USING SURF ALGORITHM FOR OBJECT RECOGNITION","text":"Video surveillance is active research topic in computer vision research area for humans & vehicles, so it is used over a great extent. Multiple images generated using a fixed camera contains various objects, which are taken under different variations, illumination changes after that the object\u2019s identity and orientation are provided to the user. This scheme is used to represent individual images as well as various objects classes in a single, scale and rotation invariant model.The objective is to improve object recognition accuracy for surveillance purposes & to detect multiple objects with sufficient level of scale invariance.Multiple objects detection& recognition is important in the analysis of video data and higher level security system. This method can efficiently detect the objects from query images as well as videos by extracting frames one by one. When given a query image at runtime, by generating the set of query features and it will find best match it to other sets within the database. Using SURF algorithm find the database object with the best feature matching, then object is present in the query image. Keywords\u2014 Image recognition, Query image, Local feature, Surveillance system, SURF algorithm."}
{"_id":"940825a6c2e4000f2f95ea70b8358e4bbd63b77c","title":"Efficient Hierarchical Identity-Based Signature With Batch Verification for Automatic Dependent Surveillance-Broadcast System","text":"The automatic-dependent surveillance-broad-cast (ADS-B) is generally regarded as the most important module in air traffic surveillance technology. To obtain better airline security, ADS-B system will be deployed in most airspace by 2020, where aircraft will be equipped with an ADS-B device that periodically broadcasts messages to other aircraft and ground station controllers. Due to the open communication environment, the ADS-B system is subject to a broad range of attacks. To simultaneously implement both integrity and authenticity of messages transmitted in the ADS-B system, Yang et al. proposed a new authentication frame based on the three-level hierarchical identity-based signature (TLHIBS) scheme with batch verification, as well as constructing two schemes for the ADS-B system. However, neither TLHIBS schemes are sufficiently lightweight for practical deployment due to the need for complex hash-to-point operation or expensive certification management. In this paper, we construct an efficient TLHIBS scheme with batch verification for the ADS-B system. Our scheme does not require hash-to-point operation or (expensive) certification management. We then prove the TLHIBS scheme secure in the random oracle model. We also demonstrate the practicality of the scheme using experiments, whose findings indicate that the TLHIBS scheme supports attributes required by the ADS-B system without the computation cost in Chow et al.'s scheme and Yang et al.'s TLHIBS schemes."}
{"_id":"5721cc62c8cd71e5302330dd2d4a1ab0b38f5553","title":"Workplace harassment: double jeopardy for minority women.","text":"To date there have been no studies of how both sex and ethnicity might affect the incidence of both sexual and ethnic harassment at work. This article represents an effort to fill this gap. Data from employees at 5 organizations were used to test whether minority women are subject to double jeopardy at work, experiencing the most harassment because they are both women and members of a minority group. The results supported this prediction. Women experienced more sexual harassment than men, minorities experienced more ethnic harassment than Whites, and minority women experienced more harassment overall than majority men, minority men, and majority women."}
{"_id":"6d60a7255878f03e3405c60c72a6420c38b165bc","title":"Exposure of Children and Adolescents to Alcohol Marketing on Social Media Websites","text":"AIMS\nIn 2011, online marketing became the largest marketing channel in the UK, overtaking television for the first time. This study aimed to describe the exposure of children and young adults to alcohol marketing on social media websites in the UK.\n\n\nMETHODS\nWe used commercially available data on the three most used social media websites among young people in the UK, from December 2010 to May 2011. We analysed by age (6-14 years; 15-24 years) and gender the reach (proportion of internet users who used the site in each month) and impressions (number of individual pages viewed on the site in each month) for Facebook, YouTube and Twitter. We further analysed case studies of five alcohol brands to assess the marketer-generated brand content available on Facebook, YouTube and Twitter in February and March 2012.\n\n\nRESULTS\nFacebook was the social media site with the highest reach, with an average monthly reach of 89% of males and 91% of females aged 15-24. YouTube had a similar average monthly reach while Twitter had a considerably lower usage in the age groups studied. All five of the alcohol brands studied maintained a Facebook page, Twitter page and YouTube channel, with varying levels of user engagement. Facebook pages could not be accessed by an under-18 user, but in most cases YouTube content and Twitter content could be accessed by those of all ages.\n\n\nCONCLUSION\nThe rise in online marketing of alcohol and the high use of social media websites by young people suggests that this is an area requiring further monitoring and regulation."}
{"_id":"31c280aa6c4f6a98c0fa5e1a843355e1e8da4007","title":"An implementation of the FP-growth algorithm","text":"The FP-growth algorithm is currently one of the fastest approaches to frequent item set mining. In this paper I describe a C implementation of this algorithm, which contains two variants of the core operation of computing a projection of an FP-tree (the fundamental data structure of the FP-growth algorithm). In addition, projected FP-trees are (optionally) pruned by removing items that have become infrequent due to the projection (an approach that has been called FP-Bonsai). I report experimental results comparing this implementation of the FP-growth algorithm with three other frequent item set mining algorithms I implemented (Apriori, Eclat, and Relim)."}
{"_id":"ee42bceb15d28ce0c7fcd3e37d9a564dfbb3ab90","title":"An analytical method for diseases prediction using machine learning techniques","text":""}
{"_id":"4d357ffc1cf60d3f34b5345899619882791474bb","title":"Deep Reinforcement Learning for General Video Game AI","text":"The General Video Game AI (GVGAI) competition and its associated software framework provides a way of benchmarking AI algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search. In this paper, we describe how we interface GVGAI to the OpenAI Gym environment, a widely used way of connecting agents to reinforcement learning problems. Using this interface, we characterize how widely used implementations of several deep reinforcement learning algorithms fare on a number of GVGAI games. We further analyze the results to provide a first indication of the relative difficulty of these games relative to each other, and relative to those in the Arcade Learning Environment under similar conditions."}
{"_id":"429e2f4d95fe77d8c61245265529963df171da68","title":"Working Memory Capacity as Executive Attention","text":"Performance on measures of working memory (WM) capacity predicts performance on a wide range of real-world cognitive tasks. I review the idea that WM capacity (a) is separable from short-term memory, (b) is an important component of general fluid intelligence, and (c) represents a domainfree limitation in ability to control attention. Studies show that individual differences in WM capacity are reflected in performance on antisaccade, Stroop, and dichotic-listening tasks. WM capacity, or executive attention, is most important under conditions in which interference leads to retrieval of response tendencies that conflict with the current task."}
{"_id":"f50bd6c24e5fef71e54d0ec873a733b6fd6302e4","title":"Characterizing Horn Antenna Signals for Breast Cancer Detection","text":"An in-depth analysis of signals from horn antennas used in breast cancer detection research has been carried out. It was found that following the excitation signal, both the double-ridged and the quad-ridged ultra-wideband horn antennas radiate additional unwanted signals that persistently oscillate. Undesirable signal oscillations sources were identified as the horn antenna cavity resonance and inherent antenna LC resonance. These signals interfere with the tumor\u2019s signal response and need to be eliminated for successful detection of the cancerous growth. This paper proposes solutions to remove or minimize these signals without affecting antenna parameters such as bandwidth, gain, ports isolation, and polarization isolation. Modification of the antenna cavity successfully suppressed the unwanted cavity oscillation. Modification of the antenna waveguide reduced inductance and consequently mitigated LC oscillation. The resulting time and frequency domain horn antenna signals demonstrate the effectiveness of the proposed methods. Finally, a breast phantom with a tumor is simulated using signals from the original and a modified horn antenna. The delay and sum method is used to create images. The breast images demonstrate enhanced image quality through the reduction of clutter using the proposed techniques. The signal-to-clutter ratios are 0.448 and 1.6823 dB for the images produced by using the original and modified antennas, respectively."}
{"_id":"bd4113b9b41f663465f2bcd5d97103715e67e273","title":"Visualising the structure of architectural open spaces based on shape analysis","text":"Visualisation and interpretation of architectural spaces are intellectually challenging and multifaceted exercises with fairly wide ranging applications and implications. In the context of urban planning, they are most commonly undertaken to evaluate the usage of the architectural space to ensure efficient navigation and accessibility [1]. These exercises clearly assume a certain influence of the built structures on the human cognition. However, what aspect of architectural space affects the human behaviour still remains an open debate. In this respect, it is closely similar to an exercise to identify the unknown visual variables in information visualization. Since a quantitative analysis of the architectural geometric structure on a large scale will be a daunting computational task, the open space bounded by the built structures is studied instead [2]. The study of architectural open spaces essentially involves the computation of the visibility polygon or isovist (space visible all around a viewpoint, see Figure 1a) from a viewpoint and calculating various shape measures of the visibility polygon. The isovist computation involves drawing rays"}
{"_id":"13b77bed4038262f2dedc3bfba8a1905e1e8dd2b","title":"Design and Implementation of the LogicBlox System","text":"The LogicBlox system aims to reduce the complexity of software development for modern applications which enhance and automate decision-making and enable their users to evolve their capabilities via a ``self-service'' model. Our perspective in this area is informed by over twenty years of experience building dozens of mission-critical enterprise applications that are in use by hundreds of large enterprises across industries such as retail, telecommunications, banking, and government. We designed and built LogicBlox to be the system we wished we had when developing those applications.\n In this paper, we discuss the design considerations behind the LogicBlox system and give an overview of its implementation, highlighting innovative aspects. These include: LogiQL, a unified and declarative language based on Datalog; the use of purely functional data structures; novel join processing strategies; advanced incremental maintenance and live programming facilities; a novel concurrency control scheme; and built-in support for prescriptive and predictive analytics."}
{"_id":"d2752e691248eb77223bf7a4a3cd4d8878d682b7","title":"Should I use TensorFlow","text":"Google\u2019s Machine Learning framework TensorFlow was opensourced in November 2015 [1] and has since built a growing community around it. TensorFlow is supposed to be flexible for research purposes while also allowing its models to be deployed productively [7]. This work is aimed towards people with experience in Machine Learning considering whether they should use TensorFlow in their environment. Several aspects of the framework important for such a decision are examined, such as the heterogenity, extensibility and its computation graph. A pure Python implementation of linear classification is compared with an implementation utilizing TensorFlow. I also contrast TensorFlow to other popular frameworks with respect to modeling capability, deployment and performance and give a brief description of the current adaption of the framework."}
{"_id":"443362dc552b36c33138c415408d307213ddfa36","title":"devices and apps for health care professionals : uses and benefits table 1 uses for Mobile devices and apps by health care professionals","text":""}
{"_id":"7d2a06f43648cf023566b58143a4e7b50f3b80ca","title":"Robust Abandoned Object Detection Using Dual Foregrounds","text":"As an alternative to the tracking-based approaches that heavily depend on accurate detection of moving objects, which often fail for crowded scenarios, we present a pixelwise method that employs dual foregrounds to extract temporally static image regions. Depending on the application, these regions indicate objects that do not constitute the original background but were brought into the scene at a subsequent time, such as abandoned and removed items, illegally parked vehicles. We construct separate longand short-term backgrounds that are implemented as pixelwise multivariate Gaussian models. Background parameters are adapted online using a Bayesian update mechanism imposed at different learning rates. By comparing each frame with these models, we estimate two foregrounds. We infer an evidence score at each pixel by applying a set of hypotheses on the foreground responses, and then aggregate the evidence in time to provide temporal consistency. Unlike optical flow-based approaches that smear boundaries, our method can accurately segment out objects even if they are fully occluded. It does not require on-site training to compensate for particular imaging conditions. While having a low-computational load, it readily lends itself to parallelization if further speed improvement is necessary."}
{"_id":"41b77840bf309358ecf45b16d00053ed12aea5c0","title":"Identifying careless responses in survey data.","text":"When data are collected via anonymous Internet surveys, particularly under conditions of obligatory participation (such as with student samples), data quality can be a concern. However, little guidance exists in the published literature regarding techniques for detecting careless responses. Previously several potential approaches have been suggested for identifying careless respondents via indices computed from the data, yet almost no prior work has examined the relationships among these indicators or the types of data patterns identified by each. In 2 studies, we examined several methods for identifying careless responses, including (a) special items designed to detect careless response, (b) response consistency indices formed from responses to typical survey items, (c) multivariate outlier analysis, (d) response time, and (e) self-reported diligence. Results indicated that there are two distinct patterns of careless response (random and nonrandom) and that different indices are needed to identify these different response patterns. We also found that approximately 10%-12% of undergraduates completing a lengthy survey for course credit were identified as careless responders. In Study 2, we simulated data with known random response patterns to determine the efficacy of several indicators of careless response. We found that the nature of the data strongly influenced the efficacy of the indices to identify careless responses. Recommendations include using identified rather than anonymous responses, incorporating instructed response items before data collection, as well as computing consistency indices and multivariate outlier analysis to ensure high-quality data."}
{"_id":"9c58c19b01b04ca7dbf122d59684bf05353cc77b","title":"A new scale of social desirability independent of psychopathology.","text":"It has long been recognized that personality test scores are influenced by non-test-relevant response determinants. Wiggins and Rumrill (1959) distinguish three approaches to this problem. Briefly, interest in the problem of response distortion has been concerned with attempts at statistical correction for \"faking good\" or \"faking bad\" (Meehl & Hathaway, 1946), the analysis of response sets (Cronbach, 1946,1950), and ratings of the social desirability of personality test items (Edwards, 19 5 7). A further distinction can be made, however, which results in a somewhat different division of approaches to the question of response distortion. Common to both the Meehl and Hathaway corrections for faking good and faking bad and Cronbach's notion of response sets is an interest in the test behavior of the subject(S). By social desirability, on the other hand, Edwards primarily means the \"scale value for any personality statement such that the scale value indicates the position of the statement on the social desirability continuum . . .\" (1957, p. 3). Social desirability, thus, has been used to refer to a characteristic of test items, i.e., their scale position on a social desirability scale. Whether the test behavior of 5s or the social desirability properties of items are the focus of interest, however, it now seems clear that underlying both these approaches is the concept of statistical deviance. In the construction of the MMPI K scale, for example, items were selected which differentiated between clinically normal persons producing abnormal te\u00a5Tpfpfiles~snd^cTinically abnormal individuals with abnormal test profiles, and between clinically abnormal persons with normal test profiles and abnormal 5s whose test records were abnormal. Keyed responses to the K scale items tend to be statistically deviant in the parent populations. Similarly, the development of the Edwards Social Desirability Scale (SDS) illustrates this procedure. Items were drawn from various MMPI scales (F, L, K, and the Manifest Anxiety Scale [Taylor, 1953]) and submitted to judges who categorized them as either socially desirable or socially undesirable. Only items on which there was unanimous agreement among the 10 judges were included in the SDS. It seems clear that the items in Edwards SDS would, of necessity, have extreme social desirability scale positions or, in other words, be statistically deviant. Some unfortunate consequences follow from the strict use of the statistical deviance model in the development of-sOcialTtesirSbTBty scales. With items drawn from the MMPI, it is apparent that in addition to their scalability for social desirability the items may also be characterized by their content which,^n a general sense, has pathological implications. When a social desrrabtltty^scale constructed according to this procedure is then applied to a college student population, the meaning of high social desirability scores is not at all clear. When 5s given the Edwards SDS deny, for example, that their sleep is fitful and disturbed (Item 6) or that they worry quite a bit over possible misfortunes (Item 35), it cannot be determined whether these responses are attributable to social desirability or to a genuine absence of such symptoms. The probability of occurrence of the symptoms represented in MMPI items (and incorportated in the SDS)"}
{"_id":"b65c9bac7a42ac4a52a7be4d8d58b152d9124d11","title":"Simultaneous administration of the Rosenberg Self-Esteem Scale in 53 nations: exploring the universal and culture-specific features of global self-esteem.","text":"The Rosenberg Self-Esteem Scale (RSES) was translated into 28 languages and administered to 16,998 participants across 53 nations. The RSES factor structure was largely invariant across nations. RSES scores correlated with neuroticism, extraversion, and romantic attachment styles within nearly all nations, providing additional support for cross-cultural equivalence of the RSES. All nations scored above the theoretical midpoint of the RSES, indicating generally positive self-evaluation may be culturally universal. Individual differences in self-esteem were variable across cultures, with a neutral response bias prevalent in more collectivist cultures. Self-competence and self-liking subscales of the RSES varied with cultural individualism. Although positively and negatively worded items of the RSES were correlated within cultures and were uniformly related to external personality variables, differences between aggregates of positive and negative items were smaller in developed nations. Because negatively worded items were interpreted differently across nations, direct cross-cultural comparisons using the RSES may have limited value."}
{"_id":"5d86a5dbcb22cee5b931d8e9d6a1a95d6d8f394d","title":"Assessing psychopathic attributes in a noninstitutionalized population.","text":"The present study examined antisocial dispositions in 487 university students. Primary and secondary psychopathy scales were developed to assess a protopsychopathic interpersonal philosophy. An antisocial action scale also was developed for purposes of validation. The primary, secondary, and antisocial action scales were correlated with each other and with boredom susceptibility and disinhibition but not with experience seeking and thrill and adventure seeking. Secondary psychopathy was associated with trait anxiety. Multiple regression analysis revealed that the strongest predictors of antisocial action were disinhibition, primary psychopathy, secondary psychopathy, and sex, whereas thrill and adventure seeking was a negative predictor. This argues against a singular behavioral inhibition system mediating both antisocial and risk-taking behavior. These findings are also consistent with the view that psychopathy is a continuous dimension."}
{"_id":"822838560825f0b58587d33ce8d28b743b8b851a","title":"Wound healing activity of the fruit skin of Punica granatum.","text":"The skin of the fruit and the bark of Punica granatum are used as a traditional remedy against diarrhea, dysentery, and intestinal parasites. The fruit skin extract of P. granatum was tested for its wound healing activity in rats using an excision wound model. The animals were divided into three groups of six each. The experimental group of animals was topically treated with P. granatum at a dose of 100 mg\/kg every day for 15 days, while the controls and standard group animals were treated with petroleum jelly and mupirocin ointment, respectively. Phytochemical analysis of the extract revealed the presence of saponins, triterpenes, tannins, alkaloids, flavonoids, and cardiac glycosides. Extract-treated animals exhibited 95% reduction in the wound area when compared with controls (84%), which was statistically significant (P<.01). The extract-treated wounds were found to epithelize faster compared with controls. The hydroxyproline content of extract-treated animals was significantly higher than controls (P<.05). The fruit skin extract did not show any antimicrobial activity against the microrganisms tested. P. granatum promotes significant wound healing in rats and further evaluation of this activity in humans is suggested."}
{"_id":"652e78b85a9b40a4dd7f389e862ff2dc9ac9c661","title":"Functional near infrared spectroscopy (fNIRS): an emerging neuroimaging technology with important applications for the study of brain disorders.","text":"Functional near-infrared spectroscopy (fNIRS) is an emerging functional neuroimaging technology offering a relatively non-invasive, safe, portable, and low-cost method of indirect and direct monitoring of brain activity. Most exciting is its potential to allow more ecologically valid investigations that can translate laboratory work into more realistic everyday settings and clinical environments. Our aim is to acquaint clinicians and researchers with the unique and beneficial characteristics of fNIRS by reviewing its relative merits and limitations vis-\u00e0-vis other brain-imaging technologies such as functional magnetic resonance imaging (fMRI). We review cross-validation work between fMRI and fNIRS, and discuss possible reservations about its deployment in clinical research and practice. Finally, because there is no comprehensive review of applications of fNIRS to brain disorders, we also review findings from the few studies utilizing fNIRS to investigate neurocognitive processes associated with neurological (Alzheimer's disease, Parkinson's disease, epilepsy, traumatic brain injury) and psychiatric disorders (schizophrenia, mood disorders, anxiety disorders)."}
{"_id":"4124dcce3d6ab7a4e1b8987c490277d2244abdf0","title":"Dose the Use of Big Data Analytics Guarantee a High Firm Performance? An Empirical Study Bases on the Dynamic Capabilities Theory","text":"For the sake of enriching relevant research field, this study focuses on two main questions: (1) What is the effect of DBA usage on firm performance (including market performance and operational performance)? and (2) What factors that drive organizations to use big data analytics (BDA) are key drivers, based on dynamic capabilities theory? And we also identified the moderating effect of environmental dynamism between BDA use and firm performance. Furthermore, we introduce the need pull\/technology push framework to identify and theorize paths via which factors influence the use of BDA. Purely from the need pull\/technology push theory perspective, we propose that perceived usefulness of BDA should belong to need pull factor. Thus, we select perceived usefulness and satisfaction level with existing IT technology as need pull factors, BDA use of competitors and technology compatibility as technology push factors for big data analytics (BDA) use, and examines if these factors have a positive effect on big data analytics (BDA)."}
{"_id":"6abe5eda71c3947013c59bbae700402813a1bc7f","title":"Performance Comparison between Five NoSQL Databases","text":"Recently NoSQL databases and their related technologies are developing rapidly and are widely applied in many scenarios with their BASE (Basic Availability, Soft state, Eventual consistency) features. At present, there are more than 225 kinds of NoSQL databases. However, the overwhelming amount and constantly updated versions of databases make it challenging for people to compare their performance and choose an appropriate one. This paper is trying to evaluate the performance of five NoSQL clusters (Redis, MongoDB, Couchbase, Cassandra, HBase) by using a measurement tool \u2013 YCSB (Yahoo! Cloud Serving Benchmark), explain the experimental results by analyzing each database's data model and mechanism, and provide advice to NoSQL developers and users."}
{"_id":"dbf2fab97f9570c196995f86eb685d8ecc2a7043","title":"Cathodic protection by zinc sacrificial anodes: impact on marine sediment metallic contamination.","text":"Cathodic protection by sacrificial zinc anodes is often applied to prevent immerged metallic structures from corrosion. But this technique induces the zinc anodes dissolution, which can induce marine sediments and seawater contamination. A large scale experiment, in natural seawater, was conducted during 12 months, in order to evaluate the potential environmental impact of this continuous zinc dissolution, and of some necessary cleaning operations of the anodes surfaces. The heavy metal (Cr, Cu, Pb and Zn) concentration in water and sediment samples was monitored. A sequential extraction procedure was applied on sediment samples to differentiate the zinc mobile fractions from the residual one. A significant increase of zinc concentration was observed in water as well as in the surface sediments under the specific operating conditions. Sediments then become a secondary pollution source, as the sorbed labile zinc can be remobilized to seawater."}
{"_id":"0c3a860f8e5452daa66b4623011567b42856f9a2","title":"Larger-Context Language Modelling with Recurrent Neural Network","text":"In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demonstrate that the proposed model improves perplexity significantly. In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained largercontext language model, we discover that content words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily."}
{"_id":"86e43d1b37518d10e803a2e6dc4bc954c88fbcbe","title":"Title Parental Expectations and Children ' s Academic Performance in Sociocultural Context","text":"In this paper, we review research on parental expectations and their effects on student achievement within and across diverse racial and ethnic groups. Our review suggests that the level of parental expectations varies by racial\/ethnic group, and that students' previous academic performance is a less influential determinant of parental expectations among racial\/ethnic minority parents than among European American parents. To explain this pattern, we identify three processes associated with race\/ethnicity that moderate the relation between students' previous performance and parental expectations. Our review also indicates that the relation of parental expectations to concurrent or future student achievement outcomes is weaker for racial\/ethnic minority families than for European American families. We describe four mediating processes by which high parental expectations may influence children's academic trajectories and show how these processes are associated with racial\/ethnic status. The article concludes with a discussion of educational implications as well as suggestions for future research."}
{"_id":"f24a1e2c670b0464d1b5503ac1bdbe8e6f0ce419","title":"Frequency Enhancement in Miller Divider with Injection-Locking Portrait","text":"In this paper, we present a methodology to enhance the operating range of a Miller frequency divider. Conventionally Miller frequency dividers are visualized as mixers. We present how injection locking portrait of Miller dividers helps us to understand the dynamics of Miller frequency dividers. Further, we discuss how to enhance the operating range while optimizing the divider between power dissipation and operating range. The Miller divider with the proposed methodology has been designed in a standard CMOS 65 nm (low leakage) process. Post layout simulation results show that Miller divider with optimization techniques presented in this paper can operate over the frequency range of 13 GHz for 0dBm input power at center frequency of 61.5 GHz."}
{"_id":"54f5d15d64b2f404c89fc1817d2eb1cda6c0fe45","title":"Does Attendance Matter ? An Examination of Student Attitudes , Participation , Performance and Attendance","text":"Non attendance of lectures and tutorials appears to be a growing trend. The literature suggests many possible reasons including students\u2019 changing lifestyle, attitudes, teaching and technology. This paper looks at the reasons for non attendance of students in the Faculty of Commerce at the University of Wollongong and identifies relationships between attendance, participation and performance. The results indicate that there are valid reasons for non attendance that are both in the control of learners and teachers. There are also clear benefits for students to be gained in attendance; however, changes in the way we learn, teach, assess and use technology are recommended if we wish to reverse the trend. This journal article is available in Journal of University Teaching & Learning Practice: http:\/\/ro.uow.edu.au\/jutlp\/vol3\/iss2\/3 Jour na l o f Un ive rs i t y Teach ing and Lear n ing Pr ac t i ce Does Attendance Matter? An Examination of Student Attitudes, Participation, Performance and Attendance"}
{"_id":"ada12ff23e862c86e4271bc6cdaaa6f92dc1bf90","title":"Applied machine vision of plants: a review with implications for field deployment in automated farming operations","text":"Automated visual assessment of plant condition, specifically foliage wilting, reflectance and growth parameters, using machine vision has potential use as input for real-time variable-rate irrigation and fertigation systems in precision agriculture. This paper reviews the research literature for both outdoor and indoor applications of machine vision of plants, which reveals that different environments necessitate varying levels of complexity in both apparatus and nature of plant measurement which can be achieved. Deployment of systems to the field environment in precision agriculture applications presents the challenge of overcoming image variation caused by the diurnal and seasonal variation of sunlight. From the literature reviewed, it is argued that augmenting a monocular RGB vision system with additional sensing techniques potentially reduces image analysis complexity while enhancing system robustness to environmental variables. Therefore, machine vision systems with a foundation in optical and lighting design may potentially expedite the transition from laboratory and research prototype to robust field tool."}
{"_id":"6cf6dc8bb7995a1f529d51b11f1677e045337337","title":"SmartPaste: Learning to Adapt Source Code","text":"Deep Neural Networks have been shown to succeed at a range of natural language tasks such as machine translation and text summarization. While tasks on source code (ie, formal languages) have been considered recently, most work in this area does not attempt to capitalize on the unique opportunities offered by its known syntax and structure. In this work, we introduce SmartPaste, a first task that requires to use such information. The task is a variant of the program repair problem that requires to adapt a given (pasted) snippet of code to surrounding, existing source code. As first solutions, we design a set of deep neural models that learn to represent the context of each variable location and variable usage in a data flow-sensitive way. Our evaluation suggests that our models can learn to solve the SmartPaste task in many cases, achieving 58.6% accuracy, while learning meaningful representation of variable usages."}
{"_id":"0760b3baa196cd449d2c81604883815a6fc73b6a","title":"Analysis and reduction of quadrature errors in the material point method ( MPM )","text":"The material point method (MPM) has demonstrated itself as a computationally effective particle method for solving solid mechanics problems involving large deformations and\/or fragmentation of structures, which are sometimes problematic for finite element methods (FEMs). However, similar to most methods that employ mixed Lagrangian (particle) and Eulerian strategies, analysis of the method is not straightforward. The lack of an analysis framework for MPM, as is found in FEMs, makes it challenging to explain anomalies found in its employment and makes it difficult to propose methodology improvements with predictable outcomes. In this paper we present an analysis of the quadrature errors found in the computation of (material) internal force in MPM and use this analysis to direct proposed improvements. In particular, we demonstrate that lack of regularity in the grid functions used for representing the solution to the equations of motion can hamper spatial convergence of the method. We propose the use of a quadratic B-spline basis for representing solutions on the grid, and we demonstrate computationally and explain theoretically why such a small change can have a significant impact on the reduction in the internal force quadrature error (and corresponding \u2018grid crossing error\u2019) often experienced when using MPM. Copyright q 2008 John Wiley & Sons, Ltd."}
{"_id":"fabddc3b57c367eef7b9f29b8bedac03c00efaf6","title":"Handbook of Natural Language Processing","text":"Handbook of Natural Language Processing and Machine Translation \u00b7 Learning Natural Language Processing With Python and NLTK p.1 Tokenizing words and By far, the most popular toolkit or API to do natural language processing... Nitin Indurkhya, Fred J. Damerau, Handbook of Natural Language Processing, Second Edition. Chapman & Hall\/CRC Machine Learning & Pattern Recognition. Arabic Natural Language Processing, Machine Translation Handbook of Natural Language Processing and Machine Translation, 164-175, 2011. 2011."}
{"_id":"569047ef761bc13403c68611bf1d5fbce945cbde","title":"A construction of cryptography system based on quantum neural network","text":"Quantum neural networks (QNNs) have been explored as one of the best approach for improving the computational efficiency of neural networks. Because of the powerful and fantastic performance of quantum computation, some researchers have begun considering the implications of quantum computation on the field of artificial neural networks (ANNs).The purpose of this paper is to introduce an application of QNNs in construction of cryptography system in which two networks exchange their outputs (in qubits) and the key to be synchronized between two communicating parties. This system is based on multilayer qubit QNNs trained with back-propagation algorithm."}
{"_id":"fd26f8069cfa528463fdf8a90864587e997ee86d","title":"A Multi-View Fusion Neural Network for Answer Selection","text":"Community question answering aims at choosing the most appropriate answer for a given question, which is important in many NLP applications. Previous neural network-based methods consider several different aspects of information through calculating attentions. These different kinds of attentions are always simply summed up and can be seen as a \u201csingle view\u201d, causing severe information loss. To overcome this problem, we propose a Multi-View Fusion Neural Network, where each attention component generates a \u201cview\u201d of the QA pair and a fusion RNN integrates the generated views to form a more holistic representation. In this fusion RNN method, a filter gate collects important information of input and directly adds it to the output, which borrows the idea of residual networks. Experimental results on the WikiQA and SemEval-2016 CQA datasets demonstrate that our proposed model outperforms the state-of-the-art methods."}
{"_id":"a6b49df6de5a1246930ae084f819d4c1d813df52","title":"Calming effects of deep touch pressure in patients with autistic disorder, college students, and animals.","text":"ABSTRACT Many people with autistic disorder have problems with oversensitivity to both touch and sound. The author (an autistic person) developed a device that delivers deep touch pressure to help her learn to tolerate touching and to reduce anxiety and nervousness. The \"squeeze machine\" applies lateral, inwardly directed pressure to both lateral aspects of a person's entire body, by compressing the user between two foam-padded panels. Clinical observations and several studies suggest that deep touch pressure is therapeutically beneficial for both children with autistic disorder and probably children with attention-deficit hyperactivity disorder. Only minor and occasional adverse effects have been noted. Data are reported that show a similar calming effect in nonreferred college students. A review of the animal literature reveals that animals have similar calming reactions, and also suggests possible additional physiological effects of deep touch pressure. At present, there are increasing anecdotal reports of the clinical value of the squeeze machine, including suggestions that it can be used to reduce required doses of psychostimulant medications. More clinical studies are needed to evaluate the potential role of this seemingly beneficial form of \"physiological\" stimulation."}
{"_id":"e5645ce314bc346b40a57bb5856c4b04bed6235c","title":"\"Oops, I Did It Again\" - Security of One-Time Signatures Under Two-Message Attacks","text":"One-time signatures (OTS) are called one-time, because the accompanying security reductions only guarantee security under single-message attacks. However, this does not imply that efficient attacks are possible under two-message attacks. Especially in the context of hash-based OTS (which are basic building blocks of recent standardization proposals) this leads to the question if accidental reuse of a one-time key pair leads to immediate loss of security or to graceful degradation. In this work we analyze the security of the most prominent hash-based OTS, Lamport\u2019s scheme, its optimized variant, and WOTS, under different kinds of two-message attacks. Interestingly, it turns out that the schemes are still secure under two message attacks, asymptotically. However, this does not imply anything for typical parameters. Our results show that for Lamport\u2019s scheme, security only slowly degrades in the relevant attack scenarios and typical parameters are still somewhat secure, even in case of a two-message attack. As we move on to optimized Lamport and its generalization WOTS, security degrades faster and faster, and typical parameters do not provide any reasonable level of security under two-message attacks."}
{"_id":"6b9c790904c8aea2b616388361cec9bef92e8bab","title":"Multi-granularity Generator for Temporal Action Proposal","text":"Temporal action proposal generation is an important task, aiming to localize the video segments containing human actions in an untrimmed video. In this paper, we propose a multi-granularity generator (MGG) to perform the temporal action proposal from different granularity perspectives, relying on the video visual features equipped with the position embedding information. First, we propose to use a bilinear matching model to exploit the rich local information within the video sequence. Afterwards, two components, namely segment proposal generator (SPG) and frame actionness generator (FAG), are combined to perform the task of temporal action proposal at two distinct granularities. SPG considers the whole video in the form of feature pyramid and generates segment proposals from one coarse perspective, while FAG carries out a finer actionness evaluation for each video frame. Our proposed MGG can be trained in an end-to-end fashion. Through temporally adjusting the segment proposals with fine-grained information based on frame actionness, MGG achieves the superior performance over state-of-the-art methods on the public THUMOS-14 and ActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to perform the classification of the proposals generated by MGG, leading to significant improvements compared against the competing methods for the video detection task."}
{"_id":"8738cf489b103d4c9fc42f17b1acf813e3495fdb","title":"The human amygdala and the emotional evaluation of sensory stimuli","text":"A wealth of animal data implicates the amygdala in aspects of emotional processing. In recent years, functional neuroimaging and neuropsychological studies have begun to refine our understanding of the functions of the amygdala in humans. This literature offers insights into the types of stimuli that engage the amygdala and the functional consequences that result from this engagement. Specific conclusions and hypotheses include: (1) the amygdala activates during exposure to aversive stimuli from multiple sensory modalities; (2) the amygdala responds to positively valenced stimuli, but these responses are less consistent than those induced by aversive stimuli; (3) amygdala responses are modulated by the arousal level, hedonic strength or current motivational value of stimuli; (4) amygdala responses are subject to rapid habituation; (5) the temporal characteristics of amygdala responses vary across stimulus categories and subject populations; (6) emotionally valenced stimuli need not reach conscious awareness to engage amygdala processing; (7) conscious hedonic appraisals do not require amygdala activation; (8) activation of the amygdala is associated with modulation of motor readiness, autonomic functions, and cognitive processes including attention and memory; (9) amygdala activations do not conform to traditional models of the lateralization of emotion; and (10) the extent and laterality of amygdala activations are related to factors including psychiatric status, gender and personality. The strengths and weakness of these hypotheses and conclusions are discussed with reference to the animal literature."}
{"_id":"cd31ecb3b58d1ec0d8b6e196bddb71dd6a921b6d","title":"Economic dispatch for a microgrid considering renewable energy cost functions","text":"Microgrids are operated by a customer or a group of customers for having a reliable, clean and economic mode of power supply to meet their demand. Understanding the economics of system is a prime factor which really depends on the cost\/kWh of electricity supplied. This paper presents an easy and simple method for analyzing the dispatch rate of power. An isolated microgrid with solar and wind is considered in this paper. Generation cost functions are modeled with the inclusion of investment cost and maintenance cost of resources. Economic dispatch problem is solved using the reduced gradient method. The effects on total generation cost, with the inclusion of wind energy and solar energy into a microgrid is studied and found the most profitable solution by considering different practical scenarios. The paper gives a detailed correlation between the cost function, investment cost, lifetime and the fluctuant energy forecasting of wind and solar resources. It also discusses the advantages of including the renewable energy credits for the solar panel."}
{"_id":"4ce94ff24cc238440a76598da13361ef4da9e5ed","title":"The Whale Optimization Algorithm","text":"This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at http:\/\/www.alimirjalili.com\/WOA.html \u00a9 2016 Elsevier Ltd. All rights reserved."}
{"_id":"8a7a9672b4981e72d6e9206024c758cc047db8cd","title":"Evolution strategies \u2013 A comprehensive introduction","text":"This article gives a comprehensive introduction into one of the main branches of evolutionary computation \u2013 the evolution strategies (ES) the history of which dates back to the 1960s in Germany. Starting from a survey of history the philosophical background is explained in order to make understandable why ES are realized in the way they are. Basic ES algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of ES research are discussed."}
{"_id":"59a3fea1f38c5dd661cc5bfec50add2c2f881454","title":"A Fast Elitist Non-dominated Sorting Genetic Algorithm for Multi-objective Optimisation: NSGA-II","text":"Multi-objectiveevolutionaryalgorithmswhichusenon-dominatedsorting andsharinghave beenmainly criticizedfor their (i) computational complexity (where is thenumberof objectivesand is thepopulationsize), (ii) non-elitismapproach, and(iii) theneedfor specifyingasharingparameter . In this paper , we suggesta non-dominatedsortingbasedmulti-objective evolutionaryalgorithm(wecalledit theNon-dominatedSortingGA-II or NSGA-II) which alleviatesall theabove threedifficulties.Specifically, a fastnon-dominatedsorting approachwith computationalcomplexity is presented. Second,a selectionoperatoris presentedwhich createsa mating pool by combiningthe parentandchild populationsandselectingthe best(with respectto fitnessand spread) solutions.Simulationresultson five difficult testproblemsshow that theproposedNSGA-II is ableto find muchbetterspreadof solutionsin all problemscomparedto PAES\u2014anotherelitist multi-objective EA which paysspecial attentiontowardscreatinga diversePareto-optimalfront. Becauseof NSGA-II\u2019s low computational requirements, elitist approach, andparameter -lesssharingapproach,NSGA-II shouldfind increasingapplicationsin theyearsto come."}
{"_id":"945790435ea901e1061df670b59808fe5764c66c","title":"Pre- and post-processes for automatic colorization using a fully convolutional network","text":"Automatic colorization is a significant task especially for Anime industry. An original trace image to be colorized contains not only outlines but also boundary contour lines of shadows and highlight areas. Unfortunately, these lines tend to decrease the consistency among all images. Thus, this paper provides a method for a cleaning pre-process of anime dataset to improve the prediction quality of a fully convolutional network, and a refinement post-process to enhance the output of the network."}
{"_id":"09ebd63a1061e1b28c0c1cd005b02a542e58f550","title":"Random Bits Regression: a Strong General Predictor for Big Data","text":"* Correspondence: momiao.xiong@gmail.com; Momiao.Xiong@uth.tmc.edu; yin.yao@nih.gov; lijin@fudan.edu.cn Equal contributors Human Genetics Center, School of Public Health, University of Texas Houston Health Sciences Center, Houston, TX, USA Unit on Statistical Genomics, Division of Intramural Division Programs, National Institute of Mental Health, National Institutes of Health, Bethesda, MD, USA Ministry of Education Key Laboratory of Contemporary Anthropology, Collaborative Innovation Center for Genetics and Development, School of Life Sciences, Fudan University, Shanghai 200433, China Full list of author information is available at the end of the article Abstract"}
{"_id":"a45d4b15ce3adbb9f755377ace8405e7cc90efd5","title":"Information Theoretic Learning with Infinitely Divisible Kernels","text":"In this paper, we develop a framework for information theore tic l arning based on infinitely divisible matrices. We formulate an entropy-lik e functional on positive definite matrices based on Renyi\u2019s axiomatic definition of en tropy and examine some key properties of this functional that lead to the conce pt of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbe rt spaces. As an application example, we derive a supervised metric learning algo rithm using a matrix based analogue to conditional entropy achieving results co mparable with the state of the art."}
{"_id":"a44364bf7c9d8d2d4b93637e81cf9d51dc3aae62","title":"Cyber Security and Power System Communication\u2014Essential Parts of a Smart Grid Infrastructure","text":"The introduction of \u201csmart grid\u201d solutions imposes that cyber security and power system communication systems must be dealt with extensively. These parts together are essential for proper electricity transmission, where the information infrastructure is critical. The development of communication capabilities, moving power control systems from \u201cislands of automation\u201d to totally integrated computer environments, have opened up new possibilities and vulnerabilities. Since several power control systems have been procured with \u201copenness\u201d requirements, cyber security threats become evident. For refurbishment of a SCADA\/EMS system, a separation of the operational and administrative computer systems must be obtained. The paper treats cyber security issues, and it highlights access points in a substation. Also, information security domain modeling is treated. Cyber security issues are important for \u201csmart grid\u201d solutions. Broadband communications open up for smart meters, and the increasing use of wind power requires a \u201csmart grid system\u201d."}
{"_id":"38e49f5ef5a0c2c99e73c2c2ec20c3c216e98e02","title":"Trustworthiness Attributes and Metrics for Engineering Trusted Internet-Based Software Systems","text":"Trustworthiness of Internet-based software systems, apps, services and platform is a key success factor for their use and acceptance by organizations and end-users. The notion of trustworthiness, though, is subject to individual interpretation and preference, e.g., organizations require confidence about how their business critical data is handled whereas end-users may be more concerned about usability. As one main contribution, we present an extensive list of software quality attributes that contribute to trustworthiness. Those software quality attributes have been identified by a systematic review of the research literature and by analyzing two real-world use cases. As a second contribution, we sketch an approach for systematically deriving metrics to measure the trustworthiness of software system. Our work thereby contributes to better understanding which software quality attributes should be considered and assured when engineering trustworthy Internet-based software systems."}
{"_id":"b6c99914d3bca48f39ca668e6bf9e1194bf95e12","title":"The foreign-language effect: thinking in a foreign tongue reduces decision biases.","text":"Would you make the same decisions in a foreign language as you would in your native tongue? It may be intuitive that people would make the same choices regardless of the language they are using, or that the difficulty of using a foreign language would make decisions less systematic. We discovered, however, that the opposite is true: Using a foreign language reduces decision-making biases. Four experiments show that the framing effect disappears when choices are presented in a foreign tongue. Whereas people were risk averse for gains and risk seeking for losses when choices were presented in their native tongue, they were not influenced by this framing manipulation in a foreign language. Two additional experiments show that using a foreign language reduces loss aversion, increasing the acceptance of both hypothetical and real bets with positive expected value. We propose that these effects arise because a foreign language provides greater cognitive and emotional distance than a native tongue does."}
{"_id":"5dc48a883f0e8ed17e58fbdfdeea7d41fa751578","title":"Fixed budget quantized kernel least-mean-square algorithm","text":"We present a quantization-based kernel least mean square (QKLMS) algorithm with a fixed memory budget. In order to deal with the growing support inherent in online kernel methods, the proposed method utilizes a growing and pruning combined technique and defines a criterion, significance, based on weighted statistical contribution of a data center. This method doesn\u2019t need any apriori information and its computational complexity is acceptable, linear with the center number. As we show theoretically and experimentally, the introduced algorithm successfully quantifies the least \u2018significant\u2019 datum and preserves the most important ones resulting in less system error."}
{"_id":"d79dd895912a36670b3477645f361e2fdd73185b","title":"On Extracting Structured Knowledge from Unstructured Business Documents","text":"Efficient management of text data is a major concern of business organizations. In this direction, we propose a novel approach to extract structured knowledge from large corpora of unstructured business documents. This knowledge is represented in the form of object instances, which are common ways of organizing the available information about entities, and are modeled here using document templates. The approach itself is based on the observation that a significant fraction of these documents are created using the cut-copy-pastemethod, and thus, it is important to factor this observation into business document analysis projects. Correspondingly, our approach solves the problem of object instance extraction in two steps, namely similarity search and then extraction of object instances from the selected documents. Early qualitative results on a couple of carefully selected document corpora indicate the effective applicability of the approach for solving an important component of the efficient text management problem."}
{"_id":"8bce31108f598986558e9afb1061eb988ea4f3be","title":"Automated Image Annotation based on YOLOv3","text":"A typical pedestrian protection system requires sophisticated hardware and robust detection algorithms. To solve these problems the existing systems use hybrid sensors where mono and stereo vision merged with active sensors. One of the most assuring pedestrian detection sensors is far infrared range camera. The classical pedestrian detection approach based on Histogram of oriented gradients is not robust enough to be applied in devices which consumers can trust. An application of deep neural network-based approach is able to perform with significantly higher accuracy. However, the deep learning approach requires a high number of labeled data examples. The investigation presented in this paper aimed the acceleration of pedestrian labeling in far-infrared image sequences. In order to accelerate pedestrian labeling in far-infrared camera videos, we have integrated the YOLOv3 object detector into labeling software. The verification of the pre-labeled results was around eleven times faster than manual labeling of every single frame."}
{"_id":"37a5c952e6e0e3d6d7cd5c30cf4307ee06e4ab5c","title":"STUDENTS ACCEPTANCE OF MOBILE LEARNING FOR HIGHER EDUCATION IN SAUDI ARABIA","text":"Mobile learning is the next step in the development of distance learning. Widespread access to mobile devices and the opportunity to learn regardless of time and place make the mobile learning an important tool for lifelong learning. The research objectives are to examine the possibility of acceptance in mobile learning (m-Learning) and study main factors that affect using mLearning that focus on higher education students in Saudi Arabia. The researcher used a quantitative approach survey of 80 students. The modified acceptance framework that based on the Unified Theory of Acceptance and Use of Technology (UTAUT) model is adopted to determine the factors that influence the students\u2019 intention to use m-Learning. The results from statistical analysis show that the acceptance level of students on m-Learning is in the high level."}
{"_id":"8c8a16d379b3dec62771fec8c1cc64e4123c5958","title":"A digitally compensated 1.5 GHz CMOS\/FBAR frequency reference","text":"A temperature-compensated 1.5 GHz film bulk acoustic wave resonator (FBAR)-based frequency reference implemented in a 0.35 \u00bfm CMOS process is presented. The ultra-small form factor (0.79 mm \u00d7 1.72 mm) and low power dissipation (515 \u00bfA with 2 V supply) of a compensated FBAR oscillator present a promising alternative for the replacement of quartz crystal frequency references. The measured post-compensation frequency drift over a 0-100\u00b0C temperature range is <\u00b110 ppm. The measured oscillator phase noise is -133 dBc\/ Hz at 100 kHz offset from the 1.5 GHz carrier."}
{"_id":"6edd1ec57bf0efaf1460ec876d9e794dacf461d1","title":"A Lightweight Virtualization Solution for Android Devices","text":"Mobile virtualization has emerged fairly recently and is considered a valuable way to mitigate security risks on Android devices. However, major challenges in mobile virtualization include runtime, hardware, resource overhead, and compatibility. In this paper, we propose a lightweight Android virtualization solution named Condroid, which is based on container technology. Condroid utilizes resource isolation based on namespaces feature and resource control based on cgroups feature. By leveraging them, Condroid can host multiple independent Android virtual machines on a single kernel to support mutilple Android containers. Furthermore, our implementation presents both a system service sharing mechanism to reduce memory utilization and a filesystem sharing mechanism to reduce storage usage. The evaluation results on Google Nexus 5 demonstrate that Condroid is feasible in terms of runtime, hardware resource overhead, and compatibility. Therefore, we find that Condroid has a higher performance than other virtualization solutions."}
{"_id":"9c1dd4348d8aa62e7236ba2a5b265b181ebe58ab","title":"Why an Intelligence Explosion is Probable","text":"If a future Artificial Intelligence were to reach the level of human intelligence there is a possibility that it would be able to rapidly redesign itself until its own capabilities far exceeded those of human beings. We analyze the principle factors that might govern the rapidity of this \u2018intelligence explosion\u2019 process. We argue that if degree of intelligence is defined using a relatively uncontroversial measure that involves only the relative speed of thought with respect to that of the human mind, and if an intelligence explosion is defined as a thousandfold increase in speed, then there are compelling reasons to believe that none of the barriers to this process look plausible, and therefore an intelligence explosion is highly likely."}
{"_id":"2e4d61cabfbbeee19a3861a1a8b663252ef673c6","title":"Combined Star-Delta Winding Analysis","text":"The combined star-delta winding has been attracting the attention of industry because of its potential to improve the efficiency of electrical machines. In order to show the efficiency benefits of this type of winding, many authors have used methods to determine the improvements in winding factors, published in different papers, which have not been validated yet. Some of these methods contradict each other. Furthermore, as a demerit of this winding, a unidirectional rotation has been reported. In this paper, a set of new formulas to calculate the winding factors for combined star-delta windings is proposed. In order to check the validity of the proposed formulas, a 2-D FEM and experimental tests on a salient pole synchronous machine are presented, which allows different combined star-delta winding connections. In contrast to other publications, the measurements show that combined windings can be applied for both directions of rotation without any problems."}
{"_id":"cff164069f3fe0c24a9b05acfdebab8473be45a0","title":"Examining Alignment of Frames Using Actor-Network Constructs: The Implementation of an IT Project","text":"IT projects often cannot succeed without bridging different stakeholders\u2019 perspectives on system design and function during implementation. These perspectives often start as competing technology frames consisting of different beliefs, interests, technology evaluation routines and artifact characteristics. For successful implementation of IT, these competing frames usually need to be stabilized into a dominant frame which is widely accepted. Prior research has offered few clues on the mechanisms that are at work in aligning competing frames. This paper uses a case study to propose mechanisms for aligning frames\u2014namely, two actor-network theory constructs of \u201cblack-boxing\u201d and \u201cobligatory points of passage\u201d\u2014that aid in translating the competing frames into a truce frame. The examined properties of these mechanisms are described and analyzed, thereby extending the literature on technology frames by providing evidence of generic aligning mechanisms."}
{"_id":"167c2993b9d15c4d8ee247f0cc1852359af63d3e","title":"Swearing, Euphemisms, and Linguistic Relativity","text":"Participants read aloud swear words, euphemisms of the swear words, and neutral stimuli while their autonomic activity was measured by electrodermal activity. The key finding was that autonomic responses to swear words were larger than to euphemisms and neutral stimuli. It is argued that the heightened response to swear words reflects a form of verbal conditioning in which the phonological form of the word is directly associated with an affective response. Euphemisms are effective because they replace the trigger (the offending word form) by another word form that expresses a similar idea. That is, word forms exert some control on affect and cognition in turn. We relate these findings to the linguistic relativity hypothesis, and suggest a simple mechanistic account of how language may influence thinking in this context."}
{"_id":"bac17e78347a071e2a640a611ef46eac3e8e8d44","title":"Game-Enhanced Second Language Vocabulary Acquisition Strategies: A Systematic Review","text":"This paper is a synthesis of 17 qualitative, quantitative, and mixed-method studies published over the last decade. The synthesis suggests that game-enhanced learning provides a set of effective strategies such as language repetitions, contextual clues, interaction with native speakers and peers, and imagery to practice and use second language vocabulary in the authentic context. Some of the strategies such as word lists, dictionaries, and vocabulary exercises are more beneficial when combined with native speakers or peers\u2019 interactions. Due to high interactivity of games, note taking and media strategies provide less support for vocabulary learning. The lack of high quality studies and empirical data makes it difficult to draw conclusions about which video games strategies provide the most benefit. The synthesis of research identifies that generally game-enhanced practices are helpful for second language vocabulary enhancement."}
{"_id":"7ab0fa87dcdd86301ed566f97d8345015e73de21","title":"Development of wearable and flexible ultrasonic sensor for skeletal muscle monitoring","text":"A wearable and flexible ultrasonic sensor was developed for monitoring of skeletal muscle contraction. The sensor was constructed using a PVDF piezoelectric polymer film without a matching layer or backing material. Due to its lightness (less than 1 gram), thinness (200 \u03bcm) and flexibility, this sensor was wearable and enabled non-invasive and continuous muscle monitoring without restricting muscle movement, which is not feasible using a conventional handheld ultrasonic probe. The developed sensor was used to monitor muscle contractions in the index finger by ultrasonic pulse-echo measurements and in the forearm by through-transmission measurements. It was successfully demonstrated that the tissue thickness variations were measured in accordance with the muscle contraction performed."}
{"_id":"e397b73a945671c0b05ab421c32fa418cf9744cb","title":"Overcoming the Bell-Shaped Dose-Response of Cannabidiol by Using Cannabis Extract Enriched in Cannabidiol","text":"Cannabidiol (CBD), a major constituent of Cannabis, has been shown to be a powerful anti\u2010in\u2010 flammatory and anti\u2010anxiety drug, without exerting a psychotropic effect. However, when given either intraperitoneally or orally as a purified product, a bell\u2010shaped dose\u2010response was observed, which limits its clinical use. In the present study, we have studied in mice the anti\u2010inflammatory and anti\u2010nociceptive activities of standardized plant extracts derived from the Cannabis sativa L., clone 202, which is highly enriched in CBD and hardly contains any psychoactive ingredients. In stark contrast to purified CBD, the clone 202 extract, when given either intraperitoneally or orally, provided a clear correlation between the anti\u2010inflammatory and anti\u2010nociceptive responses and the dose, with increasing responses upon increasing doses, which makes this plant medicine ideal for clinical uses. The clone 202 extract reduced zymosan\u2010induced paw swelling and pain in mice, and prevented TNF\u03b1 production in vivo. It is likely that other components in the extract synergize with CBD to achieve the desired anti\u2010inflammatory action that may contribute to overcoming the bell\u2010shaped dose\u2010response of purified CBD. We therefore propose that Cannabis clone 202 (Avi\u2010 dekel) extract is superior over CBD for the treatment of inflammatory conditions."}
{"_id":"8723604fd4b0f6c84878c90468e825de7fdbf653","title":"TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning","text":"The growth of robot-assisted minimally invasive surgery has led to sizable datasets of fixed-camera video and kinematic recordings of surgical subtasks. Segmentation of these trajectories into locally-similar contiguous sections can facilitate learning from demonstrations, skill assessment, and salvaging good segments from otherwise inconsistent demonstrations. Manual, or supervised, segmentation can be prone to error and impractical for large datasets. We present Transition State Clustering with Deep Learning (TSC-DL), a new unsupervised algorithm that leverages video and kinematic data for task-level segmentation, and finds regions of the visual feature space that correlate with transition events using features constructed from layers of pre-trained image classification Deep Convolutional Neural Networks (CNNs). We report results on three datasets comparing Deep Learning architectures (AlexNet and VGG), choice of convolutional layer, dimensionality reduction techniques, visual encoding, and the use of Scale Invariant Feature Transforms (SIFT). We find that the deep architectures extract features that result in up-to a 30.4% improvement in Silhouette Score (a measure of cluster tightness) over the traditional \u201cshallow\u201d features from SIFT. We also present cases where TSC-DL discovers human annotator omissions. Supplementary material, data and code is available at: http:\/\/berkeleyautomation.github.io\/tsc-dl\/."}
{"_id":"929f18130460d562d15db085b87a781a0dd34017","title":"Effects of concurrent endurance and strength training on running economy and .VO(2) kinetics.","text":"PURPOSE\nIt has been suggested that endurance training influences the running economy (CR) and the oxygen uptake (.VO(2)) kinetics in heavy exercise by accelerating the primary phase and attenuating the .VO(2) slow component. However, the effects of heavy weight training (HWT) in combination with endurance training remain unclear. The purpose of this study was to examine the influence of a concurrent HWT+endurance training on CR and the .VO(2) kinetics in endurance athletes.\n\n\nMETHODS\nFifteen triathletes were assigned to endurance+strength (ES) or endurance-only (E) training for 14 wk. The training program was similar, except ES performed two HWT sessions a week. Before and after the training period, the subjects performed 1) an incremental field running test for determination of .VO(2max) and the velocity associated (V(.VO2max)), the second ventilatory threshold (VT(2)); 2) a 3000-m run at constant velocity, calculated to require 25% of the difference between .VO(2max) and VT(2), to determine CR and the characteristics of the VO(2) kinetics; 3) maximal hopping tests to determine maximal mechanical power and lower-limb stiffness; 4) maximal concentric lower-limb strength measurements.\n\n\nRESULTS\nAfter the training period, maximal strength were increased (P < 0.01) in ES but remained unchanged in E. Hopping power decreased in E (P < 0.05). After training, economy (P < 0.05) and hopping power (P < 0.001) were greater in ES than in E. .VO(2max), leg hopping stiffness and the .VO(2) kinetics were not significantly affected by training either in ES or E.\n\n\nCONCLUSION\nAdditional HWT led to improved maximal strength and running economy with no significant effects on the .VO(2) kinetics pattern in heavy exercise."}
{"_id":"e3a2a87550a83feafc1cdcce1cc621b3a666134a","title":"Real time automation of agricultural environment","text":"The paper, \u201cReal time automation of agricultural environment\u201d, using PIC16F877A and GSM SIM300 modem is focused on automating the irrigation system for social welfare of Indian agricultural system. This system will be useful for monitoring the soil moisture condition of the farm as well as controlling the soil moisture by monitoring the level of water in the water source and accordingly switching the motor ON\/OFF for irrigation purposes. The system proposes a soil moisture sensor at each place where the moisture has to be monitored. Once the moisture reaches a particular level, the system takes appropriate steps to regulate or even stop the water flow. The circuit also monitors the water in the water source so that if the water level becomes very low, it switches off the motor to prevent damage to the motor due to dry run. The system also consists of a GSM modem through which the farmer can easily be notified about the critical conditions occurring during irrigation process."}
{"_id":"887002d61b53926d5a210b8ad8473ce951f90de3","title":"Smart home design based on ZigBee wireless sensor network","text":"This work is based on the Internet of Things and ZigBee wireless sensor network technology. A kind of smart home design based on ZigBee wireless sensor network was proposed in this paper. Texas Instruments MCU device LM3S9B96, which is the ARM Cortex-M3 based controllers, was used in this system. The entire system is running on the \u03bcC\/OS-II embedded real-time multitasking operating system. Users can access this system by a dynamic webpage of LwIP TCP\/IP protocol stack or GSM SMS. Using this system users can conveniently know the environment parameters of home, such as temperature, humidity, meter readings, light, and control the home electronic equipments, such as light, aircondition, heater, by ZigBee wireless sensor network."}
{"_id":"2bc27481fa57a1b247ab1fc5d23a07912480352a","title":"How computer games affect CS (and other) students' school performance","text":"Compulsive game playing, especially of the role-playing variety, risks failing grades and withdrawal of financial support from tuition-paying parents."}
{"_id":"3716c4896944c3461477f845319ac09e3dfe3a10","title":"eSports: collaborative and synchronous video annotation system in grid computing environment","text":"We designed eSports - a collaborative and synchronous video annotation platform, which is to be used in Internet scale cross-platform grid computing environment to facilitate computer supported cooperative work (CSCW) in education settings such as distance sport coaching, distance classroom etc. Different from traditional multimedia annotation systems, eSports provides the capabilities to collaboratively and synchronously play and archive real time live video, to take snapshots, to annotate video snapshots using whiteboard and to play back the video annotations synchronized with original video streams. eSports is designed based on the grid based collaboration paradigm $the shared event model using NaradaBrokering, which is a publish\/subscribe based distributed message passing and event notification system. In addition to elaborate the design and implementation of eSports, we analyze the potential use cases of eSports under different education settings. We believed that eSports is very useful to improve the online collaborative coaching and education."}
{"_id":"948876640d3ca519a2c625a4a52dc830fec26b29","title":"The Role of Flow Experience in Cyber-Game Addiction","text":"Consumer habit, an important key to repetitive consumption, is an interesting yet puzzling phenomenon. Sometimes this consumption becomes obsessive--consumers will continue to act a certain way even when they feel it is not in their best interests. However, not all consumers develop such addictions. This study uses cyber-game addiction syndrome as an analogue to trace the possible causes of consumer addiction. Results from structure equation modeling show that repetition of favorite activities has a moderate effect upon addiction, which is in line with the assertion of rational addiction theory. However, flow experience--the emotional state embracing perceptional distortion and enjoyment--shows a much stronger impact on addiction. This suggests that consumers who have experienced flow are more likely to be addicted."}
{"_id":"c1b66422b1dab3eeee6d6c760f4bd227a8bb16c5","title":"Being There: The Subjective Experience of Presence","text":""}
{"_id":"f40823290aaba15e8073792d302c0ff8c1d37486","title":"Analysis of Bayesian Classification based Approaches for Android Malware Detection","text":"Mobile malware has been growing in scale and complexity spurred by the unabated uptake of smartphones worldwide. Android is fast becoming the most popular mobile platform resulting in sharp increase in malware targeting the platform. Additionally, Android malware is evolving rapidly to evade detection by traditional signature-based scanning. Despite current detection measures in place, timely discovery of new malware is still a critical issue. This calls for novel approaches to mitigate the growing threat of zero-day Android malware. Hence, in this paper we develop and analyze proactive Machine Learning approaches based on Bayesian classification aimed at uncovering unknown Android malware via static analysis. The study, which is based on a large malware sample set of majority of the existing families, demonstrates detection capabilities with high accuracy. Empirical results and comparative analysis are presented offering useful insight towards development of effective static-analytic Bayesian classification based solutions for detecting unknown Android malware."}
{"_id":"7253c6cd672281576a96db1037f135ce3e78fe41","title":"Problems of Reliability and Validity in Ethnographic Research","text":"Although problems of reliability and validity have been explored thoroughly by experimenters and other quantitative researchers, their treatment by ethnographers has been sporadic and haphazard This article analyzes these constructs as defined and addressed by ethnographers. Issues of reliability and validity in ethnographic design are compared to their counterparts in experimental design. Threats to the credibility of ethnographic research are summarized and categorized from field study methodology. Strategies intended to enhance credibility are incorporated throughout the investigative process: study design, data collection, data analysis, and presentation of findings. Common approaches to resolving various categories of contamination are illustrated from the current literature in educational ethnography."}
{"_id":"bd475c9494a9e10bc711e5d8301eddb684b1663a","title":"Dairy products and colorectal cancer risk: a systematic review and meta-analysis of cohort studies.","text":"BACKGROUND\nPrevious studies of the association between intake of dairy products and colorectal cancer risk have indicated an inverse association with milk, however, the evidence for cheese or other dairy products is inconsistent.\n\n\nMETHODS\nWe conducted a systematic review and meta-analysis to clarify the shape of the dose-response relationship between dairy products and colorectal cancer risk. We searched the PubMed database for prospective studies published up to May 2010. Summary relative risks (RRs) were estimated using a random effects model.\n\n\nRESULTS\nNineteen cohort studies were included. The summary RR was 0.83 (95% CI [confidence interval]: 0.78-0.88, I2=25%) per 400 g\/day of total dairy products, 0.91 (95% CI: 0.85-0.94, I2=0%) per 200 g\/day of milk intake and 0.96 (95% CI: 0.83-1.12, I2=28%) per 50 g\/day of cheese. Inverse associations were observed in both men and women but were restricted to colon cancer. There was evidence of a nonlinear association between milk and total dairy products and colorectal cancer risk, P<0.001, and the inverse associations appeared to be the strongest at the higher range of intake.\n\n\nCONCLUSION\nThis meta-analysis shows that milk and total dairy products, but not cheese or other dairy products, are associated with a reduction in colorectal cancer risk."}
{"_id":"1037548f688bd3e566df0d4184509976695124cf","title":"Gene Expression Omnibus: NCBI gene expression and hybridization array data repository","text":"The Gene Expression Omnibus (GEO) project was initiated in response to the growing demand for a public repository for high-throughput gene expression data. GEO provides a flexible and open design that facilitates submission, storage and retrieval of heterogeneous data sets from high-throughput gene expression and genomic hybridization experiments. GEO is not intended to replace in house gene expression databases that benefit from coherent data sets, and which are constructed to facilitate a particular analytic method, but rather complement these by acting as a tertiary, central data distribution hub. The three central data entities of GEO are platforms, samples and series, and were designed with gene expression and genomic hybridization experiments in mind. A platform is, essentially, a list of probes that define what set of molecules may be detected. A sample describes the set of molecules that are being probed and references a single platform used to generate its molecular abundance data. A series organizes samples into the meaningful data sets which make up an experiment. The GEO repository is publicly accessible through the World Wide Web at http:\/\/www.ncbi.nlm.nih.gov\/geo."}
{"_id":"85bfaa2fbe1feed65f90bca30c21bea00d73097b","title":"Fine-grained Entity Recognition with Reduced False Negatives and Large Type Coverage","text":"Fine-grained Entity Recognition (FgER) is the task of detecting and classifying entity mentions to a large set of types spanning diverse domains such as biomedical, finance and sports. We observe that when the type set spans several domains, detection of entity mention becomes a limitation for supervised learning models. The primary reason being lack of dataset where entity boundaries are properly annotated while covering a large spectrum of entity types. Our work directly addresses this issue. We propose Heuristics Allied with Distant Supervision (HAnDS) framework to automatically construct a quality dataset suitable for the FgER task. HAnDS framework exploits the high interlink among Wikipedia and Freebase in a pipelined manner, reducing annotation errors introduced by naively using distant supervision approach. Using HAnDS framework, we create two datasets, one suitable for building FgER systems recognizing up to 118 entity types based on the FIGER type hierarchy and another for up to 1115 entity types based on the TypeNet hierarchy. Our extensive empirical experimentation warrants the quality of the generated datasets. Along with this, we also provide a manually annotated dataset for benchmarking FgER systems."}
{"_id":"d4b91865a3c520b7516576dba7f81c30d1e002d2","title":"Neuronal correlates of a perceptual decision","text":"THE relationship between neuronal activity and psychophysical judgement has long been of interest to students of sensory processing. Previous analyses of this problem have compared the performance of human or animal observers in detection or discrimination tasks with the signals carried by individual neurons, but have been hampered because neuronal and perceptual data were not obtained at the same time and under the same conditions1\u20134. We have now measured the performance of monkeys and of visual cortical neurons while the animals performed a psychophysical task well matched to the properties of the neurons under study. Here we report that the reliability and sensitivity of most neurons on this task equalled or exceeded that of the monkeys. We therefore suggest that under our conditions, psychophysical judgements could be based on the activity of a relatively small number of neurons."}
{"_id":"0fba92ea026075dc27df9f6d9c7ab1f856a0a3b2","title":"Clinical and microbiologic characteristics of vulvovaginitis in Korean prepubertal girls, 2009\u20132014: a single center experience","text":"OBJECTIVE\nTo update information on the clinical and microbiologic characteristics of pediatric vulvovaginitis in Korean prepubertal girls.\n\n\nMETHODS\nA total of 120 girls (aged 0 to 9 years) with culture-confirmed pediatric vulvovaginitis, diagnosed between 2009 and 2014, were enrolled in the study. The epidemiologic and microbiologic characteristics, and clinical outcomes were assessed. Patients with sexual precocity, as well as those who were referred for suspected sexual abuse, were excluded.\n\n\nRESULTS\nGirls aged 4 to 6 years were at the highest risk of pediatric vulvovaginitis. Seasonal distribution indicated obvious peaks in summer and winter. Of the 120 subjects, specific pathogens were identified in the genital specimens in only 20 cases (16.7%). Streptococcus pyogenes (n=12, 60%) was the leading cause of specific vulvovaginitis. Haemophilus influenzae was isolated in one patient. No cases presented with enteric pathogens, such as Shigella or Yersinia. A history of recent upper respiratory tract infection, swimming, and bubble bath use was reported in 37.5%, 15.8%, and 10.0% of patients, respectively. Recent upper respiratory tract infection was not significantly correlated with the detection of respiratory pathogens in genital specimens (P>0.05). Of 104 patients who underwent perineal hygienic care, 80 (76.9%) showed improvement of symptoms without antibiotic treatment. Furthermore, the efficacy of hygienic care was not significantly different between patients with or without specific pathogens (P>0.05).\n\n\nCONCLUSION\nSpecific pathogens were only found in 16.7% of pediatric vulvovaginitis cases. Our results indicate an excellent outcome with hygienic care, irrespective of the presence of specific pathogens."}
{"_id":"e75c5d1b7ecd71cd9f1fdc3d07f56290517ef1e5","title":"HyPer: A hybrid OLTP&OLAP main memory database system based on virtual memory snapshots","text":"The two areas of online transaction processing (OLTP) and online analytical processing (OLAP) present different challenges for database architectures. Currently, customers with high rates of mission-critical transactions have split their data into two separate systems, one database for OLTP and one so-called data warehouse for OLAP. While allowing for decent transaction rates, this separation has many disadvantages including data freshness issues due to the delay caused by only periodically initiating the Extract Transform Load-data staging and excessive resource consumption due to maintaining two separate information systems. We present an efficient hybrid system, called HyPer, that can handle both OLTP and OLAP simultaneously by using hardware-assisted replication mechanisms to maintain consistent snapshots of the transactional data. HyPer is a main-memory database system that guarantees the ACID properties of OLTP transactions and executes OLAP query sessions (multiple queries) on the same, arbitrarily current and consistent snapshot. The utilization of the processor-inherent support for virtual memory management (address translation, caching, copy on update) yields both at the same time: unprecedentedly high transaction rates as high as 100000 per second and very fast OLAP query response times on a single system executing both workloads in parallel. The performance analysis is based on a combined TPC-C and TPC-H benchmark."}
{"_id":"732212be0e6c5216158a7470c79fa2ff98a2da06","title":"A Ka-band asymmetrical stacked-FET MMIC Doherty power amplifier","text":"We present a stacked-FET monolithic millimeter-wave (mmW) integrated circuit Doherty power amplifier (DPA). The DPA employs a novel asymmetrical stack gate bias to achieve high power and high efficiency at 6-dB power back-off (PBO). The circuit is fabricated in a 0.15-\u00b5m enhancement mode (E-mode) Gallium Arsenide (GaAs) process. Experimental results demonstrate output power at 1-dB gain compression (P1dB) of 28.2 dBm, peak power added efficiency (PAE) of 37% and PAE at 6-dB PBO of 27% at 28 GHz. Measured small signal gain is 15 dB while the 3-dB bandwidth covers from 25.5 to 29.5 GHz. Using digital predistortion (DPD) with a 20 MHz 64 QAM modulated signal, an adjacent channel power ratio (ACPR) of \u221246 dBc has been observed."}
{"_id":"03837b659b4a8878c2a2dbef411cd986fecfef8e","title":"Autoregressive Attention for Parallel Sequence Modeling","text":"We introduce an autoregressive attention mechanism for parallelizable characterlevel sequence modeling. We use this method to augment a neural model consisting of blocks of causal convolutional layers connected by highway network skip connections. We denote the models with and without the proposed attention mechanism respectively as Highway Causal Convolution (Causal Conv) and Autoregressive-attention Causal Convolution (ARA-Conv). The autoregressive attention mechanism crucially maintains causality in the decoder, allowing for parallel implementation. We demonstrate that these models, compared to their recurrent counterparts, enable fast and accurate learning in character-level NLP tasks. In particular, these models outperform recurrent neural network models in natural language correction and language modeling tasks, and run in a fraction of the time."}
{"_id":"3466b4007e1319db8aa9cabdc001f138cffbd981","title":"Benchmarks as Limits to Arbitrage : Understanding the Low Volatility Anomaly \u2217","text":"Over the past 41 years, high volatility and high beta stocks have substantially underperformed low volatility and low beta stocks in U.S. markets. We propose an explanation that combines the average investor's preference for risk and the typical institutional investor\u2019s mandate to maximize the ratio of excess returns and tracking error relative to a fixed benchmark (the information ratio) without resorting to leverage. Models of delegated asset management show that such mandates discourage arbitrage activity in both high alpha, low beta stocks and low alpha, high beta stocks. This explanation is consistent with several aspects of the low volatility anomaly including why it has strengthened in recent years even as institutional investors have become more dominant."}
{"_id":"94e21eb765285ec9ed431006613d320586477e2c","title":"Learning Semantic Deformation Flows with 3D Convolutional Networks","text":"Shape deformation requires expert user manipulation even when the object under consideration is in a high fidelity format such as a 3D mesh. It becomes even more complicated if the data is represented as a point set or a depth scan with significant self occlusions. We introduce an end-to-end solution to this tedious process using a volumetric Convolutional Neural Network (CNN) that learns deformation flows in 3D. Our network architectures take the voxelized representation of the shape and a semantic deformation intention (e.g., make more sporty) as input and generate a deformation flow at the output. We show that such deformation flows can be trivially applied to the input shape, resulting in a novel deformed version of the input without losing detail information. Our experiments show that the CNN approach achieves comparable results with state of the art methods when applied to CAD models. When applied to single frame depth scans, and partial\/noisy CAD models we achieve \u223c60% less error compared to the state-of-the-art."}
{"_id":"615c92a9130adc40aad4268027a7af3c9ede192e","title":"The immediate effect of kinesiology taping on muscular imbalance in the lateral flexors of the neck in infants: a\u00a0randomized masked study.","text":"OBJECTIVE\nTo investigate the immediate effect of kinesiology taping (KT) on muscular imbalance in the lateral flexors of the neck.\n\n\nDESIGN\nRandomized controlled trial.\n\n\nPARTICIPANTS\nTwenty-nine infants with congenital muscular torticollis and muscular imbalance in the lateral flexors of the neck were chosen consecutively. In addition, 5 healthy infants with no signs of muscular imbalance in the neck were tested.\n\n\nMETHOD\nThe infants were randomly allocated to either an intervention group or a control group. The intervention group had kinesiology taping applied on the affected side using the muscle-relaxing technique. The healthy infants were tested both with and without kinesiology taping. The evaluator was blinded to whether the infants were or were not taped.\n\n\nRESULTS\nThere was a significant difference in the change of Muscle Function Scale (MFS) scores between the groups (P < .0001). In the intervention group, there were significantly lower scores on the affected side that had been taped (P < .0001) and also significantly higher scores on the unaffected side (P = .01). There were no significant differences in the control group. For the healthy infants, with no imbalance in the lateral flexors of the neck, there were no changes to the MFS scores regardless of whether the kinesiology tape was applied.\n\n\nCONCLUSIONS\nFor infants with congenital muscular torticollis, kinesiology taping applied on the affected side had an immediate effect on the MFS scores for the muscular imbalance in the lateral flexors of the neck."}
{"_id":"d742830713edd589a7b4f6f4c56f07392b3e3d09","title":"The emergence of psychopathy: Implications for the neuropsychological approach to developmental disorders","text":"In this paper, I am going to examine the disorder of psychopathy and consider how genetic anomalies could give rise to the relatively specific neuro-cognitive impairments seen in individuals with this disorder. I will argue that genetic anomalies in psychopathy reduce the salience of punishment information (perhaps as a function of noradrenergic disturbance). I will argue that the ability of the amygdala to form the stimulus-punishment associations necessary for successful socialization is disrupted and that because of this, individuals with psychopathy do not learn to avoid actions that will harm others. It is noted that this model follows the neuropsychological approach to the study of developmental disorders, an approach that has been recently criticized. I will argue that these criticisms are less applicable to psychopathy. Indeed, animal work on the development of the neural systems necessary for emotion, does not support a constructivist approach with respect to affect. Importantly, such work indicates that while environmental effects can alter the responsiveness of the basic neural architecture mediating emotion, environmental effects do not construct this architecture. However, caveats to the neuropsychological approach with reference to this disorder are noted."}
{"_id":"fe419be5c53e2931e1d6370c914ce166be29ff6e","title":"Dynamic Switching Networks","text":""}
{"_id":"b1e4eff874567d014482f6abd64ac59c0818ec6f","title":"Primate frontal eye fields. II. Physiological and anatomical correlates of electrically evoked eye movements.","text":"We studied single neurons in the frontal eye fields of awake macaque monkeys and compared their activity with the saccadic eye movements elicited by microstimulation at the sites of these neurons. Saccades could be elicited from electrical stimulation in the cortical gray matter of the frontal eye fields with currents as small as 10 microA. Low thresholds for eliciting saccades were found at the sites of cells with presaccadic activity. Presaccadic neurons classified as visuomovement or movement were most associated with low (less than 50 microA) thresholds. High thresholds (greater than 100 microA) or no elicited saccades were associated with other classes of frontal eye field neurons, including neurons responding only after saccades and presaccadic neurons, classified as purely visual. Throughout the frontal eye fields, the optimal saccade for eliciting presaccadic neural activity at a given recording site predicted both the direction and amplitude of the saccades that were evoked by microstimulation at that site. In contrast, the movement fields of postsaccadic cells were usually different from the saccades evoked by stimulation at the sites of such cells. We defined the low-threshold frontal eye fields as cortex yielding saccades with stimulation currents less than or equal to 50 microA. It lies along the posterior portion of the arcuate sulcus and is largely contained in the anterior bank of that sulcus. It is smaller than Brodmann's area 8 but corresponds with the union of Walker's cytoarchitectonic areas 8A and 45. Saccade amplitude was topographically organized across the frontal eye fields. Amplitudes of elicited saccades ranged from less than 1 degree to greater than 30 degrees. Smaller saccades were evoked from the ventrolateral portion, and larger saccades were evoked from the dorsomedial portion. Within the arcuate sulcus, evoked saccades were usually larger near the lip and smaller near the fundus. Saccade direction had no global organization across the frontal eye fields; however, saccade direction changed in systematic progressions with small advances of the microelectrode, and all contralateral saccadic directions were often represented in a single electrode penetration down the bank of the arcuate sulcus. Furthermore, the direction of change in these progressions periodically reversed, allowing particular saccade directions to be multiply represented in nearby regions of cortex.(ABSTRACT TRUNCATED AT 400 WORDS)"}
{"_id":"0c3078bf214cea52669ec13962a0a242243d0e09","title":"A Broadband Folded Printed Quadrifilar Helical Antenna Employing a Novel Compact Planar Feeding Circuit","text":"A broadband printed quadrifilar helical antenna employing a novel compact feeding circuit is proposed in this paper. This antenna presents an excellent axial ratio over a wide beamwidth, with a 29% bandwidth. A specific feeding circuit based on an aperture-coupled transition and including two 90\u00b0 surface mount hybrids has been designed to be integrated with the quadrifilar antenna. Over the bandwidth, the measured reflection coefficient of the antenna fed by the wideband compact circuit has been found to be equal to or lower than -12 dB and the maximum gain varies between 1.5 and 2.7 dBic from 1.18 to 1.58 GHz. The half-power beamwidth is 150\u00b0, with an axial ratio below 3 dB over this range. The compactness of the feeding circuit allows small element spacing in array arrangements."}
{"_id":"12417f4f32a3dbb6245a4c8dd345aee4d5a2f7b0","title":"Clustering Semantic Spaces of Suicide Notes and Newsgroups Articles","text":"Historically, suicide risk assessment has relied on question-and-answer type tools. These tools, built on psychometric advances, are widely used because of availability. Yet there is no known tool based on biologic and cognitive evidence. This absence often cause a vexing clinical problem for clinicians who question the value of the result as time passes. The purpose of this paper is to describe one experiment in a series of experiments to develop a tool that combines Biological Markers ( Bm) with Thought Markers ( Tm), and use machine learning to compute a real-time index for assessing the likelihood repeated suicide attempt in the next six-months. For this study we focus using unsupervised machine learning to distinguish between actual suicide notes and newsgroups. This is important because it gives us insight into how well these methods discriminate between real notes and general conversation."}
{"_id":"2d952982a5049d3315b21c8da7ebd6b165a87b0b","title":"Receiver Design for a Bionic Nervous System: Modeling the Dendritic Processing Power","text":"Intrabody nanonetworks for nervous system monitoring are envisioned as a key application of the Internet of Nano-Things (IoNT) paradigm, with the aim of developing radically new medical diagnosis and treatment techniques. Indeed, very recently, bionic devices have been implanted inside a living human brain as innovative treatment for drug-resistant epilepsy. In this context, this paper proposes a systems-theoretic communication model to capture the actual behavior of biological neurons. Specifically, biological neurons exhibit physical extension due to their projections called dendrites, which propagate the electrochemical stimulation received via synapses to the soma. Experimental evidences show that the dendrites exhibit two main features: 1) the compartmentalization at the level of the dendritic branches of the neuronal processes and 2) the location-dependent preference for different frequencies. Stemming from these experimental evidences, we propose to model the dendritic tree as a spatiotemporal filter bank, where each filter models the behavior in both space and time of a dendritic branch. Each filter is fully characterized along with the overall neuronal response. Furthermore, sufficient conditions on the incoming stimulus for inducing a null-neuronal response are derived. The conducted theoretical analysis shows that: 1) the neuronal information is encoded in the stimulus temporal pattern, i.e., it is possible to select the neuron to affect by changing the stimulus frequency content; in this sense, the communication among neurons is frequency-selective and 2) the spatial distribution of the dendrites affects the neuronal response; in this sense, the communication among neurons is spatial-selective. The theoretical analysis is validated through a real neuron morphology."}
{"_id":"2ffc77e3a847bf5cc08a40cecb107368b494d4b7","title":"Sentence Segmentation in Narrative Transcripts from Neuropsycological Tests using Recurrent Convolutional Neural Networks","text":"Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of languageimpairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech, as well as, normal, prepared speech, and presents better results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive Impairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method (F1 = 0.55 and 0.53, respectively) used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL."}
{"_id":"df39d80bfe9ea60ad165a91b9633400851b25e5d","title":"A droop control strategy of parallel-inverter-based microgrid","text":"In this paper, the control strategy for a parallel-inverters-based microgrid is presented. The control strategy includes outer active and reactive power control loop based on frequency and voltage droop method to avoid critical communications among distributed generation units (DGs). The inner inverter control loop is employed to produce the required inverter output voltage provided by the outer power loop. In addition, two inverter control schemes are introduced and compared. This microgrid can operate at both grid-connected and islanded mode with proper power sharing capability between parallel DG systems. Moreover, smooth transition between operating modes is achieved without causing negative effect on the utility and critical loads. The performance of this control strategy is verified in simulation using Matlab\/Simulink."}
{"_id":"7a11cd3434c7f1eabbf76f03182255f62fd27fa3","title":"Application of CFD in building performance simulation for the outdoor environment : an overview","text":"This paper provides an overview of the application of CFD in building performance simulation for the outdoor environment, focused on four topics: (1) pedestrian wind environment around buildings, (2) wind-driven rain on building facades, (3) convective heat transfer coefficients at exterior building surfaces, and (4) air pollutant dispersion around buildings. For each topic, its background, the need for CFD, an overview of some past CFD studies, a discussion about accuracy and some perspectives for practical application are provided. The paper indicates that for all four topics, CFD offers considerable advantages compared to wind tunnel modelling or (semi-)empirical formulae because it can provide detailed whole-flow field data under fully controlled conditions and without similarity constraints. The main limitations are the deficiencies of steady RANS modelling, the increased complexity and computational expense of LES and the requirement of systematic and time-consuming CFD solution verification and validation studies."}
{"_id":"b17dd35f5e884823fd292a6d72d8124d0758173a","title":"Numerical Study of Urban Canyon Microclimate Related to Geometrical Parameters","text":"In this study a microclimate analysis on a particular urban configuration: the\u2014street canyon\u2014has been carried out. The analysis, conducted by performing numerical simulations using the finite volumes commercial code ANSYS-Fluent, shows the flow field in an urban environment, taking into account three different aspect ratios (H\/W). This analysis can be helpful in the study on urban microclimate and on the heat exchanges with the buildings. Fluid-dynamic fields on vertical planes within the canyon, have been evaluated. The results show the importance of the geometrical configuration, in relation to the ratio between the height (H) of the buildings and the width (W) of the road. This is a very important subject from the point of view of \u201cSmart Cities\u201d, considering the urban canyon as a subsystem of a larger one (the city), which is affected by climate changes."}
{"_id":"bc39516aba1e74f7d8ed7391b4ff9e5a9ceeecf2","title":"BEST PRACTICE GUIDELINE FOR THE CFD SIMULATION OF FLOWS IN THE URBAN ENVIRONMENT","text":"Legal notice by the COST Office Neither the COST Office nor any person acting on its behalf is responsible for the use which might be made of the information contained in the present publication. The COST Office is not responsible for the external web sites referred to in the present publication. No permission to reproduce or utilize the contents of this book by any means is necessary, other than in the case of images, diagrams or other material from other copyright holders. In such cases permission of the copyright holders is required. This book may be cited as: Title of the book and Action Number"}
{"_id":"f5e717d62ee75465deb3c3495b2b867bdc17560e","title":"Urban Physics : Effect of the micro-climate on comfort , health and energy demand","text":"Y-NC-ND license. Abstract The global trend towards urbanisation explains the growing interest in the study of the modification of the urban climate due to the heat island effect and global warming, and its impact on energy use of buildings. Also urban comfort, health and durability, referring respectively to pedestrian wind\/ thermal comfort, pollutant dispersion and wind-driven rain are of interest. Urban Physics is a wellestablished discipline, incorporating relevant branches of physics, environmental chemistry, aerodynamics, meteorology and statistics. Therefore, Urban Physics is well positioned to provide keycontributions to the current urban problems and challenges. The present paper addresses the role of Urban Physics in the study of wind comfort, thermal comfort, energy demand, pollutant dispersion and wind-driven rain. Furthermore, the three major research methods applied in Urban Physics, namely field experiments, wind tunnel experiments and numerical simulations are discussed. Case studies illustrate the current challenges and the relevant contributions of Urban Physics. & 2012. Higher Education Press Limited Company. Production and hosting by Elsevier B.V. Open access under CC BY-NC-ND license."}
{"_id":"7046b8930c262c1841a7fad461dfc37eeb466771","title":"Verification , Validation , and Predictive Capability in Computational Engineering and Physics","text":"Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V&V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, i.e., experimental data, is the issue."}
{"_id":"e6886cadc783c6ca26db0ea912154d4681366492","title":"Practical path planning and path following for a non-holonomic mobile robot based on visual servoing","text":"In this paper, a practical real-time path planning and robot navigation algorithm for a non-holonomic indoor mobile robot based on visual servoing is implemented. The proposed algorithm is divided into three parts; the first part uses Multi-Stencils Fast Marching (MSFM) as a path planning method. But the generated path results from fast marching methods when used directly, not guaranteed to be safe and smooth. Subsequently, the robot can touch corners, walls and obstacles. The proposed algorithm uses image processing methods to solve this problem. The second part estimates the position and orientation of the robot, from the visual information, to follow the desired path with avoiding obstacles. The third part proposes a decentralized PD-like Fuzzy Logic Controller (FLC) to keep up the robot on the desired path. Experimental results show that the developed design is valid to estimate shortest-path by avoiding obstacles and able to guide the robot to follow the path in real-time."}
{"_id":"2bdbea0ce990ecb4f9001d3afb261f246aa8595a","title":"Analysis of Statistical Question Classification for Fact-Based Questions","text":"Question classification systems play an important role in question answering systems and can be used in a wide range of other domains. The goal of question classification is to accurately assign labels to questions based on expected answer type. Most approaches in the past have relied on matching questions against hand-crafted rules. However, rules require laborious effort to create and often suffer from being too specific. Statistical question classification methods overcome these issues by employing machine learning techniques. We empirically show that a statistical approach is robust and achieves good performance on three diverse data sets with little or no hand tuning. Furthermore, we examine the role different syntactic and semantic features have on performance. We find that semantic features tend to increase performance more than purely syntactic features. Finally, we analyze common causes of misclassification error and provide insight into ways they may be overcome."}
{"_id":"b836a5485dd5a4e39e22ec5a30bf804180a97bfe","title":"UTAssistant: A Web Platform Supporting Usability Testing in Italian Public Administrations","text":"Even if the benefits of the usability testing are remarkable, it is scarcely adopted in the software development process. To foster its adoption, this paper presents a Web platform, UTAssistant, that supports people, also without skills in Human-Computer Interaction (HCI), in evaluating Web site usability."}
{"_id":"01d023ff2450a1d0ef42a8c00592d124bbeafe69","title":"A Game Theoretic Framework for Incentives in P2P Systems","text":"Peer-To-Peer (P2P) networks are self-organizing, distributed systems, with no centralized authority or infrastructure. Because of the voluntary participation, the availability of resources in a P2P system can be highly variable and unpredictable. In this paper, we use ideas from Game Theory to study the interaction of strategic and rational peers, and propose a differential service-based incentive scheme to improve the system\u2019s performance."}
{"_id":"07fe26f10ab2eb7ba7ad7df5096813c949dcabc9","title":"Fast and Robust Joint Models for Biomedical Event Extraction","text":"Extracting biomedical events from literature has attracted much recent attention. The bestperforming systems so far have been pipelines of simple subtask-specific local classifiers. A natural drawback of such approaches are cascading errors introduced in early stages of the pipeline. We present three joint models of increasing complexity designed to overcome this problem. The first model performs joint trigger and argument extraction, and lends itself to a simple, efficient and exact inference algorithm. The second model captures correlations between events, while the third model ensures consistency between arguments of the same event. Inference in these models is kept tractable through dual decomposition. The first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart. The third model yields the best results reported so far on the BioNLP 2009 shared task, the BioNLP 2011 Genia task and the BioNLP 2011 Infectious Diseases task."}
{"_id":"535c0310c00371846ea8cecdb0957ced42f2f1ba","title":"How to improve knowledge transfer strategies and practices in education ? Answers from a systematic literature review","text":"Building on the systematic review methodology, this paper aims to examine the knowledge transfer process in education and its main determinants in this specific context. Our findings suggest that linkage agents are central actors in the knowledge transfer process. Their intervention is critical to help adapt the knowledge produced by researchers and make it easier to adopt and use by practitioners. Moreover, the effectiveness of this process hinges on several factors that were broken down into three major categories: determinants related to transferredknowledge attributes, those related to the actors involved in the process, and determinants related to transfer mechanisms."}
{"_id":"a5d1a2566d3cd4b32c2a2294019e598c0a57b219","title":"Laryngeal Tumor Detection and Classification in Endoscopic Video","text":"The development of the narrow-band imaging (NBI) has been increasing the interest of medical specialists in the study of laryngeal microvascular network to establish diagnosis without biopsy and pathological examination. A possible solution to this challenging problem is presented in this paper, which proposes an automatic method based on anisotropic filtering and matched filter to extract the lesion area and segment blood vessels. Lesion classification is then performed based on a statistical analysis of the blood vessels' characteristics, such as thickness, tortuosity, and density. Here, the presented algorithm is applied to 50 NBI endoscopic images of laryngeal diseases and the segmentation and classification accuracies are investigated. The experimental results show the proposed algorithm provides reliable results, reaching an overall classification accuracy rating of 84.3%. This is a highly motivating preliminary result that proves the feasibility of the new method and supports the investment in further research and development to translate this study into clinical practice. Furthermore, to our best knowledge, this is the first time image processing is used to automatically classify laryngeal tumors in endoscopic videos based on tumor vascularization characteristics. Therefore, the introduced system represents an innovation in biomedical and health informatics."}
{"_id":"332f0395484d174a0fd6c6a72a12054627fed3f4","title":"Generalized Simulated Annealing","text":"We propose a new stochastic algorithm (generalized simulated annealing) for computationally finding the global minimum of a given (not necessarily convex) energy\/cost function defined in a continuous D-dimensional space. This algorithm recovers, as particular cases, the so called classical (\u201cBoltzmann machine\u201d) and fast (\u201cCauchy machine\u201d) simulated annealings, and can be quicker than both. Key-words: Simulated annealing; Nonconvex optimization; Gradient descent; Generalized Statistical Mechanics."}
{"_id":"04e5b276da90c8181d6ad8397f763a181baae949","title":"Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement","text":"Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance."}
{"_id":"ce8bd8f2fb1687929ee23ebee2394a51f06872d7","title":"An Effective High Threating Alarm Mining Method for Cloud Security Management","text":"Security equipment such as intrusion prevention system is an important supplementary for security management. They reduce the difficulty of network management by giving alarms corresponding to different attacks instead of raw traffic packet inspection. But there are many false alarms due to their running mechanism, which greatly reduces its usability. In this paper, we develop a hierarchical framework to mine high threating alarms from the massive alarm logs, and aim to provide fundamental and useful information for administrators to design efficient management policy. First, the alarms are divided into two parts based on their attributes, the first part mainly includes several kinds of famous attacks which are critical for security management, we proposed a similar alarm mining method based on Choquet integral to cluster and rank the frequently occurred attacks. The rest alarms constitute the second part, which are caused by the potential threats attacks, also include many false alarms. To reduce the effect of false alarms and rank the potential threats, we employ the frequent pattern mining algorithm to mine correlation rules and then filter false alarms. Following, we proposed a self-adapting threat degree calculation method to qualify the threat degree of these alarms after filtering. To verity the methods developed, an experimental platform is constructed in the campus network of Xi\u2019an Jiaotong University. Experimental results based on the data collected verify the efficiency of the developed methods. For the first kind of alarms, the similar alarms mining accuracy is higher than 97% and the alarms are ranked with different processing urgencies. For the rest alarms, the proposed methods have filtering accuracy above 80% and can rank the potential threats. Based on the ranking results, administrators can deal with the high threats with their limited time and energy, in turn, keep the network under control."}
{"_id":"59db42a9e913bc9c56bdcd7b1303e5c5328f50a6","title":"Application of Data Mining Classification Algorithms for Breast Cancer Diagnosis","text":"Breast cancer is one of the diseases that represent a large number of incidence and mortality in the world. Data mining classifications techniques will be effective tools for classifying data of cancer to facilitate decision-making.\n The objective of this paper is to compare the performance of different machine learning algorithms in the diagnosis of breast cancer, to define exactly if this type of cancer is a benign or malignant tumor.\n Six machine learning algorithms were evaluated in this research Bayes Network (BN), Support Vector Machine (SVM), k-nearest neighbors algorithm (Knn), Artificial Neural Network (ANN), Decision Tree (C4.5) and Logistic Regression. The simulation of the algorithms is done using the WEKA tool (The Waikato Environment for Knowledge Analysis) on the Wisconsin breast cancer dataset available in UCI machine learning repository."}
{"_id":"8efac913ff430ef698dd3fa5df4cbb7ded3cab50","title":"An Unsupervised Clustering Tool for Unstructured Data","text":"We present an unsupervised clustering tool, Principal Direction Divisive Partitioning, which is a scal-able and versatile top-down method applicable to any set of data that can be represented as numerical vectors. A description of the basic method, a summary of the main application areas where this has been used, and some recent results on the selection of signiicant words as well as the process of updating clusters as new data arrives is discussed."}
{"_id":"6e5809cc76d1909a67913e2f730961dba86084c0","title":"Basic studies on wet adhesion system for wall climbing robots","text":"This paper reports a vacuum-based wet adhesion system for wall climbing robots. In this adhesion system, a suction cup adheres on a wet surface. The problems addressed are an adherability on a rough surface, which is comes from the seal action of a liquid, and low friction between suction cup and adhered rough and smooth surfaces which is comes form lubricating action of a liquid. Generally, it is difficult that a vacuumed suction cup adheres on rough surface such concrete plate and hardly slidable. In this paper, the adhesion force and friction when a suction cup adheres on smooth glass plate and concrete plate are measured and compared wet condition with dry condition. The experiment result showed that a viscosity is important at the sealing performance of adhesion on rough surface. The silicon oil of a viscosity of 5000cSt allows a suction cup to adhere on concrete surface. In this condition it comes up to the adhesion force when a suction cup adheres on smooth glass with dry condition."}
{"_id":"a0219c4ecbf7ee4603aaec5811027dfa85b3d85d","title":"Selection from alphabetic and numeric menu trees using a touch screen: breadth, depth, and width","text":"Goal items were selected by a series of touch-menu choices among sequentially subdivided ranges of integers or alphabetically ordered words. The number of alternatives at each step, <italic>b<\/italic>, was varied, and, inversely, the size of the target area for the touch. Mean response time for each screen was well described by <italic>T<\/italic> = <italic>k<\/italic>+<italic>c<\/italic>log<italic>b<\/italic>, in agreement with the Hick-Hyman and Fitts' laws for decision and movement components in series. It is shown that this function favors breadth over depth in menus, whereas others might not. Speculations are offered as to when various functions could be expected."}
{"_id":"79883a68028f206062a73ac7f32271212e92ade8","title":"FILA: Fine-grained indoor localization","text":"Indoor positioning systems have received increasing attention for supporting location-based services in indoor environments. WiFi-based indoor localization has been attractive due to its open access and low cost properties. However, the distance estimation based on received signal strength indicator (RSSI) is easily affected by the temporal and spatial variance due to the multipath effect, which contributes to most of the estimation errors in current systems. How to eliminate such effect so as to enhance the indoor localization performance is a big challenge. In this work, we analyze this effect across the physical layer and account for the undesirable RSSI readings being reported. We explore the frequency diversity of the subcarriers in OFDM systems and propose a novel approach called FILA, which leverages the channel state information (CSI) to alleviate multipath effect at the receiver. We implement the FILA system on commercial 802.11 NICs, and then evaluate its performance in different typical indoor scenarios. The experimental results show that the accuracy and latency of distance calculation can be significantly enhanced by using CSI. Moreover, FILA can significantly improve the localization accuracy compared with the corresponding RSSI approach."}
{"_id":"be4d16e6875d109288ce55d2e029122a6c5ad774","title":"Detection of covert attacks on cyber-physical systems by extending the system dynamics with an auxiliary system","text":"Securing cyber-physical systems is vital for our modern society since they are widely used in critical infrastructure like power grid control and water distribution. One of the most sophisticated attacks on these systems is the covert attack, where an attacker changes the system inputs and disguises his influence on the system outputs by changing them accordingly. In this paper an approach to detect such an attack by extending the original system with a switched auxiliary system is proposed. Furthermore, a detection system using a switched Luenberger observer is presented. The effectiveness of the proposed method is illustrated by a simulation example."}
{"_id":"905b3a30d54520055672b0f363c683b4b258e636","title":"Matchmaker: constructing constrained texture maps","text":"Texture mapping enhances the visual realism of 3D models by adding fine details. To achieve the best results, it is often necessary to force a correspondence between some of the details of the texture and the features of the model.The most common method for mapping texture onto 3D meshes is to use a planar parameterization of the mesh. This, however, does not reflect any special correspondence between the mesh geometry and the texture. The Matchmaker algorithm presented here forces user-defined feature correspondence for planar parameterization of meshes. This is achieved by adding positional constraints to the planar parameterization. Matchmaker allows users to introduce scores of constraints while maintaining a valid one-to-one mapping between the embedding and the 3D surface. Matchmaker's constraint mechanism can be used for other applications requiring parameterization besides texture mapping, such as morphing and remeshing.Matchmaker begins with an unconstrained planar embedding of the 3D mesh generated by conventional methods. It moves the constrained vertices to the required positions by matching a triangulation of these positions to a triangulation of the planar mesh formed by paths between constrained vertices. The matching triangulations are used to generate a new parameterization that satisfies the constraints while minimizing the deviation from the original 3D geometry."}
{"_id":"864605d79662579b48e248d8fb4fe36c047baaaa","title":"A First Step towards Eye State Prediction Using EEG","text":"In this paper, we investigate how the eye state (open or closed) can be predicted by measuring brain waves with an EEG. To this end, we recorded a corpus containing the activation strength of the fourteen electrodes of a commercial EEG headset as well as the manually annotated eye state corresponding to the recorded data. We tested 42 different machine learning algorithms on their performance to predict the eye state after training with the corpus. The best-performing classifier, KStar, produced a classification error rate of only 2.7% which is a 94% relative reduction over the majority vote of 44.9% classification error."}
{"_id":"4be1afa552fd9241204a878c7ae9e40dd351de52","title":"Differences in energy expenditure during high-speed versus standard-speed yoga: A randomized sequence crossover trial.","text":"OBJECTIVES\nTo compare energy expenditure and volume of oxygen consumption and carbon dioxide production during a high-speed yoga and a standard-speed yoga program.\n\n\nDESIGN\nRandomized repeated measures controlled trial.\n\n\nSETTING\nA laboratory of neuromuscular research and active aging.\n\n\nINTERVENTIONS\nSun-Salutation B was performed, for eight minutes, at a high speed versus and a standard-speed separately while oxygen consumption was recorded. Caloric expenditure was calculated using volume of oxygen consumption and carbon dioxide production.\n\n\nMAIN OUTCOME MEASURES\nDifference in energy expenditure (kcal) of HSY and SSY.\n\n\nRESULTS\nSignificant differences were observed in energy expenditure between yoga speeds with high-speed yoga producing significantly higher energy expenditure than standard-speed yoga (MD=18.55, SE=1.86, p<0.01). Significant differences were also seen between high-speed and standard-speed yoga for volume of oxygen consumed and carbon dioxide produced.\n\n\nCONCLUSIONS\nHigh-speed yoga results in a significantly greater caloric expenditure than standard-speed yoga. High-speed yoga may be an effective alternative program for those targeting cardiometabolic markers."}
{"_id":"8efe3b64ee8a936e583b18b415d0071a1cd65a7c","title":"System Design of a 77 GHz Automotive Radar Sensor with Superresolution DOA Estimation","text":"This paper introduces a novel 77 GHz FMCW automotive long range radar (LRR) system concept. High resolution direction of arrival (DOA) estimation is an important requirement for the application in automotive safety systems. The challenges in system design regarding low cost and superresolution signal processing are discussed. Dominant interferences to the MUSIC DOA estimator are amplitude and phase mismatches due to inhomogeneous antenna patterns. System simulation results deliver design guidelines for the required signal-to-noise ratio (SNR) and the antenna design. Road traffic measurements with a demonstrator system show superior DOA resolution and demonstrate the feasibility of the design goals."}
{"_id":"908f48cf72e0724a80baf87913f1b8534ed5a380","title":"Automotive Radar \u2013 Status and Trends","text":"The paper gives a brief overview of automotive radar. The status of the frequency regulation for short and long range radar is summarized because of its importance for car manufacturers and their sensor suppliers. Front end concepts and antenna techniques of 24 GHz and 77 GHz sensors are briefly described. Their impact on the sensor\u2019s field of view and on the angular measurement capability is discussed. Esp. digital beamforming concepts are considered and promising results are presented."}
{"_id":"408a8e250316863da94ffb3eab077175d08c01bf","title":"Multiple Emitter Location and Signal Parameter-- Estimation","text":""}
{"_id":"ad570ceaffa4a012ff3e0157df80c6e1229f0a73","title":"Proposal of millimeter-wave holographic radar with antenna switching","text":"This paper proposes a millimeter-wave holographic radar with a simple structure for automotive applications. The simplicity can be realized by switching both transmitting antennas and receiving antennas. Also, a super resolution technique is introduced for the detection of angular positions in the proposed radar. The radar developed experimentally has accomplished an azimuthal angular resolution of less than 2 degrees and an azimuthal field of view (FoV) of more than 20 degrees simultaneously."}
{"_id":"752a9d8506a1e67687e29f845b13f465a705a63c","title":"Planning and Control for Collision-Free Cooperative Aerial Transportation","text":"This paper presents planning and control synthesis for multiple aerial manipulators to transport a common object. Each aerial manipulator that consists of a hexacopter and a two-degree-of-freedom robotic arm is controlled by an augmented adaptive sliding mode controller based on a closed-chain robot dynamics. We propose a motion planning algorithm by exploiting rapidly exploring random tree star (RRT*) and dynamic movement primitives (DMPs). The desired path for each aerial manipulator is obtained by using RRT* with Bezier curve, which is designed to handle environmental obstacles, such as buildings or equipments. During aerial transportation, to avoid unknown obstacle, DMPs modify the trajectory based on the virtual leader\u2013follower structure. By the combination of RRT* and DMPs, the cooperative aerial manipulators can carry a common object to keep reducing the interaction force between multiple robots while avoiding an obstacle in the unstructured environment. To validate the proposed planning and control synthesis, two experiments with multiple custom-made aerial manipulators are presented, which involve user-guided trajectory and RRT*-planned trajectory tracking in unstructured environments.Note to Practitioners\u2014This paper presents a viable approach to autonomous aerial transportation using multiple aerial manipulators equipped with a multidegree-of-freedom robotic arm. Existing approaches for cooperative manipulation based on force decomposition or impedance-based control often require a heavy or expensive force\/torque sensor. However, this paper suggests a method without using a heavy or expensive force\/torque sensor based on closed-chain dynamics in joint space and rapidly exploring random tree star (RRT*) that generates the desired trajectory of aerial manipulators. Unlike conventional RRT*, in this paper, our method can also avoid an unknown moving obstacle during aerial transportation by exploiting RRT* and dynamic movement primitives. The proposed planning and control synthesis is tested to demonstrate performance in a lab environment with two custom-made aerial manipulators and a common object."}
{"_id":"9ea9b1ac918ec5d00ff352bbb3d90c31752e5a7e","title":"Reliable Client Accounting for P2P-Infrastructure Hybrids","text":"Content distribution networks (CDNs) have started to adopt hybrid designs, which employ both dedicated edge servers and resources contributed by clients. Hybrid designs combine many of the advantages of infrastructurebased and peer-to-peer systems, but they also present new challenges. This paper identifies reliable client accounting as one such challenge. Operators of hybrid CDNs are accountable to their customers (i.e., content providers) for the CDN\u2019s performance. Therefore, they need to offer reliable quality of service and a detailed account of content served. Service quality and accurate accounting, however, depend in part on interactions among untrusted clients. Using the Akamai NetSession client network in a case study, we demonstrate that a small number of malicious clients used in a clever attack could cause significant accounting inaccuracies. We present a method for providing reliable accounting of client interactions in hybrid CDNs. The proposed method leverages the unique characteristics of hybrid systems to limit the loss of accounting accuracy and service quality caused by faulty or compromised clients. We also describe RCA, a system that applies this method to a commercial hybrid content-distribution network. Using trace-driven simulations, we show that RCA can detect and mitigate a variety of attacks, at the expense of a moderate increase in logging overhead."}
{"_id":"2822a883d149956934a20614d6934c6ddaac6857","title":"A survey of appearance models in visual object tracking","text":"Visual object tracking is a significant computer vision task which can be applied to many domains, such as visual surveillance, human computer interaction, and video compression. Despite extensive research on this topic, it still suffers from difficulties in handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion. Therefore, effective modeling of the 2D appearance of tracked objects is a key issue for the success of a visual tracker. In the literature, researchers have proposed a variety of 2D appearance models.\n To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic.\n The contributions of this survey are fourfold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-construction mechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statistical modeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source codes and video datasets) are examined in this survey."}
{"_id":"056462a3d5a78362700cd964e5d0bae4a5a9f08b","title":"Polarization imaging reflectometry in the wild","text":"We present a novel approach for on-site acquisition of surface reflectance for planar, spatially varying, isotropic samples in uncontrolled outdoor environments. Our method exploits the naturally occurring linear polarization of incident and reflected illumination for this purpose. By rotating a linear polarizing filter in front of a camera at three different orientations, we measure the polarization reflected off the sample and combine this information with multi-view analysis and inverse rendering in order to recover per-pixel, high resolution reflectance and surface normal maps. Specifically, we employ polarization imaging from two near orthogonal views close to the Brewster angle of incidence in order to maximize polarization cues for surface reflectance estimation. To the best of our knowledge, our method is the first to successfully extract a complete set of reflectance parameters with passive capture in completely uncontrolled outdoor settings. To this end, we analyze our approach under the general, but previously unstudied, case of incident partial linear polarization (due to the sky) in order to identify the strengths and weaknesses of the method under various outdoor conditions. We provide practical guidelines for on-site acquisition based on our analysis, and demonstrate high quality results with an entry level DSLR as well as a mobile phone."}
{"_id":"9e2286da5ac2f3f66dbd6710c9b4fa2719ace948","title":"Product platform design and customization: Status and promise","text":"In an effort to improve customization for today\u2019s highly competitive global marketplace, many companies are utilizing product families and platform-based product development to increase variety, shorten lead times, and reduce costs. The key to a successful product family is the product platform from which it is derived either by adding, removing, or substituting one or more modules to the platform or by scaling the platform in one or more dimensions to target specific market niches. This nascent field of engineering design has matured rapidly in the past decade, and this paper provides a comprehensive review of the flurry of research activity that has occurred during that time to facilitate product family design and platform-based product development for mass customization. Techniques for identifying platform leveraging strategies within a product family are reviewed along with metrics for assessing the effectiveness of product platforms and product families. Special emphasis is placed on optimization approaches and artificial intelligence techniques to assist in the process of product family design and platform-based product development. Web-based systems for product platform customization are also discussed. Examples from both industry and academia are presented throughout the paper to highlight the benefits of product families and product platforms. The paper concludes with a discussion of potential areas of research to help bridge the gap between planning and managing families of products and designing and manufacturing them."}
{"_id":"0d14221e3bbb1a58f115a7c7301dc4d4048be13f","title":"WebWitness: Investigating, Categorizing, and Mitigating Malware Download Paths","text":"Most modern malware download attacks occur via the browser, typically due to social engineering and driveby downloads. In this paper, we study the \u201corigin\u201d of malware download attacks experienced by real network users, with the objective of improving malware download defenses. Specifically, we study the web paths followed by users who eventually fall victim to different types of malware downloads. To this end, we propose a novel incident investigation system, named WebWitness. Our system targets two main goals: 1) automatically trace back and label the sequence of events (e.g., visited web pages) preceding malware downloads, to highlight how users reach attack pages on the web; and 2) leverage these automatically labeled in-the-wild malware download paths to better understand current attack trends, and to develop more effective defenses. We deployed WebWitness on a large academic network for a period of ten months, where we collected and categorized thousands of live malicious download paths. An analysis of this labeled data allowed us to design a new defense against drive-by downloads that rely on injecting malicious content into (hacked) legitimate web pages. For example, we show that by leveraging the incident investigation information output by WebWitness we can decrease the infection rate for this type of drive-by downloads by almost six times, on average, compared to existing URL blacklisting approaches."}
{"_id":"b0fbee324607b46ae65ec4ce9601d3a2bfa17202","title":"Speech recognition in adverse conditions : A review","text":"Speech recognition in adverse conditions: A review Sven L. Mattys a , Matthew H. Davis b , Ann R. Bradlow c & Sophie K. Scott d a Department of Psychology, University of York, York, UK b Medical Research Council, Cognition and Brain Sciences Unit, Cambridge, UK c Department of Linguistics, Northwestern University, Evanston, IL, USA d Institute of Cognitive Neuroscience, University College London, London, UK"}
{"_id":"7562c087ff18553f74fae8aff29892394f181582","title":"3D free-form surface registration and object recognition","text":"A new technique to recognise 3D free-form objects via registration is proposed. This technique attempts to register a free-form surface, represented by a set of % MathType!MTEF!2!1!+-% feaafeart1ev1aaatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn% hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr% 4rNCHbGeaGqiVu0Je9sqqrpepC0xbbL8F4rqqrFfpeea0xe9Lq-Jc9% vqaqpepm0xbba9pwe9Q8fs0-yqaqpepae9pg0FirpepeKkFr0xfr-x% fr-xb9adbaqaaeGaciGaaiaabeqaamaabaabaaGcbaGaaGOmamaala% aabaGaaGymaaqaaiaaikdaaaGaamiraaaa!38F8!\\[2\\frac{1}{2}D\\] sensed data points, to the model surface, represented by another set of % MathType!MTEF!2!1!+-% feaafeart1ev1aaatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn% hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr% 4rNCHbGeaGqiVu0Je9sqqrpepC0xbbL8F4rqqrFfpeea0xe9Lq-Jc9% vqaqpepm0xbba9pwe9Q8fs0-yqaqpepae9pg0FirpepeKkFr0xfr-x% fr-xb9adbaqaaeGaciGaaiaabeqaamaabaabaaGcbaGaaGOmamaala% aabaGaaGymaaqaaiaaikdaaaGaamiraaaa!38F8!\\[2\\frac{1}{2}D\\] model data points, without prior knowledge of correspondence or view points between the two point sets. With an initial assumption that the sensed surface be part of a more complete model surface, the algorithm begins by selecting three dispersed, reliable points on the sensed surface. To find the three corresponding model points, the method uses the principal curvatures and the Darboux frames to restrict the search over the model space. Invariably, many possible model 3-typles will be found. For each hypothesized model 3-tuple, the transformation to match the sensed 3-tuple to the model 3-tuple can be determined. A heuristic search is proposed to single out the optimal transformation in low order time. For realistic object recognition or registration, where the two range images are often extracted from different view points of the model, the earlier assumption that the sensed surface be part of a more complete model surface cannot be relied on. With this, the sensed 3-tuple must be chosen such that the three sensed points lie on the common region visible to both the sensed and model views. We propose an algorithm to select a minimal non-redundant set of 3-tuples such that at least one of the 3-tuples will lie on the overlap. Applying the previous algorithm to each 3-tuple within this set, the optimal transformation can be determined. Experiments using data obtained from a range finder have indicated fast registration for relatively complex test cases. If the optimal registrations between the sensed data (candidate) and each of a set of model data are found, then, for 3D object recognition purposes, the minimal best fit error can be used as the decision rule."}
{"_id":"5656fa5aa6e1beeb98703fc53ec112ad227c49ca","title":"Multi-Prediction Deep Boltzmann Machines","text":"We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.1"}
{"_id":"66eb3adbadb7d75266428d31a82dba443cab47dd","title":"A Modified CFOA and Its Applications to Simulated Inductors, Capacitance Multipliers, and Analog Filters","text":"In this paper, using a minimum number of passive components, i.e., new grounded and floating inductance simulators, grounded capacitance multipliers, and frequency-dependent negative resistors (FDNRs) based on one\/two modified current-feedback operational amplifiers (MCFOAs), are proposed. The type of the simulators depends on the passive element selection used in the structure of the circuit without requiring critical active and passive component-matching conditions and\/or cancellation constraints. In order to show the flexibility of the proposed MCFOA, a single-input three-output (SITO) voltage-mode (VM) filter, two three-input single-output (TISO) VM filters, and an SITO current-mode (CM) filter employing a single MCFOA are reported. The layout of the proposed MCFOA is also given. A number of simulations using the SPICE program and some experimental tests are performed to exhibit the performance of the introduced structures."}
{"_id":"37c1b8775e5eb99ab21ddca50ccf109c6abf906d","title":"Data Structures for Traveling Salesmen","text":"The choice of data structure for tour representation plays a critical role in the efficiency of local improvement heuristics for the Traveling Salesman Problem. The tour data structure must permit queries about the relative order of cities in the current tour and must allow sections of the tour to be reversed. The traditional array-based representation of a tour permits the relative order of cities to be determined in small constant time, but requires worst-case \u03a9(N) time (where N is the number of cities) to implement a reversal, which renders it impractical for large instances. This paper considers alternative tour data structures, examining them from both a theoretical and experimental point of view. The first alternative we consider is a data structure based on splay trees, where all queries and updates take amortized time O( logN). We show that this is close to the best possible, because in the cell probe model of computation any data structure must take worst-case amortized time \u03a9( logN \/log logN) per operation. Empirically (for random Euclidean instances), splay trees overcome their large constant-factor overhead and catch up to arrays by N = 10 , 000, pulling ahead by a factor of 4-10 (depending on machine) when N = 100 , 000. Two alternative tree-based data structures do even better in this range, however. Although both are asymptotically inferior to the splay tree representation, the latter does not appear to pull even with them until N \u223c 1 , 000 , 000. ________________ 1 Rutgers University, New Brunswick, NJ 08903, and University of California at San Diego, La Jolla, CA 92093 2 Room 2D-150, AT&T Bell Laboratories, Murray Hill, NJ 07974 3 Department of Mathematics and Computer Science, Amherst College, Amherst, MA 01002 4 Department of Mathematics, Rutgers University, New Brunswick, NJ 08903 * A preliminary version of this paper appeared under the same title in Proceedings 4th Ann. ACM-SIAM Symp. on Discrete Algorithms (1993), 145-154."}
{"_id":"5dd204ae6b82ccaf4a840a704bfd4753e8d48add","title":"An Operating System for the Home","text":"Network devices for the home such as remotely controllable locks, lights, thermostats, cameras, and motion sensors are now readily available and inexpensive. In theory, this enables scenarios like remotely monitoring cameras from a smartphone or customizing climate control based on occupancy patterns. However, in practice today, such smarthome scenarios are limited to expert hobbyists and the rich because of the high overhead of managing and extending current technology. We present HomeOS, a platform that bridges this gap by presenting users and developers with a PC-like abstraction for technology in the home. It presents network devices as peripherals with abstract interfaces, enables cross-device tasks via applications written against these interfaces, and gives users a management interface designed for the home environment. HomeOS already has tens of applications and supports a wide range of devices. It has been running in 12 real homes for 4\u20138 months, and 42 students have built new applications and added support for additional devices independent of our efforts."}
{"_id":"3d9424dbab0ad247d47ab53b53c6cc5648dc647d","title":"How do training and competition workloads relate to injury? The workload-injury aetiology model.","text":"Injury aetiology models that have evolved over the previous two decades highlight a number of factors which contribute to the causal mechanisms for athletic injuries. These models highlight the pathway to injury, including (1) internal risk factors (eg, age, neuromuscular control) which predispose athletes to injury, (2) exposure to external risk factors (eg, playing surface, equipment), and finally (3) an inciting event, wherein biomechanical breakdown and injury occurs. The most recent aetiological model proposed in 2007 was the first to detail the dynamic nature of injury risk, whereby participation may or may not result in injury, and participation itself alters injury risk through adaptation. However, although training and competition workloads are strongly associated with injury, existing aetiology models neither include them nor provide an explanation for how workloads alter injury risk. Therefore, we propose an updated injury aetiology model which includes the effects of workloads. Within this model, internal risk factors are differentiated into modifiable and non-modifiable factors, and workloads contribute to injury in three ways: (1) exposure to external risk factors and potential inciting events, (2) fatigue, or negative physiological effects, and (3) fitness, or positive physiological adaptations. Exposure is determined solely by total load, while positive and negative adaptations are controlled both by total workloads, as well as changes in load (eg, the acute:chronic workload ratio). Finally, we describe how this model explains the load-injury relationships for total workloads, acute:chronic workload ratios and the training load-injury paradox."}
{"_id":"4869d77ae92fb93d7becfc709904cc7740b1419d","title":"Comparing and Aligning Process Representations","text":"Processes within organizations can be highly complex chains of inter-related steps, involving numerous stakeholders and information systems. Due to this complexity, having access to the right information is vital to the proper execution and effective management of an organization\u2019s business processes. A major challenge in this regard is that information on a single process is often spread out over numerous models, documents, and systems. This phenomenon results from efforts to provide a variety of process stakeholders with the information that is relevant to them, in a suitable format. However, this disintegration of process information also has considerable disadvantages for organizations. In particular, it can lead to severe maintenance issues, reduced execution efficiency, and negative effects on the quality of process results. Against this background, this doctoral thesis focuses on the spread of process information in organizations and, in particular, on the mitigation of the negative aspects associated with this phenomenon. The main contributions of this thesis are five techniques that focus on the alignment and comparison of process information from different informational artifacts. Each of these techniques tackles a specific scenario involving multiple informational artifacts that contain process information in different representation formats. Among others, we present automated techniques for the detection of inconsistencies between process models and textual process descriptions, the alignment of process performance measurements to process models, conformance-checking in the context of uncertainty, and the matching of process models through the analysis of event-log information. We demonstrate the efficacy and usefulness of these techniques through quantitative evaluations involving data obtained from real-world settings. Altogether, the presented work provides important contributions for the analysis, comparison, and alignment of process information in various representation formats through the development of novel concepts and techniques. The contributions, furthermore, provide a means for organizations to improve the efficiency and quality of their processes."}
{"_id":"1512a1cc2b8199c1e3258b1bf26bc402d42ee88f","title":"Variability of Worked Examples and Transfer of Geometrical Problem-Solving Skills : A Cognitive-Load Approach","text":"Four computer-based training strategies for geometrical problem solving in the domain of computer numerically controlled machinery programming were studied with regard to their effects on training performance, transfer performance, and cognitive load. A lowand a high-variability conventional condition, in which conventional practice problems had to be solved (followed by worked examples), were compared with a lowand a high-variability worked condition, in which worked examples had to be studied. Results showed that students who studied worked examples gained most from high-variability examples, invested less time and mental effort in practice, and attained better and less effort-demanding transfer performance than students who first attempted to solve conventional problems and then studied work examples."}
{"_id":"646181575a871cb6bc2e98005ce517ebe5772d66","title":"Comparison of the Hardware Performance of the AES Candidates Using Reconfigurable Hardware","text":"The results of implementations of all five AES finalists using Xilinx Field Programmable Gate Arrays are presented and analyzed. Performance of four alternative hardware architectures is discussed and compared. The AES candidates are divided into three classes depending on their hardware performance characteristics. Recommendation regarding the optimum choice of the algorithms for AES is provided."}
{"_id":"682a975a638bfa37fa4b4ca53222dcee756fe826","title":"Alzheimer's disease: genes, proteins, and therapy.","text":"Rapid progress in deciphering the biological mechanism of Alzheimer's disease (AD) has arisen from the application of molecular and cell biology to this complex disorder of the limbic and association cortices. In turn, new insights into fundamental aspects of protein biology have resulted from research on the disease. This beneficial interplay between basic and applied cell biology is well illustrated by advances in understanding the genotype-to-phenotype relationships of familial Alzheimer's disease. All four genes definitively linked to inherited forms of the disease to date have been shown to increase the production and\/or deposition of amyloid beta-protein in the brain. In particular, evidence that the presenilin proteins, mutations in which cause the most aggressive form of inherited AD, lead to altered intramembranous cleavage of the beta-amyloid precursor protein by the protease called gamma-secretase has spurred progress toward novel therapeutics. The finding that presenilin itself may be the long-sought gamma-secretase, coupled with the recent identification of beta-secretase, has provided discrete biochemical targets for drug screening and development. Alternate and novel strategies for inhibiting the early mechanism of the disease are also emerging. The progress reviewed here, coupled with better ability to diagnose the disease early, bode well for the successful development of therapeutic and preventative drugs for this major public health problem."}
{"_id":"1d0b61503222191fe85c7bd112f91036f6a5028e","title":"FA*IR: A Fair Top-k Ranking Algorithm","text":"In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n \u00bb k candidates, maximizing utility (i.e., select the \"best\" candidates) subject to group fairness criteria.\n Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-k should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above.\n An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list."}
{"_id":"540cc30354ed4fe646f262042de77fec4995493e","title":"Lower limb rehabilitation robot","text":"This paper describes a new prototype of lower limb rehabilitation robot (for short: LLRR), including its detailed structure, operative principles, manipulative manual and control mode which give considerate protection to patients. It implements the programmable process during the course of the limbs rehabilitation, furthermore, renders variable and predetermined step posture and force sensing. LLRR could assist patient in-- simulating normal people's footsteps, exercising leg muscles, gradually recovering the neural control toward walking function and finally walking in normal way utterly. Such robot is comprised with steps posture controlling system and weight alleviation controlling mechanism."}
{"_id":"ddf7c908fbb102fae3ea927d2dd96181ac4f1976","title":"The Air Traffic Flow Management Problem: An Integer Optimization Approach","text":"In this paper, we present a new Integer Program (IP) for the Air Traffic Flow Management (ATFM) problem. The model we propose provides a complete representation of all the phases of each flights, i.e., the phase of taking-off, of cruising and of landing; suggesting all the actions to be implemented to achieve the goal of safe, efficient, and expeditious aircraft movement. The distinctive feature of the model is that it allows rerouting decisions. These decisions are formulated by means of \u201clocal\u201d conditions, which allow us to represent such decisions in a very compact way by only introducing new constraints. Moreover, to strengthen the polyhedral structure of the underlying relaxation, we also present three classes of valid inequalities. We report short computational times (less than 15 minutes) on instances of the size of the US air traffic control system that make it realistic that our approach can be used as the main engine of managing air traffic in the US."}
{"_id":"2d248dc0c67ec82b70f8a59f1e24964916916a9e","title":"An Improved Neural Segmentation Method Based on U-NET","text":"\u6458\u8981:\u5c40\u90e8\u9ebb\u9189\u6280\u672f\u4f5c\u4e3a\u73b0\u4ee3\u793e\u4f1a\u6700\u4e3a\u5e38\u89c1\u7684\u9ebb\u9189\u6280 \u672f,\u5177\u6709\u5b89\u5168\u6027\u9ad8,\u526f\u4f5c\u7528\u5c0f\u7b49\u4f18\u52bf\u3002\u901a\u8fc7\u5206\u6790\u8d85\u58f0 \u56fe\u50cf,\u5206\u5272\u56fe\u50cf\u4e2d\u7684\u795e\u7ecf\u533a\u57df,\u6709\u52a9\u4e8e\u63d0\u5347\u5c40\u90e8\u9ebb\u9189 \u624b\u672f\u7684\u6210\u529f\u7387\u3002\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u76ee\u524d\u6700\u4e3a\u9ad8\u6548\u7684\u56fe \u50cf\u5904\u7406\u65b9\u6cd5\u4e4b\u4e00,\u5177\u6709\u51c6\u786e\u6027\u9ad8,\u9884\u5904\u7406\u5c11\u7b49\u4f18\u52bf\u3002 \u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u5bf9\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u795e\u7ecf\u533a\u57df\u8fdb\u884c\u5206 \u5272,\u901f\u5ea6\u66f4\u5feb,\u51c6\u786e\u6027\u66f4\u9ad8\u3002\u76ee\u524d\u5df2\u6709\u7684\u56fe\u50cf\u5206\u5272\u7f51 \u7edc\u7ed3\u6784\u4e3b\u8981\u6709U-NET[1],SegNet[2]\u3002U-NET\u7f51\u7edc\u8bad\u7ec3 \u65f6\u95f4\u77ed,\u8bad\u7ec3\u53c2\u6570\u8f83\u5c11,\u4f46\u6df1\u5ea6\u7565\u6709\u4e0d\u8db3\u3002SegNet \u7f51 \u7edc\u5c42\u6b21\u8f83\u6df1,\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f,\u4f46\u5bf9\u8bad\u7ec3\u6837\u672c\u9700\u6c42\u8f83\u591a \u7531\u4e8e\u533b\u5b66\u6837\u672c\u6570\u91cf\u6709\u9650,\u4f1a\u5bf9\u6a21\u578b\u8bad\u7ec3\u4ea7\u751f\u4e00\u5b9a\u5f71\u54cd\u3002 \u672c\u6587\u6211\u4eec\u5c06\u91c7\u7528\u4e00\u79cd\u6539\u8fdb\u540e\u7684 U-NET \u7f51\u7edc\u7ed3\u6784\u6765\u5bf9\u8d85 \u58f0\u56fe\u50cf\u4e2d\u7684\u795e\u7ecf\u533a\u57df\u8fdb\u884c\u5206\u5272,\u6539\u8fdb\u540e\u7684 U-NET \u7f51\u7edc \u7ed3\u6784\u52a0\u5165\u7684\u6b8b\u5dee\u7f51\u7edc(residual network)[3],\u5e76\u5bf9\u6bcf\u4e00\u5c42 \u7ed3\u679c\u8fdb\u884c\u89c4\u8303\u5316(batch normalization)\u5904\u7406[4]\u3002\u5b9e\u9a8c\u7ed3 \u679c\u8868\u660e,\u4e0e\u4f20\u7edf\u7684U-NET\u7f51\u7edc\u7ed3\u6784\u76f8\u6bd4,\u6539\u8fdb\u540e\u7684UNET \u7f51\u7edc\u5206\u5272\u6548\u679c\u5177\u6709\u663e\u8457\u63d0\u5347,\u8bad\u7ec3\u65f6\u95f4\u7565\u6709\u589e\u52a0\u3002 \u540c\u65f6,\u5c06\u6539\u8fdb\u540e\u7684 U-NET \u7f51\u7edc\u4e0e SegNet \u7f51\u7edc\u7ed3\u6784\u8fdb \u884c\u5bf9\u6bd4,\u53d1\u73b0\u6539\u8fdb\u540e\u7684 U-net \u65e0\u8bba\u4ece\u8bad\u7ec3\u901f\u5ea6\u8fd8\u662f\u4ece \u5206\u5272\u6548\u679c\u5747\u9ad8\u4e8e SegNet \u7f51\u7edc\u7ed3\u6784\u3002\u6539\u8fdb\u540e\u7684 U-net \u7f51 \u7edc\u7ed3\u6784\u5728\u795e\u7ecf\u8bc6\u522b\u65b9\u9762\u5177\u6709\u5f88\u597d\u7684\u5e94\u7528\u573a\u666f\u3002"}
{"_id":"3311d4f6f90c564bb30daa8ff159bb35649aab46","title":"Covariate Shift in Hilbert Space: A Solution via Sorrogate Kernels","text":"Covariate shift is an unconventional learning scenario in which training and testing data have different distributions. A general principle to solve the problem is to make the training data distribution similar to that of the test domain, such that classifiers computed on the former generalize well to the latter. Current approaches typically target on sample distributions in the input space, however, for kernel-based learning methods, the algorithm performance depends directly on the geometry of the kernel-induced feature space. Motivated by this, we propose to match data distributions in the Hilbert space, which, given a pre-defined empirical kernel map, can be formulated as aligning kernel matrices across domains. In particular, to evaluate similarity of kernel matrices defined on arbitrarily different samples, the novel concept of surrogate kernel is introduced based on the Mercer\u2019s theorem. Our approach caters the model adaptation specifically to kernel-based learning mechanism, and demonstrates promising results on several real-world applications. Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s)."}
{"_id":"4c99b87df6385bd945a00633f829e4a9ec5ce314","title":"Massive Social Network Analysis: Mining Twitter for Social Good","text":"Social networks produce an enormous quantity of data. Facebook consists of over 400 million active users sharing over 5 billion pieces of information each month. Analyzing this vast quantity of unstructured data presents challenges for software and hardware. We present GraphCT, a Graph Characterization Toolkit for massive graphs representing social network data. On a 128-processor Cray XMT, GraphCT estimates the betweenness centrality of an artificially generated (R-MAT) 537 million vertex, 8.6 billion edge graph in 55 minutes and a real-world graph (Kwak, et al.) with 61.6 million vertices and 1.47 billion edges in 105 minutes. We use GraphCT to analyze public data from Twitter, a microblogging network. Twitter's message connections appear primarily tree-structured as a news dissemination system. Within the public data, however, are clusters of conversations. Using GraphCT, we can rank actors within these conversations and help analysts focus attention on a much smaller data subset."}
{"_id":"daf6ddd50515d5408cb9f6183c5d308adee7d521","title":"Community detection with partially observable links and node attributes","text":"Community detection has been an important task for social and information networks. Existing approaches usually assume the completeness of linkage and content information. However, the links and node attributes can usually be partially observable in many real-world networks. For example, users can specify their privacy settings to prevent non-friends from viewing their posts or connections. Such incompleteness poses additional challenges to community detection algorithms. In this paper, we aim to detect communities with partially observable link structure and node attributes. To fuse such incomplete information, we learn link-based and attribute-based representations via kernel alignment and a co-regularization approach is proposed to combine the information from both sources (i.e., links and attributes). The link-based and attribute-based representations can lend strength to each other via the partial consensus learning. We present two instantiations of this framework by enforcing hard and soft consensus constraint respectively. Experimental results on real-world datasets show the superiority of the proposed approaches over the baseline methods and its robustness under different observable levels."}
{"_id":"0bb4401b9a1b064c513bda3001f43f8f2f3e28de","title":"Learning with Noisy Labels","text":"In this paper, we theoretically study the problem of binary classification in the presence of random classification noise \u2014 the learner, instead of seeing the true labels, sees labels that have independently been flipped with some small probability. Moreover, random label noise is class-conditional \u2014 the flip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence \u2014 methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88% accuracy even when 40% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets."}
{"_id":"165ef2b5f86b9b2c68b652391db5ece8c5a0bc7e","title":"Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation","text":"Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information, specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78:0 on the challenging PASCAL VOC 2012 dataset."}
{"_id":"32cde90437ab5a70cf003ea36f66f2de0e24b3ab","title":"The Cityscapes Dataset for Semantic Urban Scene Understanding","text":"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."}
{"_id":"3cdb1364c3e66443e1c2182474d44b2fb01cd584","title":"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation","text":"We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http:\/\/mi.eng.cam.ac.uk\/projects\/segnet\/."}
{"_id":"4fc2bbaa1f0502b5412e3d56acce2c9aa08bb586","title":"Long-Time Exposure to Violent Video Games Does Not Show Desensitization on Empathy for Pain: An fMRI Study","text":"As a typical form of empathy, empathy for pain refers to the perception and appraisal of others' pain, as well as the corresponding affective responses. Numerous studies investigated the factors affecting the empathy for pain, in which the exposure to violent video games (VVGs) could change players' empathic responses to painful situations. However, it remains unclear whether exposure to VVG influences the empathy for pain. In the present study, in terms of the exposure experience to VVG, two groups of participants (18 in VVG group, VG; 17 in non-VVG group, NG) were screened from nearly 200 video game experience questionnaires. And then, the functional magnetic resonance imaging data were recorded when they were viewing painful and non-painful stimuli. The results showed that the perception of others' pain were not significantly different in brain regions between groups, from which we could infer that the desensitization effect of VVGs was overrated."}
{"_id":"5711a5dad3fd93b14593ac46c5df0267e8991637","title":"An Augmented Reality System for Astronomical Observations","text":"Anyone who has gazed through the eyepiece of an astronomical telescope knows that, with the exception of the Moon and the planets, extra-solar astronomical objects are disappointing to observe visually. This is mainly due to their low surface brightness, but also depends on the visibility, sky brightness and telescope aperture. We propose a system which projects images of astronomical objects (with focus on nebulae and galaxies), animations and additional information directly into the eyepiece view of an astronomical telescope. As the telescope orientation is queried continuously, the projected image is adapted in real-time to the currently visible field of view. For projection, a custom-built video projection module with high contrast and low maximum luminance value was developed. With this technology visitors to public observatories have the option to experience the richness of faint astronomical objects while directly looking at them through a telescope."}
{"_id":"dbfd731c183f13d0d5c686c681d70b19e12377c5","title":"A gateway system for an automotive system: LIN, CAN, and FlexRay","text":"In the automotive industry, the usage of microcontrollers has been increasing rapidly. Many mechanical parts of automobiles are being replaced with microcontrollers, and these microcontrollers are controlling various parts of the automobile, so communications between microcontrollers must be reliable. Until now, several protocols for automotive communications have been introduced, and LIN, CAN, and FlexRay are the most widely known protocols. Different vendors of automobiles use different protocols and each protocol possess different features. This paper presents a network gateway system between LIN, low-speed CAN, high-speed CAN, and FlexRay."}
{"_id":"4164cfef02ee5e783983296e7d9914063208a203","title":"ECG-Cryptography and Authentication in Body Area Networks","text":"Wireless body area networks (BANs) have drawn much attention from research community and industry in recent years. Multimedia healthcare services provided by BANs can be available to anyone, anywhere, and anytime seamlessly. A critical issue in BANs is how to preserve the integrity and privacy of a person's medical data over wireless environments in a resource efficient manner. This paper presents a novel key agreement scheme that allows neighboring nodes in BANs to share a common key generated by electrocardiogram (ECG) signals. The improved Jules Sudan (IJS) algorithm is proposed to set up the key agreement for the message authentication. The proposed ECG-IJS key agreement can secure data commnications over BANs in a plug-n-play manner without any key distribution overheads. Both the simulation and experimental results are presented, which demonstrate that the proposed ECG-IJS scheme can achieve better security performance in terms of serval performance metrics such as false acceptance rate (FAR) and false rejection rate (FRR) than other existing approaches. In addition, the power consumption analysis also shows that the proposed ECG-IJS scheme can achieve energy efficiency for BANs."}
{"_id":"f16b23e8e0788e3298e533e71bafef7135300a5e","title":"An anomaly-based network intrusion detection system using Deep learning","text":"Recently, anomaly-based intrusion detection techniques are valuable methodology to detect both known as well as unknown\/new attacks, so they can cope with the diversity of the attacks and the constantly changing nature of network attacks. There are many problems need to be considered in anomaly-based network intrusion detection system (NIDS), such as ability to adapt to dynamic network environments, unavailability of labeled data, false positive rate. This paper, we use Deep learning techniques to implement an anomaly-based NIDS. These techniques show the sensitive power of generative models with good classification, capabilities to deduce part of its knowledge from incomplete data and the adaptability. Our experiments with KDDCup99 network traffic connections show that our work is effective to exact detect in anomaly-based NIDS and classify intrusions into five groups with the accuracy based on network data sources."}
{"_id":"987dbeff39ef3efce631e04211a748360b5d818d","title":"Smarter City : Smart Energy Grid based on Blockchain Technology","text":"The improvement of the Quality of Life (QoL) and the enhancement of the Quality of Services (QoS) represent the main goal of every city evolutionary process. It is possible making cities smarter promoting innovative solutions by use of Information and Communication Technology (ICT) for collecting and analysing large amounts of data generated by several sources, such as sensor networks, wearable devices, and IoT devices spread among the city. The integration of different technologies and different IT systems, needed to build smart city applications and services, remains the most challenge to overcome. In the Smart City context, this paper intends to investigate the Smart Environment pillar, and in particular the aspect related to the implementation of Smart Energy Grid for citizens in the urban context. The innovative characteristic of the proposed solution consists of using the Blockchain technology to join the Grid, exchanging information, and buy\/sell energy between the involved nodes (energy providers and private citizens), using the Blockchain granting ledger. Keywords\u2014 Information Technology, Smart City, Digital Revolution, Digital Innovation, Blockchain, Smart Energy Grid, Machine Learning."}
{"_id":"8ac97ef1255dffa3be216a7f6155946f7ec57c22","title":"Social interaction based video recommendation: Recommending YouTube videos to facebook users","text":"Online videos, e.g., YouTube videos, are important topics for social interactions among users of online social networking sites (OSN), e.g., Facebook. This opens up the possibility of exploiting video-related user social interaction information for better video recommendation. Towards this goal, we conduct a case study of recommending YouTube videos to Facebook users based on their social interactions. We first measure social interactions related to YouTube videos among Facebook users. We observe that the attention a video attracts on Facebook is not always well-aligned with its popularity on YouTube. Unpopular videos on YouTube can become popular on Facebook, while popular videos on YouTube often do not attract proportionally high attentions on Facebook. This finding motivates us to develop a simple top-k video recommendation algorithm that exploits user social interaction information to improve the recommendation accuracy for niche videos, that are globally unpopular, but highly relevant to a specific user or user group. Through experiments on the collected Facebook traces, we demonstrate that our recommendation algorithm significantly outperforms the YouTube-popularity based video recommendation algorithm as well as a collaborative filtering algorithm based on user similarities."}
{"_id":"7c24236fad762e5e34264c1b123e67ac04307f76","title":"TextLuas: Tracking and Visualizing Document and Term Clusters in Dynamic Text Data","text":"For large volumes of text data collected over time, a key knowledge discovery task is identifying and tracking clusters. These clusters may correspond to emerging themes, popular topics, or breaking news stories in a corpus. Therefore, recently there has been increased interest in the problem of clustering dynamic data. However, there exists little support for the interactive exploration of the output of these analysis techniques, particularly in cases where researchers wish to simultaneously explore both the change in cluster structure over time and the change in the textual content associated with clusters. In this paper, we propose a model for tracking dynamic clusters characterized by the evolutionary events of each cluster. Motivated by this model, the TextLuas system provides an implementation for tracking these dynamic clusters and visualizing their evolution using a metro map metaphor. To provide overviews of cluster content, we adapt the tag cloud representation to the dynamic clustering scenario. We demonstrate the TextLuas system on two different text corpora, where they are shown to elucidate the evolution of key themes. We also describe how TextLuas was applied to a problem in bibliographic network research."}
{"_id":"a55aeb5339a5b9e61c0fea8d836c62a7c1d19f8d","title":"Signal processing of range detection for SFCW radars using Matlab and GNU radio","text":"Development of radar technology is now rapidly. One of them is Step Frequency Continuous Wave Radar (SFCW Radar). SFCW radar can be used for various purposes. SFCW radar consists of antenna, control unit, signal processing unit. The radar advantages compared to the other radar are this radar is easier to implement and more widely within the range of radar. In this research that will be done is design SFCW radar. There are several stages to be performed in this research. At first, the simulation will be done using Matlab\u00ae. This simulation will determine the parameters needed to obtain an appropriate resolution. The second, simulations is performed by using GNU radio. This simulation will be adjusted to the parameters that have been previously designed by Matlab\u00ae. Both of the results will be analyzed. Input signal is complex. Simulation results for 5kHz-1.285MHz will display in graph. Graph will show location of the object in 5 m, 1km, and 1.1km depend on initial design."}
{"_id":"537f17624e4f8513c14f850c0e9c012c93c132b6","title":"Weather classification with deep convolutional neural networks","text":"In this paper, we study weather classification from images using Convolutional Neural Networks (CNNs). Our approach outperforms the state of the art by a huge margin in the weather classification task. Our approach achieves 82.2% normalized classification accuracy instead of 53.1% for the state of the art (i.e., 54.8% relative improvement). We also studied the behavior of all the layers of the Convolutional Neural Networks, we adopted, and interesting findings are discussed."}
{"_id":"990c5f2aefab9df89c40025d85013fe28f0a5810","title":"Image Deblurring via Enhanced Low-Rank Prior","text":"Low-rank matrix approximation has been successfully applied to numerous vision problems in recent years. In this paper, we propose a novel low-rank prior for blind image deblurring. Our key observation is that directly applying a simple low-rank model to a blurry input image significantly reduces the blur even without using any kernel information, while preserving important edge information. The same model can be used to reduce blur in the gradient map of a blurry input. Based on these properties, we introduce an enhanced prior for image deblurring by combining the low rank prior of similar patches from both the blurry image and its gradient map. We employ a weighted nuclear norm minimization method to further enhance the effectiveness of low-rank prior for image deblurring, by retaining the dominant edges and eliminating fine texture and slight edges in intermediate images, allowing for better kernel estimation. In addition, we evaluate the proposed enhanced low-rank prior for both the uniform and the non-uniform deblurring. Quantitative and qualitative experimental evaluations demonstrate that the proposed algorithm performs favorably against the state-of-the-art deblurring methods."}
{"_id":"7c5f143adf1bf182bf506bd31f9ddb0f302f3ce9","title":"10 Internet Addiction and Its Cognitive Behavioral Therapy","text":""}
{"_id":"81a81ff448b0911cbb63abecd9c54949ce8d50fd","title":"Functional connectivity dynamically evolves on multiple time-scales over a static structural connectome: Models and mechanisms","text":"Over the last decade, we have observed a revolution in brain structural and functional Connectomics. On one hand, we have an ever-more detailed characterization of the brain's white matter structural connectome. On the other, we have a repertoire of consistent functional networks that form and dissipate over time during rest. Despite the evident spatial similarities between structural and functional connectivity, understanding how different time-evolving functional networks spontaneously emerge from a single structural network requires analyzing the problem from the perspective of complex network dynamics and dynamical system's theory. In that direction, bottom-up computational models are useful tools to test theoretical scenarios and depict the mechanisms at the genesis of resting-state activity. Here, we provide an overview of the different mechanistic scenarios proposed over the last decade via computational models. Importantly, we highlight the need of incorporating additional model constraints considering the properties observed at finer temporal scales with MEG and the dynamical properties of FC in order to refresh the list of candidate scenarios."}
{"_id":"e71423028d7204d2a8f5fa1b3671f9409192b943","title":"Spider Monkey Optimization algorithm for numerical optimization","text":"Swarm intelligence is one of the most promising area for the researchers in the field of numerical optimization. Researchers have developed many algorithms by simulating the swarming behavior of various creatures like ants, honey bees, fish, birds and the findings are very motivating. In this paper, a new approach for numerical optimization is proposed by modeling the foraging behavior of spider monkeys. Spider monkeys have been categorized as fission\u2013fusion social structure based animals. The animals which follow fission\u2013 fusion social systems, split themselves from large to smaller groups and vice-versa based on the scarcity or availability of food. The proposed swarm intelligence approach is named as Spider Monkey Optimization (SMO) algorithm and can broadly be classified as an algorithm inspired by intelligent foraging behavior of fission\u2013fusion social structure based animals."}
{"_id":"3fa6ccf2e0cf89fe758b9d634030102f9c3f928a","title":"Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce","text":"Nowadays, it is a heated topic for many industries to build automatic question-answering (QA) systems. A key solution to these QA systems is to retrieve from a QA knowledge base the most similar question of a given question, which can be reformulated as a paraphrase identification (PI) or a natural language inference (NLI) problem. However, most existing models for PI and NLI have at least two problems: They rely on a large amount of labeled data, which is not always available in real scenarios, and they may not be efficient for industrial applications. In this paper, we study transfer learning for the PI and NLI problems, aiming to propose a general framework, which can effectively and efficiently adapt the shared knowledge learned from a resource-rich source domain to a resource-poor target domain. Specifically, since most existing transfer learning methods only focus on learning a shared feature space across domains while ignoring the relationship between the source and target domains, we propose to simultaneously learn shared representations and domain relationships in a unified framework. Furthermore, we propose an efficient and effective hybrid model by combining a sentence encoding-based method and a sentence interaction-based method as our base model. Extensive experiments on both paraphrase identification and natural language inference demonstrate that our base model is efficient and has promising performance compared to the competing models, and our transfer learning method can help to significantly boost the performance. Further analysis shows that the inter-domain and intra-domain relationship captured by our model are insightful. Last but not least, we deploy our transfer learning model for PI into our online chatbot system, which can bring in significant improvements over our existing system. Finally, we launch our new system on the chatbot platform Eva in our E-commerce site AliExpress."}
{"_id":"f97efbc01bea86303fcecb0719b8a76394becde2","title":"Generation of circularly polarized conical beam pattern using toroidal helical knot antenna","text":"A novel circularly polarized antenna with a conical beam radiation pattern is presented. It consists of a feeding probe and a (1, 6) helical torus knot as a polarizer. Measured results shows that, the antenna has an impedance bandwidth of 19.67% and axial ratio bandwidth of 15.7%. With the RHCP gain of about 6 dBic at 3.5 GHz. Toroidal helical knot antenna is mechanically simple to fabricate using additive manufacturing technology. The proposed prototype is suitable for mounting on vehicles to facilitate communication with geostationary satellites."}
{"_id":"330c147e02ff73f4f46aac15a2fafc6dda235b1a","title":"Towards deep learning with segregated dendrites","text":"Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations-the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons."}
{"_id":"173c819ce3fccafdc4f64af71fd6868e815580ad","title":"Introduction to E-commerce Learning Objectives","text":"\u25cf To understand the complexity of e-commerce and its many facets. \u25cf To explore how e-business and e-commerce fit together. \u25cf To identify the impact of e-commerce. \u25cf To recognise the benefits and limitations of e-commerce. \u25cf To use classification frameworks for analysing e-commerce. \u25cf To identify the main barriers to the growth and development of e-commerce in organisations. \uf8f5 Even today, some considerable time after the so called 'dot com\/Internet revolution', electronic commerce (e-commerce) remains a relatively new, emerging and constantly changing area of business management and information technology. There has been and continues to be much publicity and discussion about e-commerce. Library catalogues and shelves are filled with books and articles on the subject. However, there remains a sense of confusion, suspicion and misunderstanding surrounding the area, which has been exacerbated by the different contexts in which electronic commerce is used, coupled with the myriad related buzzwords and acronyms. This book aims to consolidate the major themes that have arisen from the new area of electronic commerce and to provide an understanding of its application and importance to management. In order to understand electronic commerce it is important to identify the different terms that are used, and to assess their origin and usage. According to the editor-in-chief of International Journal of Electronic Commerce , Vladimir Zwass, 'Electronic commerce is sharing business information , maintaining business relationships and conducting business transactions by means of telecommunications networks'. 1 He maintains that in its purest form, electronic commerce has existed for over 40 years, originating from the electronic transmission of messages during the Berlin airlift in 1948. 2 From this, electronic data interchange (EDI) was the next stage of e-commerce development. In the 1960s a cooperative effort between industry groups produced a first attempt at common electronic data formats. The formats, however, were only for purchasing, transportation and finance data, and were used primarily for intra-industry transactions. It was not until the late 1970s that work began for national Electronic Data Interchange (EDI) standards, which developed well into the early 1990s. EDI is the electronic transfer of a standardised business transaction between a sender and receiver computer, over some kind of private network or value added network (VAN). Both sides would have to have the same application software and the data would be exchanged in an extremely rigorous format. In sectors such as retail, automotive, defence and heavy manufacturing, EDI was \u2026"}
{"_id":"568a0de980b9773fe96abf3c75ac0891d1df9c2b","title":"Is e-Learning the Solution for Individual Learning?.","text":"Despite the fact that e-Learning exists for a relatively long time, it is still in its infancy. Current eLearning systems on the market are limited to technical gadgets and organizational aspects of teaching, instead of supporting the learning. As a result the learner has become deindividualized and demoted to a noncritical homogenous user. One way out of this drawback is the creation of individual e-Learning materials. For this purpose a flexible multidimensional data model and the generation of individual content are the solution. It is necessary to enable the interaction between the learners and the content in e-Learning systems in the same manner."}
{"_id":"56c16d9e2a5270ba6b1d83271e2c10916591968d","title":"Human memory ; A proposed system and its control processes","text":""}
{"_id":"47ed703b8b6a501a5eb0e07ba8bc8d27be911ce5","title":"Answering Location-Aware Graph Reachability Queries on GeoSocial Data","text":"Thanks to the wide spread use of mobile and wearable devices, popular social networks, e.g., Facebook, prompts users to add spatial attributes to social entities, e.g., check-ins, traveling posts, and geotagged photos, leading to what is known as, The GeoSocial Graph. In such graph, usersmay issue a Reachability Query with Spatial Range Predicate (abbr. RangeReach). RangeReach finds whether an input vertex can reach any spatial vertex that lies within an input spatial range. The paper proposes GEOREACH, an approach that adds spatial data awareness to a graph database management system. GEOREACH allows efficient execution of RangeReach queries, yet without compromising a lot on the overall system scalability. Experiments based on system implementation inside Neo4j prove that GEOREACH exhibits up to two orders of magnitude better query performance and up to four times less storage than the state-of-the-art spatial and reachability indexing approaches."}
{"_id":"4c6dc7218f6ba4059be6b7e77bf09492bcbae9bc","title":"Distributed Representation of Subgraphs","text":"Network embeddings have become very popular in learning effective feature representations of networks. Motivated by the recent successes of embeddings in natural language processing, researchers have tried to \u0080nd network embeddings in order to exploit machine learning algorithms for mining tasks like node classi\u0080cation and edge prediction. However, most of the work focuses on \u0080nding distributed representations of nodes, which are inherently ill-suited to tasks such as community detection which are intuitively dependent on subgraphs. Here, we propose Sub2Vec, an unsupervised scalable algorithm to learn feature representations of arbitrary subgraphs. We provide means to characterize similarties between subgraphs and provide theoretical analysis of Sub2Vec and demonstrate that it preserves the so-called local proximity. We also highlight the usability of Sub2Vec by leveraging it for network mining tasks, like community detection. We show that Sub2Vec gets signi\u0080cant gains over stateof-the-art methods and node-embedding methods. In particular, Sub2Vec o\u0082ers an approach to generate a richer vocabulary of features of subgraphs to support representation and reasoning. ACM Reference format: Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan and B. Aditya Prakash. 2016. Distributed Representation of Subgraphs. In Proceedings of ACM Conference, Washington, DC, USA, July 2017 (Conference\u201917), 9 pages. DOI: 10.1145\/nnnnnnn.nnnnnnn"}
{"_id":"4294fb388e45f3e969cb8615d8001f70eb47206e","title":"Employee job attitudes and organizational characteristics as predictors of cyberloafing","text":"Cyberloafing is the personal use of the Internet by employees while at work. The purpose of this study is to examine whether employee job attitudes, organizational characteristics, attitudes towards cyberloafing, and other non-Internet loafing behaviors serve as antecedents to cyberloafing behaviors. We hypothesize that the employee job attitudes of job involvement and intrinsic involvement are related to cyberloafing. In addition, we hypothesize that organizational characteristics including the perceived cyberloafing of one\u2019s coworkers and managerial support for internet usage are related to cyberloafing. We also hypothesize that attitudes towards cyberloafing and the extent to which employees participate in non-Internet loafing behaviors (e.g., talking with coworkers, running personal errands) will both be related to cyberloafing. One hundred and forty-three working professional from a variety of industries were surveyed regarding their Internet usage at work. As hypothesized, the employee job attitudes of job involvement and intrinsic involvement were negatively related to cyberloafing. Also as predicted, the organizational characteristics of the perceived cyberloafing of one\u2019s coworkers and managerial support for internet usage were positively related to cyberloafing. Finally, results showed that attitudes towards cyberloafing and participation in non-Internet loafing behaviors were positively related to cyberloafing. Implications for both organizations and employees are discussed. 2011 Elsevier Ltd. All rights reserved."}
{"_id":"37735f5760f8ef487791cd67a7e8fc90f79c51c9","title":"Hand Rehabilitation Learning System With an Exoskeleton Robotic Glove","text":"This paper presents a hand rehabilitation learning system, the SAFE Glove, a device that can be utilized to enhance the rehabilitation of subjects with disabilities. This system is able to learn fingertip motion and force for grasping different objects and then record and analyze the common movements of hand function including grip and release patterns. The glove is then able to reproduce these movement patterns in playback fashion to assist a weakened hand to accomplish these movements, or to modulate the assistive level based on the user's or therapist's intent for the purpose of hand rehabilitation therapy. Preliminary data have been collected from healthy hands. To demonstrate the glove's ability to manipulate the hand, the glove has been fitted on a wooden hand and the grasping of various objects was performed. To further prove that hands can be safely driven by this haptic mechanism, force sensor readings placed between each finger and the mechanism are plotted. These experimental results demonstrate the potential of the proposed system in rehabilitation therapy."}
{"_id":"0647c9d56cf11215894d57d677997826b22f6a13","title":"Transgender face recognition with off-the-shelf pre-trained CNNs: A comprehensive study","text":"Face recognition has become a ubiquitous way of establishing identity in many applications. Gender transformation therapy induces changes to face on both for structural and textural features. A challenge for face recognition system is, therefore, to reliably identify the subjects after they undergo gender change while the enrolment images correspond to pre-change. In this work, we propose a new framework based on augmenting and fine-tuning deep Residual Network-50 (ResNet-50). We employ YouTube database with 37 subjects whose images are self-captured to evaluate the performance of state-of-the-schemes. Obtained results demonstrate the superiority of the proposed scheme over twelve different state-of-the-art schemes with an improved Rank \u2014 1 recognition rate."}
{"_id":"c874f54082b6c2545babd7f5d1f447416ebba667","title":"Multi-Strategy Sentiment Analysis of Consumer Reviews Based on Semantic Fuzziness","text":"Since Internet has become an excellent source of consumer reviews, the area of sentiment analysis (also called sentiment extraction, opinion mining, opinion extraction, and sentiment mining) has seen a large increase in academic interest over the last few years. Sentiment analysis mines opinions at word, sentence, and document levels, and gives sentiment polarities and strengths of articles. As known, the opinions of consumers are expressed in sentiment Chinese phrases. But due to the fuzziness of Chinese characters, traditional machine learning techniques can not represent the opinion of articles very well. In this paper, we propose a multi-strategy sentiment analysis method with semantic fuzziness to solve the problem. The results show that this hybrid sentiment analysis method can achieve a good level of effectiveness."}
{"_id":"c86daf4fcb53be986d3b2cb1524c45b3772ac802","title":"Online shopping behavior model: A literature review and proposed model","text":"In this study, we conducted extensive reviews of online shopping literatures and proposed a hierarchy model of online shopping behavior. We collected 47 studies and classified them by variables used. Some critical points were found that research framework, methodology, and lack of cross-cultural comparison, etc So we developed a cross-cultural model of online shopping including shopping value, attitudes to online retailer's attributes and online purchasing based on the integrated V-A-B model."}
{"_id":"5ad9f87ef6f41eb91bc479cf61c850bdde5a71ef","title":"Spinal curvature determination from scoliosis X-Ray image using sum of squared difference template matching","text":"Scoliosis is a disorder in which there is a sideways curve of the spine. Curve are often S-shaped or C-shaped. One of the methods to ascertain a patient with scoliosis is through using Cobb angle measurement. The importance of the automatic spinal curve detection system is to detect any spinal disorder quicker and faster. This study is intended as a first step based on computer-aided diagnosis. The spinal detection method that we propose is using template matching based on Sum of Squared Difference (SSD). This method is used to estimate the location of the vertebra. By using polynomial curve fitting, a spinal curvature estimation can be done. This paper discusses the performance of SSD method used to detect a variety of data sources of X-Ray from numerous patients. The results from the implementation indicate that the proposed algorithm can be used to detect all the X-ray images. The best result in this experiment has 96.30% accuracy using 9-subdivisions poly 5 algorithm, and the average accuracy is 86.01%."}
{"_id":"78e5e97a082263f653ac07fbfbfc32c317ddb881","title":"Multitask Compressive Sensing","text":"Compressive sensing (CS) is a framework whereby one performs N nonadaptive measurements to constitute a vector v isin RN used to recover an approximation u isin RM desired signal u isin RM with N << M this is performed under the assumption that u is sparse in the basis represented by the matrix Psi RMtimesM. It has been demonstrated that with appropriate design of the compressive measurements used to define v, the decompressive mapping vrarru may be performed with error ||u-u||2 2 having asymptotic properties analogous to those of the best adaptive transform-coding algorithm applied in the basis Psi. The mapping vrarru constitutes an inverse problem, often solved using l1 regularization or related techniques. In most previous research, if L > 1 sets of compressive measurements {vi}i=1,L are performed, each of the associated {ui}i=1,Lare recovered one at a time, independently. In many applications the L ldquotasksrdquo defined by the mappings virarrui are not statistically independent, and it may be possible to improve the performance of the inversion if statistical interrelationships are exploited. In this paper, we address this problem within a multitask learning setting, wherein the mapping vrarru for each task corresponds to inferring the parameters (here, wavelet coefficients) associated with the desired signal vi, and a shared prior is placed across all of the L tasks. Under this hierarchical Bayesian modeling, data from all L tasks contribute toward inferring a posterior on the hyperparameters, and once the shared prior is thereby inferred, the data from each of the L individual tasks is then employed to estimate the task-dependent wavelet coefficients. An empirical Bayesian procedure for the estimation of hyperparameters is considered; two fast inference algorithms extending the relevance vector machine (RVM) are developed. Example results on several data sets demonstrate the effectiveness and robustness of the proposed algorithms."}
{"_id":"6237f7264a5c278c717d1bc625e93d0506c843cf","title":"Two cortical systems for memory-guided behaviour","text":"Although the perirhinal cortex (PRC), parahippocampal cortex (PHC) and retrosplenial cortex (RSC) have an essential role in memory, the precise functions of these areas are poorly understood. Here, we review the anatomical and functional characteristics of these areas based on studies in humans, monkeys and rats. Our Review suggests that the PRC and PHC\u2013RSC are core components of two separate large-scale cortical networks that are dissociable by neuroanatomy, susceptibility to disease and function. These networks not only support different types of memory but also appear to support different aspects of cognition."}
{"_id":"81dd4a314f8340d5e9bfd897897e8147faf8c63e","title":"Sexual arousal patterns of bisexual men.","text":"There has long been controversy about whether bisexual men are substantially sexually aroused by both sexes. We investigated genital and self-reported sexual arousal to male and female sexual stimuli in 30 heterosexual, 33 bisexual, and 38 homosexual men. In general, bisexual men did not have strong genital arousal to both male and female sexual stimuli. Rather, most bisexual men appeared homosexual with respect to genital arousal, although some appeared heterosexual. In contrast, their subjective sexual arousal did conform to a bisexual pattern. Male bisexuality appears primarily to represent a style of interpreting or reporting sexual arousal rather than a distinct pattern of genital sexual arousal."}
{"_id":"b5b0255f03233a04d5381600f79b4f0530b26467","title":"Towards a Distributed Large-Scale Dynamic Graph Data Store","text":"In many graph applications, the structure of the graph changes dynamically over time and may require real time analysis. However, constructing a large graph is expensive, and most studies for large graphs have not focused on a dynamic graph data structure, but rather a static one. To address this issue, we propose DegAwareRHH, a high performance dynamic graph data store designed for scaling out to store large, scale-free graphs by leveraging compact hash tables with high data locality. We extend DegAwareRHH for multiple processes and distributed memory, and perform dynamic graph construction on large scale-free graphs using emerging 'Big Data HPC' systems such as the Catalyst cluster at LLNL. We demonstrate that DegAwareRHH processes a request stream 206.5x faster than a state-of-the-art shared-memory dynamic graph processing framework, when both implementations use 24 threads\/processes to construct a graph with 1 billion edge insertion requests and 54 million edge deletion requests. DegAwareRHH also achieves a processing rate of over 2 billion edge insertion requests per second using 128 compute nodes to construct a large-scale web graph, containing 128 billion edges, the largest open-source real graph dataset to our knowledge."}
{"_id":"4e5326b0c248246b88f786907edb4e295eae9928","title":"Automatic assessment of macular edema from color retinal images","text":"Diabetic macular edema (DME) is an advanced symptom of diabetic retinopathy and can lead to irreversible vision loss. In this paper, a two-stage methodology for the detection and classification of DME severity from color fundus images is proposed. DME detection is carried out via a supervised learning approach using the normal fundus images. A feature extraction technique is introduced to capture the global characteristics of the fundus images and discriminate the normal from DME images. Disease severity is assessed using a rotational asymmetry metric by examining the symmetry of macular region. The performance of the proposed methodology and features are evaluated against several publicly available datasets. The detection performance has a sensitivity of 100% with specificity between 74% and 90%. Cases needing immediate referral are detected with a sensitivity of 100% and specificity of 97%. The severity classification accuracy is 81% for the moderate case and 100% for severe cases. These results establish the effectiveness of the proposed solution."}
{"_id":"e879de0d1be295288646a8308a0a01416f31db20","title":"Shoulder impingement revisited: evolution of diagnostic understanding in orthopedic surgery and physical therapy","text":"\u201cImpingement syndrome\u201d is a common diagnostic label for patients presenting with shoulder pain. Historically, it was believed to be due to compression of the rotator cuff tendons beneath the acromion. It has become evident that \u201cimpingement syndrome\u201d is not likely an isolated condition that can be easily diagnosed with clinical tests or most successfully treated surgically. Rather, it is likely a complex of conditions involving a combination of intrinsic and extrinsic factors. A mechanical impingement phenomenon as an etiologic mechanism of rotator cuff disease may be distinct from the broad diagnostic label of \u201cimpingement syndrome\u201d. Acknowledging the concepts of mechanical impingement and movement-related impairments may better suit the diagnostic and interventional continuum as they support the existence of potentially modifiable impairments within the conservative treatment paradigm. Therefore, it is advocated that the clinical diagnosis of \u201cimpingement syndrome\u201d be eliminated as it is no more informative than the diagnosis of \u201canterior shoulder pain\u201d. While both terms are ambiguous, the latter is less likely to presume an anatomical tissue pathology that may be difficult to isolate either with a clinical examination or with diagnostic imaging and may prevent potentially inappropriate surgical interventions. We further recommend investigation of mechanical impingement and movement patterns as potential mechanisms for the development of shoulder pain, but clearly distinguished from a clinical diagnostic label of \u201cimpingement syndrome\u201d. For shoulder researchers, we recommend investigations of homogenous patient groups with accurately defined specific pathologies, or with subgrouping or classification based on specific movement deviations. Diagnostic labels based on the movement system may allow more effective subgrouping of patients to guide treatment strategies."}
{"_id":"56c2fb2438f32529aec604e6fc3b06a595ddbfcc","title":"Comparison of Recent Machine Learning Techniques for Gender Recognition from Facial Images","text":"Recently, several machine learning methods for gender classification from frontal facial images have been proposed. Their variety suggests that there is not a unique or generic solution to this problem. In addition to the diversity of methods, there is also a diversity of benchmarks used to assess them. This gave us the motivation for our work: to select and compare in a concise but reliable way the main state-of-the-art methods used in automatic gender recognition. As expected, there is no overall winner. The winner, based on the accuracy of the classification, depends on the type of benchmarks used."}
{"_id":"1ba84863e2685c45c5c41953444d9383dc7aa13b","title":"Efficient Support Vector Classifiers for Named Entity Recognition","text":"NamedEntity (NE) recognitionis a task in which proper nouns and numerical information are extractedfrom documentsandareclassifiedinto categoriessuchas person,organization,and date. It is a key technologyof InformationExtractionand Open-DomainQuestionAnswering.First,weshow thatanNE recognizerbasedonSupportVectorMachines(SVMs)givesbetterscoresthanconventional systems.However, off-the-shelfSVM classifiersare too inefficient for this task.Therefore,we presenta methodthat makes the systemsubstantiallyfaster . This approachcan also be applied to other similar taskssuchaschunkingandpart-of-speechtagging. We alsopresentanSVM-basedfeatureselection methodandanefficient trainingmethod."}
{"_id":"ed7fc2b80328cea51bb11b59b15db5f6e26140fd","title":"The Bad Boys of Cyberspace: Deviant Behavior in a Multimedia Chat Community","text":"A wide variety of deviant behavior may arise as the population of an online multimedia community increases. That behavior can span the range from simple mischievous antics to more serious expressions of psychopathology, including depression, sociopathy, narcissism, dissociation, and borderline dynamics. In some cases the deviant behavior may be a process of pathological acting out\u2014in others, a healthy attempt to work through. Several factors must be taken into consideration when explaining online deviance, such as social\/cultural issues, the technical infrastructure of the environment, transference reactions, and the effects of the ambiguous, anonymous, and fantasy-driven atmosphere of cyberspace life. In what we may consider an \"online community psychology,\" intervention strategies for deviant behavior can be explored along three dimensions: preventative versus remedial, user versus superuser based, and automated versus interpersonal."}
{"_id":"d2bc84a42de360534913e8c3f239197064cf0889","title":"Bayesian Optimization with Empirical Constraints ( PhD Proposal )","text":"This work is motivated by the experimental design problem of optimizing the power output of nano-enhanced microbial fuel cells. Microbial fuel cells (MFCs) (Bond and Lovley, 2003; Fan et al., 2007; Park and Zeikus, 2003; Reguera, 2005) use micro-organisms to break down organic matter and generate electricity. For a particular MFC design, it is critical to optimize the biological energetics and the microbial\/electrode interface of the system, which research has shown to depend strongly on the surface properties of the anodes (Park and Zeikus, 2003; Reguera, 2005). This motivates the design of nano-enhanced anodes, where nano-structures (e.g. carbon nano-wire) are grown on the anode surface to improve the MFC\u2019s power output. Unfortunately, there is little understanding of the interaction between various possible nano-enhancements and MFC capabilities for different micro-organisms. Thus, optimizing anode design for a particular application is largely guess work. Our goal is to develop algorithms to aid this process. Bayesian optimization (Jones, 2001a; Brochu et al., 2009) has been widely used for experimental design problems where the goal is to optimize an unknown function f(\u00b7), that is costly to evaluate. In general, we are interested in finding the point x\u2217 \u2208 X d \u2282 Rd such that: x\u2217 = argmax x\u2208X d f(x), (1)"}
{"_id":"8b422d929c083299afc1cff7debfd2a41e94fa50","title":"I know what you did on your smartphone: Inferring app usage over encrypted data traffic","text":"Smartphones and tablets are now ubiquitous in many people's lives and are used throughout the day in many public places. They are often connected to a wireless local area network (IEEE 802.11 WLANs) and rely on encryption protocols to maintain their security and privacy. In this paper, we show that even in presence of encryption, an attacker without access to encryption keys is able to determine the users' behavior, in particular, their app usage. We perform this attack using packet-level traffic analysis in which we use side-channel information leaks to identify specific patterns in packets regardless of whether they are encrypted or not. We show that just by collecting and analyzing small amounts of wireless traffic, one can determine what apps each individual smartphone user in the vicinity is using. Furthermore, and more worrying, we show that by using these apps the privacy of the user is more at risk compared to using online services through browsers on mobile devices. This is due to the fact that apps generate more identifiable traffic patterns. Using random forests to classify the apps we show that we are able to identify individual apps, even in presence of noise, with great accuracy. Given that most online services now provide native apps that may be identified by this method, these attacks represent a serious threat to users' privacy."}
{"_id":"595e5b8d9e08d56dfcec464cfc2854e562cd7089","title":"Heart rate variability and its relation to prefrontal cognitive function: the effects of training and detraining","text":"The aim of the present study was to investigate the relationship between physical fitness, heart rate variability (HRV) and cognitive function in 37 male sailors from the Royal Norwegian Navy. All subjects participated in an 8-week training program, after which the subjects completed the initial cognitive testing (pre-test). The subjects were assigned into a detrained group (DG) and a trained group (TG) based on their application for further duty. The DG withdrew from the training program for 4 weeks after which all subjects then completed the cognitive testing again (post-test). Physical fitness, measured as maximum oxygen consumption (V\u0307O2max), resting HRV, and cognitive function, measured using a continuous performance task (CPT) and a working memory test (WMT), were recorded during the pre-test and the post-test, and the data presented as the means and standard deviations. The results showed no between-group differences in V\u0307O2max or HRV at the pre-test. The DG showed a significant decrease in V\u0307O2max from the pre- to the post-test and a lower resting HRV than the TG on the post-test. Whereas there were no between-group differences on the CPT or WMT at the pre-test, the TG had faster reaction times and more true positive responses on tests of executive function at the post-test compared to the pre-test. The DG showed faster reaction times on non-executive tasks at the post-test compared to the pre-test. The results are discussed within a neurovisceral integration framework linking parasympathetic outflow to the heart to prefrontal neural functions."}
{"_id":"46739eed6aefecd4591beed0d45b783cc0052a94","title":"Power spectrum analysis of heart rate fluctuation: a quantitative probe of beat-to-beat cardiovascular control.","text":"Power spectrum analysis of heart rate fluctuations provides a quantitative noninvasive means of assessing the functioning of the short-term cardiovascular control systems. We show that sympathetic and parasympathetic nervous activity make frequency-specific contributions to the heart rate power spectrum, and that renin-angiotensin system activity strongly modulates the amplitude of the spectral peak located at 0.04 hertz. Our data therefore provide evidence that the renin-angiotensin system plays a significant role in short-term cardiovascular control in the time scale of seconds to minutes."}
{"_id":"9684797643d13be86002683f60caa1cb69832e74","title":"Vagal influence on working memory and attention.","text":"The aim of the present study was to investigate the effect of vagal tone on performance during executive and non-executive tasks, using a working memory and a sustained attention test. Reactivity to cognitive tasks was also investigated using heart rate (HR) and heart rate variability (HRV). Fifty-three male sailors from the Royal Norwegian Navy participated in this study. Inter-beat-intervals were recorded continuously for 5 min of baseline, followed by randomized presentation of a working memory test (WMT) based on Baddeley and Hitch's research (1974) and a continuous performance test (CPT). The session ended with a 5-min recovery period. High HRV and low HRV groups were formed based on a median split of the root mean squared successive differences during baseline. The results showed that the high HRV group showed more correct responses than the low HRV group on the WMT. Furthermore, the high HRV group showed faster mean reaction time (mRT), more correct responses and less error, than the low HRV group on the CPT. Follow-up analysis revealed that this was evident only for components of the CPT where executive functions were involved. The analyses of reactivity showed a suppression of HRV and an increase in HR during presentation of cognitive tasks compared to recovery. This was evident for both groups. The present results indicated that high HRV was associated with better performance on tasks involving executive function."}
{"_id":"fd829cb98deba075f17db467d49a0391208c07f1","title":"0.9\u201310GHz low noise amplifier with capacitive cross coupling","text":"This paper presents a 0.9GHz-10GHz Ultra Wideband Low Noise Amplifier (LNA) designed for software-defined-radios (SDR). Capacitive cross coupling (CCC) is used at both input stage and cascade stage for wideband input impedance matching and small noise figure (NF). A combination of inductor peaking load and series inductor between the cascade stages of LNA is employed for a flat gain and enhanced input matching. This LNA is implemented in a standard 0.18\u00b5m CMOS technology. For 0.9GHz-10GHz ultra wideband applications, it achieves a maximum gain of 14dB, 3.5dB\u223c4.1dB NF and +1.8dBm IIP3. It consumes only7.6mW from a 1.8V power supply."}
{"_id":"518de7c7ae347fcd7ee466300c1a7853eca0538a","title":"New Direct Torque Control Scheme for BLDC Motor Drives Suitable for EV Applications","text":"This paper proposes a simple and effective scheme for Direct Torque Control (DTC) of Brushless DC Motor. Unlike the traditional DTC where both the torque and flux must to be employed in order to deliver the switching table for the inverter, the proposed DTC scheme utilizes only the torque information. By considering the particular operating principle of the motor, the instantaneous torque can be estimated only by using back EMF ratio in an unified differential equation. The simulation results have shown the good performance of the proposed scheme. It can be seen thus a very suitable technique for EV drives which need fast and precise torque response."}
{"_id":"1719aa40d501613c06f3c9a411e7bb928fb552b8","title":"Using the Facebook group as a learning management system: An exploratory study","text":"Facebook is a popular social networking site. It, like many other new technologies, has potential for teaching and learning because of its unique built-in functions that offer pedagogical, social and technological affordances. In this study, the Facebook group was used as a learning management system (LMS) in two courses for putting up announcements, sharing resources, organizing weekly tutorials and conducting online discussions at a teacher education institute in Singapore. This study explores using the Facebook group as an LMS and the students\u2019 perceptions of using it in their courses. Results showed that students were basically satisfied with the affordances of Facebook as the fundamental functions of an LMS could be easily implemented in the Facebook group. However, using the Facebook group as an LMS has certain limitations. It did not support other format files to be uploaded directly, and the discussion was not organized in a threaded structure. Also, the students did not feel safe and comfortable as their privacy might be revealed. Constraints of using the Facebook group as an LMS, implications for practice and limitations of this study are discussed. Practitioner notes What is already known about this topic \u2022 Facebook has been popularly used by tertiary students, but many students do not want their teachers to be friends on Facebook \u2022 Teacher\u2019s self-disclosure on Facebook can promote classroom atmosphere, teacher\u2019s credibility and student\u2013teacher relationship \u2022 Commercial learning management systems (LMSs) have limitations What this paper adds \u2022 The Facebook group can be used an LMS as it has certain pedagogical, social and technological affordances \u2022 Students are satisfied with the way of using the Facebook group as an LMS British Journal of Educational Technology (2011) doi:10.1111\/j.1467-8535.2011.01195.x \u00a9 2011 The Authors. British Journal of Educational Technology \u00a9 2011 Bera. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main Street, Malden, MA 02148, USA. \u2022 Younger students are more acceptable with the idea of using the Facebook group as an LMS \u2022 Using the Facebook group as an LMS has limitations: it does not support other format files; its discussions are not listed in threads; and it is not perceived as a safe environment Implications for practice and\/or policy \u2022 The Facebook group can be used an LMS substitute or supplement \u2022 Third-party applications are needed to extend the capability of the Facebook group as an LMS \u2022 Using Facebook seems to be more appropriate for young learners than adults \u2022 Teachers do not have to be students\u2019 friends on Facebook. Introduction Social networking sites (SNSs) are virtual spaces where people of similar interest gather to communicate, share photos and discuss ideas with one another (Boyd & Ellison, 2008; Raacke & Bonds-Raacke, 2008). In recent years, Facebook has become one of the most prominent SNSs. Like any new technology, Facebook seems to offer great potentials for teaching and learning as many students are using Facebook daily. One possible way of using Facebook for teaching and learning is to use its group as an LMS. Research shows that using LMSs possesses numerous benefits for teaching and learning. It enables faculty to shift the focus from content-based learning to process-based learning (Vogel & Klassen, 2001) and helps to \u201cfacilitate change from passive to active learning\u201d (Herse & Lee, 2005, p. 51). Using LMSs also has the potential to increase student enrollment (Nunes & McPherson, 2003) and to promote interaction between students and faculty members (Lonn & Teasley, 2009; West, Waddoups & Graham, 2007). Using existing commercial LMSs like Blackboard, however, often has practical constraints (Sanchez-Franco, 2010). For example, LMSs tend to be expensive and that not every school can afford to purchase and maintain them over the long run. Trainee teachers cannot access certain features such as creating a course, enrolling students and setting up student groups as these functions are usually open to instructors or administrators only. The resources in the present LMS are often no longer accessible to trainee teachers after their graduation. Also, the LMS used at school in the future may be different from the one being currently used. They have to shift to a new LMS, and research shows learning a new system is often a painful experience (Black, Beck, Dawson, Jinks & DiPietro, 2007). If the Facebook group can be used as an alternative LMS, it would help to overcome some of the abovementioned constraints. For instance, it would enable a teacher to easily create a new course and enroll students in person if the class size is small. As presented in the following literature review section, many research studies have investigated the usage of Facebook, the effect of teacher\u2019s self-disclosure via Facebook on teacher\u2013student relationship improvement and the academic performance of Facebook users. However, few studies have examined if and how Facebook can be effectively used as an LMS. In this exploratory study, the Facebook group was used as an LMS to put up announcements, share resources, organize weekly tutorial sessions and conduct online discussions. The purpose of this paper is to describe how the Facebook group was used as an LMS in the study and to report students\u2019 perceptions on it. Literature review on Facebook"}
{"_id":"ac3a313bff326f666059d797f2f505881eff3581","title":"An empirical investigation of student adoption model toward mobile e-textbook: UTAUT2 and TTF model","text":"Faculty of Economics in Universitas Atma Jaya Yogyakarta (UAJY) has replaced printed textbooks with e-textbooks for its academic activities since 2015. These e-textbooks can be accessed via iPad which is given to each student. Hence, the objective of this study is to test the proposed research model based on integrated UTAUT2 and TTF. Questionnaires were distributed to 326 students in 10 classes. Only junior and sophomore students were eligible to fulfill the questionnaires. The result of the study shows that performance expectancy, effort expectancy, social influence, facilitating condition, and habit have a direct significant relationship on behavioral intention to use mobile e-textbook. The result also shows that both Task technology and technology characteristic positively affect task technology fit. There is also a direct significant relationship between Task technology fit and performance expectancy. We suggest that further studies include experimentation to investigate performance expectancy of mobile e-book. 2) More evaluation should be held in the future in more mature environment. The finding of this study can help policymakers in the Faculty of Economics UAJY to evaluate the policy and formulate better strategy. In addition, these findings can be used by others faculties and universities as mobile e-textbook adoption case reference."}
{"_id":"871ddd72744812b4eb552a4e9a72d93e77fd87b4","title":"Ethernet-Based Real-Time and Industrial Communications","text":"Despite early attempts to use Ethernet in the industrial context, only recently has it attracted a lot of attention as a support for industrial communication. A number of vendors are offering industrial communication products based on Ethernet and TCP\/IP as a means to interconnect field devices to the first level of automation. Others restrict their offer to communication between automation devices such as programmable logic controllers and provide integration means to existing fieldbuses. This paper first details the requirements that an industrial network has to fulfill. It then shows how Ethernet has been enhanced to comply with the real-time requirements in particular in the industrial context. Finally, we show how the requirements that cannot be fulfilled at layer 2 of the OSI model can be addressed in the higher layers adding functionality to existing standard protocols."}
{"_id":"2f348a2ad3ba390ee178d400be0f09a0479ae17b","title":"Gabor-based kernel PCA with fractional power polynomial models for face recognition","text":"This paper presents a novel Gabor-based kernel principal component analysis (PCA) method by integrating the Gabor wavelet representation of face images and the kernel PCA method for face recognition. Gabor wavelets first derive desirable facial features characterized by spatial frequency, spatial locality, and orientation selectivity to cope with the variations due to illumination and facial expression changes. The kernel PCA method is then extended to include fractional power polynomial models for enhanced face recognition performance. A fractional power polynomial, however, does not necessarily define a kernel function, as it might not define a positive semidefinite Gram matrix. Note that the sigmoid kernels, one of the three classes of widely used kernel functions (polynomial kernels, Gaussian kernels, and sigmoid kernels), do not actually define a positive semidefinite Gram matrix either. Nevertheless, the sigmoid kernels have been successfully used in practice, such as in building support vector machines. In order to derive real kernel PCA features, we apply only those kernel PCA eigenvectors that are associated with positive eigenvalues. The feasibility of the Gabor-based kernel PCA method with fractional power polynomial models has been successfully tested on both frontal and pose-angled face recognition, using two data sets from the FERET database and the CMU PIE database, respectively. The FERET data set contains 600 frontal face images of 200 subjects, while the PIE data set consists of 680 images across five poses (left and right profiles, left and right half profiles, and frontal view) with two different facial expressions (neutral and smiling) of 68 subjects. The effectiveness of the Gabor-based kernel PCA method with fractional power polynomial models is shown in terms of both absolute performance indices and comparative performance against the PCA method, the kernel PCA method with polynomial kernels, the kernel PCA method with fractional power polynomial models, the Gabor wavelet-based PCA method, and the Gabor wavelet-based kernel PCA method with polynomial kernels."}
{"_id":"7ca07ca599d3811eb1d8f9c302a203c6ead0e7fe","title":"Marker-controlled watershed segmentation of nuclei in H&E stained breast cancer biopsy images","text":"In this paper we present an unsupervised automatic method for segmentation of nuclei in H&E stained breast cancer biopsy images. Colour deconvolution and morphological operations are used to preprocess the images in order to remove irrelevant structures. Candidate nuclei locations, obtained with the fast radial symmetry transform, act as markers for a marker-controlled watershed segmentation. Watershed regions that are unlikely to represent nuclei are removed in the postprocessing stage. The proposed algorithm is evaluated on a number of images that are representative of the diversity in pathology in this type of tissue. The method shows good performance in terms of the number of segmented nuclei and segmentation accuracy."}
{"_id":"827a8ab345b673ebc0896296272333d4f88bd539","title":"Empathy decline and its reasons: a systematic review of studies with medical students and residents.","text":"PURPOSE\nEmpathy is a key element of patient-physician communication; it is relevant to and positively influences patients' health. The authors systematically reviewed the literature to investigate changes in trainee empathy and reasons for those changes during medical school and residency.\n\n\nMETHOD\nThe authors conducted a systematic search of studies concerning trainee empathy published from January 1990 to January 2010, using manual methods and the PubMed, EMBASE, and PsycINFO databases. They independently reviewed and selected quantitative and qualitative studies for inclusion. Intervention studies, those that evaluated psychometric properties of self-assessment tools, and those with a sample size <30 were excluded.\n\n\nRESULTS\nEighteen studies met the inclusion criteria: 11 on medical students and 7 on residents. Three longitudinal and six cross-sectional studies of medical students demonstrated a significant decrease in empathy during medical school; one cross-sectional study found a tendency toward a decrease, and another suggested stable scores. The five longitudinal and two cross-sectional studies of residents showed a decrease in empathy during residency. The studies pointed to the clinical practice phase of training and the distress produced by aspects of the \"hidden,\" \"formal,\" and \"informal\" curricula as main reasons for empathy decline.\n\n\nCONCLUSIONS\nThe results of the reviewed studies, especially those with longitudinal data, suggest that empathy decline during medical school and residency compromises striving toward professionalism and may threaten health care quality. Theory-based investigations of the factors that contribute to empathy decline among trainees and improvement of the validity of self-assessment methods are necessary for further research."}
{"_id":"52ba56baa6f72cbb6c76529f3dc56ffc6c735558","title":"Multi-Cell Multiuser Massive MIMO Networks: User Capacity Analysis and Pilot Design","text":"We propose a novel pilot sequence design to mitigate pilot contamination in multi-cell multiuser massive multiple-input multiple-output networks. Our proposed design generates pilot sequences in the multi-cell network and devises power allocation at base stations (BSs) for downlink transmission. The pilot sequences together with the power allocation ensure that the user capacity of the network is achieved and the pre-defined signal-to-interference-plus-noise ratio (SINR) requirements of all users are met. To realize our design, we first derive new closed-form expressions for the user capacity and the user capacity region. Built upon these expressions, we then develop a new algorithm to obtain the required pilot sequences and power allocation. We further determine the minimum number of antennas required at the BSs to achieve certain SINR requirements of all users. The numerical results are presented to corroborate our analysis and to examine the impact of key parameters, such as the pilot sequence length and the total number of users, on the network performance. A pivotal conclusion is reached that our design achieves a larger user capacity region than the existing designs and needs less antennas at the BS to fulfill the pre-defined SINR requirements of all users in the network than the existing designs."}
{"_id":"9574732bdab1bea8fed896ae53641916db642156","title":"repository copy of Transformational Leadership and Burnout : The Role of Thriving and Followers ' Openness to Experience","text":"eprints@whiterose.ac.uk https:\/\/eprints.whiterose.ac.uk\/ Reuse Unless indicated otherwise, fulltext items are protected by copyright with all rights reserved. The copyright exception in section 29 of the Copyright, Designs and Patents Act 1988 allows the making of a single copy solely for the purpose of non-commercial research or private study within the limits of fair dealing. The publisher or other rights-holder may allow further reproduction and re-use of this version refer to the White Rose Research Online record for this item. Where records identify the publisher as the copyright holder, users can verify any specific terms of use on the publisher\u2019s website."}
{"_id":"5c9737f1448fac37483a94a731778b1127af23c5","title":"Adaptive scale based entropy-like estimator for robust fitting","text":"In this paper, we propose a novel robust estimator, called ASEE (Adaptive Scale based Entropy-like Estimator) which minimizes the entropy of inliers. This estimator is based on IKOSE (Iterative Kth Ordered Scale Estimator) and LEL (Least Entropy-Like Estimator). Unlike LEL, ASEE only considers inliers' entropy while excluding outliers, which makes it very robust in parametric model estimation. Compared with other robust estimators, ASEE is simple and computationally efficient. From the experiments on both synthetic and real-image data, ASEE is more robust than several state-of-the-art robust estimators, especially in handling extreme outliers."}
{"_id":"14b0b0e3cfef7a6b22fc5c3753d797cde9787a7d","title":"Home Bias in Online Investments: An Empirical Study of an Online Crowdfunding Market","text":"An extensive literature in economics and finance has documented \" home bias, \" the tendency that transactions are more likely to occur between parties in the same country or state, rather than outside. Can the Internet help overcome this spatial divide, especially in the context of online financial investments? We address this question in the context of a large online crowd funding marketplace. We analyze detailed transaction data over an extended period of time under typical market conditions, as well as those from a natural experiment, a period in which investors were restricted to only one state due to regulations. We further employ a quasi-experimental design by tracking borrowers who moved across state boundaries, and study how investors' behaviors change when those borrowers moved. Home bias appears to be a robust phenomenon under all three scenarios. This finding has important implications not just for the home bias literature, but also more broadly for the growing research and policy interest on Internet-based crowd funding, especially as a new channel for entrepreneurial financing."}
{"_id":"0b07f84c22ce01309981a02c23d5cd1770cad48b","title":"Query optimization techniques for partitioned tables","text":"Table partitioning splits a table into smaller parts that can be accessed, stored, and maintained independent of one another. From their traditional use in improving query performance, partitioning strategies have evolved into a powerful mechanism to improve the overall manageability of database systems. Table partitioning simplifies administrative tasks like data loading, removal, backup, statistics maintenance, and storage provisioning. Query language extensions now enable applications and user queries to specify how their results should be partitioned for further use. However, query optimization techniques have not kept pace with the rapid advances in usage and user control of table partitioning. We address this gap by developing new techniques to generate efficient plans for SQL queries involving multiway joins over partitioned tables. Our techniques are designed for easy incorporation into bottom-up query optimizers that are in wide use today. We have prototyped these techniques in the PostgreSQL optimizer. An extensive evaluation shows that our partition-aware optimization techniques, with low optimization overhead, generate plans that can be an order of magnitude better than plans produced by current optimizers."}
{"_id":"1b5b518fe9ebf59520f57efb880a6f93e3b20c0e","title":"Balancing minimum spanning trees and shortest-path trees","text":"We give a simple algorithm to find a spanning tree that simultaneously approximates a shortest-path tree and a minimum spanning tree. The algorithm provides a continuous tradeoff: given the two trees and a\u03b3>0, the algorithm returns a spanning tree in which the distance between any vertex and the root of the shortest-path tree is at most 1+\u221a2\u03b3 times the shortest-path distance, and yet the total weight of the tree is at most 1+\u221a2\/\u03b3 times the weight of a minimum spanning tree. Our algorithm runs in linear time and obtains the best-possible tradeoff. It can be implemented on a CREW PRAM to run a logarithmic time using one processor per vertex."}
{"_id":"4e29cef227595acdeb09a611f9728406f30baa1d","title":"The Anatomic Basis of Midfacial Aging","text":"Facial aging is a multifactorial, three-dimensional (3D) process with anatomic, biochemical, and genetic correlates. Many exogenous and endogenous factors can signifi cantly impact the perceived age of an individual. Solar exposure [ 1\u2013 3 ] , cigarette smoking [ 1, 2, 4, 5 ] , medications [ 1 ] , alcohol use [ 1 ] , body mass index [ 2 ] , and endocrinologic status [ 1, 6, 7 ] have all been implicated as factors that accelerate cutaneous and subcutaneous aging. These factors act in concert to create a variegated spectrum of facial morphologic aging changes, and thus, Mme. Chanel was partially correct in her statement from the last century. Most of the aging changes that occur in the midface, however, occur predictably in the majority of individuals. Stigmata of midfacial aging typically appear by the middle of the fourth decade. Degenerative changes occur in nearly every anatomic component of the midface and include cranial bone remodeling, tissue descent secondary to gravity, fat atrophy, and deterioration in the condition and appearance of the skin. The lower eyelids and adjacent tissues are often the initial areas of patient concern. This chapter reviews the morphologic changes that occur in the aging midface and discusses the pathogenesis of midfacial aging based upon its anatomic components. An integrated theory of facial aging will be presented. A. E. Wulc , MD, FACS ( ) Associate Clinical Professor of Ophthalmology , University of Pennsylvania"}
{"_id":"615f5506aa64d5496ebd2019f7a5661c8935fc81","title":"DGASensor: Fast Detection for DGA-Based Malwares","text":"DNS protocol has been used by many malwares for command-and-control (C&C). To improve the resiliency of C&C communication, Domain Generation Algorithm (DGA) has been utilized by recent malwares such as Locky, Conficker and Zeus. Many detection systems have been introduced for DGA-based botnets detection. However, such botnets detection approaches suffer from several limitations, for instance, requiring a group of DGA domains, period behaviors, the presence of multiple bots, and so forth. It is very hard for them to detect an individually running DGA-based malware which leave only a few traces. In this paper, we develop DGASensor to detect DGA-based malwares immediately by identifying a single DGA domain using lexical evidence. First, DGASensor automatically analyzes the lexical patterns of the most popular domains listed in Alexa top 100,000, and then extracts two templates, namely distribution template and structure template. Second, the above two templates, pronounceable attributes, and some frequently used properties like entropy and length, are used to extract features from a single domain. Third, we train our classifier using a non-DGA dataset consisting of domains obtained from Alexa rank and a DGA dataset generated by known DGAs. At last, we provide a short word filter to decrease the false positive rate. We implement a prototype system and evaluate it using the above training dataset with 10-fold cross validation. Moreover, a set of real world DNS traffic collected from a recursive DNS server is used to measure real world performance of our system. The results show that DGASensor detects DGA domains with accuracy 93% in our training dataset and is able to identify a variety of malwares in the real world dataset with an extremely high processing capability."}
{"_id":"588e95a290df5ec9a1296e11fc71db6c7a95300d","title":"On the effectiveness of address-space randomization","text":"Address-space randomization is a technique used to fortify systems against buffer overflow attacks. The idea is to introduce artificial diversity by randomizing the memory location of certain system components. This mechanism is available for both Linux (via PaX ASLR) and OpenBSD. We study the effectiveness of address-space randomization and find that its utility on 32-bit architectures is limited by the number of bits available for address randomization. In particular, we demonstrate a <i>derandomization attack<\/i> that will convert any standard buffer-overflow exploit into an exploit that works against systems protected by address-space randomization. The resulting exploit is as effective as the original exploit, although it takes a little longer to compromise a target machine: on average 216 seconds to compromise Apache running on a Linux PaX ASLR system. The attack does not require running code on the stack.\n We also explore various ways of strengthening address-space randomization and point out weaknesses in each. Surprisingly, increasing the frequency of re-randomizations adds at most 1 bit of security. Furthermore, compile-time randomization appears to be more effective than runtime randomization. We conclude that, on 32-bit architectures, the only benefit of PaX-like address-space randomization is a small slowdown in worm propagation speed. The cost of randomization is extra complexity in system support."}
{"_id":"ab491c2bd542c9fb50171f316376a0a8ac75b732","title":"Cool English: a Grammatical Error Correction System Based on Large Learner Corpora","text":"This paper presents a grammatical error correction (GEC) system that provides corrective feedback for essays. We apply the neural sequence-to-sequence model, which is frequently used in machine translation and text summarization, to this GEC task. The model is trained on EF-Cambridge Open Language Database (EFCAMDAT), a large learner corpus annotated with grammatical errors and corrections. Evaluation shows that our system achieves competitive performance on a number of publicly available testsets."}
{"_id":"5fd2e1b033eca1c3f3d9da3dc0bcb801ec054bde","title":"Uses and Gratification Theory \u2013 Why Adolescents Use Facebook ?","text":"Due to a dynamic development of the Web 2.0 and new trends in the social media field that change on a daily basis, contemporary media research is shifting its focus to a greater extent on media users, their motivation and behavior in using social network sites in order to explain the extreme popularity of Facebook, Twitter, WhatsApp and other similar SNSs and mobile chat applications among the young. In this paper we wanted to explore the benefits of Facebook use among adolescents as well as which of their needs are gratified thereat. As the theoretical background we used the uses and gratification theory due to its user oriented approach. Furthermore, we wanted to test whether the uses and gratification concept is adequate for analyzing the motivation and behavior of SNSs users as suggested by some previous research. The survey comprising 431 adolescent Facebook users was conducted from October to December 2013 in the City of Zagreb. The results have shown that most adolescents use Facebook for socializing and communicating with their friends, discussing school activities, setting up meetings and dates with friends as well as"}
{"_id":"34c658d37221dc4bdff040a0f025230adf6c26a0","title":"Entity Extraction: From Unstructured Text to DBpedia RDF triples","text":"In this paper, we describe an end-to-end system that automatically extracts RDF triples describing entity relations and properties from unstructured text. This system is based on a pipeline of text processing modules that includes a semantic parser and a coreference solver. By using coreference chains, we group entity actions and properties described in different sentences and convert them into entity triples. We applied our system to over 114,000 Wikipedia articles and we could extract more than 1,000,000 triples. Using an ontology-mapping system that we bootstrapped using existing DBpedia triples, we mapped 189,000 extracted triples onto the DBpedia namespace. These extracted entities are available online in the N-Triple format1."}
{"_id":"4c5cc16b8f890faaa23be30f752fe2a8a935f87f","title":"Improvements to BM25 and Language Models Examined","text":"Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TF1\u00b0\u0394\u00b0p\u00d7ID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best over-all."}
{"_id":"47565795cc8a46306fada69ef45b28be5b520060","title":"The Evolution of Accuracy and Bias in Social Judgment","text":"Humans are an intensely social species and therefore it is essential for our interpersonal judgments to be valid enough to help us to avoid enemies, form useful alliances and find suitable mates; flawed judgments can literally be fatal. An evolutionary perspective implies that humans ought to have developed sufficient skills at solving problems of interpersonal judgment, including gauging the personalities of others, to be useful for the basic tasks of survival and reproduction. Yet, the view to be derived from the large and influential bias-and-error literature of social psychology is decidedly different\u2014the social mind seems riddled with fundamental design flaws. We will argue in this paper that flawed design is probably the least plausible explanation for the existence of so many errors. We present an evolutionarily-based taxonomy of known bias effects that distinguishes between biases that are trivial or even artifactual and lead virtually nowhere, and those that have interesting implications and deserve further study. Finally, we present an evolutionary perspective that suggests that the ubiquity, automaticity, and success of interpersonal judgment, among other considerations, presents the possibility of a universal Personality Judgment Instinct. Archeological evidence and behavioral patterns observed in extant hunter-gatherer groups indicate that the human species has been intensely social for a long time (e.g., Chagnon, 1983, Tooby & Devore, 1987). Human offspring have a remarkably extended period of juvenile dependency, which both requires and provides the skills for surviving in a complex social world (Hrdy, 1999). Humans evolved language and universal emotional expressions which serve the social purpose of discerning and influencing the thoughts of others (e. and humans will infer social intentions on the basis of minimal cues, as Heider and Simmel (1944) demonstrated in their classic experiment involving chasing triangles and evading circles. Recent work has shown that children above age 4 and adults in disparate cultures (Germans and Amazonian Indians) can categorize intentions\u2014chasing, fighting, following, playing, and courting (for adults)\u2014 from no more than the motion patterns of computerized v-shaped arrowheads (Barrett, Todd, Miller, & Blythe, in press). Most notably, humans have a deeply-felt need for social inclusion. Deprivation of social contact produces anxiety, loneliness, and depression (Baumeister & Leary, 1995); indeed, as William James (1890) observed: \" Solitary confinement is by many regarded as a mode of torture Accuracy and Bias-3 too cruel and unnatural for civilised countries to adopt. \" Participants in laboratory studies who are left out of a face-to-face \u2026"}
{"_id":"87b1a994964d1a41858652da91e036243ed03051","title":"Pre-Prosthetic Orthodontic Implant for Management of Congenitally Unerupted Lateral Incisors \u2013 A Case Report","text":"The maxillary lateral incisor is one of the most common congenitally missing teeth of the permanent dentition. With the advent of implants in the field of restorative dentistry, a stable and predictable fixed prosthetic replacement has become a reality, especially for young adult patients who suffer from congenital absence of teeth. The dual goals of establishment of functional stability as well as enhancement of esthetic outcomes are made achievable by the placement of implants. A multidisciplinary team approach involving the triad of orthodontist, periodontist and restorative dentist will ensure the successful completion of the integrated treatment approach in these patients. The present case report achieved successful implant based oral rehabilitation in a patient diagnosed with congenital absence of bilateral maxillary lateral incisors utilizing a preprosthetic orthodontic implant site preparation for the purpose of space gain."}
{"_id":"51bb6450e617986d1bd8566878f7693ffd03132d","title":"Efficient and accurate nearest neighbor and closest pair search in high-dimensional space","text":"Nearest Neighbor (NN) search in high-dimensional space is an important problem in many applications. From the database perspective, a good solution needs to have two properties: (i) it can be easily incorporated in a relational database, and (ii) its query cost should increase sublinearly with the dataset size, regardless of the data and query distributions. Locality-Sensitive Hashing (LSH) is a well-known methodology fulfilling both requirements, but its current implementations either incur expensive space and query cost, or abandon its theoretical guarantee on the quality of query results.\n Motivated by this, we improve LSH by proposing an access method called the Locality-Sensitive B-tree (LSB-tree) to enable fast, accurate, high-dimensional NN search in relational databases. The combination of several LSB-trees forms a LSB-forest that has strong quality guarantees, but improves dramatically the efficiency of the previous LSH implementation having the same guarantees. In practice, the LSB-tree itself is also an effective index which consumes linear space, supports efficient updates, and provides accurate query results. In our experiments, the LSB-tree was faster than: (i) iDistance (a famous technique for exact NN search) by two orders of magnitude, and (ii) MedRank (a recent approximate method with nontrivial quality guarantees) by one order of magnitude, and meanwhile returned much better results.\n As a second step, we extend our LSB technique to solve another classic problem, called Closest Pair (CP) search, in high-dimensional space. The long-term challenge for this problem has been to achieve subquadratic running time at very high dimensionalities, which fails most of the existing solutions. We show that, using a LSB-forest, CP search can be accomplished in (worst-case) time significantly lower than the quadratic complexity, yet still ensuring very good quality. In practice, accurate answers can be found using just two LSB-trees, thus giving a substantial reduction in the space and running time. In our experiments, our technique was faster: (i) than distance browsing (a well-known method for solving the problem exactly) by several orders of magnitude, and (ii) than D-shift (an approximate approach with theoretical guarantees in low-dimensional space) by one order of magnitude, and at the same time, outputs better results."}
{"_id":"17f3358d219c05f3cb8d68bdfaf6424567d66984","title":"Adversarial Examples for Generative Models","text":"We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network."}
{"_id":"9a25143ad5cb06a596eb8bda2dd87c99de36d12c","title":"Exploring hacker assets in underground forums","text":"Many large companies today face the risk of data breaches via malicious software, compromising their business. These types of attacks are usually executed using hacker assets. Researching hacker assets within underground communities can help identify the tools which may be used in a cyberattack, provide knowledge on how to implement and use such assets and assist in organizing tools in a manner conducive to ethical reuse and education. This study aims to understand the functions and characteristics of assets in hacker forums by applying classification and topic modeling techniques. This research contributes to hacker literature by gaining a deeper understanding of hacker assets in well-known forums and organizing them in a fashion conducive to educational reuse. Additionally, companies can apply our framework to forums of their choosing to extract their assets and appropriate functions."}
{"_id":"e09c38c84e953f85d4ebca9b5c2b9f9e25903d9f","title":"Analysis of circulating tumor DNA to monitor metastatic breast cancer.","text":"BACKGROUND\nThe management of metastatic breast cancer requires monitoring of the tumor burden to determine the response to treatment, and improved biomarkers are needed. Biomarkers such as cancer antigen 15-3 (CA 15-3) and circulating tumor cells have been widely studied. However, circulating cell-free DNA carrying tumor-specific alterations (circulating tumor DNA) has not been extensively investigated or compared with other circulating biomarkers in breast cancer.\n\n\nMETHODS\nWe compared the radiographic imaging of tumors with the assay of circulating tumor DNA, CA 15-3, and circulating tumor cells in 30 women with metastatic breast cancer who were receiving systemic therapy. We used targeted or whole-genome sequencing to identify somatic genomic alterations and designed personalized assays to quantify circulating tumor DNA in serially collected plasma specimens. CA 15-3 levels and numbers of circulating tumor cells were measured at identical time points.\n\n\nRESULTS\nCirculating tumor DNA was successfully detected in 29 of the 30 women (97%) in whom somatic genomic alterations were identified; CA 15-3 and circulating tumor cells were detected in 21 of 27 women (78%) and 26 of 30 women (87%), respectively. Circulating tumor DNA levels showed a greater dynamic range, and greater correlation with changes in tumor burden, than did CA 15-3 or circulating tumor cells. Among the measures tested, circulating tumor DNA provided the earliest measure of treatment response in 10 of 19 women (53%).\n\n\nCONCLUSIONS\nThis proof-of-concept analysis showed that circulating tumor DNA is an informative, inherently specific, and highly sensitive biomarker of metastatic breast cancer. (Funded by Cancer Research UK and others.)."}
{"_id":"9371a8f9916c0e2bd510d82436dedfc358c74ee4","title":"Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation","text":"This paper addresses the problem of weakly supervised semantic image segmentation. Our goal is to label every pixel in a new image, given only image-level object labels associated with training images. Our problem statement differs from common semantic segmentation, where pixel-wise annotations are typically assumed available in training. We specify a novel deep architecture which fuses three distinct computation processes toward semantic segmentation &#x2013; namely, (i) the bottom-up computation of neural activations in a CNN for the image-level prediction of object classes, (ii) the top-down estimation of conditional likelihoods of the CNNs activations given the predicted objects, resulting in probabilistic attention maps per object class, and (iii) the lateral attention-message passing from neighboring neurons at the same CNN layer. The fusion of (i)-(iii) is realized via a conditional random field as recurrent network aimed at generating a smooth and boundary-preserving segmentation. Unlike existing work, we formulate a unified end-to-end learning of all components of our deep architecture. Evaluation on the benchmark PASCAL VOC 2012 dataset demonstrates that we outperform reasonable weakly supervised baselines and state-of-the-art approaches."}
{"_id":"d64b5c107c7a8710f4e413a9b1fd224478def83e","title":"Skipping Refinement","text":"We introduce skipping refinement, a new notion of correctness for reasoning about optimized reactive systems. Reasoning about reactive systems using refinement involves defining an abstract, highlevel specification system and a concrete, low-level implementation system. One then shows that every behavior allowed by the implementation is also allowed by the specification. Due to the difference in abstraction levels, it is often the case that the implementation requires many steps to match one step of the specification, hence, it is quite useful for refinement to directly account for stuttering. Some optimized implementations, however, can actually take multiple specification steps at once. For example, a memory controller can buffer the commands to the memory and at a later time simultaneously update multiple memory locations, thereby skipping several observable states of the abstract specification, which only updates one memory location at a time. We introduce skipping simulation refinement and provide a sound and complete characterization consisting of \u201clocal\u201d proof rules that are amenable to mechanization and automated verification. We present case studies that highlight the applicability of skipping refinement: a JVM-inspired stack machine, a simple memory controller and a scalar to vector compiler transformation. Our experimental results demonstrate that current model-checking and automated theorem proving tools have difficultly automatically analyzing these systems using existing notions of correctness, but they can analyze the systems if we use skipping refinement."}
{"_id":"038df8f5f00817e29bdcb8822c905fc4bb7718e9","title":"Fossil chrysophycean cyst flora of Racze Lake, Wolin Island (Poland) in relation to paleoenvironmental conditions","text":"The study presents stratigraphic distribution of fossil chrysophycean cysts in the bottom sediments of Racze Lake (Poland). Thirty morphotypes are described, most of them for the first time. The description of the cyst includes SEM microphotographs. The long-term relationship between the lake's conditions and the occurrence of characteristic morphotypes of chrysophycean cysts is discussed."}
{"_id":"b8c1f2c1f266aa37d7b82731b90ec28b72197454","title":"Design and implementation of a 120A resonant inverter for induction furnace","text":"This paper presents the design and implementation of a series resonant inverter operating at high frequency for induction heating applications. The main advantage of this work is focused in applying power electronics, particularly a series resonant inverter based on an IGBT's full bridge and a series RLC Load matched with a high frequency coupling transformer. The series resonant inverter is designed to deliver a current load up to 120A with a one phase AC source. Due to the inherent high speed operation and high efficiency of switching converters, other advantages of the induction furnace presented in this paper are that it require less time to start melting, and require less physical space than other traditional furnaces."}
{"_id":"f1e364bab4646e45a4acd5638977a6cdf508cc90","title":"Is Verbal Irony Special?","text":"The way we speak can reveal much about what we intend to communicate, but the words we use often only indirectly relate to the meanings we wish to convey. Verbal irony is a commonly studied form of indirect speech in which a speaker produces an explicit evaluative utterance that implicates an unstated, opposing evaluation. Producing and understanding ironic language, as well as many other types of indirect speech, requires the ability to recognize mental states in others, sometimes described as a capacity for metarepresentation. This article aims to connect common elements between the major theoretical approaches to verbal irony to recent psycholinguistic, developmental, and neuropsychological research demonstrating the necessity for metarepresentation in the effective use of verbal irony in social interaction. Here I will argue that verbal irony is one emergent, strategic possibility given the interface between people\u2019s ability to infer mental states, and use language. Rather than think of ironic communication as a specialized cognitive ability, I will claim that it arises from the same set of abilities that underlie a wide range of inferential communicative behaviors. Language interaction involves a complex interplay of many cognitive abilities. Theorists across many disciplines struggle with questions of how to carve up these abilities, and whether they can be carved up in the first place (e.g., Christiansen and Chater 2008). Among the most difficult questions to address are those that involve how people recognize intentions in others\u2019 behavior, and how that is achieved through language in the context of many other sources of information. The way we speak can reveal much about what we intend to communicate, but the words we use are often quite different from the meanings we wish to convey. People often speak indirectly for a variety of strategic reasons, and these strategies rely intrinsically on social cognition. One well-researched example of this is the phenomenon of verbal irony \u2013 a type of indirect speech in which a speaker produces an explicit evaluative utterance that implicates an unstated, opposing evaluation. As described below, this trope has been traditionally defined in rather vague terms, but has generated a great deal of research as a phenomenon in need of special theoretical explanation. Here I will argue that verbal irony is one emergent, strategic possibility given the interface between people\u2019s ability to infer the mental states of others, and use language. Rather than think of ironic communication as a unique skill or specialized cognitive ability, I suggest that it arises from the same set of abilities that underlie a wide range of inferential communicative behaviors. The study of verbal irony production and comprehension has been a multidisciplinary effort lacking extensive interdisciplinary exchange. There have been various taxonomies and definitions of what constitutes irony, both verbal and situational \u2013 a struggling enterprise to say the least (Colston and Gibbs 2007). The traditional notion of verbal irony dates back at least to the Roman rhetorician Quintilian, who described irony as a kind of allegory expressing the opposite of what one means, often for mocking Language and Linguistics Compass 6\/11 (2012): 673\u2013685, 10.1002\/lnc3.364 a 2012 The Author Language and Linguistics Compass a 2012 Blackwell Publishing Ltd communicative effects. Some of the assumptions of this early categorization have carried over into modern psycholinguistic research without proper consideration of underlying communicative functions (Kreuz 2000). Many scholars have linked verbal irony to situational irony \u2013 particularly in how both forms seem to involve, at some level, a contradiction between what is expected, and what occurs. While verbal irony often points to unexpected or unfortunate outcomes in people\u2019s actions (Kumon-Nakamura, Glucksberg, and Brown 1995; Pexman 2008), it does not do so necessarily. At present, most contemporary researchers of figurative language and inferential communication have settled on some version of the basic claim that verbal irony is a class of indirect language use where explicit sentence meanings are conceptually contradictory to a network of related implied propositions. Many other tropes, however, are not obviously distinct (e.g., parody, pretense, double entendre, litotes, etc). One common thread between many theoretical and empirical approaches to verbal irony is the distinction language users must make between layers of meaning. This could be the difference between actual and attributed beliefs (Kreuz and Glucksberg 1989; Sperber and Wilson 1986, 1995), real and imagined discourse acts (Clark and Gerrig 1984), relevant or inappropriate to a particular context (Attardo 2000), what is negated and what is implicated (Giora 1995), or failed expectations and attitudes toward those failures (Kumon-Nakamura, Glucksberg, and Brown 1995). Speakers implicitly highlight the contrasts between these different levels of meaning, and in doing so communicate an attitude toward an attributed belief (Sperber and Wilson 1986, 1995). This attitudinal dissociation is typically considered a hallmark of ironic communication, and necessarily requires an implicit understanding of others\u2019 mental states. Consider the following exchange: John: I got another virus on my laptop. Mary: Aren\u2019t you glad you didn\u2019t switch to Mac? In this example John and Mary are discussing John\u2019s computer problems. John reports that he got a virus on his computer \u2013 not his first. Mary asks an ironic rhetorical question expressing her negative opinion of Windows PCs. The literal question is in particular opposition to the implied evaluation \u2013 the essence of irony. In this example, the irony is largely contained in a single lexical opposition \u2013 \u2018\u2018glad\u2019\u2019 instead of, for example, \u2018\u2018sad,\u2019\u2019 but is also communicated by the rhetorical aspect of the question. The question form is one format speakers use to express ironic intentions, but ironic forms can manifest themselves in various ways linguistically, and in a variety of media including television, music, and visual art (Bryant 2011; El Refaie 2005; Ettema and Glasser 1994; Nuolijarvi and Tiittula 2010; Scott 2003). Figurative language devices such as verbal irony can be quite powerful communicative tools. The sentences we speak are often produced in a manner that will allow our audience to derive certain unstated meanings, and the way we put them is often particularly economical because of our heavy reliance on others\u2019 inferential abilities. These meanings are not just efficient, but unique to indirect communication (Gibbs 2000a). The combination of affect directed toward some attributed belief, along with a direct utterance, makes ironic meanings difficult, if not impossible, to precisely capture. But this is due to inferential processes associated with mindreading and linguistic communication, not some distinct process of irony understanding. Similarly, it is difficult to capture, in linguistic terms, many kinds of inferences regarding others\u2019 intentions that people generate in social contexts. In our example, Mary, of course, communicates much more than a negative opinion of PCs. For instance, she implicitly reminds John of previous interactions, some being debates, when their differences of opinion about computer platforms were discussed (i.e., 674 Gregory A. Bryant a 2012 The Author Language and Linguistics Compass 6\/11 (2012): 673\u2013685, 10.1002\/lnc3.364 Language and Linguistics Compass a 2012 Blackwell Publishing Ltd she echoes previous opinions expressed, perhaps even through literal quotations) (Kreuz and Glucksberg 1989; Sperber and Wilson 1986, 1995). In doing so she reiterates her claim that John\u2019s decision was a mistake, and that he would regret it. By using an ironic rhetorical question, Mary efficiently conveys a complex set of implied meanings that are relevant to John. The irony is not necessarily obvious to an outsider, as is the case with most occurrences of irony in everyday conversations (Gibbs 2000b), but John understands Mary immediately, and Mary knew he would. In the process, Mary potentially reduces John\u2019s perception that she is being critical, and makes him laugh (while actually criticizing him) (Dews and Winner 1995). Alternatively, Mary might be accentuating her criticism by highlighting a contrast between John\u2019s current situation and his earlier stated preference (Colston 1997). Speakers attempt to fulfill particular communicative goals by using indirect language such as verbal irony, and different forms can result in quite different emotional reactions \u2013 we should expect it to be used strategically and variably (Bryant 2011; Bryant and Fox Tree 2005; Leggitt and Gibbs 2000). Scholars have suggested many functions for irony in discourse, including speakers\u2019 attempts to be humorous, create solidarity, appear clever, increase memorability, save face, be polite, or alter the valence of an attack or praise (Gibbs 1994; Roberts and Kreuz 1994; Toplak and Katz 2000). The pragmatic uses of ironic speech are far-reaching \u2013 \u2018\u2018what can\u2019t irony do?\u2019\u2019 might be a better question. One reason that irony can fulfill so many discourse goals is that includable tokens in the category need only satisfy a couple conditions; thus, an enormous variety of speech acts can qualify. Burgers, van Mulken, and Schellens (2011) recently proposed a procedure for identifying ironies in discourse, and for a basic definition, they looked at the commonalities between different contemporary theories. In their view, verbal irony involves an expressed evaluative utterance that implies an opposing evaluative appraisal. This implied meaning does not need to be the opposite of the stated meaning, but just differ in a scalar manner. For instance, an understatement might still be negative, but less negative than the intended meaning (e.g., when asked about a personal experience with a"}
{"_id":"64aa6e5e7b3f8a29bad2e97463ffb7bd43a8af9d","title":"Sharpness Enhancement and Super-Resolution of Around-View Monitor Images","text":"In the wide-angle (WA) images embedded in an around-view monitor system, the subject(s) in the peripheral region is normally small and has little information. Furthermore, since the outer region suffers from the non-uniform blur phenomenon and artifact caused by the inherent optical characteristic of WA lenses, its visual quality tends to deteriorate. According to our experiments, conventional image enhancement techniques rarely improve the degraded visual quality of the outer region of WA images. In order to solve the above-mentioned problem, this paper proposes a joint sharpness enhancement (SE) and super-resolution (SR) algorithm which can improve the sharpness and resolution of WA images together. The proposed SE algorithm improves the sharpness of the deteriorated WA images by exploiting self-similarity. Also, the proposed SR algorithm generates super-resolved images by using high-resolution information which is classified according to the extended local binary pattern-based classifier and learned on a pattern basis. Experimental results show that the proposed scheme effectively improves the sharpness and resolution of the input deteriorated WA images. Even in terms of quantitative metrics such as just noticeable blur, structural similarity, and peak signal-to-noise ratio. Finally, the proposed scheme guarantees real-time processing such that it achieves 720p video at 29 Hz on a low cost GPU platform."}
{"_id":"277e6234b9f512d410302665a60c680c75be1a37","title":"Understanding the Potential of Interpreter-based Optimizations for Python","text":"The increasing popularity of scripting languages as general purpose programming environments calls for more efficient execution. Most of these languages, such as Python, Ruby, PHP, and JavaScript are interpreted. Interpretation is a natural implementation given the dynamic nature of these languages and interpreter portability has facilitated wide-spread use. In this work, we analyze the performance of CPython, a commonly used Python interpreter, to identify major sources of overhead. Based on our findings, we investigate the efficiency of a number of optimizations and explore the design options and trade-offs involved."}
{"_id":"950c8d0b042b2a55a3f32fa9bba9af485a26ad8a","title":"A Fast Parallel Algorithm for Thinning Digital Patterns","text":"A fast parallel thinning algorithm is proposed in this paper. It consists of two subiterations: one aimed at deleting the south-east boundary points and the north-west corner points while the other one is aimed at deleting the north-west boundary points and the south-east corner points. End points and pixel connectivity are preserved. Each pattern is thinned down to a \"skeleton\" of unitary thickness. Experimental results show that this method is very effective."}
{"_id":"9228aa5523fd84616b7a4d199e2178050677ad9b","title":"Lifetime study for a poly fuse in a 0.35 \/spl mu\/m polycide CMOS process","text":"Poly fuses are used as the base element for one time programmable cells in a standard CMOS process. Using a defined programming current, the resistance of the poly fuse increases irreversibly over several orders of magnitude. The goal of this study is to show that a poly fuse has a sufficient life time stability to be used as a storage element even in high reliability circuits. This paper shows the drift of the resistance of a poly fuse over the whole range of programming currents for a standard polycide 0.35 \/spl mu\/m CMOS process. The poly fuse for the selected process is build using two different layers, which gives a special performance in terms of programming current."}
{"_id":"6e07fcf8327a3f53f90f86ea86ca084d6733fb88","title":"The relative success of alternative approaches to strategic information systems planning: an empirical analysis","text":"Strategic information systems planning (SISP) is an exercise or ongoing activity that enables organisations to develop priorities for information systems development. It has been suggested that the \u2018SISP approach\u2019, a combination of method, process and implementation, is the most complete way of describing SISP activity. Based upon questionnaire responses from 267 IT Directors, four distinct approaches to SISP have been derived using cluster analysis. A comparison of these four approaches with five approaches of Earl, M.J., 1993. Experiences in SISP, MIS Quarterly, (March), 1\u201324, indicates that three bear strong similarities to the \u2018organisational\u2019, \u2018business-led\u2019, and \u2018administrative\u2019 approaches, whilst the fourth cluster is related to both Earl\u2019s \u2018method-driven\u2019 and \u2018technological\u2019 approaches. An analysis of the relationship between SISP approach and SISP success demonstrates that the \u2018organisational approach\u2019 is significantly more successful than the other three approaches. q 1999 Elsevier Science B.V. All rights reserved."}
{"_id":"84f27dd934eed1b0bde59bbcb8a562cd3b670a87","title":"Classroom Re-design to Facilitate Student Learning : A Case Study of Changes to a University Classroom","text":"This case study examines the physical aspects of a particular university classroom, and what affect specific changes to the classroom had on the perceptions of students, instructors and observers regarding the room as an effective learning space. We compare survey and focus group data collected from students taking courses in the classroom prior to changes to the physical environment with comparable data from students taking courses in the same classroom after specific changes had been made. Immediately following changes to the classroom, notable increases were observed in reported perceptions of student satisfaction with the physical environment, including perceptions of the classroom as a more effective and engaging learning space. Similar perceptions of improvement as a teaching-learning space were reported by instructors and observers. However, subsequent follow-up data collection and analyses suggested little if any sustained increase in perceptions of efficacy of the room as a learning space; indeed, most reported variables returned to baseline levels. The implications of these findings and their relevance to classroom design nevertheless may provide insight regarding the manner in which physical space might support or even enhance teaching and learning. Keywords: learning spaces, active learning, classrooms, teaching and learning environment, classroom design For a number of years there has been an on-going pedagogical shift in higher education away from a traditional content delivery model of instruction to more active models of learning in which students play more involved and interactive roles within the classroom (Cornell, 2002; Brown, 2006). This movement has been coupled with the recognition that the traditional university classroom, with its unidirectional design and tiered, fixed theatre-like seating, is insufficient to accommodate what have increasingly become more varied teaching and learning practices. This growing realization that as the nature of teaching and learning evolves so too must teaching and learning spaces has, in recent years, resulted in a heightened interest among scholars in the examination of classroom space and, more specifically, the inquiry into the connection between classroom design and pedagogy and learning (Brooks, 2011). This study presents findings from an on-going research project examining classroom spaces on a small university campus. The present case study, which is one component of the larger project, compares perceptions of a specific classroom as an effective learning space before and after relatively extensive changes were made to the physical environment of the room. These changes were guided by feedback from students and instructors as well as by existing literature on innovative classroom designs. In particular, we were interested in assessing if these changes 1\t\r  University\t\r  of\t\r  Lethbridge,\t\r  Lethbridge,\t\r  Alberta,\t\r  Canada"}
{"_id":"4f8deabc58014eae708c3e6ee27114535325067b","title":"Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models","text":"The Kullback Leibler (KL) divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaussian mixture models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments."}
{"_id":"5276d835a8cb96e06d19f0b8491dba3ab1642963","title":"Interprocedural Semantic Change-Impact Analysis using Equivalence Relations","text":"Change-impact analysis (CIA) is the task of determining the set of program elements impacted by a program change. Precise CIA has great potential to avoid expensive t sting and code reviews for (parts of) changes that are refactoring s (semantics-preserving). Existing CIA is imprecise becaus e it is coarse-grained, deals with only few refactoring patterns,or is unaware of the change semantics. We formalize the notion of change impact in terms of the trace semantics of two program versions. We show how to leverage equivalence relations to make dataflow-based CIA aware of th e change semantics, thereby improving precision in the prese nce of semantics-preserving changes. We propose an anytime algorithm that allows applying costly equivalence relation inference incrementally to refine the set of impacted statements. We ha ve implemented a prototype in SYM DIFF , and evaluated it on 322 real-world changes from open-source projects and benchmar k programs used by prior research. The evaluation results sho w an average 35% improvement in the size of the set of impacted statements compared to standard dataflow-based techniques ."}
{"_id":"c2a8d08805681ca258c101cdb850d7c3f81ed9a9","title":"Dual Learning based Multi-Objective Pairwise Ranking","text":"There are many different recommendation tasks in our real life. The item ranking task is ranking a set of items based on users\u2019 preferences. The user ranking task is referred to as user retrieval, find the potential users and recommend items to them. We find every recommendation task has it\u2019s own dual task, e.g., Recommend an item to a user versus the user is supposed to become the item\u2019s potential interested user. In this paper, we propose a novel dual learning based ranking framework with the overall aim of learning users\u2019 preferences over items and learning items\u2019 preferences over users by minimizing a pairwise ranking loss. We generate effective feedback signal from close loop formed by dual task. Through a reinforcement learning process, we can iteratively update these two tasks\u2019 models to catch relations between the item recommendation task and the user retrieval task. The experiments show that our method outperforms classical approaches for multi-object recommendation tasks, especially for user retrieval tasks."}
{"_id":"beceeaaeee67884b727248d1f9ecda075e4ce85d","title":"Business Analytics in (a) Blink","text":"The Blink project\u2019s ambitious goal is to answer all Business Intelligence (BI) queries in mere seconds, regardless of the database size, with an extremely low total cost of ownership. Blink is a new DBMS aimed primarily at read-mostly BI query processing that exploits scale-out of commodity multi-core processors and cheap DRAM to retain a (copy of a) data mart completely in main memory. Additionally, it exploits proprietary compression technology and cache-conscious algorithms that reduce memory bandwidth consumption and allow most SQL query processing to be performed on the compressed data. Blink always scans (portions of) the data mart in parallel on all nodes, without using any indexes or materialized views, and without any query optimizer to choose among them. The Blink technology has thus far been incorporated into two IBM accelerator products generally available since March 2011. We are now working on the next generation of Blink, which will significantly expand the \u201csweet spot\u201d of the Blink technology to much larger, disk-based warehouses and allow Blink to \u201cown\u201d the data, rather"}
{"_id":"52047f4929bf616ca6dfad6acf7d5da2a0c15aa8","title":"Three-dimensional menus: A survey and taxonomy","text":"Various interaction techniques have been developed in the field of virtual and augmented reality. Whereas techniques for object selection, manipulation, travel, and wayfinding have already been covered in existing taxonomies in some detail, application control techniques have not yet been sufficiently considered. However, they are needed by almost every mixed reality application, e.g. for choosing from alternative objects or options. For this purpose a great variety of distinct three-dimensional (3D) menu selection techniques is available. This paper surveys existing 3D menus from the corpus of literature and classifies them according to various criteria. The taxonomy introduced here assists developers of interactive 3D applications to better evaluate their options when choosing, optimizing, and implementing a 3D menu technique. Since the taxonomy spans the design space for 3D menu solutions, it also aids researchers in identifying opportunities to improve or create novel virtual menu techniques. r 2006 Elsevier Ltd. All rights reserved."}
{"_id":"661ce19f315aafbf5a3916684e0e7c10e642d5f1","title":"ShoeSoleSense: proof of concept for a wearable foot interface for virtual and real environments","text":"ShoeSoleSense is a proof of concept, novel body worn interface - an insole that enables location independent hands-free interaction through the feet. Forgoing hand or finger interaction is especially beneficial when the user is engaged in real world tasks. In virtual environments as moving through safety training applications is often conducted via finger input, which is not very suitable. To enable a more intuitive interaction, alternative control concepts utilize gesture control, which is usually tracked by statically installed cameras in CAVE-like-installations. Since tracking coverage is limited, problems may also occur. The introduced prototype provides a novel control concept for virtual reality as well as real life applications. Demonstrated functions include movement control in a virtual reality installation such as moving straight, turning and jumping. Furthermore the prototype provides additional feedback by heating up the feet and vibrating in dedicated areas on the surface of the insole."}
{"_id":"9810a7976242774b4d7878fd121f54094412ae40","title":"A discussion of cybersickness in virtual environments","text":"An important and troublesome problem with current virtual environment (VE) technology is the tendency for some users to exhibit symptoms that parallel symptoms of classical motion sickness both during and after the VE experience. This type of sickness, cybersickness, is distinct from motion sickness in that the user is often stationary but has a compelling sense of self motion through moving visual imagery. Unfortunately, there are many factors that can cause cybersickness and there is no foolproof method for eliminating the problem. In this paper, I discuss a number of the primary factors that contribute to the cause of cybersickness, describe three conflicting cybersickness theories that have been postulated, and discuss some possible methods for reducing cybersickness in VEs."}
{"_id":"16af753e94919ca257957cee7ab6c1b30407bb91","title":"ChairIO--the Chair-Based Interface","text":""}
{"_id":"278df68251ff50faa36585ceb05253bcd3b06c32","title":"Multi-agent quadrotor testbed control design: integral sliding mode vs. reinforcement learning","text":"The Stanford Testbed of Autonomous Rotorcraft for Multi-Agent Control (STARMAC) is a multi-vehicle testbed currently comprised of two quadrotors, also called X4-flyers, with capacity for eight. This paper presents a comparison of control design techniques, specifically for outdoor altitude control, in and above ground effect, that accommodate the unique dynamics of the aircraft. Due to the complex airflow induced by the four interacting rotors, classical linear techniques failed to provide sufficient stability. Integral sliding mode and reinforcement learning control are presented as two design techniques for accommodating the nonlinear disturbances. The methods both result in greatly improved performance over classical control techniques."}
{"_id":"5d65ac7d6ae07247a6cc12c2aa87977ceee520f9","title":"Design guidelines for wireless sensor networks: communication, clustering and aggregation","text":"When sensor nodes are organized in clusters, they could use either single hop or multi-hop mode of communication to send their data to their respective cluster heads. We present a systematic cost-based analysis of both the modes, and provide results that could serve as guidelines to decide which mode should be used for given settings. We determine closed form expressions for the required number of cluster heads and the required battery energy of nodes for both the modes. We also propose a hybrid communication mode which is a combination of single hop and multi-hop modes, and which is more cost-effective than either of the two modes. Our problem formulation also allows for the application to be taken into account in the overall design problem through a data aggregation model. 2003 Elsevier B.V. All rights reserved."}
{"_id":"60f64addc751835709c9aa10efcec36e1cd74551","title":"Grounded Learning of Color Semantics with Autoencoders","text":"Humans learn language by grounding word meaning in the physical world. Recent efforts in natural language processing have attempted to model such multimodal learning by incorporating visual information into word and sentence representations. Here, we explore the task of grounding lexical color descriptions in their visual referents. We propose an RNN-based autoencoder model to learn vector representations of sentences that reflect their associated color values. Our model effectively learns a joint visual-lexical space that demonstrates compositionality and generalizes to unseen color names. As a demonstration of such a space learned, we show that our model can predict captions from color representations and color representations from captions. In addition to successfully modeling color language, this work provides a novel framework for grounded language learning."}
{"_id":"cc75568885ab99851cc0e0ea5679121606121e5d","title":"Behavior recognition based on machine learning algorithms for a wireless canine machine interface","text":"Training and handling working dogs is a costly process and requires specialized skills and techniques. Less subjective and lower-cost training techniques would not only improve our partnership with these dogs but also enable us to benefit from their skills more efficiently. To facilitate this, we are developing a canine body-area-network (cBAN) to combine sensing technologies and computational modeling to provide handlers with a more accurate interpretation for dog training. As the first step of this, we used inertial measurement units (IMU) to remotely detect the behavioral activity of canines. Decision tree classifiers and Hidden Markov Models were used to detect static postures (sitting, standing, lying down, standing on two legs and eating off the ground) and dynamic activities (walking, climbing stairs and walking down a ramp) based on the heuristic features of the accelerometer and gyroscope data provided by the wireless sensing system deployed on a canine vest. Data was collected from 6 Labrador Retrievers and a Kai Ken. The analysis of IMU location and orientation helped to achieve high classification accuracies for static and dynamic activity recognition."}
{"_id":"6c319487df3b3ef48fe6ea4b971151bd183ae5ea","title":"Encoding User as More Than the Sum of Their Parts: Recurrent Neural Networks and Word Embedding for People-to-people Recommendation","text":"Neural networks and word embeddings are powerful tools to capture latent factors. These tools can provide effective measures of similarities between users or items in the context of sparse data. We propose a novel approach that relies on neural networks and word embeddings to the problem of matching a learner looking for mentoring, and a tutor that is willing to provide this mentoring. Tutors and learners can issue multiple offers\/requests on different topics. The approach matches over the whole array of topics specified by learners and tutors. Its performance for tutor-learner matching is compared with the state of the art. It yields similar results in terms of precision, but improves the recall."}
{"_id":"a27b8f921e475ab5c0aa93be8c566ffe1b529c79","title":"MATLAB \/ SIMULINK Based Modelling Photovoltaic Array Fed T-Source Inverter","text":"In order to utilize the solar energy for industrial, commercial and domestic applications the power conversion schemes plays an important role. The problem exists in conventional power conversion schemes are low efficiency, poor transient response, low voltage gain and more reactive components are being used .This paper proposes a single stage power conversion scheme called T-source inverter(TSI) to overcome these drawbacks. T-Source inverter (TSI) with simple boost control scheme is used as interface circuit between PV array and load. The PV array is analyzed under different irradiation and temperature value. The mathematical equations are verified with simulation and hardware. The verification shows the voltage gain of TSI was comparatively higher than ZSI. The reactive components in the circuit are less, fast transient response and low output ripple."}
{"_id":"b9225857342bf3ca201a4cca173cab3329db584f","title":"A near-threshold 7T SRAM cell with high write and read margins and low write time for sub-20 nm FinFET technologies","text":"In this paper, a 7T SRAM cell with differential write and single ended read operations working in the near-threshold region is proposed. The structure is based on modifying a recently proposed 5T cell which uses high and low VTH transistors to improve the read and write stability. To enhance the read static noise margin (RSNM) while keeping the high write margin and low write time, an extra access transistor is used and the threshold voltages of the SRAM transistors are appropriately set. In addition, to maintain the low leakage power of the cell and increase the Ion\/Ioff ratio of its access transistors, a high VTH transistor is used in the pull down path of the cell. To assess the efficacy of the proposed cell, its characteristics are compared with those of 5T, 6T, 8T, and 9T SRAM cells. The characteristics are obtained from HSPICE simulations using 20 nm, 16 nm, 14 nm, 10 nm, and 7 nm FinFET technologies assuming a supply voltage of 500 mV. The results reveal high write and read margins, the highest Ion\/Ioff ratio, a fast write, and ultra-low leakage power in the hold \u201c0\u201d state for the cell. Therefore, the suggested 7T cell may be considered as one of the better design choices for both high performance and low power applications. Also, the changes of cell parameters when the temperature rises from 40 1C to 100 1C are investigated. Finally, the write margin as well as the read and hold SNMs of the cell in the presence of the process variations are studied at two supply voltages of 400 mV and 500 mV. The study shows that the proposed cell meets the required cell sigma value (6\u03c3) under all conditions. & 2015 Elsevier B.V. All rights reserved."}
{"_id":"93a648c9d8b1d1128c9c3abf75d3c5b9b3bdcb02","title":"Microcontroller-based quadratic buck converter used as led lamp driver","text":"This paper presents a new proposal for driving LED lamps through the use of a micro controlled quadratic buck DC\/DC converter. The main focus is to improve the LED driver's characteristics using the microcontroller PIC16F873A, thus achieving greater durability and efficiency. Mathematical analyses and experimental results are presented in this paper."}
{"_id":"694a40785f480cc0d65bd94a5e44f570aff5ea37","title":"Integrating Grid-Based and Topological Maps for Mobile Robot Navigation","text":"Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are considerably difficult to learn in large-scale environments. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms\u2014grid-based and topological\u2014, the approach presented here gains the best of both worlds: accuracy\/consistency and efficiency. The paper gives results for autonomously operating a mobile robot equipped with sonar sensors in populated multi-room envi-"}
{"_id":"0c6f14e13f475c56d45f1e9c91f6f05e199ca742","title":"Seasonal Effect on Tree Species Classification in an Urban Environment Using Hyperspectral Data, LiDAR, and an Object-Oriented Approach","text":"The objective of the current study was to analyze the seasonal effect on differentiating tree species in an urban environment using multi-temporal hyperspectral data, Light Detection And Ranging (LiDAR) data, and a tree species database collected from the field. Two Airborne Imaging Spectrometer for Applications (AISA) hyperspectral images were collected, covering the Summer and Fall seasons. In order to make both datasets spatially and spectrally compatible, several preprocessing steps, including band reduction and a spatial degradation, were performed. An object-oriented classification was performed on both images using training data collected randomly from the tree species database. The seven dominant tree species (Gleditsia triacanthos, Acer saccharum, Tilia Americana, Quercus palustris, Pinus strobus and Picea glauca) were used in the classification. The results from this analysis did not show any major difference in overall accuracy between the two seasons. Overall accuracy was approximately 57% for the Summer dataset and 56% for the Fall dataset. However, the Fall dataset provided more consistent results for all tree species while the Summer dataset had a few higher individual class accuracies. Further, adding LiDAR into the classification improved the results by 19% for both fall and summer. This is mainly due to the removal of shadow effect and the addition of elevation data to separate low and high vegetation."}
{"_id":"cd7d22c8dd0bbbedd7987abff7a3ee2a76b2572f","title":"Gamification solutions for software acceptance: A comparative study of Requirements Engineering and Organizational Behavior techniques","text":"Gamification is a powerful paradigm and a set of best practices used to motivate people carrying out a variety of ICT-mediated tasks. Designing gamification solutions and applying them to a given ICT system is a complex and expensive process (in time, competences and money) as software engineers have to cope with heterogeneous stakeholder requirements on one hand, and Acceptance Requirements on the other, that together ensure effective user participation and a high level of system utilization. As such, gamification solutions require significant analysis and design as well as suitable supporting tools and techniques. In this work, we compare concepts, tools and techniques for gamification design drawn from Software Engineering and Human and Organizational Behaviors. We conduct a comparison by applying both techniques to the specific Meeting Scheduling exemplar used extensively in the Requirements Engineering literature."}
{"_id":"274166f64e23a54d47fc75bb4d0f4d0225ce7dbf","title":"Organic food: buying more safety or just peace of mind? A critical review of the literature.","text":"Consumer concern over the quality and safety of conventional food has intensified in recent years, and primarily drives the increasing demand for organically grown food, which is perceived as healthier and safer. Relevant scientific evidence, however, is scarce, while anecdotal reports abound. Although there is an urgent need for information related to health benefits and\/or hazards of food products of both origins, generalized conclusions remain tentative in the absence of adequate comparative data. Organic fruits and vegetables can be expected to contain fewer agrochemical residues than conventionally grown alternatives; yet, the significance of this difference is questionable, inasmuch as actual levels of contamination in both types of food are generally well below acceptable limits. Also, some leafy, root, and tuber organic vegetables appear to have lower nitrate content compared with conventional ones, but whether or not dietary nitrate indeed constitutes a threat to human health is a matter of debate. On the other hand, no differences can be identified for environmental contaminants (e.g. cadmium and other heavy metals), which are likely to be present in food from both origins. With respect to other food hazards, such as endogenous plant toxins, biological pesticides and pathogenic microorganisms, available evidence is extremely limited preventing generalized statements. Also, results for mycotoxin contamination in cereal crops are variable and inconclusive; hence, no clear picture emerges. It is difficult, therefore, to weigh the risks, but what should be made clear is that 'organic' does not automatically equal 'safe.' Additional studies in this area of research are warranted. At our present state of knowledge, other factors rather than safety aspects seem to speak in favor of organic food."}
{"_id":"4ce6a28609c225ed928bf585391995b985860709","title":"Induction of pluripotent stem cells from fibroblast cultures","text":"Clinical application of embryonic stem (ES) cells faces difficulties regarding use of embryos, as well as tissue rejection after implantation. One way to circumvent these issues is to generate pluripotent stem cells directly from somatic cells. Somatic cells can be reprogrammed to an embryonic-like state by the injection of a nucleus into an enucleated oocyte or by fusion with ES cells. However, little is known about the mechanisms underlying these processes. We have recently shown that the combination of four transcription factors can generate ES-like pluripotent stem cells directly from mouse fibroblast cultures. The cells, named induced pluripotent stem (iPS) cells, can be differentiated into three germ layers and committed to chimeric mice. Here we describe detailed methods and tips for the generation of iPS cells."}
{"_id":"b4fc77d6fa6bc4e7845fda834c314573b14a69f1","title":"Foundations of Augmented Cognition: Neuroergonomics and Operational Neuroscience","text":"This work evaluates the feasibility of a motor imagery-based optical brain-computer interface (BCI) for humanoid robot control. The functional near-infrared spectroscopy (fNIRS) based BCI-robot system developed in this study operates through a high-level control mechanism where user specifies a target action through the BCI and the robot performs the set of micro operations necessary to fulfill the identified goal. For the evaluation of the system, four motor imagery tasks (left hand, right hand, left foot, and right foot) were mapped to operational commands (turn left, turn right, walk forward, walk backward) that were sent to the robot in real time to direct the robot navigating a small room. An ecologically valid offline analysis with minimal preprocessing shows that seven subjects could achieve an average accuracy of 32.5 %. This was increased to 43.6 % just by including calibration data from the same day of the robot control using the same cap setup, indicating that day-of calibration following the initial training may be important for BCI control."}
{"_id":"24019386432f4b60842b86a4892d5d424e4c5e2e","title":"Understanding Availability","text":"This paper addresses a simple, yet fundamental question in the design of peer-to-peer systems: What does it mean when we say \u201cavailability\u201d and how does this understanding impact the engineering of practical systems? We argue that existing measurements and models do not capture the complex timevarying nature of availability in today\u2019s peer-to-peer environments. Further, we show that unforeseen methodological shortcomings have dramatically biased previous analyses of this phenomenon. As the basis of our study, we empirically characterize the availability of a large peer-to-peer system over a period of 7 days, analyze the dependence of the underlying availability distributions, measure host turnover in the system, and discuss how these results may affect the design of high-availability peer-to-peer services."}
{"_id":"c98fedc89caca35fe10d4118b7e984ac10737c3b","title":"Photonic Crystal-Structures for THz Vacuum Electron Devices","text":"The technology of photonic crystals (PhCs) is investigated here to improve the performance of THz vacuum electron devices. Compared with conventional metallic waveguides, the PhC arrangement alleviates typical issues in THz vacuum electron tubes, i.e. difficult vacuum pumping process and assembling, and improves the input\/output coupling. A slow-wave structure (SWS) based on a corrugated waveguide assisted by PhC lateral walls and the efficient design of a PhC coupler for sheet-beam interaction devices are demonstrated. Based on the proposed technology, a backward-wave oscillator (BWO) is designed in this paper. Cold parameters of the novel PhC SWS as well as 3-D particle-in-cell simulations of the overall BWO are investigated, obtaining more than 70-mW-peak output power at 0.650 THz for beam voltage of 11 kV and beam current of 6 mA."}
{"_id":"7d88ddeddc8a8c8e71ade8ec4bcdb3dfdd8d0526","title":"Presenting system uncertainty in automotive UIs for supporting trust calibration in autonomous driving","text":"To investigate the impact of visualizing car uncertainty on drivers' trust during an automated driving scenario, a simulator study was conducted. A between-group design experiment with 59 Swedish drivers was carried out where a continuous representation of the uncertainty of the car's ability to autonomously drive during snow conditions was displayed to one of the groups, whereas omitted for the control group. The results show that, on average, the group of drivers who were provided with the uncertainty representation took control of the car faster when needed, while they were, at the same time, the ones who spent more time looking at other things than on the road ahead. Thus, drivers provided with the uncertainty information could, to a higher degree, perform tasks other than driving without compromising with driving safety. The analysis of trust shows that the participants who were provided with the uncertainty information trusted the automated system less than those who did not receive such information, which indicates a more proper trust calibration than in the control group."}
{"_id":"5cd1c62dc99b6c3ecb2e678aa6fb2bffe3853c28","title":"Kernel-Based Learning of Hierarchical Multilabel Classification Models","text":"We present a kernel-based algorithm for hierarchical text c lassification where the documents are allowed to belong to more than one category at a time. The clas sification model is a variant of the Maximum Margin Markov Network framework, where the classifi cation hierarchy is represented as a Markov tree equipped with an exponential family defined o n the edges. We present an efficient optimization algorithm based on incremental conditi onal gradient ascent in single-example subspaces spanned by the marginal dual variables. The optim ization is facilitated with a dynamic programming based algorithm that computes best update dire ctions in the feasible set. Experiments show that the algorithm can feasibly optimize t raining sets of thousands of examples and classification hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efficient as training independent SVM-light clas sifiers for each node. The algorithm\u2019s predictive accuracy was found to be competitive with other r ecently introduced hierarchical multicategory or multilabel classification learning algorithms ."}
{"_id":"2fd54e111e77f6f5a7925592d6016af68d08c81e","title":"A novel ultra-wideband 80 GHz FMCW radar system for contactless monitoring of vital signs","text":"In this paper an ultra-wideband 80 GHz FMCW-radar system for contactless monitoring of respiration and heart rate is investigated and compared to a standard monitoring system with ECG and CO2 measurements as reference. The novel FMCW-radar enables the detection of the physiological displacement of the skin surface with submillimeter accuracy. This high accuracy is achieved with a large bandwidth of 10 GHz and the combination of intermediate frequency and phase evaluation. This concept is validated with a radar system simulation and experimental measurements are performed with different radar sensor positions and orientations."}
{"_id":"3d8c7499221033e2d1201cd842d88dfb7fed9af8","title":"Optimizing F-measure: A Tale of Two Approaches","text":"F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario."}
{"_id":"e6b6a75ac509d30089b63451c2ed640f471af18a","title":"Diffusion maps for high-dimensional single-cell analysis of differentiation data","text":"MOTIVATION\nSingle-cell technologies have recently gained popularity in cellular differentiation studies regarding their ability to resolve potential heterogeneities in cell populations. Analyzing such high-dimensional single-cell data has its own statistical and computational challenges. Popular multivariate approaches are based on data normalization, followed by dimension reduction and clustering to identify subgroups. However, in the case of cellular differentiation, we would not expect clear clusters to be present but instead expect the cells to follow continuous branching lineages.\n\n\nRESULTS\nHere, we propose the use of diffusion maps to deal with the problem of defining differentiation trajectories. We adapt this method to single-cell data by adequate choice of kernel width and inclusion of uncertainties or missing measurement values, which enables the establishment of a pseudotemporal ordering of single cells in a high-dimensional gene expression space. We expect this output to reflect cell differentiation trajectories, where the data originates from intrinsic diffusion-like dynamics. Starting from a pluripotent stage, cells move smoothly within the transcriptional landscape towards more differentiated states with some stochasticity along their path. We demonstrate the robustness of our method with respect to extrinsic noise (e.g. measurement noise) and sampling density heterogeneities on simulated toy data as well as two single-cell quantitative polymerase chain reaction datasets (i.e. mouse haematopoietic stem cells and mouse embryonic stem cells) and an RNA-Seq data of human pre-implantation embryos. We show that diffusion maps perform considerably better than Principal Component Analysis and are advantageous over other techniques for non-linear dimension reduction such as t-distributed Stochastic Neighbour Embedding for preserving the global structures and pseudotemporal ordering of cells.\n\n\nAVAILABILITY AND IMPLEMENTATION\nThe Matlab implementation of diffusion maps for single-cell data is available at https:\/\/www.helmholtz-muenchen.de\/icb\/single-cell-diffusion-map.\n\n\nCONTACT\nfbuettner.phys@gmail.com, fabian.theis@helmholtz-muenchen.de\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online."}
{"_id":"848c717ba51e48afef714dfef4bd6ab1cc050dab","title":"Algorithms for the Assignment and Transiortation Troblems*","text":"In this paper we presen algorithms for the solution of the general assignment and transportation problems. In Section 1, a statement of the algorithm for the assignment problem appears, along with a proof for the correctness of the algorithm. The remarks which constitute the proof are incorporated parenthetically into the statement of the algorithm. Following this appears a discussion of certain theoretical aspects of the problem. In Section 2, the algorithm is generalized to one for the transportation problem. The algorithm of that section is stated as concisely as possible, with theoretical remarks omitted."}
{"_id":"58bb338cab40d084321f6d1dd7f4512896249566","title":"Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices: Design Considerations","text":"In recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We evaluate the effect of various RPU device features\/non-idealities and system parameters on performance in order to derive the device and system level specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30, 000 \u00d7 compared to state-of-the-art microprocessors while providing power efficiency of 84, 000 GigaOps\u2215s\u2215W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisting of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration, and analysis of multimodal sensory data flows from a massive number of IoT (Internet of Things) sensors."}
{"_id":"09bdbdf186e24db2cf11c4c1006c718478c10b3f","title":"On-line cursive script recognition using time-delay neural networks and hidden Markov models","text":"We present a writer-independent system for online handwriting recognition that can handle a variety of writing styles including cursive script and handprinting. The input to our system contains the pen trajectory information, encoded as a time-ordered sequence of feature vectors. A time-delay neural network is used to estimate a posteriori probabilities for characters in a word. A hidden Markov model segments the word in a way that optimizes the global word score, using a dictionary in the process. A geometrical normalization scheme and a fast but efficient dictionary search are also presented. Trained on 20 k words from 59 writers, using a 25 k-word dictionary, our system reached recognition rates of 89% for characters and 80% for words on test data from a disjoint set of writers."}
{"_id":"a695c1493aab6a4aa0a22491e5c85d5bdea90cf0","title":"Force of Habit and Information Systems Usage: Theory and Initial Validation","text":"Over the last two decades, information systems (IS) research has primarily focused on people\u2019s conscious (intentional) behavior when trying to explain and predict IS usage. Consequently, almost no research has investigated the potential importance of subconscious (automatic) behaviors, also known as habits. This study represents a first step toward validating the idea that one can add explanatory power to a behavioral model such as Ajzen\u2019s [1985] theory of planned behavior (TPB) by including the habit construct. We conducted a two-stage questionnaire-based survey involving two different groups of students who had access to a sophisticated internet-based communication tool (IBCT). These data were used to test a behavioral model integrating theoretical constructs of TPB and a relevant subset of Triandis\u2019 [1980] behavioral framework. Our findings highlight the importance of considering both conscious (intentions) and subconscious (habits) factors in explaining usage behavior. Furthermore, we share our observations about antecedents of IBCT usage in the educational context. Implications for practice and research are discussed."}
{"_id":"897343e1a63761c7dd1b156f2d92ac07e6cd1fa7","title":"Interacting with in-vehicle systems: understanding, measuring, and evaluating attention","text":"In-vehicle systems research is becoming a significant field as the market for in-vehicle systems continue to grow. As a consequence, researchers are increasingly concerned with opportunities and limitations of HCI in a moving vehicle. Especially aspects of attention constitute a challenge for in-vehicle systems development. This paper seeks to remedy this by defining and exemplifying attention understandings. 100 papers were classified in a two-fold perspective; under what settings are in-vehicle systems evaluated and how is driver attention measured in regard to in-vehicle systems HCI. A breakdown of the distribution of driving settings and measures is presented and the impact of driver attention is discussed. The classification revealed that most of the studies were conducted in driving simulators and real traffic driving, while lateral and longitudinal control and eye behaviour were the most used measures. Author"}
{"_id":"c81d111aa42569aadd48eddef0f5d293067e711d","title":"Chemi-net: a graph convolutional network for accurate drug property prediction","text":"Absorption, distribution, metabolism, and excretion (ADME) studies are critical for drug discovery. Conventionally, these tasks, together with other chemical property predictions, rely on domain-specific feature descriptors, or fingerprints. Following the recent success of neural networks, we developed Chemi-Net, a completely data-driven, domain knowledge-free, deep learning method for ADME property prediction. To compare the relative performance of Chemi-Net with Cubist, one of the popular machine learning programs used by Amgen, a large-scale ADME property prediction study was performed on-site at Amgen. The results showed that our deep neural network method improved current methods by a large margin. We foresee that the significantly increased accuracy of ADME prediction seen with Chemi-Net over Cubist will greatly accelerate drug discovery."}
{"_id":"4573ec8eeb5ed4a528c6044744e48d6c2c8c7abb","title":"Threatening faces and social anxiety: a literature review.","text":"A threatening facial expression is a potent social sign of hostility or dominance. During the past 20 years, photographs of threatening faces have been increasingly included as stimuli in studies with socially anxious participants, based on the hypothesis that a threatening face is especially salient to people with fears of social interaction or negative evaluation. The purpose of this literature review is to systematically evaluate the accumulated research and suggest possible avenues for further research. The main conclusion is that photographs of threatening faces engage a broad range of perceptual processes in socially anxious participants, particularly when exposure times are very short."}
{"_id":"c9293af4e528e6d20e61f21064a44338ed3b70dd","title":"A 1 . 5V , 10-bit , 14 . 3-MS \/ s CMOS Pipeline Analog-to-Digital Converter","text":"A 1.5-V, 10-bit, 14.3-MS\/s pipeline analog-to-digital converter was implemented in a 0.6m CMOS technology. Emphasis was placed on observing device reliability constraints at low voltage. MOS switches were implemented without lowthreshold devices by using a bootstrapping technique that does not subject the devices to large terminal voltages. The converter achieved a peak signal-to-noise-and-distortion ratio of 58.5 dB, maximum differential nonlinearity of 0.5 least significant bit (LSB), maximum integral nonlinearity of 0.7 LSB, and a power consumption of 36 mW."}
{"_id":"27fca2a7c2d565be983b4a786194e92bdc3d6ac7","title":"Remote Sensing Image Classification Based on Stacked Denoising Autoencoder","text":"Focused on the issue that conventional remote sensing image classification methods have run into the bottlenecks in accuracy, a new remote sensing image classification method inspired by deep learning is proposed, which is based on Stacked Denoising Autoencoder. First, the deep network model is built through the stacked layers of Denoising Autoencoder. Then, with noised input, the unsupervised Greedy layer-wise training algorithm is used to train each layer in turn for more robust expressing, characteristics are obtained in supervised learning by Back Propagation (BP) neural network, and the whole network is optimized by error back propagation. Finally, Gaofen-1 satellite (GF-1) remote sensing data are used for evaluation, and the total accuracy and kappa accuracy reach 95.7% and 0.955, respectively, which are higher than that of the Support Vector Machine and Back Propagation neural network. The experiment results show that the proposed method can effectively improve the accuracy of remote sensing image classification."}
{"_id":"c4f08b29fe95183395f7fca6a85bbdf8c2b605f1","title":"A wearable fingertip haptic device with 3 DoF asymmetric 3-RSR kinematics","text":"A novel wearable haptic device for modulating skin stretch at the fingertip is presented. Rendering of skin stretch in 3 degrees of freedom (DoF), with contact - no contact capabilities, was implemented through rigid parallel kinematics. The novel asymmetrical three revolute-spherical-revolute (3-RSR) configuration allowed compact dimensions with minimum encumbrance of the hand workspace and minimum inter-finger interference. A differential method for solving the non-trivial inverse kinematics is proposed and implemented in real time for controlling the position of the skin tactor. Experiments involving the grasping of a virtual object were conducted using two devices (thumb and index fingers) in a group of 4 subjects: results showed that participants performed the grasping task more precisely and with grasping forces closer to the expected natural behavior when the proposed device provided haptic feedback."}
{"_id":"d589265e932f2d410fdd2ac5fa6c80e3649c49c8","title":"Deformable Classifiers","text":"Geometric variations of objects, which do not modify the object class, pose a major challenge for object recognition. These variations could be rigid as well as non-rigid transformations. In this paper, we design a framework for training deformable classifiers, where latent transformation variables are introduced, and a transformation of the object image to a reference instantiation is computed in terms of the classifier output, separately for each class. The classifier outputs for each class, after transformation, are compared to yield the final decision. As a by-product of the classification this yields a transformation of the input object to a reference pose, which can be used for downstream tasks such as the computation of object support. We apply a two-step training mechanism for our framework, which alternates between optimizing over the latent transformation variables and the classifier parameters to minimize the loss function. We show that multilayer perceptrons, also known as deep networks, are well suited for this approach and achieve state of the art results on the rotated MNIST and the Google Earth dataset, and produce competitive results on MNIST and CIFAR-10 when training on smaller subsets of training data."}
{"_id":"1531a0fddacbe55b389278abd029fb311acb1b5b","title":"Evolutionary Delivery versus the \"waterfall model\"","text":"The conventional wisdom of planning software engineering projects, using the widely cited \"waterfall model\" is not the only useful software development process model. In fact, the \"waterfall model\" may be unrealistic, and dangerous to the primary objectives of any software project.The alternative model, which I choose to call \"evolutionary delivery\" is not widely taught or practiced yet. But there is already more than a decade of practical experience in using it. In various forms. It is quite clear from these experiences that evolutionary delivery is a powerful general tool for both software development and associated systems development.Almost all experienced software developers do make use of some of the ideas in evolutionary development at one time or another. But, this is often unplanned, informal and it is an incomplete exploitation of this powerful method. This paper will try to expose the theoretical and practical aspects of the method in a fuller perspective. We need to learn the theory fully, so that we can apply and learn it completely."}
{"_id":"9f0f4ea0ec0701d0ff2738f0719ee52589cc3717","title":"Deep Generative Models with Learnable Knowledge Constraints","text":"The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is modelagnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models."}
{"_id":"412b3ef02c85087e5f1721176114672c722b17a4","title":"A Taxonomy of Deep Convolutional Neural Nets for Computer Vision","text":"Traditional architectures for solving computer vision problems and the degree of success they enjoyed have been heavily reliant on hand-crafted features. However, of late, deep learning techniques have offered a compelling alternative \u2013 that of automatically learning problem-specific features. With this new paradigm, every problem in computer vision is now being re-examined from a deep learning perspective. Therefore, it has become important to understand what kind of deep networks are suitable for a given problem. Although general surveys of this fast-moving paradigm (i.e., deep-networks) exist, a survey specific to computer vision is missing. We specifically consider one form of deep networks widely used in computer vision \u2013 convolutional neural networks (CNNs). We start with \u201cAlexNet\u201d as our base CNN and then examine the broad variations proposed over time to suit different applications. We hope that our recipe-style survey will serve as a guide, particularly for novice practitioners intending to use deep-learning techniques for computer vision."}
{"_id":"fbc27038b9c111dad2851397047c6230ece79c23","title":"Glaucoma-Deep : Detection of Glaucoma Eye Disease on Retinal Fundus Images using Deep Learning Detection of Glaucoma by Abbas","text":"Detection of glaucoma eye disease is still a challenging task for computer-aided diagnostics (CADx) systems. During eye screening process, the ophthalmologists measures the glaucoma by structure changes in optic disc (OD), loss of nerve fibres (LNF) and atrophy of the peripapillary region (APR). In retinal images, the automated CADx systems are developed to assess this eye disease through segmentation-based hand-crafted features. Therefore in this paper, the convolutional neural network (CNN) unsupervised architecture was used to extract the features through multilayer from raw pixel intensities. Afterwards, the deep-belief network (DBN) model was used to select the most discriminative deep features based on the annotated training dataset. At last, the final decision is performed by softmax linear classifier to differentiate between glaucoma and non-glaucoma retinal fundus image. This proposed system is known as Glaucoma-Deep and tested on 1200 retinal images obtained from publically and privately available datasets. To evaluate the performance of Glaucoma-Deep system, the sensitivity (SE), specificity (SP), accuracy (ACC), and precision (PRC) statistical measures were utilized. On average, the SE of 84.50%, SP of 98.01%, ACC of 99% and PRC of 84% values were achieved. Comparing to state-of-the-art systems, the Nodular-Deep system accomplished significant higher results. Consequently, the Glaucoma-Deep system can easily recognize the glaucoma eye disease to solve the problem of clinical experts during eye-screening process on large-scale environments. Keywords\u2014Fundus imaging; glaucoma; diabetic retinopathy; deep learning; convolutional neural networks; deep belief network"}
{"_id":"7dee8be2a8eccb892253a94d2fcaa0aa9971cc54","title":"On Laser Ranging Based on High-Speed\/Energy Laser Diode Pulses and Single-Photon Detection Techniques","text":"This paper discusses the construction principles and performance of a pulsed time-of-flight (TOF) laser radar based on high-speed (FWHM ~100 ps) and high-energy (~1 nJ) optical transmitter pulses produced with a specific laser diode working in an \u201cenhanced gain-switching\u201d regime and based on single-photon detection in the receiver. It is shown by analysis and experiments that single-shot precision at the level of 2W3 cm is achievable. The effective measurement rate can exceed 10 kHz to a noncooperative target (20% reflectivity) at a distance of > 50 m, with an effective receiver aperture size of 2.5 cm2. The effect of background illumination is analyzed. It is shown that the gating of the SPAD detector is an effective means to avoid the blocking of the receiver in a high-level background illumination case. A brief comparison with pulsed TOF laser radars employing linear detection techniques is also made."}
{"_id":"0f7fb0e9bd0e0e16fcfbea8c6667c9d8c13ebf72","title":"Detecting Structural Similarities between XML Documents","text":"In this paper we propose a technique for detecting the similarity in the structure of XML documents. The technique is based on the idea of representing the structure of an XML document as a time series in which each occurrence of a tag corresponds to a given impulse. By analyzing the frequencies of the corresponding Fourier transform, we can hence state the degree of similarity between documents. The efficiency and effectiveness of this approach are compelling when compared with traditional ones."}
{"_id":"85aeaa9bd5bd55082d19e211a379a17d0de4cb4c","title":"Translating Learning into Numbers: A Generic Framework for Learning Analytics","text":"With the increase in available educational data, it is expected that Learning Analytics will become a powerful means to inform and support learners, teachers and their institutions in better understanding and predicting personal learning needs and performance. However, the processes and requirements behind the beneficial application of Learning and Knowledge Analytics as well as the consequences for learning and teaching are still far from being understood. In this paper, we explore the key dimensions of Learning Analytics (LA), the critical problem zones, and some potential dangers to the beneficial exploitation of educational data. We propose and discuss a generic design framework that can act as a useful guide for setting up Learning Analytics services in support of educational practice and learner guidance, in quality assurance, curriculum development, and in improving teacher effectiveness and efficiency. Furthermore, the presented article intends to inform about soft barriers and limitations of Learning Analytics. We identify the required skills and competences that make meaningful use of Learning Analytics data possible to overcome gaps in interpretation literacy among educational stakeholders. We also discuss privacy and ethical issues and suggest ways in which these issues can be addressed through policy guidelines and best practice examples."}
{"_id":"2a99978d060b84a3397d130808cc7d32c50ce1ff","title":"Trust-aware crowdsourcing with domain knowledge","text":"The rise of social network and crowdsourcing platforms makes it convenient to take advantage of the collective intelligence to estimate true labels of questions of interest. However, input from workers is often noisy and even malicious. Trust is used to model workers in order to better estimate true labels of questions. We observe that questions are often not independent in real life applications. Instead, there are logical relations between them. Similarly, workers that provide answers are not independent of each other either. Answers given by workers with similar attributes tend to be correlated. Therefore, we propose a novel unified graphical model consisting of two layers. The top layer encodes domain knowledge which allows users to express logical relations using first-order logic rules and the bottom layer encodes a traditional crowdsourcing graphical model. Our model can be seen as a generalized probabilistic soft logic framework that encodes both logical relations and probabilistic dependencies. To solve the collective inference problem efficiently, we have devised a scalable joint inference algorithm based on the alternating direction method of multipliers. Finally, we demonstrate that our model is superior to state-of-the-art by testing it on multiple real-world datasets."}
{"_id":"a1a1c4fb58a2bc056a056795609a2be307b6b9bf","title":"GORAM \u2013 Group ORAM for Privacy and Access Control in Outsourced Personal Records 1","text":"Cloud storage has rapidly become a cornerstone of many IT infrastructures, constituting a seamless solution for the backup, synchronization, and sharing of large amounts of data. Putting user data in the direct control of cloud service providers, however, raises security and privacy concerns related to the integrity of outsourced data, the accidental or intentional leakage of sensitive information, the profiling of user activities and so on. Furthermore, even if the cloud provider is trusted, users having access to outsourced files might be malicious and misbehave. These concerns are particularly serious in sensitive applications like personal health records and credit score systems. To tackle this problem, we present GORAM, a cryptographic system that protects the secrecy and integrity of outsourced data with respect to both an untrusted server and malicious clients, guarantees the anonymity and unlinkability of accesses to such data, and allows the data owner to share outsourced data with other clients, selectively granting them read and write permissions. GORAM is the first system to achieve such a wide range of security and privacy properties for outsourced storage. In the process of designing an efficient construction, we developed two new, generally applicable cryptographic schemes, namely, batched zero-knowledge proofs of shuffle and an accountability technique based on chameleon signatures, which we consider of independent interest. We implemented GORAM in Amazon Elastic Compute Cloud (EC2) and ran a performance evaluation demonstrating the scalability and efficiency of our construction."}
{"_id":"4ad35e94ec1edf7de498bf5007c0fb90448ff0aa","title":"Position Control of a Permanent Magnet DC Motor by Model Reference Adaptive Control","text":"Model reference adaptive control (MRAC) is one of the various techniques of solving the control problem when the parameters of the controlled process are poorly known or vary during normal operation. To understand the dynamic behavior of a dc motor it is required to know its parameters; armature inductance and resistance (La, Ra), inertia of the rotor (Jm), motor constant (Km), friction coefficient (Bm), etc. To identify these parameters, some experiments should be performed. However, motor parameters change under operation according to several conditions. Therefore, the performance of controller, which has been designed considering constant motor parameters becomes poorer. For this reason, a model reference adaptive control method is proposed to control the position of a dc motor without requiring fixed motor parameters. Experimental results show how well this method controls the position of a permanent magnet dc motor."}
{"_id":"c86c3c47beec8a1db3b4d1c28f71502a6091f059","title":"Solutions Strategies for Die Shift Problem in Wafer Level Compression Molding","text":"Die shift problem that arises during the wafer molding process in embedded micro wafer level package fabrication was systematically analyzed and solution strategies were developed. A methodology to measure die shift was developed and applied to create maps of die shift on an 8 inch wafer. A total of 256 dies were embedded in an 8 inch mold compound wafer using compression molding. Thermal and cure shrinkages of mold compound are determined to be the primary reasons for die shift in wafer molding. Die shift value increases as the distance from the center of the wafer increases. Pre-compensation of die shift during pick and place is demonstrated as an effective method to control die shift. Applying pre-compensation method 99% of dies can be achieved to have die shift values of less than 40 \u03bcm. Usage of carrier wafer during wafer molding reduces the maximum die shift in a wafer from 633 \u03bcm to 79 \u03bcm. Die area\/package area ratio has a strong influence on the die shift values. Die area\/package area ratios of 0.81, 0.49, and 0.25 lead to maximum die shift values of 26, 76, and 97 \u03bc.m, respectively. Wafer molding using low coefficient of thermal expansion (7 \u00d7 10-6\/\u00b0C) and low cure shrinkage (0.094%) mold compounds is demonstrated to yield maximum die shift value of 28 \u03bcm over the whole 8 inch wafer area."}
{"_id":"60611f3395ba44d0a386d6eebdd4d0a7277d5df0","title":"Gender differences in participation and reward on Stack Overflow","text":"Programming is a valuable skill in the labor market, making the underrepresentation of women in computing an increasingly important issue. Online question and answer platforms serve a dual purpose in this field: they form a body of knowledge useful as a reference and learning tool, and they provide opportunities for individuals to demonstrate credible, verifiable expertise. Issues, such as male-oriented site design or overrepresentation of men among the site\u2019s elite may therefore compound the issue of women\u2019s underrepresentation in IT. In this paper we audit the differences in behavior and outcomes between men and women on Stack Overflow, the most popular of these Q&A sites. We observe significant differences in how men and women participate in the platform and how successful they are. For example, the average woman has roughly half of the reputation points, the primary measure of success on the site, of the average man. Using an Oaxaca-Blinder decomposition, an econometric technique commonly applied to analyze differences in wages between groups, we find that most of the gap in success between men and women can be explained by differences in their activity on the site and differences in how these activities are rewarded. Specifically, 1) men give more answers than women and 2) are rewarded more for their answers on average, even when controlling for possible confounders such as tenure or buy-in to the site. Women ask more questions and gain more reward per question. We conclude with a hypothetical redesign of the site\u2019s scoring system based on these behavioral differences, cutting the reputation gap in half."}
{"_id":"0ed269ce5462a4a0ba22a1270d7810c7b3374a09","title":"Selected techniques for data mining in medicine","text":"Widespread use of medical information systems and explosive growth of medical databases require traditional manual data analysis to be coupled with methods for efficient computer-assisted analysis. This paper presents selected data mining techniques that can be applied in medicine, and in particular some machine learning techniques including the mechanisms that make them better suited for the analysis of medical databases (derivation of symbolic rules, use of background knowledge, sensitivity and specificity of induced descriptions). The importance of the interpretability of results of data analysis is discussed and illustrated on selected medical applications."}
{"_id":"00af4fba4bc85262d381881848c3ad67536fcb6b","title":"A Multivariate Timeseries Modeling Approach to Severity of Illness Assessment and Forecasting in ICU with Sparse, Heterogeneous Clinical Data","text":"The ability to determine patient acuity (or severity of illness) has immediate practical use for clinicians. We evaluate the use of multivariate timeseries modeling with the multi-task Gaussian process (GP) models using noisy, incomplete, sparse, heterogeneous and unevenly-sampled clinical data, including both physiological signals and clinical notes. The learned multi-task GP (MTGP) hyperparameters are then used to assess and forecast patient acuity. Experiments were conducted with two real clinical data sets acquired from ICU patients: firstly, estimating cerebrovascular pressure reactivity, an important indicator of secondary damage for traumatic brain injury patients, by learning the interactions between intracranial pressure and mean arterial blood pressure signals, and secondly, mortality prediction using clinical progress notes. In both cases, MTGPs provided improved results: an MTGP model provided better results than single-task GP models for signal interpolation and forecasting (0.91 vs 0.69 RMSE), and the use of MTGP hyperparameters obtained improved results when used as additional classification features (0.812 vs 0.788 AUC)."}
{"_id":"23c3953fb45536c9129e86ac7a23098bd9f1381d","title":"Machine Learning for Sequential Data: A Review","text":"Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues."}
{"_id":"604a82697d874c4da2aa07797c4b9f24c3dd272a","title":"Lung cancer cell identification based on artificial neural network ensembles","text":"An artificial neural network ensemble is a learning paradigm where several artificial neural networks are jointly used to solve a problem. In this paper, an automatic pathological diagnosis procedure named Neural Ensemble-based Detection (NED) is proposed, which utilizes an artificial neural network ensemble to identify lung cancer cells in the images of the specimens of needle biopsies obtained from the bodies of the subjects to be diagnosed. The ensemble is built on a two-level ensemble architecture. The first-level ensemble is used to judge whether a cell is normal with high confidence where each individual network has only two outputs respectively normal cell or cancer cell. The predictions of those individual networks are combined by a novel method presented in this paper, i.e. full voting which judges a cell to be normal only when all the individual networks judge it is normal. The second-level ensemble is used to deal with the cells that are judged as cancer cells by the first-level ensemble, where each individual network has five outputs respectively adenocarcinoma, squamous cell carcinoma, small cell carcinoma, large cell carcinoma, and normal, among which the former four are different types of lung cancer cells. The predictions of those individual networks are combined by a prevailing method, i.e. plurality voting. Through adopting those techniques, NED achieves not only a high rate of overall identification, but also a low rate of false negative identification, i.e. a low rate of judging cancer cells to be normal ones, which is important in saving lives due to reducing missing diagnoses of cancer patients."}
{"_id":"1a16866c9fce54eb9c21b7730a42f1f17372907f","title":"Towards Online, Accurate, and Scalable QoS Prediction for Runtime Service Adaptation","text":"Service-based cloud applications are typically built on component services to fulfill certain application logic. To meet quality-of-service (QoS) guarantees, these applications have to become resilient against the QoS variations of their component services. Runtime service adaptation has been recognized as a key solution to achieve this goal. To make timely and accurate adaptation decisions, effective QoS prediction is desired to obtain the QoS values of component services. However, current research has focused mostly on QoS prediction of the working services that are being used by a cloud application, but little on QoS prediction of candidate services that are also important for making adaptation decisions. To bridge this gap, in this paper, we propose a novel QoS prediction approach, namely adaptive matrix factorization (AMF), which is inspired from the collaborative filtering model used in recommender systems. Specifically, our AMF approach extends conventional matrix factorization into an online, accurate, and scalable model by employing techniques of data transformation, online learning, and adaptive weights. Comprehensive experiments have been conducted based on a real-world large-scale QoS dataset of Web services to evaluate our approach. The evaluation results provide good demonstration for our approach in achieving accuracy, efficiency, and scalability."}
{"_id":"ec0c3f8206f879857b9aea6d553084058131e7c3","title":"Aromas of rosemary and lavender essential oils differentially affect cognition and mood in healthy adults.","text":"This study was designed to assess the olfactory impact of the essential oils of lavender (Lavandula angustifolia) and rosemary (Rosmarlnus officinalis) on cognitive performance and mood in healthy volunteers. One hundred and forty-four participants were randomly assigned to one of three independent groups, and subsequently performed the Cognitive Drug Research (CDR) computerized cognitive assessment battery in a cubicle containing either one of the two odors or no odor (control). Visual analogue mood questionnaires were completed prior to exposure to the odor, and subsequently after completion of the test battery. The participants were deceived as to the genuine aim of the study until the completion of testing to prevent expectancy effects from possibly influencing the data. The outcome variables from the nine tasks that constitute the CDR core battery feed into six factors that represent different aspects of cognitive functioning. Analysis of performance revealed that lavender produced a significant decrement in performance of working memory, and impaired reaction times for both memory and attention based tasks compared to controls. In contrast, rosemary produced a significant enhancement of performance for overall quality of memory and secondary memory factors, but also produced an impairment of speed of memory compared to controls. With regard to mood, comparisons of the change in ratings from baseline to post-test revealed that following the completion of the cognitive assessment battery, both the control and lavender groups were significantly less alert than the rosemary condition; however, the control group was significantly less content than both rosemary and lavender conditions. These findings indicate that the olfactory properties of these essential oils can produce objective effects on cognitive performance, as well as subjective effects on mood."}
{"_id":"a9b545a0bf2e305caff18fe94c6dad12342554a6","title":"Sentiment analysis of a document using deep learning approach and decision trees","text":"The given paper describes modern approach to the task of sentiment analysis of movie reviews by using deep learning recurrent neural networks and decision trees. These methods are based on statistical models, which are in a nutshell of machine learning algorithms. The fertile area of research is the application of Google's algorithm Word2Vec presented by Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean in 2013. The main idea of Word2Vec is the representations of words with the help of vectors in such manner that semantic relationships between words preserved as basic linear algebra operations. The extra advantage of the mentioned algorithm above the alternatives is computational efficiency. This paper focuses on using Word2Vec model for text classification by their sentiment type."}
{"_id":"76044a2bc14af5ab600908dcbc8ba8edfb58a673","title":"A Hardware Design Language for Efficient Control of Timing Channels","text":"Information security can be compromised by leakage via low-level hardware features. One recently prominent example is cache probing attacks, which rely on timing channels created by caches. We introduce a hardware design language, SecVerilog, which makes it possible to statically analyze information flow at the hardware level. With SecVerilog, systems can be built with verifiable control of timing channels and other information channels. SecVerilog is Verilog, extended with expressive type annotations that enable precise reasoning about information flow. It also comes with rigorous formal assurance: we prove that SecVerilog enforces timing-sensitive noninterference and thus ensures secure information flow. By building a secure MIPS processor and its caches, we demonstrate that SecVerilog makes it possible to build complex hardware designs with verified security, yet with low overhead in time, space, and HW designer effort."}
{"_id":"ffcfe390081aba6eb451a8465703415132cf8746","title":"A comparison of Deep Learning methods for environmental sound detection","text":"Environmental sound detection is a challenging application of machine learning because of the noisy nature of the signal, and the small amount of (labeled) data that is typically available. This work thus presents a comparison of several state-of-the-art Deep Learning models on the IEEE challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge task and data, classifying sounds into one of fifteen common indoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest path, library, train, etc. In total, 13 hours of stereo audio recordings are available, making this one of the largest datasets available."}
{"_id":"9682f17929a2a2f1ff9b812de37d23bdf40cb766","title":"A Tristate Rigid Reversible and Non-Back-Drivable Active Docking Mechanism for Modular Robotics","text":"This paper proposes a new active bonding mechanism that achieves rigid, reversible, and nonback-drivable coupling between modular mobile robots in a chain formation. The first merit of this interface lies in its ability to operate in three independent modes. In the drive mode, the motor torque is routed to drive the module. In the clamp mode, the motor torque is redirected toward an active joint that enables one module to rotate relative to its neighbors in the formation. In the neutral mode, the motor's rotation achieves alignment between the interface's components prior to the initiation of the drive and clamp modes. The second merit stems from the dual-rod slider rocker (DRSR) mechanism, which toggles between the interface's three modes of operation. The design details of the interface are presented, as well as the optimal kinematic synthesis and dynamic analysis of the DRSR mechanism. Simulation and experimental results validate the DRSR's unique kinematics, as well as the rigidity and the three operation modes of the docking interface."}
{"_id":"435277e20f62f2dcb40564a525cf7e73199eaaac","title":"Sound-model-based acoustic source localization using distributed microphone arrays","text":"Acoustic source localization and sound recognition are common acoustic scene analysis tasks that are usually considered separately. In this paper, a new source localization technique is proposed that works jointly with an acoustic event detection system. Given the identities and the end-points of simultaneous sounds, the proposed technique uses the statistical models of those sounds to compute a likelihood score for each model and for each signal at the output of a set of null-steering beamformers per microphone array. Those scores are subsequently combined to find the MAP-optimal event source positions in the room. Experimental work is reported for a scenario consisting of meeting-room acoustic events, either isolated or overlapped with speech. From the localization results, which are compared with those from the SRP-PHAT technique, it seems that the proposed model-based approach can be an alternative to current techniques for event-based localization."}
{"_id":"4a9b68db23584c3b2abfd751100e8b5b44630f9c","title":"Graph Convolutional Neural Networks for ADME Prediction in Drug Discovery","text":"ADME in-silico methods have grown increasingly powerful over the past twenty years, driven by advances in machine learning and the abundance of high-quality training data generated by laboratory automation. Meanwhile, in the technology industry, deep-learning has taken o\u21b5, driven by advances in topology design, computation, and data. The key premise of these methods is that the model is able to pass gradients back into the feature structure, engineering its own problem-specific representation for the data. Graph Convolutional Networks (GC-DNNs), a variation of neural fingerprints, allow for true deep-learning in the chemistry domain. We use this new approach to build human plasma protein binding, lipophilicty, and humans clearance models that significantly outperform random-forests and support-vector-regression."}
{"_id":"316293b426eb42853f30cd906f71fa6bc73dacce","title":"System Dynamics: Modeling, Simulation, and Control of Mechatronic Systems","text":"If your wanted solutions manual ins't on this list, also can ask me if _ is available. Theory of Applied Robotics: Kinematics, Dynamics and Control (Reza N. _ Jazar) System Dynamics: Modeling and Simulation of Mechatronic Systems (4th _. Solution Manual Dynamics of Mechanical Systems (C.T.F. Ross) Solution Manual Thermodynamics : An Integrated Learning System (Schmidt, Ezekoye, System Dynamics : Modeling, Simulation, and Control of Mechatronic Systems (5th."}
{"_id":"722ef716113443e2b13bd180dcfacf1c1f2054f2","title":"A Universal-Input Single-Stage High-Power-Factor Power Supply for HB-LEDs Based on Integrated Buck-Flyback Converter","text":"Due to the high rise in luminous efficiency that HB-LEDs have experienced in the last recent years, many new applications have been researched. In this paper, a streetlight LED application will be covered, using the Integrated Buck-Flyback Converter developed in previous works, which performs power factor correction (PFC) from a universal ac source, as well as a control loop using the LM3524 IC, which allows PWM dimming operation mode. Firstly, the LED load will be linearized and modeled in order to calculate the IBFC topology properly. Afterwards, the converter will be calculated, presenting the one built in the lab. Secondly, the converter will be modeled in order to build the closed loop system, modeling the current sensor as well in order to develop an adequate controller. Finally, experimental results obtained from the lab tests will be presented."}
{"_id":"0c23ebb3abf584fa5e0fde558584befc94fb5ea2","title":"Parsing Natural Scenes and Natural Language with Recursive Neural Networks","text":"Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-theart performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%."}
{"_id":"4cf4bdbf65d1db8fe22ac040591dba9622fce5b3","title":"Correcting the Document Layout: A Machine Learning Approach","text":"In this paper, a machine learning approach to support the user during the correction of the layout analysis is proposed. Layout analysis is the process of extracting a hierarchical structure describing the layout of a page. In our approach, the layout analysis is performed in two steps: firstly, the global analysis determines possible areas containing paragraphs, sections, columns, figures and tables, and secondly, the local analysis groups together blocks that possibly fall within the same area. The result of the local analysis process strongly depends on the quality of the results of the first step. We investigate the possibility of supporting the user during the correction of the results of the global analysis. This is done by allowing the user to correct the results of the global analysis and then by learning rules for layout correction from the sequence of user actions. Experimental results on a set of multi-page documents are reported and commented. 1. Background and motivation Strategies for the extraction of layout analysis have been traditionally classified as top-down or bottom-up [10]. In top-down methods, the document image is repeatedly decomposed into smaller and smaller components, while in bottom-up methods, basic layout components are extracted from bitmaps and then grouped together into larger blocks on the basis of their characteristics. In WISDOM++, a document image analysis system that can transform paper documents into XML format [1], the applied page decomposition method is hybrid, since it combines a top-down approach to segment the document image, and a bottom-up layout analysis method to assemble basic blocks into frames. Some attempts to learn the layout structure from a set of training examples have also been reported in the literature [2,3,4,7,11]. They are based on ad-hoc learning algorithms, which learn particular data structures, such as geometric trees and tree grammars. Results are promising, although it has been proven that good layout structures could also be obtained by exploiting generic knowledge on typographic conventions [5]. This is the case of WISDOM++, which analyzes the layout in two steps: 1. A global analysis, in order to determine possible areas containing paragraphs, sections, columns, figures and tables. This step is based on an iterative process, in which the vertical and horizontal histograms of text blocks are alternately analyzed, in order to detect columns and sections\/paragraphs, respectively. 2. A local analysis to group together blocks that possibly fall within the same area. Generic knowledge on west-style typesetting conventions is exploited to group blocks together, such as \u201cthe first line of a paragraph can be indented\u201d and \u201cin a justified text, the last line of a paragraph can be shorter than the previous one\u201d. Experimental results proved the effectiveness of this knowledge-based approach on images of the first page of papers published in conference proceedings and journals [1]. However, performance degenerates when the system is tested on intermediate pages of multi-page articles, where the structure is much more variable, due to the presence of formulae, images, and drawings that can stretch over more than one column, or are quite close. The majority of errors made by the layout analysis module were in the global analysis step, while the local analysis step performed satisfactorily when the result of the global analysis was correct. In this paper, we investigate the possibility of supporting the user during the correction of the results of the global analysis. This is done by allowing the user to correct the results of the global analysis and then by learning rules for layout correction from his\/her sequence of actions. This approach is different from those that learn the layout structure from scratch, since we try to correct the result of a global analysis returned by a bottom-up algorithm. Furthermore, we intend to capture knowledge on correcting actions performed by the user of the document image processing system. Other document processing systems allow users to correct the result of the layout analysis; nevertheless WISDOM++ is the only one that tries to learn correcting actions from user interaction with the system. Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003) 0-7695-1960-1\/03 $17.00 \u00a9 2003 IEEE In the following section, we describe the layout correction operations. The automated generation of training examples is explained in Section 3. Section 4 introduces the learning strategy, while Section 5 presents some experimental results. 2. Correcting the layout Global analysis aims at determining the general layout structure of a page and operates on a tree-based representation of nested columns and sections. The levels of columns and sections are alternated (Figure 1), which means that a column contains sections, while a section contains columns. At the end of the global analysis, the user can only see the sections and columns that have been considered atomic, that is, not subject to further decomposition (Figure 2). The user can correct this result by means of three different operations: \u2022 Horizontal splitting: a column\/section is cut horizontally. \u2022 Vertical splitting: a column\/section is cut vertically. \u2022 Grouping: two sections\/columns are merged together. The cut point in the two splitting operations is automatically determined by computing either the horizontal or the vertical histogram on the basic blocks returned by the segmentation algorithm. The horizontal (vertical) cut point corresponds to the largest gap between two consecutive bins in the horizontal (vertical) histogram. Therefore, splitting operations can be described by means of a unary function, split(X), where X represents the column\/section to be split and the range is the set {horizontal, vertical, no_split}. The grouping operation, which can be described by means of a binary predicate group(A,B), is applicable to two sections (columns) A and B and returns a new section (column) C, whose boundary is determined as follows. Let (leftX, topX) and (bottomX, rightX) be the coordinates of the top-left and bottom-right vertices of a column\/section X, respectively. Then: leftC= min(leftA, leftB), rightC=max(rightA,rightB), topC=min(topA,topB), bottomC=max(bottomA,bottomB). Grouping is possible only if the following two conditions are satisfied: 1. C does not overlap another section (column) in the document. 2. A and B are nested in the same column (section). After each splitting\/grouping operation, WISDOM++ recomputes the result of the local analysis process, so that the user can immediately perceive the final effect of the requested correction and can decide whether to confirm the correction or not. 3. Representing corrections From the user interaction, WISDOM++ implicitly generates some training observations describing when and how the user intended to correct the result of the global analysis. These training observations are used to learn correction rules of the result of the global analysis, as explained in the next section. The simplest representation describes, for each training observation, the page layout at the i-th correction step and the correcting operation performed by the user on that layout. Therefore, if the user performs n-1 correcting operations, n observations are generated. The last one corresponds to the page layout accepted by the user. In the learning phase, this representation may lead the system to generate rules which strictly take into account the exact user correction sequence. However, several alternative correction sequences, which lead to the same result, may be also possible. If they are not considered, the learning strategy will suffer from data overfitting problems. This issue was already discussed in a preliminary work [9]. A more sophisticated representation, which takes into account alternative correction sequences, is based on the Column level"}
{"_id":"a48cf75ec1105e72035c0b4969423dedb625c5bf","title":"Building Lifecycle Management System for Enhanced Closed Loop Collaboration","text":"In the past few years, the architecture, engineering and construction (AEC) industry has carried out efforts to develop BIM (Building Information Modelling) facilitating tools and standards for enhanced collaborative working and information sharing. Lessons learnt from other industries and tools such as PLM (Product Lifecycle Management) \u2013 established tool in manufacturing to manage the engineering change process \u2013 revealed interesting potential to manage more efficiently the building design and construction processes. Nonetheless, one of the remaining challenges consists in closing the information loop between multiple building lifecycle phases, e.g. by capturing information from middle-of-life processes (i.e., use and maintenance) to re-use it in end-of-life processes (e.g., to guide disposal decision making). Our research addresses this lack of closed-loop system in the AEC industry by proposing an open and interoperable Web-based building lifecycle management system. This paper gives (i) an overview of the requirement engineering process that has been set up to integrate efforts, standards and directives of both the AEC and PLM industries, and (ii) first proofs-of-concept of our system implemented on two distinct campus."}
{"_id":"dda8f969391d71dd88a4546c06f37a6c1bcd3412","title":"Bridge Resonant Inverter For Induction Heating Applications","text":"Induction heating is a well-known technique to produce very high temperature for applications. A large number of topologies have been developed in this area such as voltage and current source inverter. Recent developments in switching schemes and control methods have made the voltage-source resonant inverters widely used in several applications that require output power control. The series-resonant inverter needs an output transformer for matching the output power to the load but it carry high current as a result additional real power loss is occur and overall efficiency also is reduced. This project proposes a high efficiency LLC resonant inverter for induction heating applications by using asymmetrical voltage cancellation control .The proposed control method is implemented in a full-bridge topology for induction heating application. The output power is controlled using the asymmetrical voltage cancellation technique. The LLC resonant tank is designed without the use of output transformer. This results in an increase of the net efficiency of the induction heating system. The circuit is simulated using MATLAB .The circuit is implemented using PIC controller. Both simulation and hardware results are compared."}
{"_id":"c98bd092ebb68787da64e95202f922a0ed7fc588","title":"Development and Initial Validation of a Multidimensional Scale Assessing Subjective Well-Being: The Well-Being Scale (WeBS).","text":"Numerous scales currently exist that assess well-being, but research on measures of well-being is still advancing. Conceptualization and measurement of subjective well-being have emphasized intrapsychic over psychosocial domains of optimal functioning, and disparate research on hedonic, eudaimonic, and psychological well-being lacks a unifying theoretical model. Lack of systematic investigations on the impact of culture on subjective well-being has also limited advancement of this field. The goals of this investigation were to (1) develop and validate a self-report measure, the Well-Being Scale (WeBS), that simultaneously assesses overall well-being and physical, financial, social, hedonic, and eudaimonic domains of this construct; (2) evaluate factor structures that underlie subjective well-being; and (3) examine the measure's psychometric properties. Three empirical studies were conducted to develop and validate the 29-item scale. The WeBS demonstrated an adequate five-factor structure in an exploratory structural equation model in Study 1. Confirmatory factor analyses showed that a bifactor structure best fit the WeBS data in Study 2 and Study 3. Overall WeBS scores and five domain-specific subscale scores demonstrated adequate to excellent internal consistency reliability and construct validity. Mean differences in overall well-being and its five subdomains are presented for different ethnic groups. The WeBS is a reliable and valid measure of multiple aspects of well-being that are considered important to different ethnocultural groups."}
{"_id":"36675de101e2eb228727692e969a909d49fbbee2","title":"Adoption of Mobile Internet Services: An Exploratory Study of Mobile Commerce Early Adopters","text":"Taylor & Francis makes every effort to ensure the accuracy of all the information (the \u201cContent\u201d) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content."}
{"_id":"44c3dac2957f379e7646986f593b9a7db59bd714","title":"Literary Fiction Improves Theory of Mind","text":""}
{"_id":"0d96ac48e92b6b42737276a319f48d9d27080fce","title":"EvalAI: Towards Better Evaluation Systems for AI Agents","text":"We introduce EvalAI, an open source platform for evaluating and comparing machine learning (ML) and artificial intelligence algorithms (AI) at scale. EvalAI is built to provide a scalable solution to the research community to fulfill the critical need of evaluating machine learning models and agents acting in an environment against annotations or with a human-in-the-loop. This will help researchers, students, and data scientists to create, collaborate, and participate in AI challenges organized around the globe. By simplifying and standardizing the process of benchmarking these models, EvalAI seeks to lower the barrier to entry for participating in the global scientific effort to push the frontiers of machine learning and artificial intelligence, thereby increasing the rate of measurable progress in this domain. Our code is available here."}
{"_id":"5a4bb08d4750d27bd5a2ad0a993d144c4fb9586c","title":"Hype and Heavy Tails: A Closer Look at Data Breaches","text":"Recent widely publicized data breaches have exposed the personal information of hundreds of millions of people. Some reports point to alarming increases in both the size and frequency of data breaches, spurring institutions around the world to address what appears to be a worsening situation. But, is the problem actually growing worse? In this paper, we study a popular public dataset and develop Bayesian Generalized Linear Models to investigate trends in data breaches. Analysis of the model shows that neither size nor frequency of data breaches has increased over the past decade. We find that the increases that have attracted attention can be explained by the heavy-tailed statistical distributions underlying the dataset. Specifically, we find that data breach size is log-normally distributed and that the daily frequency of breaches is described by a negative binomial distribution. These distributions may provide clues to the generative mechanisms that are responsible for the breaches. Additionally, our model predicts the likelihood of breaches of a particular size in the future. For example, we find that in the next year there is only a 31% chance of a breach of 10 million records or more in the US. Regardless of any trend, data breaches are costly, and we combine the model with two different cost models to project that in the next three years breaches could cost up to $55 billion."}
{"_id":"70624f16968fe4f0c851398dbd46a1ebcce892ce","title":"An efficient message access quality model in vehicular communication networks","text":"In vehicular ad hoc network (VANET), vehicles equipped with computing, sensing, and communication capabilities can exchange information within a geographical area to distribute emergency messages and achieve safety system. Then how to enforce fine grained control of these messages and ensure the receiving messages coming from the claimed source in such a highly dynamic environments remains a key challenge that affects the quality of service. In this paper, we propose a hierarchical access control with authentication scheme for transmitted messages with security assurance over VANET. By extending ciphertext-policy attribute-based encryption (CP-ABE) with a hierarchical structure of multiple authorities, the scheme not only achieves scalability due to its hierarchical structure, but also inherits fine-grained access control on the transmitted messages. Also by exploiting attribute-based signature (ABS), the scheme can authorize the vehicles that can most appropriately deal with the message efficiently. The results of efficiency analysis and comparison with the related works show that the proposed scheme is efficient and scalable in dealing with access control and message authentication for data dissemination in VANET. & 2014 Elsevier B.V. All rights reserved."}
{"_id":"6c267aafaceb9de1f1af22f6f566ba1d897054e4","title":"Capture Point: A Step toward Humanoid Push Recovery","text":"It is known that for a large magnitude push a human or a humanoid robot must take a step to avoid a fall. Despite some scattered results, a principled approach towards \"when and where to take a step\" has not yet emerged. Towards this goal, we present methods for computing capture points and the capture region, the region on the ground where a humanoid must step to in order to come to a complete stop. The intersection between the capture region and the base of support determines which strategy the robot should adopt to successfully stop in a given situation. Computing the capture region for a humanoid, in general, is very difficult. However, with simple models of walking, computation of the capture region is simplified. We extend the well-known linear inverted pendulum model to include a flywheel body and show how to compute exact solutions of the capture region for this model. Adding rotational inertia enables the humanoid to control its centroidal angular momentum, much like the way human beings do, significantly enlarging the capture region. We present simulations of a simple planar biped that can recover balance after a push by stepping to the capture region and using internal angular momentum. Ongoing work involves applying the solution from the simple model as an approximate solution to more complex simulations of bipedal walking, including a 3D biped with distributed mass."}
{"_id":"16a39222c0297c55401b94aa3bed09ca825be732","title":"Graph Drawing by Stress Majorization","text":"One of the most popular graph drawing methods is based of achieving graphtheoretic target ditsances. This method was used by Kamada and Kawai [15], who formulated it as an energy optimization problem. Their energy is known in the multidimensional scaling (MDS) community as the stress function. In this work, we show how to draw graphs by stress majorization, adapting a technique known in the MDS community for more than two decades. It appears that majorization has advantages over the technique of Kamada and Kawai in running time and stability. We also present a few extensions to the basic energy model which can improve layout quality and computation speed in practice. Majorization-based optimization is essential to these extensions."}
{"_id":"198a8507c7b26f89419430ed51f1c7675e5fa6c7","title":"Eigensolver Methods for Progressive Multidimensional Scaling of Large Data","text":"We present a novel sampling-based approximation technique for classical multidimensional scaling that yields an extremely fast layout algorithm suitable even for very large graphs. It produces layouts that compare favorably with other methods for drawing large graphs, and it is among the fastest methods available. In addition, our approach allows for progressive computation, i.e. a rough approximation of the layout can be produced even faster, and then be refined until satisfaction."}
{"_id":"4c77e5650e2328390995f3219ec44a4efd803b84","title":"Accelerating Large Graph Algorithms on the GPU Using CUDA","text":"Large graphs involving millions of vertices are common in many practical applications and are challenging to process. Practical-time implementations using high-end computers are reported but are accessible only to a few. Graphics Processing Units (GPUs) of today have high computation power and low price. They have a restrictive programming model and are tricky to use. The G80 line of Nvidia GPUs can be treated as a SIMD processor array using the CUDA programming model. We present a few fundamental algorithms \u2013 including breadth first search, single source shortest path, and all-pairs shortest path \u2013 using CUDA on large graphs. We can compute the single source shortest path on a 10 million vertex graph in 1.5 seconds using the Nvidia 8800GTX GPU costing $600. In some cases optimal sequential algorithm is not the fastest on the GPU architecture. GPUs have great potential as high-performance co-processors."}
{"_id":"3578078d7071459135d4c89f2f557d29678d6be1","title":"Self-monitoring: appraisal and reappraisal.","text":"Theory and research on self-monitoring have accumulated into a sizable literature on the impact of variation in the extent to which people cultivate public appearances in diverse domains of social functioning. Yet self-monitoring and its measure, the Self-Monitoring Scale, are surrounded by controversy generated by conflicting answers to the critical question, Is self-monitoring a unitary phenomenon? A primary source of answers to this question has been largely neglected--the Self-Monitoring Scale's relations with external criteria. We propose a quantitative method to examine the self-monitoring literature and thereby address major issues of the controversy. Application of this method reveals that, with important exceptions, a wide range of external criteria tap a dimension directly measured by the Self-Monitoring Scale. We discuss what this appraisal reveals about with self-monitoring is and is not."}
{"_id":"20ee7a52a4a75762ddcb784b77286b4261e53723","title":"Low cost high performance uncertainty quantification","text":"Uncertainty quantification in risk analysis has become a key application. In this context, computing the diagonal of inverse covariance matrices is of paramount importance. Standard techniques, that employ matrix factorizations, incur a cubic cost which quickly becomes intractable with the current explosion of data sizes. In this work we reduce this complexity to quadratic with the synergy of two algorithms that gracefully complement each other and lead to a radically different approach. First, we turned to stochastic estimation of the diagonal. This allowed us to cast the problem as a linear system with a relatively small number of multiple right hand sides. Second, for this linear system we developed a novel, mixed precision, iterative refinement scheme, which uses iterative solvers instead of matrix factorizations. We demonstrate that the new framework not only achieves the much needed quadratic cost but in addition offers excellent opportunities for scaling at massively parallel environments. We based our implementation on BLAS 3 kernels that ensure very high processor performance. We achieved a peak performance of 730 TFlops on 72 BG\/P racks, with a sustained performance 73% of theoretical peak. We stress that the techniques presented in this work are quite general and applicable to several other important applications."}
{"_id":"da5075fa79da6cd7b81e5d3dc24161217ef86368","title":"ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection","text":"As the intermediate level task connecting image captioning and object detection, visual relationship detection started to catch researchers\u2019 attention because of its descriptive power and clear structure. It localizes the objects and captures their interactions with a subject-predicateobject triplet, e.g. \u3008person-ride-horse\u3009. In this paper, the visual relationship is considered as a phrase with three components. So we formulate the visual relationship detection as three inter-connected recognition problems and propose a Visual Phrase reasoning Convolutional Neural Network (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a Visual Phrase Reasoning Structure (VPRS) to set up the connection among the relationship components and help the model consider the three problems jointly. Corresponding non-maximum suppression method and model training strategy are also proposed. Experimental results show that our ViP-CNN outperforms the stateof-art method both in speed and accuracy. We further pretrain our model on our cleansed Visual Genome Relationship dataset, which is found to perform better than the pretraining on the ImageNet for this task."}
{"_id":"ac47fb864a0092a931b0b40e1821c9a0c74795ea","title":"Goal Setting and Self-Efficacy During Self-Regulated Learning","text":"This article focuses on the self-regulated learning processes of goal setting and perceived self-efficacy. Students enter learning activities with goals and self-efficacy for goal attainment. As learners work on tasks, they observe their own performances and evaluate their own goal progress. Self-efficacy and goal setting are affected by selfobservation, self-judgment, and self-reaction. When students perceive satisfactory goal progress, they feel capable of improving their skills; goal attainment, coupled with high self-efficacy, leads students to set new challenging goals. Research is reviewed on goal properties (specificity, proximity, difficulty), self-set goals, progress feedback, contracts and conferences, and conceptions of ability. Ways of teaching students to set realistic goals and evaluate progress include establishing upper and lower goal limits and employing games, contracts, and conferences. Future research might clarify the relation of goal setting and self-efficacy to transfer, goal orientations, and affective reactions. Article: Self-regulated learning occurs when students activate and sustain cognitions and behaviors systematically oriented toward attainment of learning goals. Self-regulated learning processes involve goal-directed activities that students instigate, modify, and sustain (Zimmerman, 1989). These activities include attending to instruction, processing and integrating knowledge, rehearsing information to be remembered, and developing and maintaining positive beliefs about learning capabilities and anticipated outcomes of actions (Schunk, 1989). This article focuses on two self-regulated learning processes: goal setting and perceived self-efficacy. As used in this article, a goal is what an individual is consciously trying to accomplish, goal setting involves establishing a goal and modifying it as necessary, and perceived self-efficacy refers to beliefs concerning one's capabilities to attain designated levels of performance (Bandura, 1986, 1988). I initially present a theoretical overview of self-regulated learning to include the roles of goal setting and self-efficacy. I discuss research bearing on these processes, and conclude with implications for educational practice and future research suggestions. THEORETICAL OVERVIEW Subprocesses of Self-Regulated Learning Investigators working within a social cognitive learning theory framework view self-regulation as comprising three subprocesses: self-observation, self-judgment, and self-reaction (Bandura, 1986; Kanfer & Gaelick, 1986; Schunk, 1989). A model highlighting goal setting and self-efficacy is portrayed in Figure 1. Students enter learning activities with such goals as acquiring knowledge, solving problems, and finishing workbook pages. Self-efficacy for goal attainment is influenced by abilities, prior experiences, attitudes toward learning, instruction, and the social context. As students work on tasks, they observe their performances, evaluate goal progress, and continue their work or change their task approach. Self-evaluation of goal progress as satisfactory enhances feelings of efficacy; goal attainment leads students to set new challenging goals. Self-observation. Self-observation, or deliberate attention to aspects of one's behaviors, informs and motivates. Behaviors can be assessed on such dimensions as quality, rate, quantity, and originality. The information gained is used to gauge goal progress. Self-observation also can motivate behavioral change. Many students with poor study habits are surprised to learn that they waste much study time on nonacademic activities. Sustained motivation depends on students believing that if they change their behavior they will experience better outcomes, valuing those outcomes, and feeling they can change those habits (high self-efficacy). Self-observation is aided with self-recording, where behavior instances are recorded along with such features as time, place, and duration of occurrence (Mace, Belfiore, & Shea, 1989). Without recording, observations may not faithfully reflect behaviors due to selective memory. Behaviors should be observed close in time to their occurrence and on a continuous basis rather than intermittently. Self-judgment. Self-judgment involves comparing present performance with one's goal. Self-judgments are affected by the type of standards employed, goal properties (discussed in next section), importance of goal attainment, and performance attributions. Learning goals may reflect absolute or normative standards (Bandura, 1986). Absolute standards are fixed (e.g., complete six workbook pages in 30 min). Grading systems often are based on absolute standards (A = 90-100, B = 80-89). Normative standards employ performances by others. Social comparison of one's performances with those of peers helps one determine behavioral appropriateness. Standards are informative; comparing one's performance with standards informs one of goal progress. Standards also can motivate when they show that goal progress is being made. Self-judgments can be affected by the importance of goal attainment. When individuals care little about how they perform, they may not assess their performance or expend effort to improve (Bandura, 1986). Judgments of goal progress are more likely to be made for goals one personally values. Attributions, or perceived causes of outcomes (successes, failures), influence achievement beliefs and behaviors (Weiner, 1985). Achievement outcomes often are attributed to such causes as ability, effort, task difficulty, and luck (Frieze, 1980; Weiner, 1979). Children view effort as the prime cause of outcomes (Nicholls, 1984). With development, ability attributions become increasingly important. Whether goal progress is judged as acceptable depends on its attribution. Students who attribute successes to teacher assistance may hold low self-efficacy for good performance if they believe they cannot succeed on their own. If they believe they lack ability, they may judge learning progress as deficient and be unmotivated to work harder. Self-reaction. Self-reactions to goal progress motivate behavior (Bandura, 1986). The belief that one's progress is acceptable, along with anticipated satisfaction of goal accomplishment, enhances self-efficacy and motivation. Negative evaluations will not decrease motivation if individuals believe they are capable of improving (Schunk, 1989). Motivation will not improve if students believe they lack the ability to succeed and increased effort will not help. Individuals routinely make such rewards as work breaks, new clothes, and nights on the town contingent on task progress or goal attainment. Anticipation of rewards enhances motivation and self-efficacy. Compensations raise efficacy when they are tied to accomplishments. If students are told that they will earn rewards based on what they accomplish, they become instilled with a sense of efficacy for learning. Self-efficacy is validated as students work at a task and note their own progress; receipt of the reward then becomes a symbol of the progress made. Goal Setting The effects of goals on behavior depend on their properties: specificity, proximity, and difficulty level (Bandura, 1988; Locke, Shaw, Saari, & Latham, 1981). Goals incorporating specific performance standards are more likely to enhance learning and activate self-evaluations than general goals (i.e., \"Do your best\"). Specific goals boost performance by greater specification of the amount of effort required for success and the selfsatisfaction anticipated. Specific goals promote self-efficacy because progress is easy to gauge. Proximal goals result in greater motivation than distant goals. It is easier to gauge progress toward a proximal goal, and the perception of progress raises self-efficacy. Proximal goals are especially influential with young children, who do not represent distant outcomes in thought. Goal difficulty, or the level of task proficiency required as assessed against a standard, influences the effort learners expend to attain a goal. Assuming requisite skills, individuals expend greater effort to attain difficult goals than when standards are lower. Learners initially may doubt whether they can attain difficult goals, but working toward them builds self-efficacy. Self-Efficacy Self-efficacy is hypothesized to influence choice of activities, effort expended, and persistence (Bandura, 1986). Students who hold low self-efficacy for learning may avoid tasks; those who judge themselves efficacious are more likely to participate. When facing difficulties, self-efficacious learners expend greater effort and persist longer than students who doubt their capabilities. Students acquire information about their self-efficacy in a given domain from their performances, observations of models (i.e., vicarious experiences), forms of social persuasion, and physiological indexes (e.g., heart rate, sweating). Information acquired from these sources does not influence efficacy automatically but is cognitively appraised. Efficacy appraisal is an inferential process; persons weigh and combine the contributions of personal and situational factors. In assessing self-efficacy, students take into account such factors as perceived ability, expended effort, task difficulty, teacher assistance, other situational factors, and patterns of successes and failures. The notion that personal expectations influence behavior is not unique to self-efficacy theory. Self-efficacy is conceptually similar to such other constructs as perceived competence, expectations of success, and selfconfidence. One means of distinguishing constructs involves the generality of the constructs. Some constructs (e.g., self-concept, self-esteem) are hypothesized to affect diverse areas of human functioning. Though perceptions of efficacy can generalize, they offer the best prediction of behavior within specific domains (e.g., selfefficacy for acquiring fraction"}
{"_id":"094ca99cc94e38984823776158da738e5bc3963d","title":"Learning to predict by the methods of temporal differences","text":"This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage."}
{"_id":"d379e858069861ca388659e578abb7a96c571e85","title":"A review on methods and classifiers in lip reading","text":"The idea of lip reading as a visual technique which people may use to translate lip movement into phrases without relying on speech itself is fascinating. There are numerous application areas in which lip reading could provide full assistance. Although there may be a downside to using the lip reading system, whether it may range from problems such as time constraint to minor word recognition mistakes, further development of the system should be an active cycle. Ongoing research into improvement of the lip reading system performance by way of suitable choices of feature extraction and classifiers is essential to track the developing trend in both fields of technology as well as in pattern recognition. This paper discusses a short review on the existing methods and classifiers which have been used in previous work in the field of lip reading."}
{"_id":"bba260607d53209373176bce4b515441de510d1e","title":"Classical grasp quality evaluation: New algorithms and theory","text":"This paper investigates theoretical properties of a well-known L<sup>1<\/sup> grasp quality measure Q whose approximation Q<sup>-<\/sup><sub>l<\/sub> is commonly used for the evaluation of grasps and where the precision of Q<sup>-<\/sup><sub>l<\/sub> depends on an approximation of a cone by a convex polyhedral cone with l edges. We prove the Lipschitz continuity of Q and provide an explicit Lipschitz bound that can be used to infer the stability of grasps lying in a neighbourhood of a known grasp. We think of Q<sup>-<\/sup><sub>l<\/sub> as a lower bound estimate to Q and describe an algorithm for computing an upper bound Q<sup>+<\/sup>. We provide worst-case error bounds relating Q and Q<sup>-<\/sup><sub>l<\/sub>. Furthermore, we develop a novel grasp hypothesis rejection algorithm which can exclude unstable grasps much faster than current implementations. Our algorithm is based on a formulation of the grasp quality evaluation problem as an optimization problem, and we show how our algorithm can be used to improve the efficiency of sampling based grasp hypotheses generation methods."}
{"_id":"cd74b296bed69cb6e9f838e9330a975ebac5139c","title":"A high frequency high voltage power supply","text":"Novel compact high frequency high voltage (HV) power supply is introduced for small volume, lightweight, ultra fast high voltage output application. The two key factors high frequency HV transformer and voltage multiplier diodes are evaluated in this paper. The HV resonant tank size is only 1L for 400kHz\u223c500kHz 35kV 2kW output, the rise time for output HV is around 100us by lab prototype experiment results."}
{"_id":"45a33bddf460554b7c1f550aa382d63345a20704","title":"Data-Driven Intelligent Transportation Systems: A Survey","text":"For the last two decades, intelligent transportation systems (ITS) have emerged as an efficient way of improving the performance of transportation systems, enhancing travel security, and providing more choices to travelers. A significant change in ITS in recent years is that much more data are collected from a variety of sources and can be processed into various forms for different stakeholders. The availability of a large amount of data can potentially lead to a revolution in ITS development, changing an ITS from a conventional technology-driven system into a more powerful multifunctional data-driven intelligent transportation system (D2ITS) : a system that is vision, multisource, and learning algorithm driven to optimize its performance. Furthermore, D2ITS is trending to become a privacy-aware people-centric more intelligent system. In this paper, we provide a survey on the development of D2ITS, discussing the functionality of its key components and some deployment issues associated with D2ITS Future research directions for the development of D2ITS is also presented."}
{"_id":"f032295b9f8e7771c43a1296a1e305f23e5f4fcb","title":"Modified Convolutional Neural Network Based on Dropout and the Stochastic Gradient Descent Optimizer","text":"This study proposes a modified convolutional neural network (CNN) algorithm that is based on dropout and the stochastic gradient descent (SGD) optimizer (MCNN-DS), after analyzing the problems of CNNs in extracting the convolution features, to improve the feature recognition rate and reduce the time-cost of CNNs. The MCNN-DS has a quadratic CNN structure and adopts the rectified linear unit as the activation function to avoid the gradient problem and accelerate convergence. To address the overfitting problem, the algorithm uses an SGD optimizer, which is implemented by inserting a dropout layer into the all-connected and output layers, to minimize cross entropy. This study used the datasets MNIST, HCL2000, and EnglishHand as the benchmark data, analyzed the performance of the SGD optimizer under different learning parameters, and found that the proposed algorithm exhibited good recognition performance when the learning rate was set to [0.05, 0.07]. The performances of WCNN, MLP-CNN, SVM-ELM, and MCNN-DS were compared. Statistical results showed the following: (1) For the benchmark MNIST, the MCNN-DS exhibited a high recognition rate of 99.97%, and the time-cost of the proposed algorithm was merely 21.95% of MLP-CNN, and 10.02% of SVM-ELM; (2) Compared with SVM-ELM, the average improvement in the recognition rate of MCNN-DS was 2.35% for the benchmark HCL2000, and the time-cost of MCNN-DS was only 15.41%; (3) For the EnglishHand test set, the lowest recognition rate of the algorithm was 84.93%, the highest recognition rate was 95.29%, and the average recognition rate was 89.77%."}
{"_id":"2fa4e164014a36c55b6cf6ed283333fb0bfa82e7","title":"Designing Sketches for Similarity Filtering","text":"The amounts of currently produced data emphasize the importance of techniques for efficient data processing. Searching big data collections according to similarity of data well corresponds to human perception. This paper is focused on similarity search using the concept of sketches \u2013 a compact bit string representations of data objects compared by Hamming distance, which can be used for filtering big datasets. The object-to-sketch transformation is a form of the dimensionality reduction and thus there are two basic contradictory requirements: (1) The length of the sketches should be small for efficient manipulation, but (2) longer sketches retain more information about the data objects. First, we study various sketching methods for data modeled by metric space and we analyse their quality. Specifically, we study importance of several sketch properties for similarity search and we propose a high quality sketching technique. Further, we focus on the length of sketches by studying mutual influence of sketch properties such as correlation of their bits and the intrinsic dimensionality of a set of sketches. The outcome is an equation that allows us to estimate a suitable length of sketches for an arbitrary given dataset. Finally, we empirically verify proposed approach on two real-life datasets."}
{"_id":"163131a9ad9d106058000a45bb8f4b65599859b3","title":"Are emoticons good enough to train emotion classifiers of Arabic tweets?","text":"Nowadays, the automatic detection of emotions is employed by many applications across different fields like security informatics, e-learning, humor detection, targeted advertising, etc. Many of these applications focus on social media. In this study, we address the problem of emotion detection in Arabic tweets. We focus on the supervised approach for this problem where a classifier is trained on an already labeled dataset. Typically, such a training set is manually annotated, which is expensive and time consuming. We propose to use an automatic approach to annotate the training data based on using emojis, which are a new generation of emoticons. We show that such an approach produces classifiers that are more accurate than the ones trained on a manually annotated dataset. To achieve our goal, a dataset of emotional Arabic tweets is constructed, where the emotion classes under consideration are: anger, disgust, joy and sadness. Moreover, we consider two classifiers: Support Vector Machine (SVM) and Multinomial Naive Bayes (MNB). The results of the tests show that the automatic labeling approaches using SVM and MNB outperform manual labeling approaches."}
{"_id":"82f9ed9ea80957a74bfe57affe1a017c913be5e1","title":"X-Ray PoseNet: 6 DoF Pose Estimation for Mobile X-Ray Devices","text":"Precise reconstruction of 3D volumes from X-ray projections requires precisely pre-calibrated systems where accurate knowledge of the systems geometric parameters is known ahead. However, when dealing with mobile X-ray devices such calibration parameters are unknown. Joint estimation of the systems calibration parameters and 3d reconstruction is a heavily unconstrained problem, especially when the projections are arbitrary. In industrial applications, that we target here, nominal CAD models of the object to be reconstructed are usually available. We rely on this prior information and employ Deep Learning to learn the mapping between simulated X-ray projections and its pose. Moreover, we introduce the reconstruction loss in addition to the pose loss to further improve the reconstruction quality. Finally, we demonstrate the generalization capabilities of our method in case where poses can be learned on instances of the objects belonging to the same class, allowing pose estimation of unseen objects from the same category, thus eliminating the need for the actual CAD model. We performed exhaustive evaluation demonstrating the quality of our results on both synthetic and real data."}
{"_id":"56121c8b56688a0fcce0358f74a2c98566ff80e2","title":"A low-cost smart sensor for non intrusive load monitoring applications","text":"Next generation Smart Cities have the potential to enhance the citizens quality of life and to reduce the overall energy expenditure. In particular, emerging smart metering technologies promise to greatly increase energy efficiency in the residential and industrial areas. In this context, new power metering methods such as Non-Intrusive Load Monitoring (NILM) can provide important real-time information about the contribution of any single appliance. In this paper, we present a complete hardware-software design that concentrates locally an efficient event-based supervised NILM algorithm for load disaggregation. This new kind of power analysis, which usually requires high computing capability, is executed real-time on a low-cost and easy-to-install smart meter ready for the Internet of Things (IoT)."}
{"_id":"ac474d5a3cf7afeba424e9e64984f13eb22e2ec6","title":"Linearity and efficiency enhancement strategies for 4G wireless power amplifier designs","text":"Next generation wireless transmitters will rely on highly integrated silicon-based solutions to realize the cost and performance goals of the 4G market. This will require increased use of digital compensation techniques and innovative circuit approaches to maximize power and efficiency and minimize linearity degradation. This paper summarizes the circuit and system strategies being developed to meet these aggressive performance goals."}
{"_id":"51354699bfd423bb99d34c06d0061540d8ed178e","title":"Vector space model adaptation and pseudo relevance feedback for content-based image retrieval","text":"Image retrieval is an important problem for researchers in computer vision and content-based image retrieval (CBIR) fields. Over the last decades, many image retrieval systems were based on image representation as a set of extracted low-level features such as color, texture and shape. Then, systems calculate similarity metrics between features in order to find similar images to a query image. The disadvantage of this approach is that images visually and semantically different may be similar in the low level feature space. So, it is necessary to develop tools to optimize retrieval of information. Integration of vector space models is one solution to improve the performance of image retrieval. In this paper, we present an efficient and effective retrieval framework which includes a vectorization technique combined with a pseudo relevance model. The idea is to transform any similarity matching model (between images) to a vector space model providing a score. A study on several methodologies to obtain the vectorization is presented. Some experiments have been undertaken on Wang, Oxford5k and Inria Holidays datasets to show the performance of our proposed framework."}
{"_id":"8da17dde7a90af885d8d7413b2ae44202607ff54","title":"Deep Network Embedding with Aggregated Proximity Preserving","text":"Network embedding is an effective method to learn a low-dimensional feature vector representation for each node of a given network. In this paper, we propose a deep network embedding model with aggregated proximity preserving (DNE-APP). Firstly, an overall network proximity matrix is generated to capture both local and global network structural information, by aggregating different k-th order network proximities between different nodes. Then, a semi-supervised stacked auto-encoder is employed to learn the hidden representations which can best preserve the aggregated proximity in the original network, and also map the node pairs with higher proximity closer to each other in the embedding space. With the hidden representations learned by DNE-APP, we apply vector-based machine learning techniques to conduct node classification and link label prediction tasks on the real-world datasets. Experimental results demonstrate the superiority of our proposed DNE-APP model over the state-of-the-art network embedding algorithms."}
{"_id":"515ccd058edbbc588b4e98926897af5b1969b559","title":"Counterfactual Fairness","text":"Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school."}
{"_id":"1e295f4e195cf3d63a90ce85ca7ce29e0b42d8b7","title":"A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication","text":"The strength with which a statement is made can have a significant impact on the audience. For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings. It is thus important to understand the effects of statement strength. A first step is to be able to distinguish between strong and weak statements. However, even this problem is understudied, partly due to a lack of data. Since strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences. In this paper, we introduce a corpus of sentence-level revisions from academic writing. We also describe insights gained from our annotation efforts for this task."}
{"_id":"69b755a7593a94d39af20f781d8db0c6fd62be53","title":"CVSS Attack Graphs","text":"Attack models and attack graphs are efficient tools to describe and analyse attack scenarios aimed at computer networks. More precisely, attack graphs give all possible scenarios for an attacker to reach a certain goal, exploiting vulnerabilities of the targeted network. Nevertheless they give no information about the damages induced by these attacks, nor about the probability of exploitation of these scenarios. In this paper, we propose to combine attack graphs and CVSS framework, in order to add damage and exploitability probability information. Then, we define a notion of risk for each attack scenario, which is based on quantitative information added to attack graphs."}
{"_id":"604809686a977df714f76b823d4fce48e67ee3ba","title":"Better Summarization Evaluation with Word Embeddings for ROUGE","text":"ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank co-"}
{"_id":"191e6f705beb6d37eb610763bd885e43b96988e8","title":"QUOTUS: The Structure of Political Media Coverage as Revealed by Quoting Patterns","text":"Given the extremely large pool of events and stories available, media outlets need to focus on a subset of issues and aspects to convey to their audience. Outlets are often accused of exhibiting a systematic bias in this selection process, with different outlets portraying different versions of reality. However, in the absence of objective measures and empirical evidence, the direction and extent of systematicity remains widely disputed. In this paper we propose a framework based on quoting patterns for quantifying and characterizing the degree to which media outlets exhibit systematic bias. We apply this framework to a massive dataset of news articles spanning the six years of Obama's presidency and all of his speeches, and reveal that a systematic pattern does indeed emerge from the outlet's quoting behavior. Moreover, we show that this pattern can be successfully exploited in an unsupervised prediction setting, to determine which new quotes an outlet will select to broadcast. By encoding bias patterns in a low-rank space we provide an analysis of the structure of political media coverage. This reveals a latent media bias space that aligns surprisingly well with political ideology and outlet type. A linguistic analysis exposes striking differences across these latent dimensions, showing how the different types of media outlets portray different realities even when reporting on the same events. For example, outlets mapped to the mainstream conservative side of the latent space focus on quotes that portray a presidential persona disproportionately characterized by negativity."}
{"_id":"f82e86b853fcda968d63c0196bf2df6748b13233","title":"Greedy Attribute Selection","text":"Many real-world domains bless us with a wealth of attributes to use for learning. This blessing is often a curse: most inductive methods generalize worse given too many attributes than if given a good subset of those attributes. We examine this problem for two learning tasks taken from a calendar scheduling domain. We show that ID3\/C4.5 generalizes poorly on these tasks if allowed to use all available attributes. We examine five greedy hillclimbing procedures that search for attribute sets that generalize well with ID3\/C4.5. Experiments suggest hillclimbing in attribute space can yield substantial improvements in generalization performance. We present a caching scheme that makes attribute hillclimbing more practical computationally. We also compare the results of hillclimbing in attribute space with FOCUS and RELIEF on the two tasks."}
{"_id":"503e91d69742a723be55ff4143a6c02457a6caca","title":"A systematic review of implementation strategies for assessment, prevention, and management of ICU delirium and their effect on clinical outcomes","text":"INTRODUCTION\nDespite recommendations from professional societies and patient safety organizations, the majority of ICU patients worldwide are not routinely monitored for delirium, thus preventing timely prevention and management. The purpose of this systematic review is to summarize what types of implementation strategies have been tested to improve ICU clinicians' ability to effectively assess, prevent and treat delirium and to evaluate the effect of these strategies on clinical outcomes.\n\n\nMETHOD\nWe searched PubMed, Embase, PsychINFO, Cochrane and CINAHL (January 2000 and April 2014) for studies on implementation strategies that included delirium-oriented interventions in adult ICU patients. Studies were suitable for inclusion if implementation strategies' efficacy, in terms of a clinical outcome, or process outcome was described.\n\n\nRESULTS\nWe included 21 studies, all including process measures, while 9 reported both process measures and clinical outcomes. Some individual strategies such as \"audit and feedback\" and \"tailored interventions\" may be important to establish clinical outcome improvements, but otherwise robust data on effectiveness of specific implementation strategies were scarce. Successful implementation interventions were frequently reported to change process measures, such as improvements in adherence to delirium screening with up to 92%, but relating process measures to outcome changes was generally not possible. In meta-analyses, reduced mortality and ICU length of stay reduction were statistically more likely with implementation programs that employed more (six or more) rather than less implementation strategies and when a framework was used that either integrated current evidence on pain, agitation and delirium management (PAD) or when a strategy of early awakening, breathing, delirium screening and early exercise (ABCDE bundle) was employed. Using implementation strategies aimed at organizational change, next to behavioral change, was also associated with reduced mortality.\n\n\nCONCLUSION\nOur findings may indicate that multi-component implementation programs with a higher number of strategies targeting ICU delirium assessment, prevention and treatment and integrated within PAD or ABCDE bundle have the potential to improve clinical outcomes. However, prospective confirmation of these findings is needed to inform the most effective implementation practice with regard to integrated delirium management and such research should clearly delineate effective practice change from improvements in clinical outcomes."}
{"_id":"42628ebecf7debccb9717251d7d02c9a5d040ecf","title":"Emotion Detection From Text Documents","text":"Emotion Detection is one of the most emerging issues in human computer interaction. A sufficient amount of work has been done by researchers to detect emotions from facial and audio information whereas recognizing emotions from textual data is still a fresh and hot research area. This paper presented a knowledge based survey on emotion detection based on textual data and the methods used for this purpose. At the next step paper also proposed a new architecture for recognizing emotions from text document. Proposed architecture is composed of two main parts, emotion ontology and emotion detector algorithm. Proposed emotion detector system takes a text document and the emotion ontology as inputs and produces one of the six emotion classes (i.e. love, joy, anger, sadness, fear and surprise) as the output."}
{"_id":"30f11d456739d8d83d8cbf240dea46a26bca6509","title":"A power line communication network infrastructure for the smart home","text":"Low voltage electrical wiring in homes has largely been dismissed as too noisy and unpredictable to support high speed communication signals. However, recent advances in communication and modulation methodologies as well as in adaptive digital signal processing and error detection and correction have spawned novel media access control (MAC) and physical layer (PHY) protocols, capable of supporting power line communication networks at speeds comparable to wired local area networks (LANs). In this paper we motivate the use of power line LAN\u2019s as a basic infrastructure for building integrated \u201dsmart homes,\u201d wherein information appliances (IA)\u2014ranging from simple control or monitoring devices to multimedia entertainment systems\u2014are seamlessly interconnected by the very wires which provide them electricity. By simulation and actual measurements using \u201dreference design\u201d prototype commercial powerline products, we show that the HomePlugMAC and PHY layers can guarantee QoS for real-time communications, supporting delay sensitive data streams for \u201dSmart Home\u201d applications."}
{"_id":"6032773345c73957f87178fd5d0556870299c4e1","title":"Learning Deep Boltzmann Machines using Adaptive MCMC","text":"When modeling high-dimensional richly structured data, it is often the case that the distribution defined by the Deep Boltzmann Machine (DBM) has a rough energy landscape with many local minima separated by high energy barriers. The commonly used Gibbs sampler tends to get trapped in one local mode, which often results in unstable learning dynamics and leads to poor parameter estimates. In this paper, we concentrate on learning DBM\u2019s using adaptive MCMC algorithms. We first show a close connection between Fast PCD and adaptive MCMC. We then develop a Coupled Adaptive Simulated Tempering algorithm that can be used to better explore a highly multimodal energy landscape. Finally, we demonstrate that the proposed algorithm considerably improves parameter estimates, particularly when learning large-scale DBM\u2019s."}
{"_id":"6e7dadd63455c194e3472bb181aaf509f89b9166","title":"Classification using discriminative restricted Boltzmann machines","text":"Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting."}
{"_id":"a0117ec4cd582974d06159644d12f65862a8daa3","title":"Deep Belief Networks Are Compact Universal Approximators","text":"Deep belief networks (DBN) are generative models with many layers of hidden causal variables, recently introduced by Hinton, Osindero, and Teh (2006), along with a greedy layer-wise unsupervised learning algorithm. Building on Le Roux and Bengio (2008) and Sutskever and Hinton (2008), we show that deep but narrow generative networks do not require more parameters than shallow ones to achieve universal approximation. Exploiting the proof technique, we prove that deep but narrow feedforward neural networks with sigmoidal units can represent any Boolean expression."}
{"_id":"4a92f809913c9e6d4f50b8ceaaf6bb7a5a43c8f7","title":"Global, regional, and national causes of child mortality: an updated systematic analysis for 2010 with time trends since 2000","text":"BACKGROUND\nInformation about the distribution of causes of and time trends for child mortality should be periodically updated. We report the latest estimates of causes of child mortality in 2010 with time trends since 2000.\n\n\nMETHODS\nUpdated total numbers of deaths in children aged 0-27 days and 1-59 months were applied to the corresponding country-specific distribution of deaths by cause. We did the following to derive the number of deaths in children aged 1-59 months: we used vital registration data for countries with an adequate vital registration system; we applied a multinomial logistic regression model to vital registration data for low-mortality countries without adequate vital registration; we used a similar multinomial logistic regression with verbal autopsy data for high-mortality countries; for India and China, we developed national models. We aggregated country results to generate regional and global estimates.\n\n\nFINDINGS\nOf 7\u00b76 million deaths in children younger than 5 years in 2010, 64\u00b70% (4\u00b7879 million) were attributable to infectious causes and 40\u00b73% (3\u00b7072 million) occurred in neonates. Preterm birth complications (14\u00b71%; 1\u00b7078 million, uncertainty range [UR] 0\u00b7916-1\u00b7325), intrapartum-related complications (9\u00b74%; 0\u00b7717 million, 0\u00b7610-0\u00b7876), and sepsis or meningitis (5\u00b72%; 0\u00b7393 million, 0\u00b7252-0\u00b7552) were the leading causes of neonatal death. In older children, pneumonia (14\u00b71%; 1\u00b7071 million, 0\u00b7977-1\u00b7176), diarrhoea (9\u00b79%; 0\u00b7751 million, 0\u00b7538-1\u00b7031), and malaria (7\u00b74%; 0\u00b7564 million, 0\u00b7432-0\u00b7709) claimed the most lives. Despite tremendous efforts to identify relevant data, the causes of only 2\u00b77% (0\u00b7205 million) of deaths in children younger than 5 years were medically certified in 2010. Between 2000 and 2010, the global burden of deaths in children younger than 5 years decreased by 2 million, of which pneumonia, measles, and diarrhoea contributed the most to the overall reduction (0\u00b7451 million [0\u00b7339-0\u00b7547], 0\u00b7363 million [0\u00b7283-0\u00b7419], and 0\u00b7359 million [0\u00b7215-0\u00b7476], respectively). However, only tetanus, measles, AIDS, and malaria (in Africa) decreased at an annual rate sufficient to attain the Millennium Development Goal 4.\n\n\nINTERPRETATION\nChild survival strategies should direct resources toward the leading causes of child mortality, with attention focusing on infectious and neonatal causes. More rapid decreases from 2010-15 will need accelerated reduction for the most common causes of death, notably pneumonia and preterm birth complications. Continued efforts to gather high-quality data and enhance estimation methods are essential for the improvement of future estimates.\n\n\nFUNDING\nThe Bill & Melinda Gates Foundation."}
{"_id":"274946a974bc2bbbfe89c7f6fd3751396f295625","title":"Theory and Applications of Robust Optimization","text":"In this paper we survey the primary research, both theoretical and applied, in the area of Robust Optimization (RO). Our focus will be on the computational attractiveness of RO approaches, as well as the modeling power and broad applicability of the methodology. In addition to surveying the most prominent theoretical results of RO over the past decade, we will also present some recent results linking RO to adaptable models for multi-stage decision-making problems. Finally, we will highlight applications of RO across a wide spectrum of domains, including finance, statistics, learning, and various areas of engineering."}
{"_id":"40768956409dbafee1bb6c89a329bdf17e0efb0f","title":"Building a motor simulation de novo: Observation of dance by dancers","text":"Research on action simulation identifies brain areas that are active while imagining or performing simple overlearned actions. Are areas engaged during imagined movement sensitive to the amount of actual physical practice? In the present study, participants were expert dancers who learned and rehearsed novel, complex whole-body dance sequences 5 h a week across 5 weeks. Brain activity was recorded weekly by fMRI as dancers observed and imagined performing different movement sequences. Half these sequences were rehearsed and half were unpracticed control movements. After each trial, participants rated how well they could perform the movement. We hypothesized that activity in premotor areas would increase as participants observed and simulated movements that they had learnt outside the scanner. Dancers' ratings of their ability to perform rehearsed sequences, but not the control sequences, increased with training. When dancers observed and simulated another dancer's movements, brain regions classically associated with both action simulation and action observation were active, including inferior parietal lobule, cingulate and supplementary motor areas, ventral premotor cortex, superior temporal sulcus and primary motor cortex. Critically, inferior parietal lobule and ventral premotor activity was modulated as a function of dancers' ratings of their own ability to perform the observed movements and their motor experience. These data demonstrate that a complex motor resonance can be built de novo over 5 weeks of rehearsal. Furthermore, activity in premotor and parietal areas during action simulation is enhanced by the ability to execute a learned action irrespective of stimulus familiarity or semantic label."}
{"_id":"4891676f862db6800afbfea4503835d957fe2458","title":"A Novel Miniature YIG Tuned Oscillator Achieves Octave Tuning Bandwidth with Ultra Low Phase Noise in X and Ku Bands","text":"Traditional YIG tuned oscillators use negative resistance type circuits and large magnetic structure to produce excellent phase noise with octave plus tuning ranges, but at the expense of size and weight. This paper describes a revolutionary approach to YIG tuned oscillator design which makes use of a unique combination of a miniaturized magnetic structure, and a GaAs HBT ring oscillator circuit topology, working through a YIG tuned filter to produce octave tuning ranges in X and Ku bands with phase noise less than -125 dBC per Hz at 100 KHz. Frequency pulling is less than 10 KHz into a 2:1 VSWR load over all phases. Simulations and measured data are in excellent agreement"}
{"_id":"4ee7f38c36959f673937d0504120e829f5e719cd","title":"Perceptual category mapping between English and Korean obstruents in non-CV positions: Prosodic location effects in second language identification skills","text":"This study examines the degree to which mapping patterns between native language (L1) and second language (L2) categories for one prosodic context will generalize to other prosodic contexts, and how position-specific neutralization in the L1 influences the category mappings. Forty L1-Korean learners of English listened to English nonsense words consisting of \/p b t d f v h \u00f0\/ and \/\u0251\/, with the consonants appearing in pre-stressed intervocalic, poststressed intervocalic, or coda context, and were asked to identify the consonant with both Korean and English labeling and to give gradient evaluations of the goodness of each label to the stimuli. Results show that the mapping patterns differ extensively from those found previously with the same subjects for consonants in initial, onset context. The mapping patterns for the intervocalic context also differed by position with respect to stress location. Coda consonants elicited poor goodness-of-fit and noisier mapping patterns for all segments, suggesting that an L1 coda neutralization process put all L2-English sounds in codas as \u201cnew\u201d sounds under the Speech Learning Model (SLM) framework (Flege, 1995). Taken together, the results indicate that consonant learning needs to be evaluated in terms of position-by-position variants, rather than just being a general property of the overall"}
{"_id":"a6de0d1389e897cb2c7266401a57e6f10beddcf8","title":"Viraliency: Pooling Local Virality","text":"In our overly-connected world, the automatic recognition of virality &#x2013; the quality of an image or video to be rapidly and widely spread in social networks &#x2013; is of crucial importance, and has recently awaken the interest of the computer vision community. Concurrently, recent progress in deep learning architectures showed that global pooling strategies allow the extraction of activation maps, which highlight the parts of the image most likely to contain instances of a certain class. We extend this concept by introducing a pooling layer that learns the size of the support area to be averaged: the learned top-N average (LENA) pooling. We hypothesize that the latent concepts (feature maps) describing virality may require such a rich pooling strategy. We assess the effectiveness of the LENA layer by appending it on top of a convolutional siamese architecture and evaluate its performance on the task of predicting and localizing virality. We report experiments on two publicly available datasets annotated for virality and show that our method outperforms state-of-the-art approaches."}
{"_id":"dc2dcde8e068ba9a9073972cd406df873e9dfd93","title":"Indirect Image Registration with Large Diffeomorphic Deformations","text":"The paper adapts the large deformation diffeomorphic metric mapping framework for image registration to the indirect setting where a template is registered against a target that is given through indirect noisy observations. The registration uses diffeomorphisms that transform the template through a (group) action. These diffeomorphisms are generated by solving a flow equation that is defined by a velocity field with certain regularity. The theoretical analysis includes a proof that indirect image registration has solutions (existence) that are stable and that converge as the data error tends so zero, so it becomes a well-defined regularization method. The paper concludes with examples of indirect image registration in 2D tomography with very sparse and\/or highly noisy data."}
{"_id":"573cd4046fd8b899a7753652cd0f4cf6e351c5ae","title":"Shape-based recognition of wiry objects","text":"We present an approach to the recognition of complex-shaped objects in cluttered environments based on edge information. We first use example images of a target object in typical environments to train a classifier cascade that determines whether edge pixels in an image belong to an instance of the desired object or the clutter. Presented with a novel image, we use the cascade to discard clutter edge pixels and group the object edge pixels into overall detections of the object. The features used for the edge pixel classification are localized, sparse edge density operations. Experiments validate the effectiveness of the technique for recognition of a set of complex objects in a variety of cluttered indoor scenes under arbitrary out-of-image-plane rotation. Furthermore, our experiments suggest that the technique is robust to variations between training and testing environments and is efficient at runtime."}
{"_id":"e73b9e7dd4f948553e8de4a0d46c24508b7a00f9","title":"Consistent Linear Tracker With Converted Range, Bearing, and Range Rate Measurements","text":"Active sonar and radar systems often include a measurement of range rate in addition to the position-based measurements of range and bearing. Due to the nonlinearity of the range rate measurement with respect to a target state in the Cartesian coordinate system, this measurement is not always fully exploited. The state of the art methods to utilize range rate include the extended Kalman filter and the unscented Kalman filter with sequential processing of the position based measurements and the range rate measurement. Common to these approaches is that the measurement prediction function remains nonlinear. The goal of this work is to develop a measurement conversion from range, bearing, and range rate to Cartesian position and velocity that is unbiased and consistent, with appropriate elimination of estimation bias. The converted measurement is then used with a linear Kalman filter. Performance of this new method is compared to state of the art techniques and shown to match or exceed that of existing techniques over a wide range of scenarios."}
{"_id":"2c92e8f93be31eb7d82f21b64e5b72fec735c169","title":"A term-based methodology for query reformulation understanding","text":"Key to any research involving session search is the understanding of how a user\u2019s queries evolve throughout the session. When a user creates a query reformulation, he or she is consciously retaining terms from their original query, removing others and adding new terms. By measuring the similarity between queries we can make inferences on the user\u2019s information need and how successful their new query is likely to be. By identifying the origins of added terms we can infer the user\u2019s motivations and gain an understanding of their interactions. In this paper we present a novel term-based methodology for understanding and interpreting query reformulation actions. We use TREC Session Track data to demonstrate how our technique is able to learn from query logs and we make use of click data to test user interaction behavior when reformulating queries. We identify and evaluate a range of term-based query reformulation strategies and show that our methods provide valuable insight into understanding query reformulation in session search."}
{"_id":"592885ae6cd235f8c642c2ec8765a857b84fee66","title":"Object-Aware Identification of Microservices","text":"Microservices is an architectural style inspired by service-oriented computing that structures an application as a collection of cohesive and loosely coupled components, which implement business capabilities. One of today\u2019s problems in designing microservice architectures is to decompose a system into cohesive, loosely coupled, and fine-grained microservices. Identification of microservices is usually performed intuitively, based on the experience of the system designers, however, if the functionalities of a system are highly interconnected, it is a challenging task to decompose the system into appropriate microservices. To tackle this challenge, we present a microservice identification method that decomposes a system using clustering technique. To this end, we model a system as a set of business processes and take two aspects of structural dependency and data object dependency of functionalities into account. Furthermore, we conduct a study to evaluate the effect of process characteristics on the accuracy of identification approaches."}
{"_id":"c1792ff52189bdec1de609cb0d00d64a7d7f128f","title":"On the Reception and Detection of Pseudo-profound Bullshit","text":"Although bullshit is common in everyday life and has attracted attention from philosophers, its reception (critical or ingenuous) has not, to our knowledge, been subject to empirical investigation. Here we focus on pseudo-profound bullshit, which consists of seemingly impressive assertions that are presented as true and meaningful but are actually vacuous. We presented participants with bullshit statements consisting of buzzwords randomly organized into statements with syntactic structure but no discernible meaning (e.g., \u201cWholeness quiets infinite phenomena\u201d). Across multiple studies, the propensity to judge bullshit statements as profound was associated with a variety of conceptually relevant variables (e.g., intuitive cognitive style, supernatural belief). Parallel associations were less evident among profundity judgments for more conventionally profound (e.g., \u201cA wet person does not fear the rain\u201d) or mundane (e.g., \u201cNewborn babies require constant attention\u201d) statements. These results support the idea that some people are more receptive to this type of bullshit and that detecting it is not merely a matter of indiscriminate skepticism but rather a discernment of deceptive vagueness in otherwise impressive sounding claims. Our results also suggest that a bias toward accepting statements as true may be an important component of pseudo-profound bullshit receptivity."}
{"_id":"2980fd9cc4f599bb93e0a5a11f4bab67364a4dde","title":"Model Shrinking for Embedded Keyword Spotting","text":"In this paper we present two approaches to improve computational efficiency of a keyword spotting system running on a resource constrained device. This embedded keyword spotting system detects a pre-specified keyword in real time at low cost of CPU and memory. Our system is a two stage cascade. The first stage extracts keyword hypotheses from input audio streams. After the first stage is triggered, hand-crafted features are extracted from the keyword hypothesis and fed to a support vector machine (SVM) classifier on the second stage. This paper focuses on improving the computational efficiency of the second stage SVM classifier. More specifically, select a subset of feature dimensions and merge the SVM classifier to a smaller size, while maintaining the keyword spotting performance. Experimental results indicate that we can remove more than 36% of the non-discriminative SVM features, and reduce the number of support vectors by more than 60% without significant performance degradation. This results in more than 15% relative reduction in CPU utilization."}
{"_id":"afe45eea3e954805df2dac45b55726773d221f1b","title":"Predictive Analytics On Public Data - The Case Of Stock Markets","text":"This work examines the predictive power of public data by aggregating information from multiple online sources. Our sources include microblogging sites like Twitter, online message boards like Yahoo! Finance, and traditional news articles. The subject of prediction are daily stock price movements from Standard & Poor\u2019s 500 index (S&P 500) during a period from June 2011 to November 2011. To forecast price movements we filter messages by stocks, apply state-of-the-art sentiment analysis to message texts, and aggregate message sentiments to generate trading signals for daily buy and sell decisions. We evaluate prediction quality through a simple trading model considering real-world limitations like transaction costs or broker commission fees. Considering 833 virtual trades, our model outperformed the S&P 500 and achieved a positive return on investment of up to ~0.49% per trade or ~0.24% when adjusted by market, depending on supposed trading costs."}
{"_id":"45a4fe01625c47f335e12d67602d3a02d713f095","title":"Soft pneumatic actuator with adjustable stiffness layers for Multi-DoF Actuation","text":"The soft pneumatic actuators (SPAs) are a solution toward the highly customizable and light actuators with the versatility of actuation modes, and an inherent compliance. Such flexibility allows SPAs to be considered as alternative actuators for wearable rehabilitative devices and search and rescue robots. The actuator material and air-chamber design dictate the actuator's mechanical performance. Therefore, each actuator design with a single pressure source produces a highly customized motion but only a single degree of freedom (DoF). We present a novel design and fabrication method for a SPA with different modes of actuation using integrated adjustable stiffness layers (ASLs). Unlike the most SPA designs where one independent chamber is needed for each mode of actuation, here we have a single chamber that drives three different modes of actuation by activating different combinations of ASLs. Adapting customized micro heaters and thermistors for modulating the temperature and stiffness of ASLs, we considerably broaden the work space of the SPA actuator. Here, a thorough characterization of the materials and the modeling of the actuator are presented. We propose a design methodology for developing application specific actuators with multi-DoFs that are light and compact."}
{"_id":"3f4cb985e14b975e2b588c8ebd0685ad1d895c23","title":"Deep learning traffic sign detection, recognition and augmentation","text":"Driving is a complex, continuous, and multitask process that involves driver's cognition, perception, and motor movements. The way road traffic signs and vehicle information is displayed impacts strongly driver's attention with increased mental workload leading to safety concerns. Drivers must keep their eyes on the road, but can always use some assistance in maintaining their awareness and directing their attention to potential emerging hazards. Research in perceptual and human factors assessment is needed for relevant and correct display of this information for maximal road traffic safety as well as optimal driver comfort. In-vehicle contextual Augmented Reality (AR) has the potential to provide novel visual feedbacks to drivers for an enhanced driving experience. In this paper, we present a new real-time approach for fast and accurate framework for traffic sign recognition, based on Cascade Deep learning and AR, which superimposes augmented virtual objects onto a real scene under all types of driving situations, including unfavorable weather conditions. Experiments results show that, by combining the Haar Cascade and deep convolutional neural networks show that the joint learning greatly enhances the capability of detection and still retains its realtime performance."}
{"_id":"5c6caa27256bea3190bab5c9e3ffcba271806321","title":"A Design and Assessment of a Direction Finding Proximity Fuze Sensor","text":"This paper presents the implementation and assessment of a direction finding proximity fuze sensor for anti-aircrafts or anti-air missiles. A higher rejection of clutter signals is achieved by employing a binary phase shift keying modulation using Legendre sequence. The direction finding is implemented by comparing received powers from six receiving antennas equally spaced by an angle of 60\u00b0 around a cylindrical surface. In addition, target detection algorithms have been developed for a robust detection of the target, taking the wide variation of target related parameters into considerations. The performances of the developed fuze sensor are experimentally verified by constructing the fuze-specific encounter simulation test apparatus, which collects and analyzes reflected signals from a standard target. The developed fuze sensor can successfully detect the -10 dBsm target over a 10 m range as well as the direction with an out-of-range rejection of about 40 dB. Furthermore, the developed fuze sensor can clearly detect the target with mesh clutter environment. To assess realistic operation, the fuze sensor is tested using 155 mm gun firing test setup. Through the gun firing test, the successful fuzing range and direction finding performances are demonstrated."}
{"_id":"211cd0a2ea0039b459fe3bbd06a2b34cddfb4cfe","title":"Cognitive Context Models and Discourse","text":"1. Mental models Since the early 1980s, the notion of mental model has been quite successful in cognitive psychology in general, and in the theory of text processing in particu-Such models have been conceptualized as representations in episodic memory of situations, acts or events spoken or thought about, observed or participated in by human actors, that is of experiences (Ehrlich, Tardieu & Cavazza 1993). In the theory of text processing, such (situation or event) models played a crucial role in establishing the necessary referential basis for the processing of anaphora and other phenomena of coherence (Albrecht & O Brien 1993). They further explained, among many other things, why text recall does not seem to be based on semantic representations of texts, but rather on the mental model construed or updated of the event the text is about (Bower & Morrow 1990). Conversely, mental models also play a role in the much neglected theory of discourse production, viz., as the mental point of departure of all text and talk, from which relevant information may be selected for the strategic construction of their global and local semantic structures. Many experiments have confirmed these hypotheses, and have shown at text comprehension and recall essentially involve a strategic manipulation of models, for instance by matching text information with structures of the The notion of mental space is sometimes also used in formal linguistics as a construct that has similar functions as our notion of a mental model (Faucormier 1985)."}
{"_id":"940279fe8a2a6e9a2614b8cbe391e56c0fb5ceab","title":"XBOS: An Extensible Building Operating System","text":"We present XBOS, an eXtensible Building Operating System for integrated management of previously isolated building subsystems. The key contribution of XBOS is the development of the Building Profile, a canonical, executable description of a building and its subsystems that changes with a building and evolves the control processes and applications accordingly. We discuss the design and implementation of XBOS in the context of 6 dimensions of an effective BAS \u2013 hardware presentation layer, canonical metadata, control process management, building evolution management, security, scalable UX and API \u2013 and evaluate against several recent building and sensor management systems. Lastly, we examine the evolution of a real, 10 month deployment of XBOS in a 7000 sq ft office building."}
{"_id":"0b05900471e6c7599028141836a742c25b84a671","title":"Survey on Collaborative AR for Multi-user in Urban Simulation","text":"This paper describes an augmented reality (AR) environment that allows multiple participants or multi-user to interact with 2D and 3D data. AR simply can provide a collaborative interactive AR environment for urban simulation, where users can interact naturally and intuitively. In addition, the collaborative AR makes multi-user in urban simulation to share simultaneously a real world and virtual world. The fusion between real and virtual world, existed in AR environment by see-through HMDs, achieves higher interactivity as a key features of collaborative AR. In real-time, precise registration between both worlds and multiuser are crucial for the collaborations. Collaborative AR allow multi-user to simultaneously share a real world surrounding them and a virtual world. Common problems in AR environment will be discussed and major issues in collaborative AR will be explained details in this paper. The features of collaboration in AR environment are will be identified and the requirements of collaborative AR will be defined. This paper will give an overview on collaborative AR environment for multi-user in urban studies and planning. The work will also cover numerous systems of collaborative AR environments for multi-user."}
{"_id":"2fb19f33df18e6975653b7574ab4c897d9b6ba06","title":"Multi-scale retinex for color image enhancement","text":"The retinex is a human perception-based image processing algorithm which provides color constancy and dynamic range compression. We have previously reported on a single-scale retinex (SSR) and shown that it can either achieve color\/lightness rendition or dynamic range compression, but not both simultaneously. We now present a multi-scale retinex (MSR) which overcomes this limitation for most scenes. Both color rendition and dynamic range compression are successfully accomplished except for some \\patho-logical\" scenes that have very strong spectral characteristics in a single band."}
{"_id":"916823fe0525f8db8bf5a6863bcab1c7077ff59e","title":"Properties and performance of a center\/surround retinex","text":"The last version of Land's (1986) retinex model for human vision's lightness and color constancy has been implemented and tested in image processing experiments. Previous research has established the mathematical foundations of Land's retinex but has not subjected his lightness theory to extensive image processing experiments. We have sought to define a practical implementation of the retinex without particular concern for its validity as a model for human lightness and color perception. We describe the trade-off between rendition and dynamic range compression that is governed by the surround space constant. Further, unlike previous results, we find that the placement of the logarithmic function is important and produces best results when placed after the surround formation. Also unlike previous results, we find the best rendition for a \"canonical\" gain\/offset applied after the retinex operation. Various functional forms for the retinex surround are evaluated, and a Gaussian form is found to perform better than the inverse square suggested by Land. Images that violate the gray world assumptions (implicit to this retinex) are investigated to provide insight into cases where this retinex fails to produce a good rendition."}
{"_id":"9fa3c3f1fb6f1566638f97fcb993fe121646433e","title":"Real-time single image dehazing using block-to-pixel interpolation and adaptive dark channel prior","text":""}
{"_id":"2d6d5cfa8e99dd53e50bf870e24e72b0be7f7aeb","title":"Decision Combination in Multiple Classifier Systems","text":"A multiple classifier system is a powerful solution to difficult pattern recognition problems involving large class sets and noisy input because it allows simultaneous use of arbitrary feature descriptors and classification procedures. Decisions by the classifiers can be represented as rankings of classes so that they are comparable across different types of classifiers and different instances of a problem. The rankings can be combined by methods that either reduce or rerank a given set of classes. An intersection method and a union method are proposed for class set reduction. Three methods based on the highest rank, the Borda count, and logistic regression are proposed for class set reranking. These methods have been tested in applications on degraded machine-printed characters and words from large lexicons, resulting in substantial improvement in overall correctness."}
{"_id":"6baaf1b2dc375e21a8ca8e8d17e5dc9d7483f4e8","title":"Keyword Search on Spatial Databases","text":"Many applications require finding objects closest to a specified location that contains a set of keywords. For example, online yellow pages allow users to specify an address and a set of keywords. In return, the user obtains a list of businesses whose description contains these keywords, ordered by their distance from the specified address. The problems of nearest neighbor search on spatial data and keyword search on text data have been extensively studied separately. However, to the best of our knowledge there is no efficient method to answer spatial keyword queries, that is, queries that specify both a location and a set of keywords. In this work, we present an efficient method to answer top-k spatial keyword queries. To do so, we introduce an indexing structure called IR2-Tree (Information Retrieval R-Tree) which combines an R-Tree with superimposed text signatures. We present algorithms that construct and maintain an IR2-Tree, and use it to answer top-k spatial keyword queries. Our algorithms are experimentally compared to current methods and are shown to have superior performance and excellent scalability."}
{"_id":"271c1be41d746e146c62313f810476daa21523bf","title":"Imitation, mirror neurons and autism","text":"Various deficits in the cognitive functioning of people with autism have been documented in recent years but these provide only partial explanations for the condition. We focus instead on an imitative disturbance involving difficulties both in copying actions and in inhibiting more stereotyped mimicking, such as echolalia. A candidate for the neural basis of this disturbance may be found in a recently discovered class of neurons in frontal cortex, 'mirror neurons' (MNs). These neurons show activity in relation both to specific actions performed by self and matching actions performed by others, providing a potential bridge between minds. MN systems exist in primates without imitative and 'theory of mind' abilities and we suggest that in order for them to have become utilized to perform social cognitive functions, sophisticated cortical neuronal systems have evolved in which MNs function as key elements. Early developmental failures of MN systems are likely to result in a consequent cascade of developmental impairments characterised by the clinical syndrome of autism."}
{"_id":"7181a8918e34f13402ec8ecf5ff92dff9ddc03f9","title":"Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation","text":"This paper presents a novel unsupervised domain adaptation framework, called Synergistic Image and Feature Adaptation (SIFA), to effectively tackle the problem of domain shift. Domain adaptation has become an important and hot topic in recent studies on deep learning, aiming to recover performance degradation when applying the neural networks to new testing domains. Our proposed SIFA is an elegant learning diagram which presents synergistic fusion of adaptations from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features towards the segmentation task. The feature encoder layers are shared by both perspectives to grasp their mutual benefits during the end-to-end learning procedure. Without using any annotation from the target domain, the learning of our unified model is guided by adversarial losses, with multiple discriminators employed from various aspects. We have extensively validated our method with a challenging application of crossmodality medical image segmentation of cardiac structures. Experimental results demonstrate that our SIFA model recovers the degraded performance from 17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant margin."}
{"_id":"be61fc5903f22384c90b94013a51448e8d5610eb","title":"Lower bounds by probabilistic arguments","text":"The purpose of this paper is to resolve several open problems in the current literature on Boolean circuits, communication complexity, and hashing functions. These lower bound results share the common feature that their proofs utilize probabilistic arguments in an essential way. Specifically, we prove that, to compute the majority function of n Boolean variables, the size of any depth-3 monotone circuit must be greater than 2n\u03b5, and the size of any width-2 branching program must have super-polynomial growth. We also show that, for the problem of deciding whether i \u2264 j for two n-bit integers i and j, the probabilistic \u03b5-error one-way communication complexity is of order \u03b8(n), while the two-way \u03b5-error complexity is O((log n)2). We will also prove that, to compute i \u00bf j mod p for an n-bit prime p, the probabilistic \u03b5-error two-way communication complexity is of order \u03b8(n). Finally, we prove a conjecture of Ullman that uniform hashing is asymptotically optimal in its expected retrieval cost among open address hashing schemes."}
{"_id":"d632593703a604d9ab27e9310ecd9a849d405346","title":"Analysis of Load Balancing Techniques in Cloud Computing","text":"Cloud Computing is an emerging computing paradigm. It aims to share data, calculations, and service transparently over a scalable network of nodes. Since Cloud computing stores the data and disseminated resources in the open environment. So, the amount of data storage increases quickly. In the cloud storage, load balancing is a key issue. It would consume a lot of cost to maintain load information, since the system is too huge to timely disperse load. Load balancing is one of the main challenges in cloud computing which is required to distribute the dynamic workload across multiple nodes to ensure that no single node is overwhelmed. It helps in optimal utilization of resources and hence in enhancing the performance of the system. A few existing scheduling algorithms can maintain load balancing and provide better strategies through efficient job scheduling and resource allocation techniques as well. In order to gain maximum profits with optimized load balancing algorithms, it is necessary to utilize resources efficiently. This paper discusses some of the existing load balancing algorithms in cloud computing and also their challenges."}
{"_id":"1464776f20e2bccb6182f183b5ff2e15b0ae5e56","title":"Benchmarking Deep Reinforcement Learning for Continuous Control","text":"Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released open-source in order to facilitate experimental reproducibility and to encourage adoption by other researchers."}
{"_id":"eae004dd2cd6ef7e31824ac1f50473812101fda0","title":"Leaky wave antenna integrated into gap waveguide technology","text":"A novel leaky wave antenna, based on the gap waveguide technology, is here proposed. A groove gap-waveguide is used as feeding and it also acts as antenna at the same time. The proposed antenna provides an excellent performance while maintaining a simple design. To demonstrate the potential of this radiation mechanism, an antenna was designed to operate between 9 GHz and 11.5 GHz. The antenna has a gain around 12 dB and provides a radiation pattern that steers its radiation direction with the frequency."}
{"_id":"40388bf28adc81ee9208217e7966e9b0b8e81456","title":"What is Gab: A Bastion of Free Speech or an Alt-Right Echo Chamber","text":"Over the past few years, a number of new \u201cfringe\u201d communities, like 4chan or certain subreddits, have gained traction on the Web at a rapid pace. However, more often than not, little is known about how they evolve or what kind of activities they attract, despite recent research has shown that they influence how false information reaches mainstream communities. This motivates the need to monitor these communities and analyze their impact on the Web\u2019s information ecosystem. In August 2016, a new social network called Gab was created as an alternative to Twitter. It positions itself as putting \u201cpeople and free speech first\u201d, welcoming users banned or suspended from other social networks. In this paper, we provide, to the best of our knowledge, the first characterization of Gab. We collect and analyze 22M posts produced by 336K users between August 2016 and January 2018, finding that Gab is predominantly used for the dissemination and discussion of news and world events, and that it attracts alt-right users, conspiracy theorists, and other trolls. We also measure the prevalence of hate speech on the platform, finding it to be much higher than Twitter, but lower than 4chan\u2019s Politically Incorrect board."}
{"_id":"0c6345c83c6454d26124c583f8d00b935eefefba","title":"Lesion-specific coronary artery calcium quantification better predicts cardiac events","text":"CT-based coronary artery calcium (CAC) scanning has been introduced as a non-invasive, low-radiation imaging technique for the assessment of the overall coronary arterial atherosclerotic burden. A three dimensional CAC volume contains significant clinically relevant information, which is unused by conventional whole-heart CAC quantification methods. In this paper, we have developed a more detailed distance-weighted lesion-specific CAC quantification framework that predicts cardiac events better than the conventional whole-heart CAC measures. This framework consists of (1) a novel lesion-specific CAC quantification tool that measures each calcific lesion's attenuation, morphologic and geometric statistics; (2) a distance-weighted event risk model to estimate the risk probability caused by each lesion, and (3) a Naive Bayesian technique for risk integration. We have tested our lesion-specific event predictor on 30 CAC positive scans (10 with events and 20 without events), and compared it with conventional whole-heart CAC scores. Experiment results showed our novel approach significantly improves the prediction accuracy, including AUC of ROC analysis was improved from 66 \u223c 68% to 75%, and sensitivities was improved by 20 \u223c 30% at the cutpoints of 80% specificity."}
{"_id":"a190742917469c60e96e401140a73de5548be360","title":"Link prediction in bipartite graphs using internal links and weighted projection","text":"Many real-world complex networks, like client-product or file-provider relations, have a bipartite nature and evolve during time. Predicting links that will appear in them is one of the main approach to understand their dynamics. Only few works address the bipartite case, though, despite its high practical interest and the specific challenges it raises. We define in this paper the notion of internal links in bipartite graphs and propose a link prediction method based on them. We describe the method and experimentally compare it to a basic collaborative filtering approach. We present results obtained for two typical practical cases. We reach the conclusion that our method performs very well, and that internal links play an important role in bipartite graphs and their dynamics."}
{"_id":"b25613704201c0c9dbbb8223c61cd3cbb0c51168","title":"Modeling social interestingness in conversational stories","text":"Telling stories about our daily lives is one of the most ubiquitous, consequential and seamless ways in which we socialize. Current narrative generation methods mostly require specification of a priori knowledge or comprehensive domain models, which are not generalizable across contexts. Hence, such approaches do not lend themselves well to new and unpredictable domains of observation and interaction, in which social stories usually occur. In this paper, we describe a methodology for categorizing event descriptions as being socially interesting. The event sequences are drawn from crowd-sourced Plot Graphs. The models include low-level natural language and higher-level features. The results from classification and regression tasks look promising overall, indicating that general metrics of social interestingness of stories could be modeled for sociable agents."}
{"_id":"682a6668e853992ef57b2d20f4e5cb67f09aa87f","title":"Depth-of-field-based alpha-matte extraction","text":"In compositing applications, objects depicted in images frequently have to be separated from their background, so that they can be placed in a new environment. Alpha mattes are important tools aiding the selection of objects, but cannot normally be created in a fully automatic way. We present an algorithm that requires as input two images---one where the object is in focus, and one where the background is in focus---and then automatically produces an alpha matte indicating which pixels belong to the object. This algorithm is inspired by human visual processing and involves nonlinear response compression, center-surround mechanisms as well as a filling-in stage. The output can then be refined with standard computer vision techniques."}
{"_id":"f8f92624c8794d54e08b3a8f94910952ae03cade","title":"CamStyle: A Novel Data Augmentation Method for Person Re-Identification","text":"Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https:\/\/github.com\/zhunzhong07\/CamStyle."}
{"_id":"42fdf2999c46babe535974e14375fbb224445757","title":"Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables","text":"This study compared two alternative techniques for predicting forest cover types from cartographic variables. The study evaluated four wilderness areas in the Roosevelt National Forest, located in the Front Range of northern Colorado. Cover type data came from US Forest Service inventory information, while the cartographic variables used to predict cover type consisted of elevation, aspect, and other information derived from standard digital spatial data processed in a geographic information system (GIS). The results of the comparison indicated that a feedforward artificial neural network model more accurately predicted forest cover type than did a traditional statistical model based on Gaussian discriminant analysis. \u00a9 1999 Elsevier Science B.V. All rights reserved."}
{"_id":"d4884e0c1059046b42c0770f051e485275b93724","title":"Integrating Algorithmic Planning and Deep Learning for Partially Observable Navigation","text":"We propose to take a novel approach to robot system design where each building block of a larger system is represented as a differentiable program, i.e. a deep neural network. This representation allows for integrating algorithmic planning and deep learning in a principled manner, and thus combine the benefits of model-free and model-based methods. We apply the proposed approach to a challenging partially observable robot navigation task. The robot must navigate to a goal in a previously unseen 3-D environment without knowing its initial location, and instead relying on a 2-D floor map and visual observations from an onboard camera. We introduce the Navigation Networks (NavNets) that encode state estimation, planning and acting in a single, end-to-end trainable recurrent neural network. In preliminary simulation experiments we successfully trained navigation networks to solve the challenging partially observable navigation task."}
{"_id":"1c66fa1f3f189ca25bd657f53d77fde64c75f3da","title":"Adaptive Color Attributes for Real-Time Visual Tracking","text":"Visual tracking is a challenging problem in computer vision. Most state-of-the-art visual trackers either rely on luminance information or use simple color representations for image description. Contrary to visual tracking, for object recognition and detection, sophisticated color features when combined with luminance have shown to provide excellent performance. Due to the complexity of the tracking problem, the desired color feature should be computationally efficient, and possess a certain amount of photometric invariance while maintaining high discriminative power. This paper investigates the contribution of color in a tracking-by-detection framework. Our results suggest that color attributes provides superior performance for visual tracking. We further propose an adaptive low-dimensional variant of color attributes. Both quantitative and attribute-based evaluations are performed on 41 challenging benchmark color sequences. The proposed approach improves the baseline intensity-based tracker by 24 % in median distance precision. Furthermore, we show that our approach outperforms state-of-the-art tracking methods while running at more than 100 frames per second."}
{"_id":"fb340484981edfa25965c8b5e9751a1e3e7b5a41","title":"SDN-Based Data Center Networking With Collaboration of Multipath TCP and Segment Routing","text":"Large-scale data centers are major infrastructures in the big data era. Therefore, a stable and optimized architecture is required for data center networks (DCNs) to provide services to the applications. Many studies use software-defined network (SDN)-based multipath TCP (MPTCP) implementation to utilize the entire DCN\u2019s performance and achieve good results. However, the deployment cost is high. In SDN-based MPTCP solutions, the flow allocation mechanism leads to a large number of forwarding rules, which may lead to storage consumption. Considering the advantages and limitations of the SDN-based MPTCP solution, we aim to reduce the deployment cost due to the use of an extremely expensive storage resource\u2014ternary content addressable memory (TCAM). We combine MPTCP and segment routing (SR) for traffic management to limit the storage requirements. And to the best of our knowledge, we are among the first to use the collaboration of MPTCP and SR in multi-rooted DCN topologies. To explain how MPTCP and SR work together, we use four-layer DCN architecture for better description, which contains physical topology, SR over the topology, multiple path selection supplied by MPTCP, and traffic scheduling on the selected paths. Finally, we implement the proposed design in a simulated SDN-based DCN environment. The simulation results reveal the great benefits of such a collaborative approach."}
{"_id":"141e6c1dd532504611266d08458dbe2a0dbb4e98","title":"Multiple kernel learning, conic duality, and the SMO algorithm","text":"While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes."}
{"_id":"e0cf9c51192f63c3a9e3b23aa07ee5654fc97b68","title":"Large Margin Methods for Structured and Interdependent Output Variables","text":"Learning general functional dependencies between arbitra ry input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representa tions, this paper addresses the complementary issue of designing classification algorithms that c an deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider pr oblems involving multiple dependent output variables, structured output spaces, and classifica tion problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulat ion. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem i n polynomial time for a large class of problems. The proposed method has important applications i n areas such as computational biology, natural language processing, information retrieval\/extr action, and optical character recognition. Experiments from various domains involving different types o f output spaces emphasize the breadth and generality of our approach."}
{"_id":"0948365ef39ef153e61e9569ade541cf881c7c2a","title":"Learning the Kernel Matrix with Semi-Definite Programming","text":"Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive definite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space\u2014classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semi-definite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm\u2014 using the labelled part of the data one can learn an embedding also for the unlabelled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method to learn the 2-norm soft margin parameter in support vector machines, solving another important open problem. Finally, the novel approach presented in the paper is supported by positive empirical results."}
{"_id":"b85509aa44db10d3116b36599014df39164c2a2a","title":"Open-World Knowledge Graph Completion","text":"Knowledge Graphs (KGs) have been applied to many tasks including Web search, link prediction, recommendation, natural language processing, and entity linking. However, most KGs are far from complete and are growing at a rapid pace. To address these problems, Knowledge Graph Completion (KGC) has been proposed to improve KGs by filling in its missing connections. Unlike existing methods which hold a closed-world assumption, i.e., where KGs are fixed and new entities cannot be easily added, in the present work we relax this assumption and propose a new open-world KGC task. As a first attempt to solve this task we introduce an openworld KGC model called ConMask. This model learns embeddings of the entity\u2019s name and parts of its text-description to connect unseen entities to the KG. To mitigate the presence of noisy text descriptions, ConMask uses a relationshipdependent content masking to extract relevant snippets and then trains a fully convolutional neural network to fuse the extracted snippets with entities in the KG. Experiments on large data sets, both old and new, show that ConMask performs well in the open-world KGC task and even outperforms existing KGC models on the standard closed-world KGC task. Introduction Knowledge Graphs (KGs) are a special type of information network that represents knowledge using RDF-style triples \u3008h, r, t\u3009, where h represents some head entity and r represents some relationship that connects h to some tail entity t. In this formalism a statement like \u201cSpringfield is the capital of Illinois\u201d can be represented as \u3008Springfield, capitalOf, Illinois\u3009. Recently, a variety of KGs, such as DBPedia (Lehmann et al. 2015), and ConceptNet (Speer, Chin, and Havasi 2017), have been curated in the service of fact checking (Shi and Weninger 2016), question answering (Lukovnikov et al. 2017), entity linking (Hachey et al. 2013), and for many other tasks (Nickel et al. 2016). Despite their usefulness and popularity, KGs are often noisy and incomplete. For example, DBPedia, which is generated from Wikipedia\u2019s infoboxes, contains 4.6 million entities, but half of these entities contain less than 5 relationships. Based on this observation, researchers aim to improve the accuracy and reliability of KGs by predicting the existence Copyright c \u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. (or probability) of relationships. This task is often called Knowledge Graph Completion (KGC). Continuing the example from above, suppose the relationship capitalOf is missing between Indianapolis and Indiana; the KGC task might predict this missing relationship based on the topological similarity between this part of the KG and the part containing Springfield and Illinois. Progress in vector embeddings originating with word2vec has produced major advancements in the KGC task. Typical embedding-based KGC algorithms like TransE (Bordes et al. 2013) and others learn lowdimensional representations (i.e., embeddings) for entities and relationships using topological features. These models are able to predict the existence of missing relationships thereby \u201ccompleting\u201d the KG. Existing KGC models implicitly operate under the Closed-World Assumption (Reiter 1978) in which all entities and relationships in the KG cannot be changed \u2013 only discovered. We formally define the Closed-word KGC task as follows: Definition 1 Given an incomplete Knowledge Graph G = (E,R,T), where E, R, and T are the entity set, relationship set, and triple set respectively, Closed-World Knowledge Graph Completion completes G by finding a set of missing triples T = {\u3008h, r, t\u3009|h \u2208 E, r \u2208 R, t \u2208 E, \u3008h, r, t\u3009 \/ \u2208 T} in the incomplete Knowledge Graph G. Closed-world KGC models heavily rely on the connectivity of the existing KG and are best able to predict relationships between existing, well-connected entities. Unfortunately, because of their strict reliance on the connectivity of the existing KG, closed-world KGC models are unable to predict the relationships of poorly connected or new entities. Therefore, we assess that closed-world KGC is most suitable for fixed or slowly evolving KGs. However, most real-world KGs evolve quickly with new entities and relationships being added by the minute. For example, in the 6 months between DBPedia\u2019s October 2015 release and its April 2016 release 36, 340 new English entities were added \u2013 a rate of 200 new entities per day. Recall that DBPedia merely tracks changes to Wikipedia infoboxes, so these updates do not include newly added articles without valid infobox data. Because of the accelerated growth of online information, repeatedly re-training closed-world models every day (or hour) has become impractical. In the present work we borrow the idea of openworld assumption from probabilistic database literature (Ceylan, Darwiche, and Van den Broeck 2016) and relax the closed-world assumption to develop an Open-World Knowledge Graph Completion model capable of predicting relationships involving unseen entities or those entities that have only a few connections. Formally we define the openworld KGC task as follows: Definition 2 Given an incomplete Knowledge Graph G = (E,R,T), where E, R, and T are the entity set, relationship set, and triple set respectively, Open-World Knowledge Graph Completion completes G by finding a set of missing triples T = {\u3008h, r, t\u3009|\u3008h, r, t\u3009 \/ \u2208 T, h \u2208 E, t \u2208 E, r \u2208 R} in the incomplete Knowledge Graph G where E is an entity superset. In Defn. 2 we relax the constraint on the triple set T so that triples in T can contain entities that are absent from the original entity set E. Closed-world KGC models learn entity and relationship embedding vectors by updating an initially random vector based on the KG\u2019s topology. Therefore, any triple \u3008h, r, t\u3009 \u2208 T \u2032 such that h \/ \u2208 E or t \/ \u2208 E will only ever be represented by its initial random vector because its absence does not permit updates from any inference function. In order to predict the missing connections for unseen entities, it is necessary to develop alternative features to replace the topological features used by closed-world models. Text content is a natural substitute for the missing topological features of disconnected or newly added entities. Indeed, most KGs such as FreeBase (Bollacker et al. 2008), DBPedia (Lehmann et al. 2015), and SemMedDB (Kilicoglu et al. 2012) were either directly extracted from (Lin et al. 2016; Ji and Grishman 2011), or are built in parallel to some underlying textual descriptions. However, open-world KGC differs from the standard information extraction task because 1) Rather than extracting triples from a large text corpus, the goal of open-world KGC is to discover missing relationships; and 2) Rather than a pipeline of independent subtasks like Entity Linking (Francis-Landau, Durrett, and Klein 2016) and Slotfilling (Liu and Lane 2016), etc., open-world KGC is a holistic task that operates as a single model. Although it may seem intuitive to simply include an entity\u2019s description into an existing KGC model, we find that learning useful vector embeddings from unstructured text is much more challenging than learning topology-embeddings as in the closed-world task. First, in closed-world KGC models, each entity will have a unique embedding, which is learned from its directly connected neighbors; whereas open-world KGC models must fuse entity embeddings with the word embeddings of the entity\u2019s description. These word embeddings must be updated by entities sharing the same words regardless of their connectivity status. Second, because of the inclusion of unstructured content, open-world models are likely to include noisy or redundant information. With respect to these challenges, the present work makes the following contributions: 1. We describe an open-world KGC model called ConMask that uses relationship-dependent content masking to reduce noise in the given entity description and uses fully convolutional neural networks (FCN) to fuse related text into a relationship-dependent entity embedding. 2. We release two new Knowledge Graph Completion data sets constructed from DBPedia and Wikipedia for use in closed-world and open-world KGC evaluation. Before introduce the ConMask model, we first present preliminary material by describing relevant KGC models. Then we describe the methodology, data sets, and a robust case study of closed-world and open-world KGC tasks. Finally, we draw conclusions and offer suggestions for future work. Closed-World Knowledge Graph Completion A variety of models have been developed to solve the closedworld KGC task. The most fundamental and widely used model is a translation-based Representation Learning (RL) model called TransE (Bordes et al. 2013). TransE assumes there exists a simple function that can translate the embedding of the head entity to the embedding of some tail entity via some relationship:"}
{"_id":"232b43584b2236669c0a53702ad89ab10c3886ea","title":"Implicit Quantile Networks for Distributional Reinforcement Learning","text":"For the merging function m, the simplest choice would be a simple vector concatenation of \u03c8(x) and \u03c6(\u03c4). Note however, that the MLP f which takes in the output of m and outputs the action-value quantiles, only has a single hidden layer in the DQN network. Therefore, to force a sufficiently early interaction between the two representations, we also considered a multiplicative function m(\u03c8, \u03c6) = \u03c8 \u03c6, where denotes the element-wise (Hadamard) product of two vectors, as well as a \u2018residual\u2019 function m(\u03c8, \u03c6) = \u03c8 (1 + \u03c6)."}
{"_id":"476f764f44d9d31b6bb86dc72b34de681b8b4b03","title":"DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image","text":"In this paper, we propose a deep learning architecture that produces accurate dense depth for the outdoor scene from a single color image and a sparse depth. Inspired by the indoor depth completion, our network estimates surface normals as the intermediate representation to produce dense depth, and can be trained end-to-end. With a modified encoder-decoder structure, our network effectively fuses the dense color image and the sparse LiDAR depth. To address outdoor specific challenges, our network predicts a confidence mask to handle mixed LiDAR signals near foreground boundaries due to occlusion, and combines estimates from the color image and surface normals with learned attention maps to improve the depth accuracy especially for distant areas. Extensive experiments demonstrate that our model improves upon the state-of-the-art performance on KITTI depth completion benchmark. Ablation study shows the positive impact of each model components to the final performance, and comprehensive analysis shows that our model generalizes well to the input with higher sparsity or from indoor scenes."}
{"_id":"e5aad460c7b8b1fff70e45a4d1fb79180cf4d098","title":"Introduction Welcome to this early seminal work , Fostering Resiliency in Kids : Protective Factors in the Family , School and Community","text":"The field of prevention, both research and practice, came a long way in the1980s: from short-term, even one-shot, individual-focused interventions in the school classroom to a growing awareness and beginning implementation of long-term, comprehensive, environmental-focused interventions expanding beyond the school to include the community. Furthermore, in the mid-1980s we finally started to hear preventionists talking about prevention strategies and programs based on research identifying the underlying risk factors for problems such as alcohol and other drug abuse, teen pregnancy, delinquency and gangs, and dropping out (Hawkins, Lishner, and Catalano, 1985). While certainly a giant step in the right direction, the identification of risks does not necessarily provide us with a clear sense of just what strategies we need to implement to reduce the risks. More recently, we are hearing preventionists talk about \u201cprotective factors,\u201d about building \u201cresiliency\u201d in youth, about basing our strategies on what research hast old us about the environmental factors that facilitate the development of youth who do not get involved in life-compromising problems (Benard, March 1987). What clearly becomes the challenge for the l990s is the implementation of prevention strategies that strengthen protective factors in our families, schools, an communities. As Gibbs and Bennett (1990) conceptualize the process, we must\u201dturn the situation around by translating negative risk factors into positive action strategies\u201d which are, in essence, protective factors. After a brief overview of the protective factor research phenomenon, this paper will discuss the major protective factors that research has identified as contributing to the development of resiliency in youth and the implications of this for building effective prevention programs."}
{"_id":"acdbcb6f40a178b4a1cb95bdfc812f21e3110d54","title":"A Novel AHRS Inertial Sensor-Based Algorithm for Wheelchair Propulsion Performance Analysis","text":"With the increasing rise of professionalism in sport, athletes, teams, and coaches are looking to technology to monitor performance in both games and training in order to find a competitive advantage. The use of inertial sensors has been proposed as a cost effective and adaptable measurement device for monitoring wheelchair kinematics; however, the outcomes are dependent on the reliability of the processing algorithms. Though there are a variety of algorithms that have been proposed to monitor wheelchair propulsion in court sports, they all have limitations. Through experimental testing, we have shown the Attitude and Heading Reference System (AHRS)-based algorithm to be a suitable and reliable candidate algorithm for estimating velocity, distance, and approximating trajectory. The proposed algorithm is computationally inexpensive, agnostic of wheel camber, not sensitive to sensor placement, and can be embedded for real-time implementations. The research is conducted under Griffith University Ethics (GU Ref No: 2016\/294)."}
{"_id":"75514c2623a0c02776b42689963e987c29b42c05","title":"Tuning of PID controller based on Fruit Fly Optimization Algorithm","text":"The Proportional - Integral - Derivative (PID) controllers are one of the most popular controllers used in industry because of their remarkable effectiveness, simplicity of implementation and broad applicability. PID tuning is the key issue in the design of PID controllers and most of the tuning processes are implemented manually resulting in difficulty and time consuming. To enhance the capabilities of traditional PID parameters tuning techniques, modern heuristics approaches, such as Genetic Algorithm (GA) and Particle Swarm Optimization (PSO), are employed recent years. In this paper, a novel tuning method based on Fruit Fly Optimization Algorithm (FOA) is proposed to optimize PID controller parameters. Each fruit fly's position represents a candidate solution for PID parameters. When the fruit fly swarm flies towards one location, it is treated as the evolution of each iterative swarm. After hundreds of iteration, the tuning results - the best PID controller parameters can be obtained. The main advantages of the proposed method include ease of implementation, stable convergence characteristic, large searching range, ease of transformation of such concept into program code and ease of understanding. Simulation results demonstrate that the FOA-Based optimized PID (FOA - PID) controller is with the capability of providing satisfactory closed - loop performance."}
{"_id":"91cf9beb696cbb0818609614f4da7351262eac86","title":"Event detection over twitter social media streams","text":"In recent years, microblogs have become an important source for reporting real-world events. A real-world occurrence reported in microblogs is also called a social event. Social events may hold critical materials that describe the situations during a crisis. In real applications, such as crisis management and decision making, monitoring the critical events over social streams will enable watch officers to analyze a whole situation that is a composite event, and make the right decision based on the detailed contexts such as what is happening, where an event is happening, and who are involved. Although there has been significant research effort on detecting a target event in social networks based on a single source, in crisis, we often want to analyze the composite events contributed by different social users. So far, the problem of integrating ambiguous views from different users is not well investigated. To address this issue, we propose a novel framework to detect composite social events over streams, which fully exploits the information of social data over multiple dimensions. Specifically, we first propose a graphical model called location-time constrained topic (LTT) to capture the content, time, and location of social messages. Using LTT, a social message is represented as a probability distribution over a set of topics by inference, and the similarity between two messages is measured by the distance between their distributions. Then, the events are identified by conducting efficient similarity joins over social media streams. To accelerate the similarity join, we also propose a variable dimensional extendible hash over social streams. We have conducted extensive experiments to prove the high effectiveness and efficiency of the proposed approach."}
{"_id":"e10642453c5c99442eb24743c4bab60a3a0b6273","title":"Kullback-Leibler Divergence Constrained Distributionally Robust Optimization","text":"In this paper we study distributionally robust optimization (DRO) problems where the ambiguity set of the probability distribution is defined by the Kullback-Leibler (KL) divergence. We consider DRO problems where the ambiguity is in the objective function, which takes a form of an expectation, and show that the resulted minimax DRO problems can be formulated as a one-layer convex minimization problem. We also consider DRO problems where the ambiguity is in the constraint. We show that ambiguous expectation-constrained programs may be reformulated as a one-layer convex optimization problem that takes the form of the Benstein approximation of Nemirovski and Shapiro (2006). We further consider distributionally robust probabilistic programs. We show that the optimal solution of a probability minimization problem is also optimal for the distributionally robust version of the same problem, and also show that the ambiguous chance-constrained programs (CCPs) may be reformulated as the original CCP with an adjusted confidence level. A number of examples and special cases are also discussed in the paper to show that the reformulated problems may take simple forms that can be solved easily. The main contribution of the paper is to show that the KL divergence constrained DRO problems are often of the same complexity as their original stochastic programming problems and, thus, KL divergence appears a good candidate in modeling distribution ambiguities in mathematical programming."}
{"_id":"33dc678ba56819b2fd05c1b538b0398125f4ccd3","title":"Measuring the effectiveness of answers in Yahoo! Answers","text":"Purpose \u2013 This study investigates the ways in which effectiveness of answers in Yahoo! Answers, one of the largest community question answering sites (CQAs), is related to question types and answerer reputation. Effective answers are defined as those that are detailed, readable, superior in quality, and contributed promptly. Five question types that were studied include factoid, list, definition, complex interactive, and opinion. Answerer reputation refers to the past track record of answerers in the community. Design\/Methodology\/Approach \u2013 The dataset comprises 1,459 answers posted in Yahoo! Answers in response to 464 questions that were distributed across the five question types. The analysis was done using factorial analysis of variance. Findings \u2013 The results indicate that factoid, definition and opinion questions were comparable in attracting high quality as well as readable answers. Although reputed answerers generally fared better in offering detailed and high quality answers, novices were found to submit more readable responses. Moreover, novices were more prompt in answering factoid, list and definition questions. Originality\/value \u2013 By analyzing variations in answer effectiveness with a twin-focus on question types and answerer reputation, this study explores a strand of CQA research that has hitherto received limited attention. The findings offer insights to users and designers of CQAs."}
{"_id":"41e07d21451df21dacda2fea6f90b53bf4b89b27","title":"An Inference Model for Semantic Entailment in Natural Language","text":"Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. We present a principled approach to this problem that builds on inducing rerepresentations of text snippets into a hierarchical knowledge representation along with a sound inferential mechanism that makes use of it to prove semantic entailment."}
{"_id":"d744e447012f2dea175462c1a15472787b171adf","title":"The social role of social media: the case of Chennai rains-2015","text":"Social media has altered the way individuals communicate in present scenario. Individuals feel more connected on Facebook and Twitter with greater communication freedom to chat, share pictures, and videos. Hence, social media is widely employed by various companies to promote their product and services and establish better customer relationships. Owing to the increasing popularity of these social media platforms, their usage is also expanding significantly. Various studies have discussed the importance of social media in the corporate world for effective marketing communication, customer relationships, and firm performance, but no studies have focused on the social role of social media, i.e., in disaster resilience in India. Various academicians and practitioners have advocated the importance and use of social media in disaster resilience. This article focuses on the role that social media can play during the time of natural disasters, with the help of the recent case of Chennai floods in India. This study provides a better understanding about the role social media can play in natural disaster resilience in Indian context."}
{"_id":"9c40ba65099210f9af2a25567fd1a5461e1fb80d","title":"A New Algorithm for Detecting Text Line in Handwritten Documents","text":"Curvilinear text line detection and segmentation in handwritten documents is a significant challenge for handwriting recognition. Given no prior knowledge of script, we model text line detection as an image segmentation problem by enhancing text line structure using a Gaussian window, and adopting the level set method to evolve text line boundaries. Experiments show that the proposed method achieves high accuracy for detecting text lines in both handwritten and machine printed documents with many scripts."}
{"_id":"e43651a1dc78f5420268c49508dce13e1f154a7a","title":"Modelling and control of a 2-DOF planar parallel manipulator for semiconductor packaging systems","text":"A novel direct-drive planar parallel manipulator for high-speed and high-precision semiconductor packaging systems is presented. High precision kinematics design, significant reduction on moving mass and driving power of the actuators over traditional XY motion stages are the benefits of the proposed manipulator. The mathematical model of the manipulator is obtained using the Newton-Euler method and a practical model-based control design approach is employed to design the PID computed-torque controller. Experimental results demonstrate that the proposed planar parallel manipulator has significant improvements on motion performance in terms of positioning accuracy, settling time and stability when compared with traditional XY stages. This shows that the proposed planar parallel manipulator can provide a superior alternative for replacing traditional XY motion stages in high precision low-payload applications"}
{"_id":"5e38a1f41a5ab3c04cfbfb1d9de4ba8a7721d8b7","title":"BuildingRules: a trigger-action based system to manage complex commercial buildings","text":"Modern Building Management Systems (BMSs) provide limited amount of control to its occupants, and typically allow only the facility manager to set the building policies. BuildingRules let occupants to customise their office spaces using trigger-action programming. In order to accomplish this task, BuildingRules automatically detects conflicts among the policies expressed by the users using a SMT based logic. We tested our system with 23 users across 17 days in a virtual office building, and evaluate the effectiveness and scalability of the system."}
{"_id":"eb70e45a5f4b74edc1e1fdfa052905184daf655c","title":"Social Media, News and Political Information during the US Election: Was Polarizing Content Concentrated in Swing States?","text":"US voters shared large volumes of polarizing political news and information in the form of links to content from Russian, WikiLeaks and junk news sources. Was this low quality political information distributed evenly around the country, or concentrated in swing states and particular parts of the country? In this data memo we apply a tested dictionary of sources about political news and information being shared over Twitter over a ten day period around the 2016 Presidential Election. Using self-reported location information, we place a third of users by state and create a simple index for the distribution of polarizing content around the country. We find that (1) nationally, Twitter users got more misinformation, polarizing and conspiratorial content than professionally produced news. (2) Users in some states, however, shared more polarizing political news and information than users in other states. (3) Average levels of misinformation were higher in swing states than in uncontested states, even when weighted for the relative size of the user population in each state. We conclude with some observations about the impact of strategically disseminated polarizing information on public life. COMPUTATIONAL PROPAGANDA AND THE 2016 US ELECTION Social media plays an important role in the circulation of ideas about public policy and politics. Political actors and governments worldwide are deploying both people and algorithms to shape public life. Bots are pieces of software intended to perform simple, repetitive, and robotic tasks. They can perform legitimate tasks on social media like delivering news and information\u2014real news as well as junk\u2014or undertake malicious activities like spamming, harassment and hate speech. Whatever their uses, bots on social media platforms are able to rapidly deploy messages, replicate themselves, and pass as human users. They are also a pernicious means of spreading junk news over social networks of family and friends. Computational propaganda flourished during the 2016 US Presidential Election. There were numerous examples of misinformation distributed online with the intention of misleading voters or simply earning a profit. Multiple media reports have investigated how \u201cfake news\u201d may have propelled Donald J. Trump to victory. What kinds of political news and information were social media users in the United States sharing in advance of voting day? How much of it was extremist, sensationalist, conspiratorial, masked commentary, fake, or some other form of junk news? Was this misleading information concentrated in the battleground states where the margins of victory for candidates had big consequences for electoral outcomes? SOCIAL MEDIA AND JUNK NEWS Junk news, widely distributed over social media platforms, can in many cases be considered to be a form of computational propaganda. Social media platforms have served significant volumes of fake, sensational, and other forms of junk news at sensitive moments in public life, though most platforms reveal little about how much of this content there is or what its impact on users may be. The World Economic Forum recently identified the rapid spread of misinformation online as among the top 10 perils to society. Prior research has found that social media favors sensationalist content, regardless of whether the content has been fact checked or is from a reliable source. When junk news is backed by automation, either through dissemination algorithms that the platform operators cannot fully explain or through political bots that promote content in a preprogrammed way, political actors have a powerful set of tools for computational propaganda. Both state and non-state political actors deliberately manipulate and amplify non-factual information online. Fake news websites deliberately publish misleading, deceptive or incorrect information purporting to be real news for political, economic or cultural gain. These sites often rely on social media to attract web traffic and drive engagement. Both fake news websites and political bots are crucial tools in digital propaganda attacks\u2014they aim to influence conversations, demobilize opposition and generate false support. SAMPLING AND METHOD Our analysis is based on a dataset of 22,117,221 tweets collected between November 1-11, 2016, that contained hashtags related to politics and the election in the US. Our previous analyses have been based on samples of political conversation, over Twitter that used hashtags that were relevant to the US election as a whole. In this analysis, we selected users who provided some evidence of physical location across the United States in their profiles. Within our initial sample, approximately 7,083,691 tweets, 32 percent of the total"}
{"_id":"122ab8b1ac332bceacf556bc50268b9d80552bb3","title":"Uptane: Security and Customizability of Software Updates for Vehicles","text":"A widely accepted premise is that complex software frequently contains bugs that can be remotely exploited by attackers. When this software is on an electronic control unit (ECU) in a vehicle, exploitation of these bugs can have life or death consequences. Since software for vehicles is likely to proliferate and grow more complex in time, the number of exploitable vulnerabilities will increase. As a result, manufacturers are keenly aware of the need to quickly and efficiently deploy updates so that software vulnerabilities can be remedied as soon as possible."}
{"_id":"8d41667f77c8d2aad1eb0ad2b8501e6080f235f1","title":"Automated classification of security requirements","text":"Requirement engineers are not able to elicit and analyze the security requirements clearly, that are essential for the development of secure and reliable software. Proper identification of security requirements present in the Software Requirement Specification (SRS) document has been a problem being faced by the developers. As a result, they are not able to deliver the software free from threats and vulnerabilities. Thus, in this paper, we intend to mine the descriptions of security requirements present in the SRS document and thereafter develop the classification models. The security-based descriptions are analyzed using text mining techniques and are then classified into four types of security requirements viz. authentication-authorization, access control, cryptography-encryption and data integrity using J48 decision tree method. Corresponding to each type of security requirement, a prediction model has been developed. The effectiveness of the prediction models is evaluated against requirement specifications collected from 15 projects which have been developed by MS students at DePaul University. The result analysis indicated that all the four models have performed very well in predicting their respective type of security requirements."}
{"_id":"735762290da16163e6f408e1dd138d47ae400745","title":"CitySpectrum: a non-negative tensor factorization approach","text":"People flow at a citywide level is in a mixed state with several basic patterns (e.g. commuting, working, commercial), and it is therefore difficult to extract useful information from such a mixture of patterns directly. In this paper, we proposed a novel tensor factorization approach to modeling city dynamics in a basic life pattern space (CitySpectral Space). To obtain the CitySpectrum, we utilized Non-negative Tensor Factorization (NTF) to decompose a people flow tensor into basic life pattern tensors, described by three bases i.e. the intensity variation among different regions, the time-of-day and the sample days. We apply our approach to a big mobile phone GPS log dataset (containing 1.6 million users) to model the fluctuation in people flow before and after the Great East Japan Earthquake from a CitySpectral perspective. In addition, our framework is extensible to a variety of auxiliary spatial-temporal data. We parametrize a people flow with a spatial distribution of the Points of Interest (POIs) to quantitatively analyze the relationship between human mobility and POI distribution. Based on the parametric people flow, we propose a spectral approach for a site-selection recommendation and people flow simulation in another similar area using POI distribution."}
{"_id":"2f3d178126225f8618ab01cfd3786edcec0d30b7","title":"A Novel Artificial Bee Colony Algorithm Based on Modified Search Equation and Orthogonal Learning","text":"The artificial bee colony (ABC) algorithm is a relatively new optimization technique which has been shown to be competitive to other population-based algorithms. However, ABC has an insufficiency regarding its solution search equation, which is good at exploration but poor at exploitation. To address this concerning issue, we first propose an improved ABC method called as CABC where a modified search equation is applied to generate a candidate solution to improve the search ability of ABC. Furthermore, we use the orthogonal experimental design (OED) to form an orthogonal learning (OL) strategy for variant ABCs to discover more useful information from the search experiences. Owing to OED's good character of sampling a small number of well representative combinations for testing, the OL strategy can construct a more promising and efficient candidate solution. In this paper, the OL strategy is applied to three versions of ABC, i.e., the standard ABC, global-best-guided ABC (GABC), and CABC, which yields OABC, OGABC, and OCABC, respectively. The experimental results on a set of 22 benchmark functions demonstrate the effectiveness and efficiency of the modified search equation and the OL strategy. The comparisons with some other ABCs and several state-of-the-art algorithms show that the proposed algorithms significantly improve the performance of ABC. Moreover, OCABC offers the highest solution quality, fastest global convergence, and strongest robustness among all the contenders on almost all the test functions."}
{"_id":"76d259940fe57f399a992ea0e2bb6cd5304b6a23","title":"Why did my car just do that ? Explaining semi-autonomous driving actions to improve driver understanding , trust , and performance","text":"This study explores, in the context of semiautonomous driving, how the content of the verbalized message accompanying the car\u2019s autonomous action affects the driver\u2019s attitude and safety performance. Using a driving simulator with an auto-braking function, we tested different messages that provided advance explanation of the car\u2019s imminent autonomous action. Messages providing only \u201chow\u201d information describing actions (e.g., \u201cThe car is braking\u201d) led to poor driving performance, whereas \u201cwhy\u201d information describing reasoning for actions (e.g., \u201cObstacle ahead\u201d) was preferred by drivers and led to better driving performance. Providing both \u201chow and why\u201d resulted in the safest driving performance but increased negative feelings in drivers. These results suggest that, to increase overall safety, car makers need to attend not only to the design of autonomous actions but also to the right way to explain these actions to the drivers."}
{"_id":"8eea0da60738a54c0fc6a092aecf0daf0c51cee3","title":"Public opinion on automated driving : Results of an international questionnaire among 5000 respondents","text":"This study investigated user acceptance, concerns, and willingness to buy partially, highly, and fully automated vehicles. By means of a 63-question Internet-based survey, we collected 5000 responses from 109 countries (40 countries with at least 25 respondents). We determined cross-national differences, and assessed correlations with personal variables, such as age, gender, and personality traits as measured with a short version of the Big Five Inventory. Results showed that respondents, on average, found manual driving the most enjoyable mode of driving. Responses were diverse: 22% of the respondents did not want to pay more than $0 for a fully automated driving system, whereas 5% indicated they would be willing to pay more than $30,000, and 33% indicated that fully automated driving would be highly enjoyable. 69% of respondents estimated that fully automated driving will reach a 50% market share between now and 2050. Respondents were found to be most concerned about software hacking\/misuse, and were also concerned about legal issues and safety. Respondents scoring higher on neuroticism were slightly less comfortable about data transmitting, whereas respondents scoring higher on agreeableness were slightly more comfortable with this. Respondents from more developed countries (in terms of lower accident statistics, higher education, and higher income) were less comfortable with their vehicle transmitting data, with cross-national correlations between q = 0.80 and q = 0.90. The present results indicate the major areas of promise and concern among the international public, and could be useful for vehicle developers and other stakeholders. 2015 Elsevier Ltd. All rights reserved."}
{"_id":"eab0415ebbf5a2e163737e34df4d008405c2be5f","title":"Effects of adaptive cruise control and highly automated driving on workload and situation awareness : A review of the empirical evidence","text":"Department of BioMechanical Engineering, Faculty of Mechanical, Maritime and Materials Engineering, Delft University of Technology, Mekelweg 2, 2628 CD Delft, The Netherlands Centre for Transport Studies, University of Twente, Drienerlolaan 5, 7522 NB Enschede, The Netherlands c TNO Human Factors, Kampweg 5, 3769 DE Soesterberg, The Netherlands d Transportation Research Group, Civil, Maritime, Environmental Engineering and Science, Engineering and the Environment, University of Southampton, United Kingdom"}
{"_id":"2160b8f38538320ff6bbd1c35de7b441541b8007","title":"The expansion of Google Scholar versus Web of Science: a longitudinal study","text":"Web of Science (WoS) and Google Scholar (GS) are prominent citation services with distinct indexing mechanisms. Comprehensive knowledge about the growth patterns of these two citation services is lacking. We analyzed the development of citation counts in WoS and GS for two classic articles and 56 articles from diverse research fields, making a distinction between retroactive growth (i.e., the relative difference between citation counts up to mid-2005 measured in mid-2005 and citation counts up to mid-2005 measured in April 2013) and actual growth (i.e., the relative difference between citation counts up to mid-2005 measured in April 2013 and citation counts up to April 2013 measured in April 2013). One of the classic articles was used for a citation-by-citation analysis. Results showed that GS has substantially grown in a retroactive manner (median of 170\u00a0% across articles), especially for articles that initially had low citations counts in GS as compared to WoS. Retroactive growth of WoS was small, with a median of 2\u00a0% across articles. Actual growth percentages were moderately higher for GS than for WoS (medians of 54 vs. 41\u00a0%). The citation-by-citation analysis showed that the percentage of citations being unique in WoS was lower for more recent citations (6.8\u00a0% for citations from 1995 and later vs. 41\u00a0% for citations from before 1995), whereas the opposite was noted for GS (57 vs. 33\u00a0%). It is concluded that, since its inception, GS has shown substantial expansion, and that the majority of recent works indexed in WoS are now also retrievable via GS. A discussion is provided on quantity versus quality of citations, threats for WoS, weaknesses of GS, and implications for literature research and research evaluation."}
{"_id":"2d4f10ccd2503c37ec32aa0033d3e5b3559f4404","title":"Toward a Theory of Situation Awareness in Dynamic Systems","text":"Situational awareness has become an increasingly salient factor contributing to flight safety and operational performance, and the research has burgeoned to cope with the human performance challenges associated with the installation of advanced avionics systems in modern aircraft. The systematic study and application of situational awareness has also extended beyond the cockpit to include air traffic controllers and personnel operating within other complex, high consequence work domains. This volume offers a collection of essays that have made important contributions to situational awareness research and practice. To this end, it provides unique access to key readings that address the conceptual development of situational awareness, methods for its assessment, and applications to enhance situational awareness through training and design."}
{"_id":"643bbe8450a52ff474b8194a2c95097d02387610","title":"The Influence of Culture on Memory","text":"The study of cognition across cultures offers a useful approach to both identifying bottlenecks in information processing and suggesting culturespecific strategies to alleviate these limitations. The recent emphasis on applying cognitive neuroscience methods to the study of culture further aids in specifying which processes differ cross-culturally. By localizing cultural differences to distinct neural regions, the comparison of cultural groups helps to identify candidate information processing mechanisms that can be made more efficient with augmented cognition and highlights the unique solutions that will be required for different groups of information processors."}
{"_id":"8fdeffbf8d1fa688d74db882a1d8c7ebc596534b","title":"Multidisciplinary Perspectives on Music Emotion Recognition : Implications for Content and Context-Based Models","text":"The prominent status of music in human culture and every day life is due in large part to its striking ability to elicit emotions, which may manifest from slight variation in mood to changes in our physical condition and actions. In this paper, we first review state of the art studies on music and emotions from different disciplines including psychology, musicology and music information retrieval. Based on these studies, we then propose new insights to enhance automated music emotion recog-"}
{"_id":"62bdc8ec0f4987f80100cd825dce9cb102c1e7fa","title":"Spiking Neural Networks: Principles and Challenges","text":"Over the last decade, various spiking neural network models have been proposed, along with a similarly increasing interest in spiking models of computation in computational neuroscience. The aim of this tutorial paper is to outline some of the common ground in state-of-the-art spiking neural networks as well as open challenges."}
{"_id":"bafd94ad2520a4e10ba3e9b5b665a3a474624cf4","title":"STRATOS: Using Visualization to Support Decisions in Strategic Software Release Planning","text":"Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous trade-offs-constraints and factors that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans in order to make an informed choice. We present STRATOS, a tool that simultaneously visualizes several software release plans. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between alternative plans. We evaluated our tool via a qualitative study and found that STRATOS enables a range of decision-making processes, helping participants decide on which plan is most optimal."}
{"_id":"5100b28dd2098b4b4aaa29655c4fd7af7ee8e4e9","title":"Rainy weather recognition from in-vehicle camera images for driver assistance","text":"We propose a weather recognition method from in-vehicle camera images that uses a subspace method to judge rainy weather by detecting raindrops on the windshield. \"Eigendrops\" represent the principal components extracted from raindrop images in the learning stage. Then the method detects raindrops by template matching. In experiments using actual video sequences, our method showed good detection ability of raindrops and promising results for rainfall judgment from detection results."}
{"_id":"3542254ecc9f57d19f00e9fcc645b3d44469a6ba","title":"Scaling Up Mixed Workloads: A Battle of Data Freshness, Flexibility, and Scheduling","text":"The common \u201cone size does not fit all\u201d paradigm isolates transactional and analytical workloads into separate, specialized database systems. Operational data is periodically replicated to a data warehouse for analytics. Competitiveness of enterprises today, however, depends on real-time reporting on operational data, necessitating an integration of transactional and analytical processing in a single database system. The mixed workload should be able to query and modify common data in a shared schema. The database needs to provide performance guarantees for transactional workloads, and, at the same time, efficiently evaluate complex analytical queries. In this paper, we share our analysis of the performance of two main-memory databases that support mixed workloads, SAP HANA and HyPer, while evaluating the mixed workload CHbenCHmark. By examining their similarities and differences, we identify the factors that affect performance while scaling the number of concurrent transactional and analytical clients. The three main factors are (a) data freshness, i.e., how recent is the data processed by analytical queries, (b) flexibility, i.e., restricting transactional features in order to increase optimization choices and enhance performance, and (c) scheduling, i.e., how the mixed workload utilizes resources. Specifically for scheduling, we show that the absence of workload management under cases of high concurrency leads to analytical workloads overwhelming the system and severely hurting the performance of transactional workloads."}
{"_id":"2d3ab651aa843229932ed63f19986dae0c6aace0","title":"Locally Weighted Regression : An Approach to Regression Analysis by Local Fitting","text":"Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http:\/\/www.jstor.org\/about\/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http:\/\/www.jstor.org\/journals\/astata.html. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission."}
{"_id":"fb8f7d172d47286101fc7f8b34a863162adc0e8e","title":"Histogram of Oriented Lines for Palmprint Recognition","text":"Subspace learning methods are very sensitive to the illumination, translation, and rotation variances in image recognition. Thus, they have not obtained promising performance for palmprint recognition so far. In this paper, we propose a new descriptor of palmprint named histogram of oriented lines (HOL), which is a variant of histogram of oriented gradients (HOG). HOL is not very sensitive to changes of illumination, and has the robustness against small transformations because slight translations and rotations make small histogram value changes. Based on HOL, even some simple subspace learning methods can achieve high recognition rates."}
{"_id":"4a00102f2a850bb208911a7541b4174e3b7ac71c","title":"Automatically verifying and reproducing event-based races in Android apps","text":"Concurrency has been a perpetual problem in Android apps, mainly due to event-based races. Several event-based race detectors have been proposed, but they produce false positives, cannot reproduce races, and cannot distinguish be- tween benign and harmful races. To address these issues, we introduce a race verification and reproduction approach named ERVA. Given a race report produced by a race detector, ERVA uses event dependency graphs, event flipping, and replay to verify the race and determine whether it is a false positive, or a true positive; for true positives, ERVA uses state comparison to distinguish benign races from harmful races. ERVA automatically produces an event schedule that can be used to deterministically reproduce the race, so developers can fix it. Experiments on 16 apps indicate that only 3% of the races reported by race detectors are harmful, and that ERVA can verify an app in 20 minutes on average."}
{"_id":"658ae58cc25748e5767ca17710c42d9b0d9061be","title":"Adolescents' attitudes toward sports, exercise, and fitness predict physical activity 5 and 10 years later.","text":"OBJECTIVE\nTo determine whether adolescent attitudes towards sports, exercise, and fitness predict moderate-to-vigorous physical activity 5 and 10 years later.\n\n\nMETHOD\nA diverse group of 1902 adolescents participating in Project Eating and Activity in Teens, reported weekly moderate-to-vigorous physical activity and attitudes toward sports, exercise, and fitness in Eating and Activity in Teens-I (1998-99), Eating and Activity in Teens-II (2003-04), and Eating and Activity in Teens-III (2008-09).\n\n\nRESULTS\nMean moderate-to-vigorous physical activity was 6.4, 5.1, and 4.0 hours\/week at baseline, 5-year, and 10-year follow-up, respectively. Attitudes toward sports, exercise, and fitness together predicted moderate-to-vigorous physical activity at 5 and 10 years. Among the predictors of 5- and 10-year moderate-to-vigorous physical activity, attitude's effect size, though modest, was comparable to the effect sizes for sports participation and body mass index. Adolescents with more-favorable attitudes toward sports, exercise, and fitness engaged in approximately 30%-40% more weekly moderate-to-vigorous physical activity at follow-up (2.1 hour\/week at 5 years and 1.2 hour\/week at 10 years) than those with less-favorable attitudes.\n\n\nCONCLUSION\nAdolescents' exercise-related attitudes predict subsequent moderate-to-vigorous physical activity independent of baseline behavior suggesting that youth moderate-to-vigorous physical activity promotion efforts may provide long-term benefits by helping youth develop favorable exercise attitudes."}
{"_id":"c57111a50b9e0818138b19cb9d8f01871145ee0a","title":"Validation of the Ten-Item Internet Gaming Disorder Test (IGDT-10) and evaluation of the nine DSM-5 Internet Gaming Disorder criteria.","text":"INTRODUCTION\nThe inclusion of Internet Gaming Disorder (IGD) in the DSM-5 (Section 3) has given rise to much scholarly debate regarding the proposed criteria and their operationalization. The present study's aim was threefold: to (i) develop and validate a brief psychometric instrument (Ten-Item Internet Gaming Disorder Test; IGDT-10) to assess IGD using definitions suggested in DSM-5, (ii) contribute to ongoing debate regards the usefulness and validity of each of the nine IGD criteria (using Item Response Theory [IRT]), and (iii) investigate the cut-off threshold suggested in the DSM-5.\n\n\nMETHODS\nAn online gamer sample of 4887 gamers (age range 14-64years, mean age 22.2years [SD=6.4], 92.5% male) was collected through Facebook and a gaming-related website with the cooperation of a popular Hungarian gaming magazine. A shopping voucher of approx. 300 Euros was drawn between participants to boost participation (i.e., lottery incentive). Confirmatory factor analysis and a structural regression model were used to test the psychometric properties of the IGDT-10 and IRT analysis was conducted to test the measurement performance of the nine IGD criteria. Finally, Latent Class Analysis along with sensitivity and specificity analysis were used to investigate the cut-off threshold proposed in the DSM-5.\n\n\nRESULTS\nAnalysis supported IGDT-10's validity, reliability, and suitability to be used in future research. Findings of the IRT analysis suggest IGD is manifested through a different set of symptoms depending on the level of severity of the disorder. More specifically, \"continuation\", \"preoccupation\", \"negative consequences\" and \"escape\" were associated with lower severity of IGD, while \"tolerance\", \"loss of control\", \"giving up other activities\" and \"deception\" criteria were associated with more severe levels. \"Preoccupation\" and \"escape\" provided very little information to the estimation IGD severity. Finally, the DSM-5 suggested threshold appeared to be supported by our statistical analyses.\n\n\nCONCLUSIONS\nIGDT-10 is a valid and reliable instrument to assess IGD as proposed in the DSM-5. Apparently the nine criteria do not explain IGD in the same way, suggesting that additional studies are needed to assess the characteristics and intricacies of each criterion and how they account to explain IGD."}
{"_id":"91d056b60f2391317c1dd2adaad01a92087b066e","title":"Optimizing Hierarchical Visualizations with the Minimum Description Length Principle","text":"In this paper we examine how the Minimum Description Length (MDL) principle can be used to efficiently select aggregated views of hierarchical datasets that feature a good balance between clutter and information. We present MDL formulae for generating uneven tree cuts tailored to treemap and sunburst diagrams, taking into account the available display space and information content of the data. We present the results of a proof-of-concept implementation. In addition, we demonstrate how such tree cuts can be used to enhance drill-down interaction in hierarchical visualizations by implementing our approach in an existing visualization tool. Validation is done with the feature congestion measure of clutter in views of a subset of the current DMOZ web directory, which contains nearly half million categories. The results show that MDL views achieve near constant clutter level across display resolutions. We also present the results of a crowdsourced user study where participants were asked to find targets in views of DMOZ generated by our approach and a set of baseline aggregation methods. The results suggest that, in some conditions, participants are able to locate targets (in particular, outliers) faster using the proposed approach."}
{"_id":"a7621b4ec18719b08f3a2a444b6d37a2e20227b7","title":"Fast Training of Convolutional Networks through FFTs","text":"Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges."}
{"_id":"6ef1e1c54855fc1d1de1066510e7ea3e588a38c6","title":"Buccinator myomucosal flap: clinical results and review of anatomy, surgical technique and applications.","text":"BACKGROUND\nThe buccinator musculomucosal flap is an axial-pattern flap based on either the buccal or the facial artery. We present our experience with this flap and describe its surgical anatomy, the surgical techniques utilised to raise the flap and its clinical applications.\n\n\nMATERIALS AND METHODS\nWe retrospectively reviewed all patients who had had buccinator myomucosal flaps created at the Groote Schuur Hospital between 1999 and 2004. Patients were also recalled to assess flap sensation and to record reduction of mouth opening as a consequence of donor site scarring.\n\n\nRESULTS\nOf the 14 patients who had had a buccinator myomucosal flap created, there was one flap failure. Sensation was present in 71 per cent of flaps, and there was no trismus due to donor site scarring.\n\n\nCONCLUSIONS\nThe buccinator myomycosal flap is a dependable flap with good functional outcome and low morbidity."}
{"_id":"24851ce5f8e097b1053d372cd81717d183b11d96","title":"Signal Space CoSaMP for Sparse Recovery With Redundant Dictionaries","text":"Compressive sensing (CS) has recently emerged as a powerful framework for acquiring sparse signals. The bulk of the CS literature has focused on the case where the acquired signal has a sparse or compressible representation in an orthonormal basis. In practice, however, there are many signals that cannot be sparsely represented or approximated using an orthonormal basis, but that do have sparse representations in a redundant dictionary. Standard results in CS can sometimes be extended to handle this case provided that the dictionary is sufficiently incoherent or well conditioned, but these approaches fail to address the case of a truly redundant or overcomplete dictionary. In this paper, we describe a variant of the iterative recovery algorithm CoSaMP for this more challenging setting. We utilize the \\mbi D-RIP, a condition on the sensing matrix analogous to the well-known restricted isometry property. In contrast to prior work, the method and analysis are \u201csignal-focused\u201d; that is, they are oriented around recovering the signal rather than its dictionary coefficients. Under the assumption that we have a near-optimal scheme for projecting vectors in signal space onto the model family of candidate sparse signals, we provide provable recovery guarantees. Developing a practical algorithm that can provably compute the required near-optimal projections remains a significant open problem, but we include simulation results using various heuristics that empirically exhibit superior performance to traditional recovery algorithms."}
{"_id":"15a4cfb61baedfeccd741c1a09001a9b4e560663","title":"Traffic sign recognition based on color, shape, and pictogram classification using support vector machines","text":"Traffic sign recognition is the second part of traffic sign detection and recognition systems. It plays a crucial role in driver assistance systems and provides drivers with crucial safety and precaution information. In this study, the recognition of the TS is performed based on its border color, shape, and pictogram information. This technique breaks down the recognition system into small parts, which makes it efficient and accurate. Moreover, this makes it easy to understand TS components. The proposed technique is composed of three independent stages. The first stage involves extracting the border colors using an adaptive image segmentation technique that is based on learning vector quantization. Then, the shape of the TS is detected using a fast and simple matching technique based on the logical exclusive OR operator. Finally, the pictogram is extracted and classified using a support vector machines classifier model. The proposed technique is applied on the German traffic sign recognition benchmark and achieves an overall recognition rate of 98.23%, with an average computational speed of 30\u00a0ms."}
{"_id":"742c49afaa003dcbed07e4b7aabd32f14f5e2617","title":"Adaptive item-based learning environments based on the item response theory: possibilities and challenges","text":"The popularity of intelligent tutoring systems (ITSs) is increasing rapidly. In order to make learning environments more efficient, researchers have been exploring the possibility of an automatic adaptation of the learning environment to the learner or the context. One of the possible adaptation techniques is adaptive item sequencing by matching the difficulty of the items to the learner\u2019s knowledge level. This is already accomplished to a certain extent in adaptive testing environments, where the test is tailored to the person\u2019s ability level by means of the item response theory (IRT). Even though IRT has been a prevalent computerized adaptive test (CAT) approach for decades and applying IRT in item-based ITSs could lead to similar advantages as in CAT (e.g. higher motivation and more efficient learning), research on the application of IRT in such learning environments is highly restricted or absent. The purpose of this paper was to explore the feasibility of applying IRT in adaptive item-based ITSs. Therefore, we discussed the two main challenges associated with IRT application in such learning environments: the challenge of the data set and the challenge of the algorithm. We concluded that applying IRT seems to be a viable solution for adaptive item selection in item-based ITSs provided that some modifications are implemented. Further research should shed more light on the adequacy of the proposed solutions."}
{"_id":"5e3fd9e6e7bcfc37fa751385ea3c8c7c7ac80c43","title":"Maximum Principle Based Algorithms for Deep Learning","text":"The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin\u2019s maximum principle (PMP). A modification of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on flat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate periteration, provided Hamiltonian maximization can be efficiently carried out a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables."}
{"_id":"7023fcaa06fc9e66f1e5d35a0b13548e2a8ebb66","title":"Word Spotting in Cursive Handwritten Documents using Modified Character Shape Codes","text":"There is a large collection of Handwritten English paper documents of Historical and Scientific importance. But paper documents are not recognised directly by computer. Hence the closest way of indexing these documents is by storing their document digital image. Hence a large database of document images can replace the paper documents. But the document and data corresponding to each image cannot be directly recognised by the computer. This paper applies the technique of word spotting using Modified Character Shape Code to Handwritten English document images for quick and efficient query search of words on a database of document images. It is different from other Word Spotting techniques as it implements two level of selection for word segments to match search query. First based on word size and then based on character shape code of query. It makes the process faster and more efficient and reduces the need of multiple pre-processing."}
{"_id":"7276372b4685c49c1f4fc394fb3fd0ab5fc9db3a","title":"Visualization of boundaries in volumetric data sets using LH histograms","text":"A crucial step in volume rendering is the design of transfer functions that highlights those aspects of the volume data that are of interest to the user. For many applications, boundaries carry most of the relevant information. Reliable detection of boundaries is often hampered by limitations of the imaging process, such as blurring and noise. We present a method to identify the materials that form the boundaries. These materials are then used in a new domain that facilitates interactive and semiautomatic design of appropriate transfer functions. We also show how the obtained boundary information can be used in region-growing-based segmentation."}
{"_id":"107a53a46f3acda939d93c47009ab960d6a33464","title":"Improving Adversarial Robustness by Data-Specific Discretization","text":"A recent line of research proposed (either implicitly or explicitly) gradient-masking preprocessing techniques to improve adversarial robustness. However, as shown by Athaley-Carlini-Wagner, essentially all these defenses can be circumvented if an attacker leverages approximate gradient information with respect to the preprocessing. This thus raises a natural question of whether there is a useful preprocessing technique in the context of white-box attacks, even just for only mildly complex datasets such as MNIST. In this paper we provide an affirmative answer to this question. Our key observation is that for several popular datasets, one can approximately encode entire dataset using a small set of separable codewords derived from the training set, while retaining high accuracy on natural images. The separability of the codewords in turn prevents small perturbations as in `\u221e attacks from changing feature encoding, leading to adversarial robustness. For example, for MNIST our code consists of only two codewords, 0 and 1, and the encoding of any pixel is simply 1[x > 0.5] (i.e., whether a pixel x is at least 0.5). Applying this code to a naturally trained model already gives high adversarial robustness even under strong white-box attacks based on Backward Pass Differentiable Approximation (BPDA) method of Athaley-Carlini-Wagner that takes the codes into account. We give density-estimation based algorithms to construct such codes, and provide theoretical analysis and certificates of when our method can be effective. Systematic evaluation demonstrates that our method is effective in improving adversarial robustness on MNIST, CIFAR-10, and ImageNet, for either naturally or adversarially trained models."}
{"_id":"1f779bb04bf3510cc8b0fc8f5dba896f5e4e6b95","title":"The DARPA Twitter Bot Challenge","text":"From politicians and nation states to terrorist groups, numerous organizations reportedly conduct explicit campaigns to influence opinions on social media, posing a risk to freedom of expression. Thus, there is a need to identify and eliminate \"influence bots\" - realistic, automated identities that illicitly shape discussions on sites like Twitter and Facebook - before they get too influential."}
{"_id":"48b38420f9c39c601dcf81621609d131b8035f94","title":"Smart Health Monitoring Systems: An Overview of Design and Modeling","text":"Health monitoring systems have rapidly evolved during the past two decades and have the potential to change the way health care is currently delivered. Although smart health monitoring systems automate patient monitoring tasks and, thereby improve the patient workflow management, their efficiency in clinical settings is still debatable. This paper presents a review of smart health monitoring systems and an overview of their design and modeling. Furthermore, a critical analysis of the efficiency, clinical acceptability, strategies and recommendations on improving current health monitoring systems will be presented. The main aim is to review current state of the art monitoring systems and to perform extensive and an in-depth analysis of the findings in the area of smart health monitoring systems. In order to achieve this, over fifty different monitoring systems have been selected, categorized, classified and compared. Finally, major advances in the system design level have been discussed, current issues facing health care providers, as well as the potential challenges to health monitoring field will be identified and compared to other similar systems."}
{"_id":"02460220d72144f971f39613897b6fb67d2253b1","title":"Aerial-guided navigation of a ground robot among movable obstacles","text":"We demonstrate the fully autonomous collaboration of an aerial and a ground robot in a mock-up disaster scenario. Within this collaboration, we make use of the individual capabilities and strengths of both robots. The aerial robot first maps an area of interest, then it computes the fastest mission for the ground robot to reach a spotted victim and deliver a first-aid kit. Such a mission includes driving and removing obstacles in the way while being constantly monitored and commanded by the aerial robot. Our mission-planning algorithm distinguishes between movable and fixed obstacles and considers both the time for driving and removing obstacles. The entire mission is executed without any human interaction once the aerial robot is launched and requires a minimal amount of communication between the robots. We describe both the hardware and software of our system and detail our mission-planning algorithm. We present exhaustive results of both simulation and real experiments. Our system was successfully demonstrated more than 20 times at a trade fair."}
{"_id":"08d65470ed0994085a2bfbed17052e50023feeb5","title":"Beat them or ban them: the characteristics and social functions of anger and contempt.","text":"This article reports 3 studies in which the authors examined (a) the distinctive characteristics of anger and contempt responses and (b) the interpersonal causes and effects of both emotions. In the 1st study, the authors examined the distinction between the 2 emotions; in the 2nd study, the authors tested whether contempt could be predicted from previous anger incidents with the same person; and in the 3rd study, the authors examined the effects of type of relationship on anger and contempt reactions. The results of the 3 studies show that anger and contempt often occur together but that there are clear distinctions between the 2 emotions: Anger is characterized more by short-term attack responses but long-term reconciliation, whereas contempt is characterized by rejection and social exclusion of the other person, both in the short-term and in the long-term. The authors also found that contempt may develop out of previously experienced anger and that a lack of intimacy with and perceived control over the behavior of the other person, as well as negative dispositional attributions about the other person, predicted the emergence of contempt."}
{"_id":"93a3703a113eb7726345f7cc3c8dbc564d4ece15","title":"FPMR: MapReduce Framework on FPGA A Case Study of RankBoost Acceleration","text":"Machine learning and data mining are gaining increasing attentions of the computing society. FPGA provides a highly parallel, low power, and flexible hardware platform for this domain, while the difficulty of programming FPGA greatly limits its prevalence. MapReduce is a parallel programming framework that could easily utilize inherent parallelism in algorithms. In this paper, we describe FPMR, a MapReduce framework on FPGA, which provides programming abstraction, hardware architecture, and basic building blocks to developers. An on-chip processor scheduler is implemented to maximize the utilization of computation resources and achieve better load balancing. An efficient data access scheme is carefully designed to maximize data reuse and throughput. Meanwhile, the FPMR framework hides the task control, synchronization, and communication away from designers so that more attention can be paid to the application itself. A case study of RankBoost acceleration based on FPMR demonstrates that FPMR efficiently helps with the development productivity; and the speedup is 31.8x versus CPU-based implementation. This performance is comparable to a fully manually designed version, which achieves 33.5x speedup. Two other applications: SVM, PageRank are also discussed to show the generalization of the framework."}
{"_id":"7e58e877c4236913a7a2dd0e3a1e6f9b60721cd7","title":"Area-efficient high-speed hybrid 1-bit full adder circuit using modified XNOR gate","text":"A hybrid 1-bit full adder design is presented here using modified 3T-XNOR gate to improve the area and speed performance. The design is implemented for 1-bit full adder and then is scaled to 32-bit adder. Combination of CMOS and transmission gate logic is used to enhance the performance in terms of area, delay and power. The performance of the proposed design is evaluated through the simulation analysis in 90-nm technology with 1.2v supply voltage. The effect of scaling on the overall performance is also analyzed through the performance evaluation of 1-bit and 32-bit adder. The performance of proposed design is also compared with conventional design to verify the effectiveness in terms of area, power, delay."}
{"_id":"69f1806ca756846d144e552770e1c71592809397","title":"Evaluating Knowledge Representation and Reasoning Capabilites of Ontology Specification Languages","text":"The interchange of ontologies across the World Wide Web (WWW) and the cooperation among heterogeneous agents placed on it is the main reason for the development of a new set of ontology specification languages, based on new web standards such as XML or RDF. These languages (SHOE, XOL, RDF, OIL, etc) aim to represent the knowledge contained in an ontology in a simple and human-readable way, as well as allow for the interchange of ontologies across the web. In this paper, we establish a common framework to compare the expressiveness of \u201c traditional\u201d ontology languages (Ontolingua, OKBC, OCML, FLogic, LOOM) and \u201cweb-based\u201d ontology languages. As a result of this study, we conclude that different needs in KR and reasoning may exist in the building of an ontology-based application, and these needs must be evaluated in order to choose the most suitable ontology language(s)."}
{"_id":"ebbf93b4c79d6bdac6ce5b56aa9cf24b3bb1f542","title":"The SWAL-QOL Outcomes Tool for Oropharyngeal Dysphagiain Adults: II. Item Reduction and Preliminary Scaling","text":"The SWAL-QOL outcomes tool was constructed for use in clinical research for patients with oropharyngeal dysphagia. The SWAL-QOL was constructed a priori to enable preliminary psychometric analyses of items and scales before its final validation. This article describes data analysis from a pretest of the SWAL-QOL. We evaluated the different domains of the SWAL-QOL for respondent burden, data quality, item variability, item convergent validity, internal consistency reliability as measured by Cronbach's alpha, and range and skewness of scale scores upon aggregation and floor and ceiling effects. The item reduction techniques outlined reduced the SWAL-QOL from 185 to 93 items. The pretest of the SWAL-QOL afforded us the opportunity to select items for the ongoing validation study which optimally met our a priori psychometric criteria of high data quality, normal item distributions, and robust evidence of item convergent validity."}
{"_id":"5fe85df7321aad95d4a5ec58889ea55f7f0c5ff2","title":"A Descriptive Content Analysis of Trust-Building Measures in B2B Electronic Marketplaces","text":"Because business-to-business (B2B) electronic marketplaces (e-marketplaces) facilitate transactions between buyers and sellers, they strive to foster a trustworthy trading environment with a variety of trust-building measures. However, little research has been undertaken to explore trust-building measures used in B2B e-marketplaces, or to determine to what extent these measures are applied in B2B e-marketplaces and how they are applied. Based on reviews of the scholarly, trade, and professional literature on trust in electronic commerce, we identified 11 trustbuilding measures used to create trust in B2B e-marketplaces. Zucker\u2019s trust production theory [1986] was applied to understand how these trust-building measures will enhance participants\u2019 trust in buyers and sellers in B2B e-marketplaces or in B2B e-marketplace providers. A descriptive content analysis of 100 B2B e-marketplaces was conducted to survey the current usage of the 11 trust-building measures. Many of the trust-building measures were found to be widely used in the B2B e-marketplaces. However, although they were proven to be effective in building trust-related beliefs in online business environments, several institutional-based trustbuilding measures, such as escrow services, insurance and third-party assurance seals, are not widely used in B2B e-marketplaces."}
{"_id":"66336d0b89c3eca3dec0a41d2696a0fda23b6957","title":"A Low-Profile Broadband 32-Slot Continuous Transverse Stub Array for Backhaul Applications in $E$ -Band","text":"A high-gain, broadband, and low-profile continuous transverse stub antenna array is presented in E-band. This array comprises 32 long slots excited in parallel by a uniform corporate parallel-plate-waveguide beamforming network combined to a pillbox coupler. The radiating slots and the corporate feed network are built in aluminum whereas the pillbox coupler and its focal source are fabricated in printed circuit board technology. Specific transitions have been designed to combine both fabrication technologies. The design, fabrication, and measurement results are detailed, and a simple design methodology is proposed. The antenna is well matched ( $S_{11} < -13.6$  dB) between 71 and 86 GHz, and an excellent agreement is found between simulations and measurements, thus validating the proposed design. The antenna gain is higher than 29.3 dBi over the entire bandwidth, with a peak gain of 30.8 dBi at 82.25 GHz, and a beam having roughly the same half-power beamwidth in E- and H-planes. This antenna architecture is considered as an innovative solution for long-distance millimeter-waves telecommunication applications such as fifth-generation backhauling in E-band."}
{"_id":"663444f7bb70eb20c1f9c6c084db4d7a1dff21c4","title":"News Article Teaser Tweets and How to Generate Them","text":"We define the task of teaser generation and provide an evaluation benchmark and baseline systems for it. A teaser is a short reading suggestion for an article that is illustrative and includes curiosity-arousing elements to entice potential readers to read the news item. Teasers are one of the main vehicles for transmitting news to social media users. We compile a novel dataset of teasers by systematically accumulating tweets and selecting ones that conform to the teaser definition. We compare a number of neural abstractive architectures on the task of teaser generation and the overall best performing system is See et al. (2017)\u2019s seq2seq with pointer network."}
{"_id":"07d9dd5c25c944bf009256cdcb622feda53dabba","title":"Markov Chain Monte Carlo Maximum Likelihood","text":"Markov chain Monte Carlo (e. g., the Metropolis algorithm and Gibbs sampler) is a general tool for simulation of complex stochastic processes useful in many types of statistical inference. The basics of Markov chain Monte Carlo are reviewed, including choice of algorithms and variance estimation, and some new methods are introduced. The use of Markov chain Monte Carlo for maximum likelihood estimation is explained, and its performance is compared with maximum pseudo likelihood estimation."}
{"_id":"5e7cf311e1d0f0a137add1b1d373b185c52f04bc","title":"Modeling and power conditioning for thermoelectric generation","text":"In this paper, the principle and basic structure of the thermoelectric module is introduced. The steady- state and dynamic behaviors of a single TE module are characterized. An electric model of TE modules is developed and can be embedded in the simulation software for circuit analysis and design. The issues associated with the application of the TEG models is analyzed and pointed out. Power electronic technologies provide solutions for thermoelectric generation with features such as load interfacing, maximum power point tracking, power conditioning and failed module bypassing. A maximum power point tracking algorithm is developed and implemented with a DC-DC converter and low cost microcontroller. Experimental results demonstrated that the power electronic circuit can extract the maximum electrical power from the thermoelectric modules and feed electric loads regardless of the thermoelectric module's heat flux and load impedance or conditions."}
{"_id":"411500f7aaa0f3c00d492b78b24b33da0fd0d58d","title":"Identifying Analogies Across Domains","text":"Identifying analogies across domains without supervision is an important task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision."}
{"_id":"a05e4272d00860c701fa2110365bddaa5a169b80","title":"Photovoltaic based water pumping system","text":"In this paper a stand-alone Photovoltaic (PV) systems is presented for water pumping. Solar PV water pumping systems are used for irrigation and drinking water. PV based pumping systems without battery can provide a cost-effective use of solar energy. For the purpose of improving efficiency of the system perturb and observe (P&O) algorithm based Maximum Power Point Tracker (MPPT) is connected to this system. The aim of this paper is to show how to achieve an effective photovoltaic pumping system without battery storage. Results are presented based on different cases of irrigation pumping application and availability of solar irradiance. Simulation results using MATLAB\/SIMULINK show that the performance of the controllers both in transient as well as in steady state is quite satisfactory."}
{"_id":"4fa65adc0ad69471b9f146407399db92d17b5140","title":"Quality Management in Systems Development: An Organizational System Perspective","text":"We identify top management leadership, a sophisticated management infrastructure, process management efficacy, and stakeholder participation as important elements of a quality-oriented organizational system for software development. A model interrelating these constructs and quality performance is proposed. Data collected through a national survey of IS executives in Fortune 1000 companies and government agencies was used to Robert Zmud was the accepting senior editor for this paper. test the model using a Partial Least Squares analysis methodology. Our results suggest that software quality goals are best attained when top management creates a management infrastructure that promotes improvements in process design and encourages stakeholders to evolve the design of the development processes. Our results also suggest that all elements of the organizational system need to be developed in order to attain quality goals and that piecemeal adoption of select quality management practices are unlikely to be effective. Implications of this research for IS theory and practice are discussed."}
{"_id":"83bd2592ae88603a397d63815b353816a7cec4b1","title":"Experiences with Selecting Search Engines Using Metasearch","text":"Search engines are among the most useful and high-profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve, and how to use them. This article describes and evaluates SavvySearch, a metasearch engine designed to intelligently select and interface with multiple remote search engines. The primary metasearch issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired metaindex approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the metaindex approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme."}
{"_id":"8a76de2c9fc7021e360c770b96640860571037a4","title":"Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1","text":"We introduce a method to train Binarized Neural Networks (BNNs) neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line."}
{"_id":"e86f2ad8b768df4f34031e20f07fdeb19af505a9","title":"An Accurate Multi-Row Panorama Generation Using Multi-Point Joint Stitching","text":"Most of the existing panorama generation tools require the input images to be captured along one direction, and yield a narrow strip panorama. To generate a large viewing field panorama, this paper proposes a multi-row panorama generation (MRPG) method. For a pan\/tilt camera whose scanning path covers a wide range of horizontal and vertical views, the image frames in different views correspond to different coordinate benchmarks and different projections. And the image frame should not only be aligned with the continuous frames in timeline but also be aligned with other frames in spatial neighborhood even with long time intervals. For these problems, MRPG first designs an optimal scanning path to cover the large viewing field, and chooses the center frame as the reference frame to start to stitch. The stitching order of multi-frame is arranged in first-column and second-row to ensure a small alignment error. Moreover, MRPG proposes a multi-point joint stitching method to eliminate the seams and correct the distortions, which makes the current frame accurately integrated into the panoramic canvas from all directions. Experimental results show that MRPG can generate a more accurate panorama than other state-of-the-art image stitching methods, and give a better visual effect for a large viewing field panorama."}
{"_id":"13f4d056f6ba318074187ea5ade7f231803e2879","title":"Privacy-Preserving Image Denoising From External Cloud Databases","text":"Along with the rapid advancement of digital image processing technology, image denoising remains a fundamental task, which aims to recover the original image from its noisy observation. With the explosive growth of images on the Internet, one recent trend is to seek high quality similar patches at cloud image databases and harness rich redundancy therein for promising denoising performance. Despite the well-understood benefits, such a cloud-based denoising paradigm would undesirably raise security and privacy issues, especially for privacy-sensitive image data sets. In this paper, we initiate the first endeavor toward privacy-preserving image denoising from external cloud databases. Our design enables the cloud hosting encrypted databases to provide secure query-based image denoising services. Considering that image denoising intrinsically demands high quality similar image patches, our design builds upon recent advancements on secure similarity search, Yao\u2019s garbled circuits, and image denoising operations, where each is used at a different phase of the design for the best performance. We formally analyze the security strengths. Extensive experiments over real-world data sets demonstrate that our design achieves the denoising quality close to the optimal performance in plaintext."}
{"_id":"e3435df055d3ea85b5e213c5a9bceb542eeb236c","title":"Induction and synchronous reluctance motors comparison","text":"The aim of this paper is to investigate and compare the performances of two induction motors and two transverse laminated synchronous reluctance motors (induction motor nameplate data: 2.2 and 4 kW, 380 V, 50 Hz, 4 pole). Each induction motor is compared with a synchronous reluctance motor, that has the same stator lamination and winding, but, obviously, different rotor. The analysis has been based both on an analytical and experimental approach. The results have shown that the synchronous reluctance motor, compared to the induction motor, is capable of around a 10% to 15% larger rated torque for a given frame size. The direct comparison of the performances of the motor types has been made both for constant load and constant dissipated power conditions."}
{"_id":"24e56a1b8e1d4f32efc64a220da19f948a2942e6","title":"Leveraging the Crowd to Detect and Reduce the Spread of Fake News and Misinformation","text":"Online social networking sites are experimenting with the following crowd-powered procedure to reduce the spread of fake news and misinformation: whenever a user is exposed to a story through her feed, she can flag the story as misinformation and, if the story receives enough flags, it is sent to a trusted third party for fact checking. If this party identifies the story as misinformation, it is marked as disputed. However, given the uncertain number of exposures, the high cost of fact checking, and the trade-off between flags and exposures, the above mentioned procedure requires careful reasoning and smart algorithms which, to the best of our knowledge, do not exist to date. In this paper, we first introduce a flexible representation of the above procedure using the framework of marked temporal point processes. Then, we develop a scalable online algorithm, CURB, to select which stories to send for fact checking and when to do so to efficiently reduce the spread of misinformation with provable guarantees. In doing so, we need to solve a novel stochastic optimal control problem for stochastic differential equations with jumps, which is of independent interest. Experiments on two real-world datasets gathered from Twitter and Weibo show that our algorithm may be able to effectively reduce the spread of fake news and misinformation."}
{"_id":"7cfab12f2258c12ef68901b1f966803a575847f2","title":"The wisdom of the few: a collaborative filtering approach based on expert opinions from the web","text":"Nearest-neighbor collaborative filtering provides a successful means of generating recommendations for web users. However, this approach suffers from several shortcomings, including data sparsity and noise, the cold-start problem, and scalability. In this work, we present a novel method for recommending items to users based on expert opinions. Our method is a variation of traditional collaborative filtering: rather than applying a nearest neighbor algorithm to the user-rating data, predictions are computed using a set of expert neighbors from an independent dataset, whose opinions are weighted according to their similarity to the user. This method promises to address some of the weaknesses in traditional collaborative filtering, while maintaining comparable accuracy. We validate our approach by predicting a subset of the Netflix data set. We use ratings crawled from a web portal of expert reviews, measuring results both in terms of prediction accuracy and recommendation list precision. Finally, we explore the ability of our method to generate useful recommendations, by reporting the results of a user-study where users prefer the recommendations generated by our approach."}
{"_id":"94a62f470aeea69af436e2dd0b54cd50eaaa4b23","title":"A Survey of Collaborative Filtering Techniques","text":"As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, modelbased, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area."}
{"_id":"ced981c28215dd218f05ecbba6512671b22d1cc6","title":"Features for Measuring Credibility on Facebook Information","text":"Abstract\u2014Nowadays social media information, such as news, links, images, or VDOs, is shared extensively. However, the effectiveness of disseminating information through social media lacks in quality: less fact checking, more biases, and several rumors. Many researchers have investigated about credibility on Twitter, but there is no the research report about credibility information on Facebook. This paper proposes features for measuring credibility on Facebook information. We developed the system for credibility on Facebook. First, we have developed FB credibility evaluator for measuring credibility of each post by manual human\u2019s labelling. We then collected the training data for creating a model using Support Vector Machine (SVM). Secondly, we developed a chrome extension of FB credibility for Facebook users to evaluate the credibility of each post. Based on the usage analysis of our FB credibility chrome extension, about 81% of users\u2019 responses agree with suggested credibility automatically computed by the proposed system."}
{"_id":"c632af0d740a551a148e48091499ca4ddba45881","title":"Neural Kinematic Networks for Unsupervised Motion Retargetting","text":"We propose a recurrent neural network architecture with a Forward Kinematics layer and cycle consistency based adversarial training objective for unsupervised motion retargetting. Our network captures the high-level properties of an input motion by the forward kinematics layer, and adapts them to a target character with different skeleton bone lengths (e.g., shorter, longer arms etc.). Collecting paired motion training sequences from different characters is expensive. Instead, our network utilizes cycle consistency to learn to solve the Inverse Kinematics problem in an unsupervised manner. Our method works online, i.e., it adapts the motion sequence on-the-fly as new frames are received. In our experiments, we use the Mixamo animation data1 to test our method for a variety of motions and characters and achieve state-of-the-art results. We also demonstrate motion retargetting from monocular human videos to 3D characters using an off-the-shelf 3D pose estimator."}
{"_id":"1a8fa469e92c8c39bb6e44bcd1d650a307041bb6","title":"A Supervised Learning Framework for Arbitrary Lagrangian-Eulerian Simulations","text":"The Arbitrary Lagrangian-Eulerian (ALE) method is used in a variety of engineering and scientific applications for enabling multi-physics simulations. Unfortunately, the ALE method can suffer from simulation failures that require users to adjust parameters iteratively in order to complete a simulation. In this paper, we present a supervised learning framework for predicting conditions leading to simulation failures. To our knowledge, this is the first time machine learning has been applied to ALE simulations. We propose a novel learning representation for mapping the ALE domain onto a supervised learning formulation. We analyze the predictability of these failures and evaluate our framework using well-known test problems."}
{"_id":"e7d8cc6b166a204b5321bcf6392c75750427a2d5","title":"A Compact Multipath Mitigating Ground Plane for Multiband GNSS Antennas","text":"This paper presents the design of a novel multipath mitigating ground plane for global navigation satellite system (GNSS) antennas. First, the concept of a compact low multipath cross-plate reflector ground plane (CPRGP) is presented. In comparison with the choke ring and electromagnetic band gap (EBG) ground planes, the proposed CPRGP has compact size, low mass, wide operational bandwidth, and simple configuration. The proposed CPRGP is then integrated with a circularly polarized dual-band GNSS antenna in order to assess the multipath mitigating performance over two frequency bands. Measurement results of the proposed CPRGP with GNSS antenna achieves a front-to-back ratio (FBR) over 25 dB at L1 (1.575 GHz) and L2 (1.227 GHz) bands and maximum backward cross-polarization levels below -23 dB at both bands. Antenna phase center variation remains less than 2 mm across both L1 and L2 bands. Furthermore, the performance comparison of the proposed CPRGP with the commercially available pinwheel antenna and the shallow corrugated ground plane is presented, showing the advantages of CPRGP for high precision GNSS applications."}
{"_id":"f5fdb64a4a2e96702488153e87efea838d2c7e1f","title":"Emotion regulation through listening to music in everyday situations.","text":"Music is a stimulus capable of triggering an array of basic and complex emotions. We investigated whether and how individuals employ music to induce specific emotional states in everyday situations for the purpose of emotion regulation. Furthermore, we wanted to examine whether specific emotion-regulation styles influence music selection in specific situations. Participants indicated how likely it would be that they would want to listen to various pieces of music (which are known to elicit specific emotions) in various emotional situations. Data analyses by means of non-metric multidimensional scaling revealed a clear preference for pieces of music that were emotionally congruent with an emotional situation. In addition, we found that specific emotion-regulation styles might influence the selection of pieces of music characterised by specific emotions. Our findings demonstrate emotion-congruent music selection and highlight the important role of specific emotion-regulation styles in the selection of music in everyday situations."}
{"_id":"3d54fea211981e91b1cba184833679cf08bfe483","title":"Moving beyond the Turing Test with the Allen AI Science Challenge","text":"Answering questions correctly from standardized eighth-grade science tests is itself a test of machine intelligence."}
{"_id":"d7a458dff8dec9bb6d4ecaf14fa10569c3a9ea19","title":"Artificial intelligence in medicine and cardiac imaging: harnessing big data and advanced computing to provide personalized medical diagnosis and treatment.","text":"Although advances in information technology in the past decade have come in quantum leaps in nearly every aspect of our lives, they seem to be coming at a slower pace in the field of medicine. However, the implementation of electronic health records (EHR) in hospitals is increasing rapidly, accelerated by the meaningful use initiatives associated with the Center for Medicare & Medicaid Services EHR Incentive Programs. The transition to electronic medical records and availability of patient data has been associated with increases in the volume and complexity of patient information, as well as an increase in medical alerts, with resulting \"alert fatigue\" and increased expectations for rapid and accurate diagnosis and treatment. Unfortunately, these increased demands on health care providers create greater risk for diagnostic and therapeutic errors. In the near future, artificial intelligence (AI)\/machine learning will likely assist physicians with differential diagnosis of disease, treatment options suggestions, and recommendations, and, in the case of medical imaging, with cues in image interpretation. Mining and advanced analysis of \"big data\" in health care provide the potential not only to perform \"in silico\" research but also to provide \"real time\" diagnostic and (potentially) therapeutic recommendations based on empirical data. \"On demand\" access to high-performance computing and large health care databases will support and sustain our ability to achieve personalized medicine. The IBM Jeopardy! Challenge, which pitted the best all-time human players against the Watson computer, captured the imagination of millions of people across the world and demonstrated the potential to apply AI approaches to a wide variety of subject matter, including medicine. The combination of AI, big data, and massively parallel computing offers the potential to create a revolutionary way of practicing evidence-based, personalized medicine."}
{"_id":"1b2a8dc42d6eebc937c9642799a6de87985c3da6","title":"Arabic language sentiment analysis on health services","text":"The social media network phenomenon creates massive amounts of valuable data that is available online and easy to access. Many users share images, videos, comments, reviews, news and opinions on different social networks sites, with Twitter being one of the most popular ones. Data collected from Twitter is highly unstructured, and extracting useful information from tweets is a challenging task. Twitter has a huge number of Arabic users who mostly post and write their tweets using the Arabic language. While there has been a lot of research on sentiment analysis in English, the amount of researches and datasets in Arabic language is limited. This paper introduces an Arabic language dataset, which is about opinions on health services and has been collected from Twitter. The paper will first detail the process of collecting the data from Twitter and also the process of filtering, pre-processing and annotating the Arabic text in order to build a big sentiment analysis dataset in Arabic. Several Machine Learning algorithms (Na\u00efve Bayes, Support Vector Machine and Logistic Regression) alongside Deep and Convolutional Neural Networks were utilized in our experiments of sentiment analysis on our health dataset."}
{"_id":"6fa84cb2356ed30ca4ab6b747e3aa23f2b9ac7b4","title":"Neck lift my way: an update.","text":"BACKGROUND\nThe author updates prior descriptions of an approach to the surgical neck lift that aims for a maximum degree of control over the size, shape, and position of every anatomical feature of the neck that is negatively affecting its appearance.\n\n\nMETHODS\nA 38-year clinical experience guided the development of the operative tactics that define the strategy. Data collected from a records review of 522 consecutive neck lifts performed during the 10-year period 2004 through 2013 further inform the report. The approach has eight features: (1) nearly routine use of open submental access to all tissue layers of the central neck, including a regimen that curbed the problems that may attend an extensive tissue dissection; (2) management of lax neck skin by lateral excision using a specific postauricular incision, or by using the nonexcisional method of redistribution; (3) open lipectomy for precise removal of excess subcutaneous neck and jawline fat; (4) individualized modifications to subplatysmal fat, perihyoid fascia, and anterior digastric muscles; (5) treatment of large, ptotic, or malpositioned submandibular salivary glands by partial excision using a transcutaneous traction suture; (6) the current version of the corset platysmaplasty, which is used to treat static paramedian platysma muscle bands, and to avoid contour imperfections following subplatysmal maneuvers; (7) an approach that facilitates an isolated neck lift; and (8) durable results.\n\n\nRESULTS\nCase examples demonstrate outcomes.\n\n\nCONCLUSION\nAlthough the updated approach remains relatively complex and invasive, the author believes that the ends justify the means."}
{"_id":"d228e3e200c2c6f757b9b3579fa058b2953083c0","title":"Synthetic Biology for Space Exploration : Promises and Societal Implications","text":""}
{"_id":"bb6b5584bc8f0cc74d0a5a08293a87901d953446","title":"Emulation of Software Defined Networks Using Mininet in Different Simulation Environments","text":"In this paper an evaluation of an SDN emulation tool called Mininet is conducted. Tests were conducted to study Mininnet limitations related to the simulation environment, resource capabilities. To evaluate the later, the scalability of Mininet in term of creating many topologies is tested with varying number of nodes and two different environment scenarios. Results show that the simulation environment has a remarkable effect on the required time for building a topology, for instance, the powerful resources scenario needed only 0.19 sec, whereas, 5.611 sec were needed when the resources were less. However, the required time were increased in both scenarios when the number of nodes was increased into 242.842 and 3718.117 sec for the powerful and less capabilities resources respectively."}
{"_id":"52c3248151e1d1bee68eb1d9507bf4edcffff0bb","title":"Identifying Women's Experiences With and Strategies for Mitigating Negative Effects of Online Harassment","text":"The popularity, availability, and ubiquity of information and communication technologies create new opportunities for online harassment. The present study evaluates factors associated with young adult women's online harassment experiences through a multi-factor measure accounting for the frequency and severity of negative events. Findings from a survey of 659 undergraduate and graduate students highlight the relationship between harassment, well-being, and engagement in strategies to manage one's online identity. We further identify differences in harassment experiences across three popular social media platforms: Facebook, Twitter, and Instagram. We conclude by discussing this study's contribution to feminist theory and describing five potential design interventions derived from our data that may minimize these negative experiences, mitigate the psychological harm they cause, and provide women with more proactive ways to regain agency when using communication technologies."}
{"_id":"6669972a5a7e0f8afaf56cb91c3270b50f931bf9","title":"Silhouette Smoothing for Real-Time Rendering of Mesh Surfaces","text":"Coarse piecewise linear approximation of surfaces causes the undesirable polygonal appearance of silhouettes. We present an efficient method for smoothing the silhouettes of coarse triangle meshes using efficient 3D curve reconstruction and simple local remeshing. It does not assume the availability of a fine mesh and generates only a moderate amount of additional data at runtime. Furthermore, polygonal feature edges are also smoothed in a unified framework. Our method is based on a novel interpolation scheme over silhouette triangles, and this ensures that smooth silhouettes are faithfully reconstructed and always change continuously with respect to the continuous movement of the viewpoint or objects. We speed up computation with GPU assistance to achieve real-time rendering of coarse meshes with the smoothed silhouettes. Experiments show that this method outperforms previous methods for silhouette smoothing."}
{"_id":"0667440f001de6191dc5ce45b6300bf8fbd39cee","title":"Robustness Analysis of Artificial Neural Networks and Support Vector Machine in Making Prediction","text":"This This study aims to investigate the robustness of prediction model by comparing artificial neural networks (ANNs), and support vector machine (SVMs) model. The study employs ten years monthly data of six types of macroeconomic variables as independent variables and the average rate of return of one-month time deposit of Indonesian Islamic banks (RR) as dependent variable. Finally, the performance is evaluated through graph analysis, statistical parameters and accuracy rate measurement. This research found that ANNs outperforms SVMs empirically resulted from the training process and overall data prediction. This is indicating that ANNs model is better in the context of capturing all data pattern and explaining the volatility of RR."}
{"_id":"612b3d72fd6004390c4d15dd0c754a2fd6457456","title":"Biofeedback-based behavioral treatment for chronic tinnitus: results of a randomized controlled trial.","text":"Many tinnitus sufferers believe that their tinnitus has an organic basis and thus seek medical rather than psychological treatments. Tinnitus has been found to be associated with negative appraisal, dysfunctional attention shift, and heightened psychophysiological arousal, so cognitive-behavioral interventions and biofeedback are commonly suggested as treatments. This study developed and investigated the efficacy of a biofeedback-based cognitive-behavioral treatment for tinnitus. In total, 130 tinnitus patients were randomly assigned to an intervention or a wait-list control group. Treatment consisted of 12 sessions of a biofeedback-based behavioral intervention over a 3-month period. Patients in the wait-list group participated in the treatment after the intervention group had completed the treatment. Results showed clear improvements regarding tinnitus annoyance, diary ratings of loudness, and feelings of controllability. Furthermore, changes in coping cognitions as well as changes in depressive symptoms were found. Improvements were maintained over a 6-month follow-up period in which medium-to-large effect sizes were observed. The treatment developed and investigated in this study is well accepted and leads to clear and stable improvements. Through demonstration of psychophysiological interrelationships, the treatment enables patients to change their somatic illness perceptions to a more psychosomatic point of view."}
{"_id":"49949855c6440569175bb926bcc6f9162ad55cdb","title":"Lung nodule detection in CT images using deep convolutional neural networks","text":"Early detection of lung nodules in thoracic Computed Tomography (CT) scans is of great importance for the successful diagnosis and treatment of lung cancer. Due to improvements in screening technologies, and an increased demand for their use, radiologists are required to analyze an ever increasing amount of image data, which can affect the quality of their diagnoses. Computer-Aided Detection (CADe) systems are designed to assist radiologists in this endeavor. Here, we present a CADe system for the detection of lung nodules in thoracic CT images. Our system is based on (1) the publicly available Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) database, which contains 1018 thoracic CT scans with nodules of different shape and size, and (2) a deep Convolutional Neural Network (CNN), which is trained, using the back-propagation algorithm, to extract valuable volumetric features from the input data and detect lung nodules in sub-volumes of CT images. Considering only those test nodules that have been annotated by four radiologists, our CADe system achieves a sensitivity (true positive rate) of 78.9% with 20 false positives (FPs) per scan, or a sensitivity of 71.2% with 10 FPs per scan. This is achieved without using any segmentation or additional FP reduction procedures, both of which are commonly used in other CADe systems. Furthermore, our CADe system is validated on a larger number of lung nodules compared to other studies, which increases the variation in their appearance, and therefore, makes their detection by a CADe system more challenging."}
{"_id":"f481c55c0c2a6fd3505165227355774ae15faef8","title":"MAPGEN: Mixed-Initiative Planning and Scheduling for the Mars Exploration Rover Mission","text":"8 1094-7167\/04\/$20.00 \u00a9 2004 IEEE IEEE INTELLIGENT SYSTEMS to elucidate the planet\u2019s past climate, water activity, and habitability. Science is MER\u2019s primary driver, so making best use of the scientific instruments, within the available resources, is a crucial aspect of the mission. To address this criticality, the MER project team selected MAPGEN (Mixed Initiative Activity Plan Generator) as an activity-planning tool. MAPGEN combines two existing systems, each with a strong heritage: the APGEN activity-planning tool1 from the Jet Propulsion Laboratory and the Europa planning and scheduling system2 from NASA Ames Research Center. This article discusses the issues arising from combining these tools in this mission\u2019s context."}
{"_id":"f87b713182d39297e930c41e23ff26394cbdcade","title":"Face recognition using HOG-EBGM","text":""}
{"_id":"96e1b9945861b5e2ec4450ec91716ae0de08f9b1","title":"Robot control design using virtual path from android smartphone","text":"Progress robot development very varied almost all fields seize the attention of the world's scientists and researchers. For some cases, robots are generally used to help humans in relieving job. The robot can replace human mobilization for move so that the efficiency of wasted time will be less. Many things need to be note what kind of robot control reliable to replace human work in mobilization. Virtual world as in android smartphone greatly benefit humans to creating control tools easy to use anywhere and could represent a desire humans. By making control robot using virtual paths on the Android smartphone which have an accuracy of more than 70% to a scale of 1:50 with the actual track in an environment that is not wide enough, the advantage can be used to control the robot move easily and efficiently compared to the usual manual control robot that will take more time to control The robot moves its place as desired."}
{"_id":"54b5aab87dbe38803935789c4d730bd203d198a1","title":"3D Human Pose Estimation in RGBD Images for Robotic Task Learning","text":"We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher."}
